O’Reilly books may be purchased for educational, business, or sales promotional use.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in this book, and O’Reilly Media, Inc., was aware of a trademark claim, the designations have been printed in caps or initial caps.
While every precaution has been taken in the preparation of this book, the publisher and author assume no responsibility for errors or omissions, or for damages resulting from the use of the information contained herein.
In the last 10 years, the Internet has challenged relational databases in ways nobody could have foreseen.
Having used MySQL at large and growing Internet companies during this time, I’ve seen this happen firsthand.
First you have a single server with a small data set.
Then you find yourself setting up replication so you can scale out reads and deal with potential failures.
And, before too long, you’ve added a caching layer, tuned all the queries, and thrown even more hardware at the problem.
Eventually you arrive at the point when you need to shard the data across multiple clusters and rebuild a ton of application logic to deal with it.
And soon after that you realize that you’re locked into the schema you modeled so many months before.
Why? Because there’s so much data in your clusters now that altering the schema will take a long time and involve a lot of precious DBA time.
This can keep a small team of developers busy for many months.
In the end, you’ll always find yourself wondering if there’s a better way—or why more of these features are not built into the core database server.
Keeping with tradition, the Open Source community has created a plethora of “better ways” in response to the ballooning data needs of modern web applications.
They span the spectrum from simple in-memory key/value stores to complicated SQL-speaking MySQL/InnoDB derivatives.
But the sheer number of choices has made finding the right solution more difficult.
Instead it strikes the right balance between features and complexity, with a clear bias toward making previously difficult tasks far easier.
In other words, it has the features that really matter to the vast majority of today’s web applications: indexes, replication, sharding, a rich query syntax, and a very flexible data model.
Like MongoDB itself, this book is very straightforward and approachable.
New MongoDB users can start with Chapter 1 and be up and running in no time.
It’s a solid reference for advanced administrative topics such as replication, backups, and sharding, as well as popular client APIs.
Having recently started to use MongoDB in my day job, I have no doubt that this book will be at my side for the entire journey—from the first install to production deployment of a sharded and replicated cluster.
It’s an essential reference to anyone seriously looking at using MongoDB.
How This Book Is Organized This book is split up into six sections, covering development, administration, and deployment information.
Getting Started with MongoDB In Chapter 1 we provide background about MongoDB: why it was created, the goals it is trying to accomplish, and why you might choose to use it for a project.
We go into more detail in Chapter 2, which provides an introduction to the core concepts and vocabulary of MongoDB.
Chapter 2 also provides a first look at working with MongoDB, getting you started with the database and the shell.
The next two chapters cover the basic material that developers need to know to work with MongoDB.
In Chapter 3, we describe how to perform those basic write operations, including how to do them with different levels of safety and speed.
Chapter 4 explains how to find documents and create complex queries.
This chapter also covers how to iterate through results and gives options for limiting, skipping, and sorting results.
Developing with MongoDB Chapter 5 covers what indexing is and how to index your MongoDB collections.
Chapter 6 explains how to use several special types of indexes and collections.
Chapter 7 covers a number of techniques for aggregating data with MongoDB, including counting, finding distinct values, grouping documents, the aggregation framework, and using MapReduce.
Finally, this section finishes with a chapter on designing your application: Chapter 8 goes over tips for writing an application that works well with MongoDB.
Replication The replication section starts with Chapter 9, which gives you a quick way to set up a replica set locally and covers many of the available configuration options.
Chapter 10 then covers the various concepts related to replication.
Sharding The sharding section starts in Chapter 13 with a quick local setup.
Chapter 14 then gives an overview of the components of the cluster and how to set them up.
Chapter 15 has advice on choosing a shard key for a variety of application.
Application Administration The next two chapters cover many aspects of MongoDB administration from the perspective of your application.
Chapter 17 discusses how to introspect what MongoDB is doing.
Chapter 18 covers administrative tasks such as building indexes, and moving and compacting data.
Server Administration The final section is focused on server administration.
Chapter 20 covers common options when starting and stopping MongoDB.
Chapter 21 discusses what to look for and how to read stats when monitoring.
Chapter 22 describes how to take and restore backups for each type of deployment.
Finally, Chapter 23 discusses a number of system settings to keep in mind when deploying MongoDB.
Appendix B details ow MongoDB works internally: its storage engine, data format, and wire protocol.
Conventions Used in This Book The following typographical conventions are used in this book: Italic.
Indicates new terms, URLs, email addresses, collection names, database names, filenames, and file extensions.
Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, command-line utilities, environment variables, statements, and keywords.
Shows commands or other text that should be typed literally by the user.
Constant width italic Shows text that should be replaced with user-supplied values or by values determined by context.
Using Code Examples This book can help you get your job done.
In general, you may use the code in this book in your programs and documentation.
You do not need to contact us for permission unless you’re reproducing a significant portion of the code.
For example, writing a program that uses several chunks of code from this book does not require permission.
Selling or distributing a CD-ROM of examples from O’Reilly books does require permission.
Answering a question by citing this book and quoting example code does not require permission.
Incorporating a significant amount of example code from this book into your product’s documentation does require permission.
An attribution usually includes the title, author, publisher, and ISBN.
Technology professionals, software developers, web designers, and business and creative professionals use Safari Books Online as their primary resource for research, problem solving, learning, and certification training.
Safari Books Online offers a range of product mixes and pricing programs for organizations, government agencies, and individuals.
For more information about Safari Books Online, please visit us online.
How to Contact Us Please address comments and questions concerning this book to the publisher:
We have a web page for this book, where we list errata, examples, and any additional information.
To comment or ask technical questions about this book, send email to:
For more information about our books, conferences, Resource Centers, and the O’Reilly Network, see our website at:
You guys made this book immeasurably better (and more correct)
Thank you, Ann Spencer, for being such a terrific editor and for helping me every step of the way.
Thanks to all of my coworkers at 10gen for sharing your knowledge and advice on MongoDB as well as Eliot Horowitz and Dwight Merriman, for starting the MongoDB project.
And thank you, Andrew, for all of your support and suggestions.
It combines the ability to scale out with features such as secondary indexes, range queries, sorting, aggregations, and geospatial indexes.
This chapter covers the major design decisions that made MongoDB what it is.
Ease of Use MongoDB is a document-oriented database, not a relational one.
The primary reason for moving away from the relational model is to make scaling out easier, but there are some other advantages as well.
A document-oriented database replaces the concept of a “row” with a more flexible model, the “document.” By allowing embedded documents and arrays, the documentoriented approach makes it possible to represent complex hierarchical relationships with a single record.
This fits naturally into the way developers in modern objectoriented languages think about their data.
There are also no predefined schemas: a document’s keys and values are not of fixed types or sizes.
Without a fixed schema, adding or removing fields as needed becomes easier.
Generally, this makes development faster as developers can quickly iterate.
Developers can try dozens of models for the data and then choose the best one to pursue.
Easy Scaling Data set sizes for applications are growing at an incredible pace.
Increases in available bandwidth and cheap storage have created an environment where even small-scale applications need to store more data than many databases were meant to handle.
A terabyte of data, once an unheard-of amount of information, is now commonplace.
As the amount of data that developers need to store grows, developers face a difficult decision: how should they scale their databases? Scaling a database comes down to the choice between scaling up (getting a bigger machine) or scaling out (partitioning data across more machines)
Scaling up is often the path of least resistance, but it has drawbacks: large machines are often very expensive, and eventually a physical limit is reached where a more powerful machine cannot be purchased at any cost.
The alternative is to scale out: to add storage space or increase performance, buy another commodity server and add it to your cluster.
This is both cheaper and more scalable; however, it is more difficult to administer a thousand machines than it is to care for one.
Its document-oriented data model makes it easier for it to split up data across multiple servers.
MongoDB automatically takes care of balancing data and load across a cluster, redistributing documents automatically and routing user requests to the correct machines.
This allows developers to focus on programming the application, not scaling it.
When a cluster need more capacity, new machines can be added and MongoDB will figure out how the existing data should be spread to them.
Tons of Features… MongoDB is intended to be a general-purpose database, so aside from creating, reading, updating, and deleting data, it provides an ever-growing list of unique features: Indexing.
MongoDB supports generic secondary indexes, allowing a variety of fast queries, and provides unique, compound, geospatial, and full-text indexing capabilities as well.
Aggregation MongoDB supports an “aggregation pipeline” that allows you to build complex aggregations from simple pieces and allow the database to optimize it.
Special collection types MongoDB supports time-to-live collections for data that should expire at a certain time, such as sessions.
It also supports fixed-size collections, which are useful for holding recent data, such as logs.
File storage MongoDB supports an easy-to-use protocol for storing large files and file metadata.
Some features common to relational databases are not present in MongoDB, notably joins and complex multirow transactions.
Omitting these was an architectural decision to allow for greater scalability, as both of those features are difficult to provide efficiently in a distributed system.
Without Sacrificing Speed Incredible performance is a major goal for MongoDB and has shaped much of its design.
MongoDB adds dynamic padding to documents and preallocates data files to trade extra space usage for consistent performance.
It uses as much of RAM as it can as its cache and attempts to automatically choose the correct indexes for queries.
In short, almost every aspect of MongoDB was designed to maintain high performance.
Although MongoDB is powerful and attempts to keep many features from relational systems, it is not intended to do everything that a relational database does.
Whenever possible, the database server offloads processing and logic to the client side (handled either by the drivers or by a user’s application code)
Maintaining this streamlined design is one of the reasons MongoDB can achieve such high performance.
Let’s Get Started Throughout the course of the book, we will take the time to note the reasoning or motivation behind particular decisions made in the development of MongoDB.
Through those notes we hope to share the philosophy behind MongoDB.
The best way to summarize the MongoDB project, however, is through its main focus—to create a full-featured data store that is scalable, flexible, and fast.
In this chapter we’ll introduce some of the basic concepts of MongoDB:
A document is the basic unit of data for MongoDB and is roughly equivalent to a row in a relational database management system (but much more expressive)
Similarly, a collection can be thought of as a table with a dynamic schema.
A single instance of MongoDB can host multiple independent databases, each of.
Every document has a special key, "_id", that is unique within a collection.
MongoDB comes with a simple but powerful JavaScript shell, which is useful for.
Documents At the heart of MongoDB is the document: an ordered set of keys with associated values.
The representation of a document varies by programming language, but most languages have a data structure that is a natural fit, such as a map, hash, or dictionary.
This simple document contains a single key, "greeting", with a value of "Hello, world!"
Most documents will be more complex than this simple one and often will contain multiple key/value pairs:
In this example the value for "greeting" is a string, whereas the value for "foo" is an integer.
Any UTF-8 character is allowed in a key, with a few notable exceptions:
Keys must not contain the character \0 (the null character)
This character is used to signify the end of a key.
In general, they should be considered reserved, and drivers will complain if they are used inappropriately.
A final important thing to note is that documents in MongoDB cannot contain duplicate keys.
In some programming languages the default representation of a document does not even maintain ordering (e.g., dictionaries in Python and hashes in Perl or Ruby 1.8)
Drivers for those languages usually have some mechanism for specifying documents with ordering, when necessary.
If a document is the MongoDB analog of a row in a relational database, then a collection can be thought of as the analog to a table.
This means that the documents within a single collection can have any number of different “shapes.” For example, both of the following documents could be stored in a single collection:
Note that the previous documents not only have different types for their values (string versus integer) but also have entirely different keys.
Because any document can be put into any collection, the question often arises: “Why do we need separate collections at all?” It’s a good question—with no need for separate schemas for different kinds of documents, why should we use more than one collection? There are several good reasons:
Keeping different kinds of documents in the same collection can be a nightmare for developers and admins.
Developers need to make sure that each query is only returning documents of a certain type or that the application code performing a query can handle documents of different shapes.
If we’re querying for blog posts, it’s a hassle to weed out documents containing author data.
It is much faster to get a list of collections than to extract a list of the types in a collection.
For example, if we had a "type" field in each document that specified whether the document was a “skim,” “whole,” or “chunky monkey,” it would be much slower to find those three values in a single collection than to have three separate collections and query the correct collection.
Grouping documents of the same kind together in the same collection allows for data locality.
Getting several blog posts from a collection containing only posts will likely require fewer disk seeks than getting the same posts from a collection containing posts and author data.
We begin to impose some structure on our documents when we create indexes.
This is especially true in the case of unique indexes.
By putting only documents of a single type into the same collection, we can index our collections more efficiently.
As you can see, there are sound reasons for creating a schema and for grouping related types of documents together, even though MongoDB does not enforce it.
Collection names can be any UTF-8 string, with a few restrictions:
The empty string ("") is not a valid collection name.
Collection names may not contain the character \0 (the null character) because.
You should not create any collections that start with system., a prefix reserved for.
User-created collections should not contain the reserved character $ in the name.
The various drivers available for the database do support using $ in collection names because some system-generated collections contain it.
You should not use $ in a name unless you are accessing one of these collections.
One convention for organizing collections is to use namespaced subcollections separated by the.
For example, an application containing a blog might have a collection named blog.posts and a separate collection named blog.authors.
This is for organizational purposes only—there is no relationship between the blog collection (it doesn’t even have to exist) and its “children.”
Although subcollections do not have any special properties, they are useful and incorporated into many MongoDB tools:
GridFS, a protocol for storing large files, uses subcollections to store file metadata separately from content chunks (see Chapter 6 for more information about GridFS)
Most drivers provide some syntactic sugar for accessing a subcollection of a given collection.
For example, in the database shell, db.blog will give you the blog collection, and db.blog.posts will give you the blog.posts collection.
Subcollections are a great way to organize data in MongoDB, and their use is highly recommended.
Databases In addition to grouping documents by collection, MongoDB groups collections into databases.
A single instance of MongoDB can host several databases, each grouping together zero or more collections.
A database has its own permissions, and each database is stored in separate files on disk.
A good rule of thumb is to store all data for a single application in the same database.
Separate databases are useful when storing data for several application or users on the same MongoDB server.
Database names can be any UTF-8 string, with the following restrictions:
The empty string ("") is not a valid database name.
To keep things simple, try to just use lowercase characters.
Database names are limited to a maximum of 64 bytes.
One thing to remember about database names is that they will actually end up as files on your filesystem.
This explains why many of the previous restrictions exist in the first place.
There are also several reserved database names, which you can access but which have special semantics.
If a user is added to the admin database, the user automatically inherits permissions for all databases.
There are also certain server-wide commands that can be run only from the admin database, such as listing all of the databases or shutting down the server.
By concatenating a database name with a collection in that database you can get a fully qualified collection name called a namespace.
For instance, if you are using the blog.posts collection in the cms database, the namespace of that collection would be cms.blog.posts.
For more on namespaces and the internal representation of collections in MongoDB, see Appendix B.
Getting and Starting MongoDB MongoDB is almost always run as a network server that clients can connect to and perform operations on.
For detailed information on installing MongoDB on your system, see Appendix A.
When run with no arguments, mongod will use the default data directory, /data/db/ (or \data\db\ on the current volume on Windows)
If the data directory does not already exist or is not writable, the server will fail to start.
It is important to create the data directory (e.g., mkdir -p /data/db/) and to make sure your user has permission to write to the directory before starting MongoDB.
On startup, the server will print some version and system information and then begin waiting for connections.
The server will fail to start if the port is not available—the most common cause of this is another instance of MongoDB that is already running.
This means that you can get some administrative information about your database by opening a web browser and going to http://local host:28017
You can safely stop mongod by typing Ctrl-C in the shell that is running the server.
Introduction to the MongoDB Shell MongoDB comes with a JavaScript shell that allows interaction with a MongoDB instance from the command line.
The mongo shell is a crucial tool for using MongoDB and is used extensively throughout the rest of the text.
Running the Shell To start the shell, run the mongo executable:
The shell automatically attempts to connect to a MongoDB server on startup, so make sure you start mongod before starting the shell.
The shell is a full-featured JavaScript interpreter, capable of running arbitrary JavaScript programs.
The shell will detect whether the JavaScript statement is complete when you press Enter.
If the statement is not complete, the shell will allow you to continue writing it on the next line.
Pressing Enter three times in a row will cancel the half-formed command and get you back to the >-prompt.
A MongoDB Client Although the ability to execute arbitrary JavaScript is cool, the real power of the shell lies in the fact that it is also a standalone MongoDB client.
On startup, the shell connects to the test database on a MongoDB server and assigns this database connection to the.
This variable is the primary access point to your MongoDB server through the shell.
To see the database db is currently assigned to, type in db and hit Enter: > db test.
The shell contains some add-ons that are not valid JavaScript syntax but were implemented because of their familiarity to users of SQL shells.
The add-ons do not provide any extra functionality, but they are nice syntactic sugar.
For instance, one of the most important operations is selecting which database to use:
Now if you look at the db variable, you can see that it refers to the foobar database: > db foobar.
Because this is a JavaScript shell, typing a variable will convert the variable to a string (in this case, the database name) and print it.
For example, db.baz returns the baz collection in the current database.
Now that we can access a collection in the shell, we can perform almost any database operation.
Basic Operations with the Shell We can use the four basic operations, create, read, update, and delete (CRUD) to manipulate and view data in the shell.
For example, suppose we want to store a blog post.
First, we’ll create a local variable called post that is a JavaScript object representing our document.
It will have the keys "title", "content", and "date" (the date that it was published):
This object is a valid MongoDB document, so we can save it to the blog collection using the insert method:
We can see it by calling find on the collection:
You can see that an "_id" key was added and that the other key/value pairs were saved as we entered them.
The reason for the sudden appearance of the "_id" field is explained at the end of this chapter.
If we just want to see one document from a collection, we can use findOne:
The shell will automatically display up to 20 documents matching a find, but more can be fetched.
If we would like to modify our post, we can use update.
Suppose we decide to enable comments on the blog post we created earlier.
We’ll need to add an array of comments as the value for a new key in our document.
The first step is to modify the variable post and add a "comments" key: > post.comments = [] [  ]
Then we perform the update, replacing the post titled “My Blog Post” with our new version of the document:
Called with no parameters, it removes all documents from a collection.
It can also take a document specifying criteria for removal.
For example, this would remove the post we just created:
Data Types The beginning of this chapter covered the basics of what a document is.
Now that you are up and running with MongoDB and can try things on the shell, this section will dive a little deeper.
MongoDB supports a wide range of data types as values in documents.
Basic Data Types Documents in MongoDB can be thought of as “JSON-like” in that they are conceptually similar to objects in JavaScript.
This is a good thing in many ways: it’s easy to understand, parse, and remember.
On the other hand, JSON’s expressive capabilities are limited because the only types are null, boolean, numeric, string, array, and object.
Although these types allow for an impressive amount of expressivity, there are a couple of additional types that are crucial for most applications, especially when working with a database.
For example, JSON has no date type, which makes working with dates even more annoying than it usually is.
There is no way to represent other commonly used types, either, such as regular expressions or functions.
MongoDB adds support for a number of additional data types while keeping JSON’s essential key/value pair nature.
There are also a few less common types that you may need, including: binary data.
Binary data is the only way to save non-UTF-8 strings to the database.
There are a few types that are mostly used internally (or superseded by other types)
For more information on MongoDB’s data format, see Appendix B.
Calling the constructor as a function (that is, not including new) returns a string representation of the date, not an actual Date object.
This is not MongoDB’s choice; it is how JavaScript works.
If you are not careful to always use the Date constructor, you can end up with a mishmash of strings and dates.
Strings do not match dates and vice versa, so this can cause problems with removing, updating, querying…pretty much everything.
For a full explanation of JavaScript’s Date class and acceptable formats for the constructor, see ECMAScript specification section 15.9
Dates in the shell are displayed using local time zone settings.
However, dates in the database are just stored as milliseconds since the epoch, so they have no time zone information associated with them.
Time zone information could, of course, be stored as the value for another key.
Arrays Arrays are values that can be interchangeably used for both ordered operations (as though they were lists, stacks, or queues) and unordered operations (as though they were sets)
As we can see from the example, arrays can contain different data types as values (in this case, a string and a floating-point number)
In fact, array values can be any of the supported values for normal key/value pairs, even nested arrays.
One of the great things about arrays in documents is that MongoDB “understands” their structure and knows how to reach inside of arrays to perform operations on their contents.
This allows us to query on arrays and build indexes using their contents.
For instance, in the previous example, MongoDB can query for all documents where 3.14 is an element of the "things" array.
If this is a common query, you can even create an index on the "things" key to improve the query’s speed.
MongoDB also allows atomic updates that modify the contents of arrays, such as reaching into the array and changing the value pie to pi.
We’ll see more examples of these types of operations throughout the text.
Embedded Documents Documents can be used as the value for a key.
Embedded documents can be used to organize data in a more natural way than just a flat structure of key/value pairs.
For example, if we have a document representing a person and want to store his address, we can nest this information in an embedded "address" document:
The value for the "address" key in the previous example is an embedded document with its own key/value pairs for "street", "city", and "state"
As with arrays, MongoDB “understands” the structure of embedded documents and is able to reach inside them to build indexes, perform queries, or make updates.
We’ll discuss schema design in depth later, but even from this basic example we can begin to see how embedded documents can change the way we work with data.
In a relational database, the previous document would probably be modeled as two separate rows in two different tables (one for “people” and one for “addresses”)
With MongoDB we can embed the address document directly within the person document.
When used properly, embedded documents can provide a more natural representation of information.
The flip side of this is that there can be more data repetition with MongoDB.
Suppose “addresses” were a separate table in a relational database and we needed to fix a typo in an address.
When we did a join with “people” and “addresses,” we’d get the updated address for everyone who shares it.
With MongoDB, we’d need to fix the typo in each person’s document.
The "_id" key’s value can be any type, but it defaults to an ObjectId.
In a single collection, every document must have a unique value for "_id", which ensures that every document in a collection can be uniquely identified.
The ObjectId class is designed to be lightweight, while still being easy to generate in a globally unique way across different machines.
MongoDB’s distributed nature is the main reason why it uses ObjectIds as opposed to something more traditional, like an autoincrementing primary key: it is difficult and time-consuming to synchronize autoincrementing primary keys across multiple servers.
Because MongoDB was designed to be a distributed database, it was important to be able to generate unique identifiers in a sharded environment.
This causes them to appear larger than they are, which makes some people nervous.
It’s important to note that even though an ObjectId is often represented as a giant hexadecimal string, the string is actually twice as long as the data being stored.
If you create multiple new ObjectIds in rapid succession, you can see that only the last few digits change each time.
In addition, a couple of digits in the middle of the Objec tId will change (if you space the creations out by a couple of seconds)
This is because of the manner in which ObjectIds are created.
The 12 bytes of an ObjectId are generated as follows:
The first four bytes of an ObjectId are a timestamp in seconds since the epoch.
The timestamp, when combined with the next five bytes (which will be described in a moment), provides uniqueness at the granularity of a second.
Because the timestamp comes first, it means that ObjectIds will sort in roughly insertion order.
This is not a strong guarantee but does have some nice properties, such as making ObjectIds efficient to index.
In these four bytes exists an implicit timestamp of when each document was created.
Most drivers expose a method for extracting this information from an ObjectId.
Because the current time is used in ObjectIds, some users worry that their servers will need to have synchronized clocks.
Although synchronized clocks are a good idea for other reasons (see “Synchronizing Clocks” on page 383), the actual timestamp doesn’t matter to ObjectIds, only that it is often new (once per second) and increasing.
The next three bytes of an ObjectId are a unique identifier of the machine on which it was generated.
By including these bytes, we guarantee that different machines will not generate colliding ObjectIds.
These first nine bytes of an ObjectId guarantee its uniqueness across machines and processes for a single second.
The last three bytes are simply an incrementing counter that is responsible for uniqueness within a second in a single process.
As stated previously, if there is no "_id" key present when a document is inserted, one will be automatically added to the inserted document.
This can be handled by the MongoDB server but will generally be done by the driver on the client side.
The decision to generate them on the client side reflects an overall philosophy of MongoDB: work should be pushed out of the server and to the drivers whenever possible.
This philosophy reflects the fact that, even with scalable databases like MongoDB, it is easier to scale out at the application layer than at the database layer.
Moving work to the client side reduces the burden requiring the database to scale.
Using the MongoDB Shell This section covers how to use the shell as part of your command line toolkit, customize it, and use some of its more advanced functionality.
Although we connected to a local mongod instance above, you can connect your shell to any MongoDB instance that your machine can reach.
To connect to a mongod on a different machine or port, specify the hostname, port, and database when starting the shell:
Sometimes it is handy to not connect to a mongod at all when starting the mongo shell.
If you start the shell with --nodb, it will start up without attempting to connect to anything:
Once started, you can connect to a mongod at your leisure by running new Mon go(hostname):
You can use these commands to connect to a different database or server at any time.
Tips for Using the Shell Because mongo is simply a JavaScript shell, you can get a great deal of help for it by simply looking up JavaScript documentation online.
For MongoDB-specific functionality, the shell includes built-in help that can be accessed by typing help:
Database-level help is provided by db.help() and collection-level help by db.foo.help()
A good way of figuring out what a function is doing is to type it without the parentheses.
This will print the JavaScript source code for the function.
For example, if we are curious about how the update function works or cannot remember the order of parameters, we can do the following:
Running Scripts with the Shell Other chapters have used the shell interactively, but you can also pass the shell JavaScript files to execute.
The mongo shell will execute each script listed and exit.
If you want to run a script using a connection to a non-default host/port mongod, specify the address first, then the script(s):
This would execute the three scripts with db set to the foo database on server-1:30000
As shown above, any command line options for running the shell go before the address.
You can print to stdout in scripts (as the scripts above did) using the print() function.
This allows you to use the shell as part of a pipeline of commands.
If you’re planning to pipe the output of a shell script to another command use the --quiet option to prevent the “MongoDB shell version...” banner from printing.
Scripts have access to the db variable (as well as any other globals)
However, shell helpers such as "use db" or "show collections" do not work from files.
There are valid JavaScript equivalents to each of these, as shown in Table 2-1
You can also use scripts to inject variables into the shell.
For example, we could have a script that simply initializes helper functions that you commonly use.
The script below, for instance, may be helpful for the replication and sharding sections of the book.
It defines a function, connectTo(), that connects to the locally-running database on the given port and sets db to that connection:
In addition to adding helper functions, you can use scripts to automate common tasks and administrative activities.
By default, the shell will look in the directory that you started the shell in (use run("pwd") to see what directory that is)
If the script is not in your current directory, you can give the shell a relative or absolute path to it.
You can use run() to run command-line programs from the shell.
This is of limited use, generally, as the output is formatted oddly and it doesn’t support pipes.
Creating a .mongorc.js If you have frequently-loaded scripts you might want to put them in your mongorc.js file.
This file is run whenever you start up the shell.
For example, suppose we would like the shell to greet us when we log in.
Create a file called .mongorc.js in your home directory, and then add the following lines to it:
More practically, you can use this script to set up any global variables you’d like to use, alias long names to shorter ones, and override built-in functions.
One of the most common uses for .mongorc.js is remove some of the more “dangerous” shell helpers.
You can override functions like dropDatabase or deleteIndexes with no-ops or undefine them altogether:
Make sure that, if you change any database functions, you do so on both the db variable and the DB prototype (as shown in the example above)
If you change only one, either the db variable won’t see the change or all new databases you use (when you run use anotherDB) won’t see your change.
Now if you try to call any of these functions, it will simply print an error message.
Note that this technique does not protect you against malicious users; it can only help with fat-fingering.
You can disable loading your .mongorc.js by using the --norc option when starting the shell.
Customizing Your Prompt The default shell prompt can be overridden by setting the prompt variable to either a string or a function.
For example, if you are running a query that takes minutes to complete, you may want to have a prompt that prints the current time when it is drawn so you can see when the last operation finished:
Note that prompt functions should return strings and be very cautious about catching exceptions: it can be very confusing if your prompt turns into an exception!
In general, your prompt function should include a call to getLastError.
This catches errors on writes and reconnects you “automatically” if the shell gets disconnected (e.g., if you restart mongod)
The .mongorc.js file is a good place to set your prompt if you want to always use a custom one (or set up a couple of custom prompts that you can switch between in the shell)
Thus, for larger blocks of code or objects, you may want to edit them in an editor.
To do so, set the EDITOR variable in the shell (or in your environment, but since you’re already in the shell):
When you’re done making changes, save and exit the editor.
The variable will be parsed and loaded back into the shell.
For example, suppose we are trying to access the version collection.
We cannot say db.version because db.version is a method on db (it returns the version of the running MongoDB server):
Another way of getting around invalid properties is to use array-access syntax: in JavaScript, x.y is identical to x['y']
This means that subcollections can be accessed using variables, not just literal names.
Thus, if you needed to perform some operation on every blog subcollection, you could iterate through them with something like this:
You must use the db.blog[i] syntax for i to be interpreted as a variable.
Attempting to query db.@#&! would be illegal, but db[name] would work.
This chapter covers the basics of moving data in and out of the database, including the following:
Choosing the correct level of safety versus speed for all of these operations.
Inserting and Saving Documents Inserts are the basic method for adding data to MongoDB.
To insert a document into a collection, use the collection’s insert method:
This will add an "_id" key to the document (if one does not already exist) and store it in MongoDB.
Batch Insert If you have a situation where you are inserting multiple documents into a collection, you can make the insert faster by using batch inserts.
Batch inserts allow you to pass an array of documents to the database.
In the shell, you can try this out using the batchInsert function, which is similar to insert except that it takes an array of documents to insert:
Sending dozens, hundreds, or even thousands of documents at a time can make inserts significantly faster.
Batch inserts are only useful if you are inserting multiple documents into a single collection: you cannot use batch inserts to insert into multiple collections with a single request.
If you are just importing raw data (for example, from a data feed or MySQL), there are command-line tools like mongoimport that can be used instead of batch insert.
On the other hand, it is often handy to munge data before saving it to MongoDB (converting dates to the date type or adding a custom "_id") so batch inserts can be used for importing data, as well.
Current versions of MongoDB do not accept messages longer than 48 MB, so there is a limit to how much can be inserted in a single batch insert.
If you are importing a batch and a document halfway through the batch fails to be inserted, the documents up to that document will be inserted and everything after that document will not:
Only the first two documents will be inserted, as the third will produce an error: you cannot insert two documents with the same "_id"
If you want to ignore errors and make batchInsert attempt to insert the rest of the batch, you can use the continueOnError option to continue after an insert failure.
This would insert the first, second, and fourth documents above.
The shell does not support this option, but all the drivers do.
These minimal checks also mean that it is fairly easy to insert invalid data (if you are trying to)
Thus, you should only allow trusted sources, such as your application servers, to connect to the database.
All of the drivers for major languages (and most of the minor ones, too) do check for a variety of invalid data (documents that are too large, contain non-UTF-8 strings, or use unrecognized types) before sending anything to the database.
Removing Documents Now that there’s data in our database, let’s delete it:
This will remove all of the documents in the foo collection.
This doesn’t actually remove the collection, and any meta information about it will still exist.
The remove function optionally takes a query document as a parameter.
When it’s given, only documents that match the criteria will be removed.
Suppose, for instance, that we want to remove everyone from the mailing.list collection where the value for "optout" is true:
There is no way to undo the remove or recover deleted documents.
Remove Speed Removing documents is usually a fairly quick operation, but if you want to clear an entire collection, it is faster to drop it (and then recreate any indexes on the empty collection)
Now we’ll try to remove all of the documents we just inserted, measuring the time it takes.
On a MacBook Air, this script prints “Remove took: 9676ms”
The whole collection is dropped, and all of its metadata is deleted.
Updating Documents Once a document is stored in the database, it can be changed using the update method.
Updating a document is atomic: if two updates happen at the same time, whichever one reaches the server first will be applied, and then the next one will be applied.
Thus, conflicting updates can safely be sent in rapid-fire succession without any documents being corrupted: the last update will “win.”
Document Replacement The simplest type of update fully replaces a matching document with a new one.
This can be useful to do a dramatic schema migration.
For example, suppose we are making major changes to a user document, which looks like the following:
We want to move the "friends" and "enemies" fields to a "relationships" subdocument.
We can change the structure of the document in the shell and then replace the database’s version with an update:
A common mistake is matching more than one document with the criteria and then creating a duplicate "_id" value with the second parameter.
The database will throw an error for this, and no documents will be updated.
For example, suppose we create several documents with the same value for "name", but we don’t realize it:
Now, if it’s Joe #2’s birthday, we want to increment the value of his "age" key, so we might say this:
Using Modifiers Usually only certain portions of a document need to be updated.
You can update specific fields in a document using atomic update modifiers.
Update modifiers are special keys that can be used to specify complex update operations, such as altering, adding, or removing keys, and even manipulating arrays and embedded documents.
Suppose we were keeping website analytics in a collection and wanted to increment a counter each time someone visited a page.
We can use update modifiers to do this increment atomically.
Each URL and its number of page views is stored in a document that looks like this:
Every time someone visits a page, we can find the page by its URL and use the "$inc" modifier to increment the value of the "pageviews" key:
When using modifiers, the value of "_id" cannot be changed.
Note that "_id" can be changed by using whole-document replacement.
Values for any other key, including other uniquely indexed keys, can be modified.
If the field does not yet exist, it will be created.
This can be handy for updating schema or adding user-defined keys.
For example, suppose you have a simple user profile stored as a document that looks something like the following:
If the user wanted to store his favorite book in his profile, he could add it using "$set":
If the user decides that he actually enjoys a different book, "$set" can be used again to change the value:
For instance, if our fickle user decides that he actually likes quite a few books, he can change the value of the “favorite book” key into an array:
If the user realizes that he actually doesn’t like reading, he can remove the key altogether with "$unset":
Now the document will be the same as it was at the beginning of this example.
You must always use a $-modifier for adding, changing, or removing keys.
A common error people make when starting out is to try to set the value of "foo" to "bar" by doing an update that looks like this:
The "$inc" modifier can be used to change the value for an existing key or to create a new key if it does not already exist.
It is very useful for updating analytics, karma, votes, or anything else that has a changeable, numeric value.
Suppose we are creating a game collection where we want to save games and update scores as they change.
When a user starts playing, say, a game of pinball, we can insert a document that identifies the game by name and user playing it:
When the ball hits a bumper, the game should increment the player’s score.
If the ball lands in a “bonus” slot, we want to add 10,000 to the score.
This can be accomplished by passing a different value to "$inc":
The "score" key existed and had a numeric value, so the server added 10,000 to it.
If it is used on any other type of value, it will fail.
This includes types that many languages will automatically cast into numbers, like nulls, booleans, or strings of numeric characters:
Also, the value of the "$inc" key must be a number.
You cannot increment by a string, array, or other non-numeric value.
Doing so will give a “Modifier "$inc" allowed for numbers only” error message.
To modify other types, use "$set" or one of the following array operations.
Arrays are common and powerful data structures: not only are they lists that can be referenced by index, but they can also double as sets.
For example, suppose that we are storing blog posts and want to add a "comments" key containing an array.
We can push a comment onto the nonexistent "comments" array, which will create the array and add the comment:
This is the “simple” form of push, but you can use it for more complex array operations as well.
You can push multiple values in one operation using the "$each" suboperator:
This example would limit the array to the last 10 elements pushed.
If the array was smaller than 10 elements (after the push), all elements would be kept.
Thus, "$slice" can be used to create a queue in a document.
Finally, you can "$sort" before trimming, so long as you are pushing subobjects onto the array:
You might want to treat an array as a set, only adding values if they are not present.
This can be done using a "$ne" in the query document.
For example, to push an author onto a list of citations, but only if he isn’t already there, use the following:
For instance, suppose you have a document that represents a user.
You might have a set of email addresses that they have added:
For instance, we could use these modifiers if the user wanted to add more than one email address:
Sometimes an element should be removed based on specific criteria, rather than its position in the array.
For example, suppose we have a list of things that need to be done but not in any specific order:
Pulling removes all matching documents, not just a single match.
Array operators can be used only on keys with array values.
For example, you cannot push on to an integer or pop off of a string, for example.
Array manipulation becomes a little trickier when we have multiple values in an array and want to modify some of them.
There are two ways to manipulate values in arrays: by position or by using the position operator (the "$" character)
Arrays use 0-based indexing, and elements can be selected as though their index were a document key.
For example, suppose we have a document containing an array with a few embedded documents, such as a blog post with comments:
If we want to increment the number of votes for the first comment, we can say the following:
In many cases, though, we don’t know what index of the array to modify without querying for the document first and examining it.
To get around this, MongoDB has a positional operator, "$", that figures out which element of the array the query document matched and updates that element.
For example, if we have a user named John who updates his name to Jim, we can replace it in the comments by using the positional operator:
Thus, if John had left more than one comment, his name would be changed only for the first comment he left.
On the other hand, array modifiers might change the size of a document and can be slow.
When you start inserting documents into MongoDB, it puts each document right next to the previous one on disk.
Thus, if a document gets bigger, it will no longer fit in the space it was originally written to and will be moved to another part of the collection.
You can see this in action by creating a new collection with just a few documents and then making a document that is sandwiched between two other documents larger.
It will be bumped to the end of the collection:
When MongoDB has to move a document, it bumps the collection’s padding factor, which is the amount of extra space MongoDB leaves around new documents to give them room to grow.
If subsequent updates cause more moves, the padding factor will continue to grow (although not as dramatically as it did on the first move)
If there aren’t more moves, the padding factor will slowly go down, as shown in Figure 3-3
If a document grows and must be moved, free space is left behind and the padding size in increased.
Subsequent documents are inserted with the padding factor in free space between them.
If moves do not occur on subsequent inserts, this padding factor will decrease.
MongoDB has to free the space the document was in and write the document somewhere else.
Thus, you should try to keep the padding factor as close to 1 as possible.
You cannot manually set the padding factor (unless you’re compacting the collection: see “Compacting Data” on page 320), but you can design a schema that does not depend on documents growing arbitrarily large.
A simple program demonstrates the speed difference between in-place updates and moves.
The program below inserts a single key and increments its value 100,000 times:
Now, let’s try it with a document with a single array key, pushing new values onto that array 100,000 times.
Using "$push" and other array modifiers is encouraged and often necessary, but it is good to keep in mind the trade-offs of such updates.
As of this writing, MongoDB is not great at reusing empty space, so moving documents around a lot can result in large swaths of empty data file.
If you have a lot of empty space, you’ll start seeing messages that look like this in the logs:
That means that, while querying, MongoDB looked through an entire extent (see Appendix B for a definition, but it’s basically a subset of your collection) without finding any documents: it was just empty space.
The message itself is harmless, but it indicates that you have fragmentation and may wish to perform a compact.
If your schema requires lots of moves or lots of churn through inserts and deletes, you can improve disk reuse by using the usePowerOf2Sizes option.
All subsequent allocations made by the collection will be in power-of-two-sized blocks.
Only use this option on high-churn collections, though, as this makes initial space allocation less efficient.
Running this command with "usePowerOf2Sizes" : false turns off the special allocation.
The option only affects newly allocated records, so there is no harm in running it on an existing collection or toggling the value.
If no document is found that matches the update criteria, a new document will be created by combining the criteria and updated documents.
If a matching document is found, it will be updated normally.
Upserts can be handy because they can eliminate the need to “seed” your collection: you can often have the same code create and update documents.
Let’s go back to our example that records the number of views for each page of a website.
Without an upsert, we might try to find the URL and increment the number of views or create a new document if the URL doesn’t exist.
If we were to write this out as a JavaScript program it might look something like the following:
This means we are making a round trip to the database, plus sending an update or insert, every time someone visits a page.
If we are running this code in multiple processes, we.
We can eliminate the race condition and cut down on the amount of code by just sending an upsert (the third parameter to update specifies that this should be an upsert):
This line does exactly what the previous code block does, except it’s faster and atomic! The new document is created using the criteria document as a base and applying any modifier documents to it.
For example, if you do an upsert that matches a key and has an increment to the value of that key, the increment will be applied to the match:
Sometimes a field needs to be seeded when a document is created, but not changed on subsequent updates.
If we run this update again, it will match the existing document, nothing will be inserted, and so the "createdAt" field will not be changed:
Note that you generally do not need to keep a "createdAt" field, as ObjectIds contain a timestamp of when the document was created.
However, "$setOnInsert" can be useful for creating padding, initializing counters, and for collections that do not use ObjectIds.
If the document contains an "_id" key, save will do an upsert.
Updating Multiple Documents Updates, by default, update only the first document found that matches the criteria.
If there are more matching documents, they will remain unchanged.
To modify all of the documents matching the criteria, you can pass true as the fourth parameter to update.
Not only is it more obvious what the update should be doing, but your program also won’t break if the default is ever changed.
Multiupdates are a great way of performing schema migrations or rolling out new features to certain users.
Suppose, for example, we want to give a gift to every user who has a birthday on a certain day.
We can use multiupdate to add a "gift" to their account:
The "n" key will contain the number of documents affected by the update:
Returning Updated Documents You can get some limited information about what was updated by calling getLastError, but it does not actually return the updated document.
It is handy for manipulating queues and performing other operations that need get-and-set style atomicity.
Suppose we have a collection of processes run in a certain order.
Each is represented with a document that has the following form:
We need to find the job with the highest priority in the "READY" state, run the process function, and then update the status to "DONE"
We might try querying for the ready processes, sorting by priority, and updating the status of the highest-priority process to mark it is "RUNNING"
Once we have processed it, we update the status to "DONE"
This algorithm isn’t great because it is subject to a race condition.
If one thread (call it A) retrieved the document and another thread (call it B) retrieved the same document before A had updated its status to "RUNNING", then both threads would be running the same process.
We can avoid this by checking the status as part of the update query, but this becomes complex:
Also, depending on timing, one thread may end up doing all the work while another thread uselessly trails it.
Thread A could always grab the process, and then B would try to get the same process, fail, and leave A to do all the work.
Notice that the status is still "READY" in the returned document as findAndModify defaults to returning the document in its pre-modified state.
If you do a find on the collection, though, you can see that the document’s "status" has been updated to "RUNNING":
A "remove" key indicates that the matching document should be removed from the collection.
For instance, if we wanted to simply remove the job instead of updating its status, we could run the following:
A query document; the criteria with which to search for documents.
A modifier document; the update to perform on the document found (either this or "remove" must be specified)
Boolean specifying whether the document should be removed (either this or "up date" must be specified)
Boolean specifying whether the document returned should be the updated document or the pre-update document, to which it defaults.
Boolean specifying whether or not this should be an upsert, and which defaults to false.
Either "update" or "remove" must be included, but not both.
If no matching document is found, the command will return an error.
Setting a Write Concern Write concern is a client setting used to describe how safely a write should be stored before the application continues.
By default, inserts, removes, and updates wait for a database response—did the write succeed or not?—before continuing.
Generally, clients will throw an exception (or whatever the language’s version of an exception is) on failure.
There are a number of options available to tune exactly what you want the application to wait for.
The two basic write concerns are acknowledged or unacknowledged writes.
Acknowledged writes are the default: you get a response that tells you whether or not the database successfully processed your write.
Unacknowledged writes do not return any response, so you do not know if the write succeeded or not.
However, for low-value data (e.g., logs or bulk data loading), you may not want to wait for a response you don’t care about.
Although unacknowledged writes will not return database errors, they do not eliminate the need for error checking in your application.
If the socket was closed or there was an error writing to it, attempting a write will cause an exception.
One type of error that is easy to miss when using unacknowledged writes is inserting invalid data.
For example, if we attempt to insert two documents with the same "_id", the shell will throw an exception:
Were the second write sent with “unacknowledged” write concern, the second insert would not throw an exception.
Duplicate key exceptions are a common source of errors, but there are many others, from invalid $-modifiers to running out of disk space.
The shell does not actually support write concerns in the same way that the client libraries do: it does unacknowledged writes and then checks that the last operation was successful before drawing the prompt.
Thus, if you do a series of invalid operations on a collection, finishing with a valid operation, the shell will not complain:
You can manually force a check in the shell by calling getLastError, which checks for an error on the last operation:
The default write concern was changed in 2012, so legacy code may behave differently.
Fortunately, there is an easy way to tell if you’re using code written before or after the write concern switch: all of the drivers began using a class called MongoClient when they began defaulting to safe writes.
If your program is using a connection object called Mongo or Connec tion or something else, you are using the old, default-unsafe API.
No language used MongoClient as a class name prior to the switch, so if your code is using that, your writes are safe.
If you are using non-MongoClient connections, you should changed unacknowledged writes to acknowledged writes wherever possible in old code.
You can perform ad hoc queries on the database using the find or findOne functions and a query document.
You can query for ranges, set inclusion, inequalities, and more by using $-conditionals.
Queries return a database cursor, which lazily returns batches of documents as you need them.
There are a lot of metaoperations you can perform on a cursor, including skipping a certain number of results, limiting the number of results returned, and sorting results.
Introduction to find The find method is used to perform queries in MongoDB.
Querying returns a subset of documents in a collection, from no documents at all to the entire collection.
Which documents get returned is determined by the first argument to find, which is a document specifying the query criteria.
When we start adding key/value pairs to the query document, we begin restricting our search.
This works in a straightforward way for most types: numbers match numbers, booleans match booleans, and strings match strings.
For example, to find all documents where the value for "age" is 27, we can add that key/value pair to the query document:
If we have a string we want to match, such as a "username" key with the value "joe", we use that key/value pair instead:
Specifying Which Keys to Return Sometimes you do not need all of the key/value pairs in a document returned.
If this is the case, you can pass a second argument to find (or findOne) specifying the keys you want.
This reduces both the amount of data sent over the wire and the time and memory used to decode documents on the client side.
For example, if you have a user collection and you are interested only in the "user name" and "email" keys, you could return just those keys with the following query:
As you can see from the previous output, the "_id" key is returned by default, even if it isn’t specifically requested.
You can also use this second parameter to exclude specific key/value pairs from the results of a query.
For instance, you may have documents with a variety of keys, and the only thing you know is that you never want to return the "fatal_weakness" key:
The value of a query document must be a constant as far as the database is concerned.
It can be a normal variable in your own code.
That is, it cannot refer to the value of another key in the document.
Then, every time someone buys an item, we decrement the value of the "in_stock" key by one.
Finally, we can do a simple query to check which items are out of stock:
Query Criteria Queries can go beyond the exact matching described in the previous section; they can match more complex criteria, such as ranges, OR-clauses, and negation.
They can be combined to look for a range of values.
These types of range queries are often useful for dates.
An exact match on a date is less useful, since dates are only stored with millisecond precision.
Often you want a whole day, week, or month, making a range query necessary.
To query for documents where a key’s value is not equal to a certain value, you must use another conditional operator, "$ne", which stands for “not equal.” If you want to find all users who do not have the username “joe,” you can query for them using this:
If you have more than one possible value to match for a single key, use an array of criteria with "$in"
To find all three of these documents, we can construct the following query:
For example, if we are gradually migrating our schema to use usernames instead of user ID numbers, we can query for either by using this:
If we want to return all of the people who didn’t win anything in the raffle, we can query for them with this:
This query returns everyone who did not have tickets with those numbers.
In the raffle case, using "$or" would look like this:
If, for example, we want to match any of the three "ticket_no" values or the "winner" key, we can use this:
With a normal AND-type query, you want to narrow down your results as far as possible in as few arguments as possible.
OR-type queries are the opposite: they are most efficient if the first arguments match as many documents as possible.
Conditional Semantics If you look at the update modifiers in the previous chapter and previous query documents, you’ll notice that the $-prefixed keys are in different positions.
This generally holds true: conditionals are an inner document key, and modifiers are always a key in the outer document.
Although these seem like contradictory conditions, it is possible to fulfill if the "x" field.
Type-Specific Queries As covered in Chapter 2, MongoDB has a wide variety of types that can be used in a document.
It does match itself, so if we have a collection with the following documents:
However, null not only matches itself but also matches “does not exist.” Thus, querying for a key with the value null will return all documents lacking that key:
If we only want to find keys whose value is null, we can check that the key is null and exists using the "$exists" conditional:
Regular Expressions Regular expressions are useful for flexible string matching.
For example, if we want to find all users with the name Joe or joe, we can use a regular expression to do caseinsensitive matching:
Regular expression flags (for example, i) are allowed but not required.
If we want to match not only various capitalizations of joe, but also joey, we can continue to improve our regular expression:
MongoDB uses the Perl Compatible Regular Expression (PCRE) library to match regular expressions; any regular expression syntax allowed by PCRE is allowed in MongoDB.
It is a good idea to check your syntax with the JavaScript shell before using it in a query to make sure it matches what you think it matches.
MongoDB can leverage an index for queries on prefix regular expressions (e.g., /^joey/)
Very few people insert regular expressions into the database, but if you insert one, you can match it with itself:
Querying Arrays Querying for elements of an array is designed to behave the way querying for scalars does.
For example, if the array is a list of fruits, like this:
If you need to match arrays by more than one element, you can use "$all"
For example, suppose we created a collection with three elements:
Then we can find all documents with both "apple" and "banana" elements by querying with "$all":
You can also query by exact match using the entire array.
However, exact match will not match a document if any elements are missing or superfluous.
If you want to query for a specific element of an array, you can specify an index using the syntax key.index:
Arrays are always 0-indexed, so this would match the third array element against the string "peach"
A useful conditional for querying arrays is "$size", which allows you to query for arrays of a given size.
One common query is to get a range of sizes.
Then, every time you add an element to the array, increment the value of "size"
Incrementing is extremely fast, so any performance penalty is negligible.
Storing documents like this allows you to do queries such as this:
Unfortunately, this technique doesn’t work as well with the "$addToSet" operator.
As mentioned earlier in this chapter, the optional second argument to find specifies the keys to be returned.
The special "$slice" operator can be used to return a subset of elements for an array key.
For example, suppose we had a blog post document and we wanted to return the first 10 comments:
If there were fewer than 33 elements in the array, it would return as many as possible.
Unless otherwise specified, all keys in a document are returned when "$slice" is used.
This is unlike the other key specifiers, which suppress unmentioned keys from being returned.
For instance, if we had a blog post document that looked like this:
Both "title" and "content" are still returned, even though they weren’t explicitly included in the key specifier.
Given the blog example above, you could get Bob’s comment back with:
Note that this only returns the first match for each document: if Bob had left multiple comments on this post, only the first one in the "comments" array would be returned.
The best way to understand this behavior is to see an example.
This makes range queries against arrays essentially useless: a range will match any multielement array.
There are a couple of ways to get the expected behavior.
First, you can use "$elemMatch" to force MongoDB to compare both clauses with a single array element.
However, the catch is that "$elemMatch" won’t match non-array elements:
You can only use min() and max() when you have an index on the field you are querying for, though, and you must pass all fields of the index to min() and max()
Using min() and max() when querying for ranges over documents that may include arrays is generally a good idea: if you look at the index bounds for a "$gt“/”$lt" query over an array, you can see that it’s horribly inefficient.
It basically accepts any value, so it will search every index entry, not just those in the range.
Querying on Embedded Documents There are two ways of querying for an embedded document: querying for the whole document or querying for its individual key/value pairs.
Querying for an entire embedded document works identically to a normal query.
For example, if we have a document that looks like this:
If possible, it’s usually a good idea to query for just a specific key or keys of an embedded document.
Then, if your schema changes, all of your queries won’t suddenly break because they’re no longer exact matches.
Now, if Joe adds more keys, this query will still match his first and last names.
This dot notation is the main difference between query documents and other document types.
Query documents can contain dots, which mean “reach into an embedded document.” Dot notation is also the reason that documents to be inserted cannot contain the.
Oftentimes people run into this limitation when trying to save URLs as keys.
One way to get around it is to always perform a global replace before inserting or after retrieving, substituting a character that isn’t legal in URLs for the dot character.
Embedded document matches can get a little tricky as the document structure gets more complicated.
To correctly group criteria without needing to specify every key, use "$elemMatch"
This vaguely-named conditional allows you to partially specify criteria to match a single embedded document in an array.
As such, it’s only needed when you have more than one key you want to match on in an embedded document.
For queries that cannot be done any other way, there are "$where" clauses, which allow you to execute arbitrary JavaScript as part of your query.
This allows you to do (almost) anything within a query.
For security, use of "$where" clauses should be highly restricted or eliminated.
End users should never be allowed to execute arbitrary "$where" clauses.
The most common case for using "$where" is to compare the values for two keys in a document.
For instance, suppose we have documents that look like this:
We’d like to return documents where any two of the fields are equal.
For example, in the second document, "spinach" and "watermelon" have the same value, so we’d like that document returned.
If the function returns true, the document will be part of the result set; if it returns false, it won’t be.
Each document has to be converted from BSON to a JavaScript object and then run through the "$where" expression.
Hence, you should use "$where" only when there is no other way of doing the query.
You can cut down on the penalty by using other query filters in combination with "$where"
Server-Side Scripting You must be very careful with security when executing JavaScript on the server.
If done incorrectly, server-side JavaScript is susceptible to injection attacks similar to those that occur in a relational database.
However, by following certain rules around accepting input, you can use JavaScript safely.
Alternatively, you can turn off JavaScript execution altogether by running mongod with the --noscripting option.
The security issues with JavaScript are all related to executing user-provided programs on the server.
You want to avoid doing that, so make sure you aren’t accepting user input and passing it directly to mongod.
For example, suppose you want to print “Hello, name!”, where name is provided by the user.
A naive approach might be to write a JavaScript function such as the following:
Now, if you run this code, your entire database will be dropped!
To prevent this, you should use a scope to pass in the name.
Most drivers have a special type for sending code to the database, since code can actually be a composite of a string and a scope.
A scope is a document that maps variable names to values.
This mapping becomes a local scope for the JavaScript function being.
Thus, in the example above, the function would have access to a variable called username, whose value would be the string that the user gave.
The shell does not have a code type that includes scope; you can only use strings or JavaScript functions with it.
Cursors The database returns results from find using a cursor.
The client-side implementations of cursors generally allow you to control a great deal about the eventual output of a query.
You can limit the number of results, skip over some number of results, sort results by any combination of keys in any direction, and perform a number of other powerful operations.
To create a cursor with the shell, put some documents into a collection, do a query on them, and assign the results to a local variable (variables defined with "var" are local)
Here, we create a very simple collection and query it, storing the results in the cursor variable:
The advantage of doing this is that you can look at one result at a time.
If you store the results in a global variable or no variable at all, the MongoDB shell will automatically iterate through and display the first couple of documents.
This is what we’ve been seeing up until this point, and it is often the behavior you want for seeing what’s in a collection but not for doing actual programming with the shell.
To iterate through the results, you can use the next method on the cursor.
You can use hasNext to check whether there is another result.
The cursor class also implements JavaScript’s iterator interface, so you can use it in a forEach loop:
When you call find, the shell does not query the database immediately.
It waits until you start requesting results to send the query, which allows you to chain additional options onto a query before it is performed.
Almost every method on a cursor object returns the cursor itself so that you can chain options in any order.
At this point, the query has not been executed yet.
At this point, the query will be sent to the server.
After the client has run through the first set of results, the shell will again contact the database and ask for more results with a getMore request.
This process continues until the cursor is exhausted and all results have been returned.
Limits, Skips, and Sorts The most common query options are limiting the number of results returned, skipping a number of results, and sorting.
All these options must be added before a query is sent to the database.
To set a limit, chain the limit function onto your call to find.
If there are fewer than three documents matching your query in the collection, only the number of matching documents will be returned; limit sets an upper limit, not a lower limit.
This will skip the first three matching documents and return the rest of the matches.
If there are fewer than three documents in your collection, it will not return any documents.
For example, suppose that you are running an online store and someone searches for mp3
If you want 50 results per page sorted by price from high to low, you can do the following:
However, large skips are not very performant; there are suggestions for how to avoid them in the next section.
Sometimes you will have a single key with multiple types: for instance, integers and booleans, or strings and nulls.
If you do a sort on a key with a mix of types, there is a predefined order that they will be sorted in.
From least to greatest value, this ordering is as follows:
Avoiding Large Skips Using skip for a small number of documents is fine.
For a large number of results, skip can be slow, since it has to find and then discard all the skipped results.
Most databases keep more metadata in the index to help with skips, but MongoDB does not yet support this, so large skips should be avoided.
Often you can calculate the next query based on the result from the previous one.
The easiest way to do pagination is to return the first page of results using limit and then return each subsequent page as an offset from the beginning:
However, depending on your query, you can usually find a way to paginate without skips.
For example, suppose we want to display documents in descending order based on "date"
We can get the first page of results with the following:
Then, we can use the "date" value of the last document as the criteria for fetching the next page:
Now the query does not need to include a skip.
One fairly common problem is how to get a random document from a collection.
The naive (and slow) solution is to count the number of documents and then do a find, skipping a random number of documents between 0 and the size of the collection:
It is actually highly inefficient to get a random element this way: you have to do a count (which can be expensive if you are using criteria), and skipping large numbers of elements can be time-consuming.
It takes a little forethought, but if you know you’ll be looking up a random element on a collection, there’s a much more efficient way to do so.
The trick is to add an extra random key to each document when it is inserted.
Now, when we want to find a random document from the collection, we can calculate a random number and use that as query criteria, instead of doing a skip:
There is a slight chance that random will be greater than any of the "random" values in the collection, and no results will be returned.
We can guard against this by simply returning a document in the other direction:
If there aren’t any documents in the collection, this technique will end up returning null, which makes sense.
This technique can be used with arbitrarily complex queries; just make sure to have an index that includes the random key.
For example, if we want to find a random plumber in California, we can create an index on "profession", "state", and "random":
This allows us to quickly find a random result (see Chapter 5 for more information on indexing)
Advanced Query Options There are two types of queries: wrapped and plain.
Most drivers provide helpers for adding arbitrary options to queries.
Specify the maximum number of documents that should be scanned for the query.
This can be useful if you want a query to not to take too long but are not sure how much of a collection will need to be scanned.
This will limit your results to whatever was found in the part of the collection that was scanned (i.e., you may miss other documents that match)
This forces the given index to be used for the query.
You can use "$min" to force the lower bound on an index scan, which may be helpful for complex queries.
This forces the given index to be used for the query.
You can use "$max" to force bounds on an index scan, which may be helpful for complex queries.
Adds a "$diskLoc" field to the results that shows where on disk that particular result lives.
The file number shows which file the document is in.
In this case, if we’re using the test database, the document is in test.2
The second field gives the byte offset of each document within the file.
Getting Consistent Results A fairly common way of processing data is to pull it out of MongoDB, change it in some way, and then save it again:
This is fine for a small number of results, but MongoDB can return the same result multiple times for a large result set.
To see why, imagine how the documents are being stored.
You can picture a collection as a list of documents that looks something like Figure 4-1
Snowflakes represent documents, since every document is beautiful and unique.
Now, when we do a find, the cursor starts returning results from the beginning of the collection and moves right.
Your program grabs the first 100 documents and processes them.
When you save them back to the database, if a document does not have the padding available to grow to its new size, like in Figure 4-2, it needs to be relocated.
Usually, a document will be relocated to the end of a collection (Figure 4-3)
An enlarged document may not fit where it did before.
MongoDB relocates updated documents that don’t fit in their original position.
When it gets toward the end, it will return the relocated documents again (Figure 4-4)!
A cursor may return these relocated documents again in a later batch.
The solution to this problem is to snapshot your query.
If you add the option, the query will be run by traversing the "_id" index, which guarantees that you’ll only return each document once.
Snapshotting makes queries slower, so only use snapshotted queries when necessary.
For example, mongodump (a backup utility covered in Chapter 22) uses snapshotted queries by default.
All queries that return a single batch of results are effectively snapshotted.
Inconsistencies arise only when the collection changes under a cursor while it is waiting to get another batch of results.
Immortal Cursors There are two sides to a cursor: the client-facing cursor and the database cursor that the client-side one represents.
We have been talking about the client-side one up until now, but we are going to take a brief look at what’s happening on the server.
On the server side, a cursor takes up memory and resources.
Once a cursor runs out of results or the client sends a message telling it to die, the database can free the resources it was using.
Freeing these resources lets the database use them for other things, which is good, so we want to make sure that cursors can be freed quickly (within reason)
There are a couple of conditions that can cause the death (and subsequent cleanup) of a cursor.
First, when a cursor finishes iterating through the matching results, it will clean itself up.
Another way is that, when a cursor goes out of scope on the client side, the drivers send the database a special message to let it know that it can kill that cursor.
Finally, even if the user hasn’t iterated through all the results and the cursor is still in scope, after 10 minutes of inactivity, a database cursor will automatically “die.” This way, if a client crashes or is buggy, MongoDB will not be left with thousands of open cursors.
This “death by timeout” is usually the desired behavior: very few applications expect their users to sit around for minutes at a time waiting for results.
However, sometimes you might know that you need a cursor to last for a long time.
In that case, many drivers have implemented a function called immortal, or a similar mechanism, which tells the database not to time out the cursor.
If you turn off a cursor’s timeout, you must iterate through all of its results or kill it to make sure it gets closed.
Otherwise, it will sit around in the database hogging resources until the server is restarted.
Database Commands There is one very special type of query called a database command.
Database commands do “everything else,” from administrative tasks like shutting down the server and cloning databases to counting documents in a collection and performing aggregations.
Commands are mentioned throughout this text, as they are useful for data manipulation, administration, and monitoring.
For example, dropping a collection is done via the "drop" database command:
You might be more familiar with the shell helper, which wraps the command and provides a simpler interface:
Often you can just use the shell helpers, but knowing the underlying commands can be helpful if you’re stuck on a box with an old version of the shell and connected to a new version of the database: the shell might not have the wrappers for new database commands, but you can still run them with runCommand()
We’ve already seen a couple of commands in the previous chapters; for instance, we used the getLastError command in Chapter 3 to check the number of documents affected by an update:
In this section, we’ll take a closer look at commands to see exactly what they are and how they’re implemented.
We’ll also describe some of the most useful commands that are supported by MongoDB.
You can see all commands by running the db.listCom mands() command.
How Commands Work A database command always returns a document containing the key "ok"
If "ok" is 0 then an additional key will be present, "errmsg"
The value of "errmsg" is a string explaining why the command failed.
As an example, let’s try running the drop command again, on the collection that was dropped in the previous section:
Commands in MongoDB are implemented as a special type of query that gets performed on the $cmd collection.
When the MongoDB server gets a query on the $cmd collection, it handles it using special logic, rather than the normal code for handling queries.
Almost all MongoDB drivers provide a helper method like runCommand for running commands, but commands can always be run using a simple query.
Some commands require administrator access and must be run on the admin database.
If such a command is run on any other database, it will return an "access denied" error.
If you’re working on another database and you need to run an admin command, you can use the adminCommand function, instead of runCommand:
This chapter introduces MongoDB’s indexing, which allows you to optimize your queries and is even required for certain types of queries:
What indexing is and why you’d want to use it.
Choosing the right indexes for your collections is critical to performance.
Introduction to Indexing A database index is similar to a book’s index.
Instead of looking through the whole book, the database takes a shortcut and just looks at an ordered list that points to the content, which allows it to query orders of magnitude faster.
A query that does not use an index is called a table scan (a term inherited from relational databases), which means that the server has to “look through the whole book” to find a query’s results.
This process is basically what you’d do if you were looking for information in a book without an index: you start at page 1 and read through the whole thing.
In general, you want to avoid making the server do table scans because it is very slow for large collections.
If we do a query on this collection, we can use the explain() function to see what MongoDB is doing when it executes the query.
Try querying on a random username to see an example.
That is, MongoDB had to look through every field in every document.
This took nearly a second to accomplish: the "millis" field shows the number of milliseconds it took to execute the query.
Note that MongoDB had to look through every document in the collection for matches because it did not know that usernames are unique.
To optimize this query, we could limit it to one result so that MongoDB would stop looking after it found user101:
The number scanned has now been cut way down and the query is almost instantaneous.
However, this is an impractical solution in general: what if we were looking for user999999? Then we’d still have to traverse the entire collection and our service would just get slower and slower as we added users.
Indexes are a great way to fix queries like this because they organize data by a given field to let MongoDB find it quickly.
Depending on your machine and how large you made the collection, creating an index may take a few minutes.
If the ensureIndex call does not return after a few seconds, run db.currentOp() (in a different shell) or check your mongod’s log to see the index build’s progress.
This explain() output is more complex, but continue to ignore all the fields other than "n", "nscanned", and "millis" for now.
As you can see, the query is now almost instantaneous and, even better, has a similar runtime when querying for any username:
As you can see, an index can make a dramatic difference in query times.
However, indexes have their price: every write (insert, update, or delete) will take longer for every.
This is because MongoDB has to update all your indexes whenever your data changes, as well as the document itself.
Generally you should not have more than a couple of indexes on any given collection.
The tricky part becomes figuring out which fields to index.
MongoDB’s indexes work almost identically to typical relational database indexes, so if you are familiar with those, you can skim this section for syntax specifics.
To choose which fields to create indexes for, look through your common queries and queries that need to be fast and try to find a common set of keys from those.
For instance, in the example above, we were querying on "username"
If that was a particularly common query or was becoming a bottleneck, indexing "username" would be a good choice.
However, if this was an unusual query or one that was only done by administrators who didn’t care how long it took, it would not be a good choice of index.
Introduction to Compound Indexes An index keeps all of its values in a sorted order so it makes sorting documents by the indexed key much faster.
However, an index can only help with sorting if it is a prefix of the sort.
For example, the index on "username" wouldn’t help much for this sort:
This sorts by "age" and then "username", so a strict sorting by "username" isn’t terribly helpful.
To optimize this sort, you could make an index on "age" and "username":
This is called a compound index and is useful if your query has multiple sort directions or multiple keys in the criteria.
A compound index is an index on more than one field.
Suppose we have a users collection that looks something like this, if we run a query with no sorting (called natural order):
Each index entry contains an age and a username and points to the location of a document on disk (represented by the hexadecimal numbers, which can be ignored)
Note that "age" fields are ordered to be strictly ascending and, within each age, "user name"s are also in ascending order.
As each age has approximately 8,000 usernames associated with it, only those necessary to convey the general idea have been included.
This type of query is very efficient: MongoDB can jump directly to the correct age and doesn’t need to sort the results as traversing the index returns the data in the correct order.
Note that sort direction doesn’t matter: MongoDB is comfortable traversing the index in either direction.
MongoDB will use the first key in the index, "age", to return the matching documents, like so:
In general, if MongoDB uses an index for a query it will return the resulting documents in index order.
This is a multi-value query, like the previous one, but this time it has a sort.
As before, MongoDB will use the index to match the criteria:
However, the index doesn’t return the usernames in sorted order and the query requested that the results be sorted by username, so MongoDB has to sort the results in memory before returning them.
Thus, this query is usually less efficient than the queries above.
Of course, the speed depends on how many results match your criteria: if your result set is only a couple of documents, MongoDB won’t have much work to do to sort them.
If there are more results, it will be slower or may not work at all: if you have more than 32 MB of results MongoDB will just error out, refusing to sort that much data:
This is good in that it does not require any giant in-memory sorts.
However, it does have to scan the entire index to find all matches.
Thus, putting the sort key first is generally a good strategy when you’re using a limit so MongoDB can stop scanning the index after a couple of matches.
Note that this took nearly 15 seconds to run, making the first index the clear winner.
However, if we limit the number of results for each query, a new winner emerges:
The first query is still hovering between two and three seconds, but the second query now takes less than a fifth of a second! Thus, you should always run explain()s on exactly the queries that your application is running.
Using Compound Indexes In the section above, we’ve been using compound indexes, which are indexes with more than one key in them.
Compound indexes are a little more complicated to think about than single-key indexes, but they are very powerful.
So far, all of our index entries have been sorted in ascending, or least-to-greatest, order.
However, if you need to sort on two (or more) criteria, you may need to have index keys go in different directions.
For example, suppose we want to sort the collection above by youngest to oldest and usernames from Z-A.
Our previous indexes would not be very efficient for this problem: within each age group users were sorted by "username" ascending A-Z, not Z-A.
The compound indexes above do not hold the values in any useful order for getting "age" ascending and "username" descending.
In the examples above, the query was always used to find the correct document, and then follow a pointer back to fetch the actual document.
However, if your query is only looking for the fields that are included in the index, it does not need to fetch the document.
When an index contains all the values requested by the user, it is considered to.
Whenever practical, use covered indexes in preference to going back to documents.
You can make your working set much smaller that way, especially if you can combine this with a right-balanced index.
You may also have to index fields that you aren’t querying on, so you should balance your need for faster queries with the overhead this will add on writes.
If you run an explain on a covered query, the "indexOnly" field will be true.
If you index a field containing arrays, that index can never cover a query (due to the way arrays are stored in indexes, this is covered in more depth in “Indexing Objects and Arrays” on page 95)
Even if you exclude the array field from the fields returned, you cannot cover a query using such an index.
How $-Operators Use Indexes Some queries can use indexes more efficiently than others; some queries cannot using indexes at all.
This section covers how various query operators are handled by MongoDB.
They must look at all the index entries other than the one specified by the "$ne", so it basically has to scan the entire index.
For example, here are the index ranges traversed for such a query:
This can be efficient if a large swath of your collection is 3, but otherwise it must check almost everything.
If you need to perform one of these types of queries quickly, figure out if there’s another clause that you could add to the query that could use an index to filter the result set down to a small number of documents before MongoDB attempts to do non-indexed matching.
For example, suppose we were finding all users without a "birthday" field.
If we knew that the application started adding a birthday field on March 20th, we could limit the query to users created before then:
The order of fields in a query is irrelevant: MongoDB will find which fields it can use indexes on regardless of ordering.
This forces MongoDB to scan 10 times the number of index entries as using the previous index would.
Using two ranges in a query basically always forces this less-efficient query plan.
As you can see, this explain is the conglomerate of two separate queries.
Indexing Objects and Arrays MongoDB allows you to reach into your documents and create indexes on nested fields and arrays.
Embedded object and array fields can be combined with top-level fields in.
Indexes can be created on keys in embedded documents in the same way that they are created on normal keys.
If we had a collection where each document represented a user, we might have an embedded document that described each user’s location:
We could put an index on one of the subfields of "loc", say "loc.city", to speed up queries using that field:
You can also index arrays, which allows you to use the index to search for specific array elements efficiently.
Suppose we have a collection of blog posts where each document was a post.
Each post has a "comments" field, which is an array of comment subdocuments.
Indexing an array creates an index entry for each element of the array, so if a post had.
Unlike the "loc" example in the previous section, you cannot index an entire array as a single entity: indexing an array field indexes each element of the array, not the array itself.
Indexes on array elements do not keep any notion of position: you cannot use an index for a query that is looking for a specific array element, such as "comments.4"
If any document has an array field for the indexed key, the index immediately is flagged as a multikey index.
You can see whether an index is multikey from explain()’s output: if a multikey index was used, the "isMultikey" field will be true.
Once an index has been flagged as multikey, it can never be un-multikeyed, even if all of the documents containing arrays in that field are removed.
The only way to un-multikey it is to drop and recreate it.
Multikey indexes may be a bit slower than non-multikey indexes.
Many index entries can point at a single document so MongoDB may need to do some de-duping before returning results.
Index Cardinality Cardinality refers to how many distinct values there are for a field in a collection.
Some fields, such as "gender" or "newsletter opt-out", might only have two possible values, which is considered a very low cardinality.
Others, such as "username" or "email", might have a unique value for every document in the collection, which is high cardinality.
Still others fall somewhere in between, such as "age" or "zip code"
In general, the greater the cardinality of a field, the more helpful an index on that field can be.
This is because the index can quickly narrow the search space to a much smaller result set.
For a low cardinality field, an index generally cannot eliminate as many possible matches.
For example, suppose we had an index on "gender" and were looking for women named Susan.
We could only narrow down the result space by approximately 50% before referring to individual documents to look up "name"
Conversely, if we indexed by "name", we could immediately narrow down our result set to the tiny fraction of users named Susan and then we could refer to those documents to check the gender.
As a rule of thumb, try to create indexes on high-cardinality keys or at least put highcardinality keys first in compound indexes (before low-cardinality keys)
Using explain() and hint() As you have seen above, explain() gives you lots of information about your queries.
It is one of the most important diagnostic tools there is for slow queries.
You can find out which indexes are being used and how by looking at a query’s explain.
For any query, you can add a call to explain() at the end (the way you would add a sort() or lim it(), but explain() must be the last call)
There are two types of explain() output that you’ll see most commonly: indexed and non-indexed queries.
Special index types may create slightly different query plans, but most fields should be similar.
Also, sharding returns a conglomerate of explain()s (as covered in Chapter 13), as it runs the query on multiple servers.
The most basic type of explain() is on a query that doesn’t use an index.
You can tell that a query doesn’t use an index because it uses a "BasicCursor"
Conversely, most queries that use an index use a BtreeCursor (some special types of indexes, such as geospatial indexes, use their own type of cursor)
The output to an explain() on a query that uses an index varies, but in the simplest case, it looks something like this:
This output first tells you what index was used: age_1_username_1
However, it may not always be the number you are looking for.
If MongoDB tried multiple query plans, "millis" will reflect how long it took all of them to run, not the one chosen as the best.
Next is how many documents were actually returned as a result: "n"
This doesn’t reflect how much work MongoDB did to answer the query: how many index entries and documents did it have to search? Index entries are described by "nscanned"
Finally, if you were using a sort and MongoDB could not use an index for it, "scanAndOrder" would be true.
This means that MongoDB had to sort the results in memory, which is generally quite slow and limited to a small number of results.
Now that you know the basics, here is a breakdown of the all of the fields in more detail: "cursor" : "BtreeCursor age_1_username_1"
If this query used a multikey index (see “Indexing Objects and Arrays” on page 95)
This is the number of documents returned by the query.
This is a count of the number of times MongoDB had to follow an index pointer to the actual document on disk.
If the query contains criteria that is not part of the index or requests fields back that aren’t contained in the index, MongoDB must look up the document each index entry points to.
The number of index entries looked at if an index was used.
If this was a table scan, it is the number of documents examined.
If MongoDB was able to fulfill this query using only the index entries (as discussed in “Using covered indexes” on page 90)
The number of times this query yielded (paused) to allow a write request to proceed.
If there are writes waiting to go, queries will periodically release their lock and allow them to do so.
However, on this system, there were no writes waiting so the query never yielded.
The number of milliseconds it took the database to execute the query.
This field describes how the index was used, giving ranges of the index traversed.
The second index key was a free variable, as the query didn’t specify.
If a query is not using the index that you want it to and you use a hint to change it, run an explain() on the hinted query before deploying.
If you force MongoDB to use an index on a query that it does not know how to use an index for, you could end up making the query less efficient than it was without the index.
The Query Optimizer MongoDB’s query optimizer works a bit differently than any other database’s.
Basically, if an index exactly matches a query (you are querying for "x" and have an index on "x"), the query optimizer will use that index.
Otherwise, there might be a few possible indexes that could work well for your query.
MongoDB will select a subset of likely indexes and run the query once with each plan, in parallel.
The first plan to return 100 results is the “winner” and the other plans’ executions are halted.
This plan is cached and used subsequently for that query until the collection has seen a certain amount of churn.
Once the collection has changed a certain amount since the initial plan evaluation, the query optimizer will re-race the possible plans.
Plans will also be reevaluated after index creation or every 1,000 queries.
The "allPlans" field in explain()’s output shows each plan the query tried running.
When Not to Index Indexes are most effective at retrieving small subsets of data and some types of queries are faster without indexes.
Indexes become less and less efficient as you need to get larger percentages of a collection because using an index requires two lookups: one to.
A table scan only requires one: looking at the document.
In the worst case (returning all of the documents in a collection) using an index would take twice as many lookups and would generally be significantly slower than a table scan.
Unfortunately, there isn’t a hard-and-fast rule about when an index helps and when it hinders as it really depends on the size of your data, size of your indexes, size of your documents, and the average result set size (Table 5-1)
As a rule of thumb: if a query is returning 30% or more of the collection, start looking at whether indexes or table scans are faster.
Properties that affect the effectiveness of indexes Indexes often work well for Table scans often work well for.
Let’s say we have an analytics system that collects statistics.
Your application queries the system for all documents for a given account to generate a nice graph of all data from an hour ago to the beginning of time:
When we first launch, this is a tiny result set and returns instantly.
But after a couple weeks, it starts being a lot of data, and after a month this query is already taking too long to run.
For most applications, this is probably the “wrong” query: do you really want a query that’s returning most of your data set? Most applications, particularly those with large data sets, do not.
However, there are some legitimate cases where you may want most or all of your data: you might be exporting this data to a reporting system or using it for a batch job.
In these cases, you would like to return this large proportion of the data set as fast as possible.
One side effect of sorting by "$natural" is that it gives you results in on-disk order.
This is generally meaningless for an active collection: as documents grow and shrink they’ll be moved around on disk and new documents will be written in the “holes” they left.
However, for insert-only workloads, $natural can be useful for giving you the latest (or earliest) documents.
Types of Indexes There are a few index options you can specify when building an index that change the way the index behaves.
The most common variations are described in the following sections, and more advanced or special-case options are described in the next chapter.
Unique Indexes Unique indexes guarantee that each value will appear at most once in the index.
For example, if you want to make sure no two documents can have the same value in the "username" key, you can create a unique index:
For example, suppose that we try to insert the following documents on the collection above:
If you check the collection, you’ll see that only the first "bob" was stored.
Throwing duplicate key exceptions is not very efficient, so use the unique constraint for the occasional duplicate, not to filter out zillions of duplicates a second.
A unique index that you are probably already familiar with is the index on "_id", which is automatically created whenever you create a collection.
This is a normal unique index (aside from the fact that it cannot be dropped as other unique indexes can be)
If a key does not exist, the index stores its value as null for that document.
This means that if you create a unique index and try to insert more than one document that is missing the indexed field, the inserts will fail because you already have a document with a value of null.
See “Sparse Indexes” on page 106 for advice on handling this.
Index buckets are of limited size and if an index entry exceeds it, it just won’t be included in the index.
This can cause confusion as it makes a document “invisible” to queries that use the index.
All fields must be smaller than 1024 bytes to be included in an index.
MongoDB does not return any sort of error or warning if a document’s fields cannot be indexed due to size.
If you do this, individual keys can have the same values, but the combination of values across all keys in an index entry can appear in the index at most once.
However, attempting to insert a second copy of any of these documents would cause a duplicate key exception.
Note that all of the values for "files_id" are the same, but "n" is different.
If you attempt to build a unique index on an existing collection, it will fail to build if there are any duplicate values:
Generally, you’ll need to process your data (the aggregation framework can help) and figure out where the duplicates are and what to do with them.
In a few rare cases, you may just want to delete documents with duplicate values.
The "dropDups" option will save the first document found and remove any subsequent documents with duplicate values:
If your data is of any importance, do not use "dropDups"
Sparse Indexes As mentioned in an earlier section, unique indexes count null as a value, so you cannot have a unique index with more than one document missing the key.
However, there are lots of cases where you may want the unique index to be enforced only if the key exists.
If you have a field that may or may not exist but must be unique when it does, you can combine the unique option with the sparse option.
If you are familiar with sparse indexes on relational databases, MongoDB’s sparse indexes are a completely different concept.
MongoDB sparse indexes are basically indexes that need not include every document as an entry.
For example, if providing an email address was optional but, if provided, should be unique, we could do:
To make a non-unique sparse index, simply do not include the unique option.
One thing to be aware of is that the same query can return different results depending on whether or not it uses the sparse index.
For example, suppose we had a collection where most of the documents had "x" fields, but one does not:
You can use hint() to force it to do a table scan if you need documents with missing fields.
Index Administration As shown in the previous section, you can create new indexes using the ensureIndex function.
An index only needs to be created once per collection.
If you try to create the same index again, nothing will happen.
All of the information about a database’s indexes is stored in the system.indexes collection.
This is a reserved collection, so you cannot modify its documents or remove documents from it.
You can manipulate it only through ensureIndex and the dropIn dexes database command.
When you create an index, you can see its meta information in system.indexes.
If you have any indexes that do not have a "v" : 1 field, they are being stored in an older, less efficient format.
You can upgrade them by ensuring that you’re running at least MongoDB version 2.0 and dropping and rebuilding the index.
Identifying Indexes Each index in a collection has a name that uniquely identifies the index and is used by the server to delete or manipulate it.
This can get unwieldy if indexes contain more than a couple keys, so you can specify your own name as one of the options to ensur eIndex:
There is a limit to the number of characters in an index name, so complex indexes may need custom names to be created.
A call to getLastError will show if the index creation succeeded or why it didn’t.
Changing Indexes As your application grows and changes, you may find that your data or queries have changed and that indexes that used to work well no longer do.
Use the "name" field from the index description to specify which index to drop.
By default, MongoDB will build an index as fast as possible, blocking all reads and writes on a database until the index build has finished.
If you would like your database to remain somewhat responsive to reads and writes, use the background option when building an index.
This forces the index build to occasionally yield to other operations, but may still have a severe impact on your application (see “Building Indexes” on page 222 for more information)
If you have the choice, creating indexes on existing documents is slightly faster than creating the index first and then inserting all documents.
This chapter covers the special collections and index types MongoDB has available, including:
Capped Collections “Normal” collections in MongoDB are created dynamically and automatically grow in size to fit additional data.
MongoDB also supports a different type of collection, called a capped collection, which is created in advance and is fixed in size (see Figure 6-1)
Having fixed-size collections brings up an interesting question: what happens when we try to insert into a capped collection that is already full? The answer is that capped collections behave like circular queues: if we’re out of space, the oldest document will be deleted, and the new one will take its place (see Figure 6-2)
This means that capped collections automatically age-out the oldest documents as new documents are inserted.
Documents cannot be removed or deleted (aside from the automatic age-out described earlier), and updates that would cause documents to grow in size are disallowed.
By preventing these two operations, we guarantee that documents in a capped collection are stored in insertion order and that there is no need to maintain a free list for space from removed documents.
New documents are inserted at the end of the queue.
When the queue is full, the oldest element will be replaced by the newest.
Capped collections have a different access pattern than most MongoDB collections: data is written sequentially over a fixed section of disk.
This makes them tend to perform writes quickly on spinning disk, especially if they can be given their own disk (so as not to be “interrupted” by other collections’ random writes)
Capped collections tend to be useful for logging, although they lack flexibility: you cannot control when data ages out, other than setting a size when you create the collection.
Creating Capped Collections Unlike normal collections, capped collections must be explicitly created before they are used.
Once a capped collection has been created, it cannot be changed (it must be dropped and recreated if you wish to change its properties)
Thus, you should think carefully about the size of a large collection before creating it.
When limiting the number of documents in a capped collection, you must specify a size limit as well.
Age-out will be based on whichever limit is reached first: it cannot hold more than "max" documents nor take up more than "size" space.
Another option for creating a capped collection is to convert an existing, regular collection into a capped collection.
This can be done using the convertToCapped command—in the following example, we convert the test collection to a capped collection of 10,000 bytes:
There is no way to “uncap” a capped collection (other than dropping it)
Sorting Au Naturel There is a special type of sort that you can do with capped collections, called a natural sort.
A natural sort returns the documents in the order that they appear on disk (see Figure 6-3)
For most collections, this isn’t a very useful sort because documents move around.
However, documents in a capped collection are always kept in insertion order so that natural order is the same as insertion order.
Thus, a natural sort gives you documents from oldest to newest.
You can also sort from newest to oldest (see Figure 6-4):
Tailable Cursors Tailable cursors are a special type of cursor that are not closed when their results are exhausted.
They were inspired by the tail -f command and, similar to the command, will continue fetching output for as long as possible.
Because the cursors do not die when they run out of results, they can continue to fetch new results as documents are added to the collection.
Tailable cursors can be used only on capped collections, since insert order is not tracked for normal collections.
Tailable cursors are often used for processing documents as they are inserted onto a “work queue” (the capped collection)
Because tailable cursors will time out after 10 minutes of no results, it is important to include logic to re-query the collection if they die.
The mongo shell does not allow you to use tailable cursors, but using one in PHP looks something like the following:
The cursor will process results or wait for more results to arrive until the cursor dies (it will time out if there are no inserts for 10 minutes or someone kills the query operation)
However, you can create collections with "_id" indexes by setting the autoIndexId option to false when calling createCol lection.
This is not recommended but can give you a slight speed boost on an insertonly collection.
If you create a collection without an "_id" index, you will never be able replicate the mongod it lives on.
Replication requires the "_id" index on every collection (it is important that replication can uniquely identify each document in a collection)
Do a practice run before creating the index in production, as unlike other indexes, the "_id" index cannot be dropped once created.
Thus, you must get it right the first time! If you do not, you cannot change it without dropping the collection and recreating it.
Time-To-Live Indexes As mentioned in the previous section, capped collections give you limited control over when their contents are overwritten.
If you need a more flexible age-out system, timeto-live (TTL) indexes allow you to set a timeout for each document.
When a document reaches a preconfigured age, it will be deleted.
This type of index is useful for caching problems like session storage.
You can create a TTL index by specifying the expireAfterSecs option in the second argument to ensureIndex:
If a document’s "lastUpdated" field exists and is a date, the document will be removed once the server time is expir eAfterSecs seconds ahead of the document’s time.
To prevent an active session from being removed, you can update the "lastUpdated" field to the current time whenever there is activity.
Once "lastUpdated" is 24 hours old, the document will be removed.
MongoDB sweeps the TTL index once per minute, so you should not depend on tothe-second granularity.
You can have multiple TTL indexes on a given collection.
They cannot be compound indexes but can be used like “normal” indexes for the purposes of sorting and query optimization.
Full-Text Indexes MongoDB has a special type of index for searching for text within documents.
In previous chapters, we’ve queried for strings using exact matches and regular expressions, but these techniques have some limitations.
Searching a large block of text for a regular expression is slow and it’s tough to take linguistic issues into account (e.g., that “entry” should match “entries”)
Full-text indexes give you the ability to search text quickly, as well as provide built-in support for multi-language stemming and stop words.
While all indexes are expensive to create, full-text indexes are particularly heavyweight.
Creating a full-text index on a busy collection can overload MongoDB, so adding this type of index should always be done offline or at a time when performance does not matter.
You should be wary of creating full-text indexes that will not fit in RAM (unless you have SSDs)
See Chapter 18 for more information on creating indexes with minimal impact on your application.
Full-text search will also incur more severe performance penalties on writes than “normal” indexes, since all strings must be split, stemmed, and stored in a few places.
Thus, you will tend to see poorer write performance on full-text-indexed collections than on others.
It will also slow down data movement if you are sharding: all text must be reindexed when it is migrated to a new shard.
As of this writing, full text indexes are an “experimental” feature, so you must enable them specifically.
You can either start MongoDB with the --setParameter textSearch Enabled=true option or set it at runtime by running the setParameter command:
Suppose we use the unofficial Hacker News JSON API to load some recent stories into MongoDB.
Now, to use the index, we must use the text command (as of this writing, full text indexes cannot be used with “normal” queries):
The matching documents are returned in order of decreasing relevance: “Ask HN” is first, then two “Show HN” partial matches.
The "score" field before each object describes how closely the result matched the query.
As you can see from the results, the search is case insensitive, at least for characters in [a-zA-Z]
Full-text indexes use toLower to lowercase words, which is locale-dependant, so users of other languages may find MongoDB unpredictably case sensitive, depending on how toLower behaves on their character set.
Full text indexes only index string data: other data types are ignored and not included in the index.
Only one full-text index is allowed per collection, but it may contain multiple fields:
This is not like “nomal” multikey indexes where there is an ordering on the keys: each field is given equal consideration.
You can control the relative importance MongoDB attaches to each field by specifying a weight:
The weights above would weight "title" fields the most, followed by "author" and then "desc" (not specified in the weight list, so given a default weight of 1)
You cannot change field weights after index creation (without dropping the index and recreating it), so you may want to play with weights on a sample data set before creating the index on your production data.
For some collections, you may not know which fields a document will contain.
You can create a full-text index on all string fields in a document by creating an index on "$**": this not only indexes all top-level string fields, but also searches embedded documents and arrays for string fields:
As the weights specify that you’re indexing all fields, MongoDB does not require you to give a field list.
Search Syntax By default, MongoDB queries for an OR of all the words: “ask OR hn”
This is the most efficient way to perform a full text query, but you can also do exact phrase searches and NOT.
To search for the exact phrase “ask hn”, you can query for that by including the query in quotes:
This is slower than the OR-type match, since MongoDB first performs an OR match and then post-processes the documents to ensure that they are AND matches, as well.
This will search for exactly "ask hn" and, optionally, "ipod"
This will return results that match “vc” and don’t include the word “startup”
Full-Text Search Optimization There are a couple ways to optimize full text searches.
If you can first narrow your search results by other criteria, you can create a compound index with a prefix of the other criteria and then the full-text fields:
This is referred to as partitioning the full-text index, as it breaks it into several smaller trees based on "date" (in the example above)
This makes full-text searches for a certain date much faster.
You can also use a postfix of other criteria to cover queries with the index.
For example, if we were only returning the "author" and "post" fields, we could create a compound index on both:
You cannot use a multikey field for any of the prefix or postfix index fields.
Creating a full-text index automatically enables the usePowerOf2Sizes option on the collection, which controls how space is allocated.
Do not disable this option, since it should improve writes speed.
Searching in Other Languages When a document is inserted (or the index is first created), MongoDB looks at the indexes fields and stems each word, reducing it to an essential unit.
However, different languages stem words in different ways, so you must specify what language the index or document is.
Thus, text-type indexes allow a "default_language" option to be specified, which defaults to "english" but can be set to a number of other languages (see the online documentation for an up-to-date list)
Then French would be used for stemming, unless otherwise specified.
You can, on a per-document basis, specify another stemming language by having a "language" field that describes the document’s language:
Geospatial Indexing MongoDB has a few types of geospatial indexes.
A point is given by a two-element array, representing [longitude, latitude]:
A polygon is specified the same way a line is (an array of points), but with a different "type":
The "loc" field can be called anything, but the field names within its subobject are specified by GeoJSON and cannot be changed.
For example, you can find documents that intersect the query’s location using the "$ge oIntersects" operator:
This would find all point-, line-, and polygon-containing documents that had a point in the East Village.
You can use "$within" to query for things that are completely contained in an area, for instance: “What restaurants are in the East Village?”
Unlike our first query, this would not return things that merely pass through the East Village (such as streets) or partially overlap it (such as a polygon describing Manhattan)
However, having an index on your geo field will speed up queries significantly, so it’s usually recommended.
Compound Geospatial Indexes As with other types of indexes, you can combine geospatial indexes with other fields to optimize more complex queries.
A possible query mentioned above was: “What restaurants are in the East Village?” Using only a geospatial index, we could narrow the field to everything in the East Village, but narrowing it down to only “restaurants” or “pizza” would require another field in the index:
We can have the “vanilla” index field either before or after the "2dsphere" field, depending on whether we’d like to filter by the vanilla field or the location first.
Choose whichever will filter out more results as the first index term.
Thus, "2d" indexes should not be used with spheres unless you don’t mind massive distortion around the poles.
Documents should use a two-element array for their 2d indexed field (which is not a GeoJSON document, as of this writing)
You can store an array of points, but it will be stored as exactly that: an array of points, not a line.
This is an important distinction for "$with in" queries, in particular.
If you store a street as an array of points, the document will match $within if one of those points is within the given shape.
However, the line created by those points might not be wholly contained in the shape.
If you are expecting larger or smaller bounds, you can specify what the minimum and maximum values will be as options to ensureIndex:
A default limit of 100 documents is applied if no limit is specified.
If you don’t need that many results, you should set a limit to conserve server resources.
Similarly, you can find all points within a circle with "$center", which takes an array with the center point and then a radius:
This example would locate all documents containing points within the given triangle.
The final point in the list will be “connected to” the first point to form the polygon.
Storing Files with GridFS GridFS is a mechanism for storing large binary files in MongoDB.
There are several reasons why you might consider using GridFS for file storage:
If you’re already using MongoDB, you might be able to use GridFS instead of a separate tool for file storage.
GridFS will leverage any existing replication or autosharding that you’ve set up for MongoDB, so getting failover and scale-out for file storage is easier.
GridFS can alleviate some of the issues that certain filesystems can exhibit when being used to store user uploads.
For example, GridFS does not have issues with storing large numbers of files in the same directory.
You can get great disk locality with GridFS, because MongoDB allocates data files in 2 GB chunks.
Slower performance: accessing files from MongoDB will not be as fast as going directly through the filesystem.
You can only modify documents by deleting them and resaving the whole thing.
MongoDB stores files as multiple documents so it cannot lock all of the chunks in a file at the same time.
GridFS is generally best when you have large files you’ll be accessing in a sequential fashion that won’t be changing much.
Getting Started with GridFS: mongofiles The easiest way to try out GridFS is by using the mongofiles utility.
As with any of the other command-line tools, run mongofiles --help to see the options available for mongofiles.
The following session shows how to use mongofiles to upload a file from the filesystem to GridFS, list all of the files in GridFS, and download a file that we’ve previously uploaded:
In the previous example, we perform three basic operations using mongofiles: put, list, and get.
The put operation takes a file in the filesystem and adds it to GridFS; list will list any files that have been added to GridFS; and get does the inverse of put: it takes a file from GridFS and writes it to the filesystem.
For example, with PyMongo (the Python driver for MongoDB) you can perform the same series of operations as we did with mongo files:
The API for working with GridFS from PyMongo is very similar to that of mongo files: we can easily perform the basic put, get, and list operations.
Almost all the MongoDB drivers follow this basic pattern for working with GridFS, while often exposing more advanced functionality as well.
For driver-specific information on GridFS, please check out the documentation for the specific driver you’re using.
Under the Hood GridFS is a lightweight specification for storing files that is built on top of normal MongoDB documents.
The MongoDB server actually does almost nothing to “special-case” the handling of GridFS requests; all the work is handled by the client-side drivers and tools.
The basic idea behind GridFS is that we can store large files by splitting them up into chunks and storing each chunk as a separate document.
Because MongoDB supports storing binary data in documents, we can keep storage overhead for chunks to a minimum.
In addition to storing each chunk of a file, we store a single document that groups the chunks together and contains metadata about the file.
The chunks for GridFS are stored in their own collection.
By default chunks will use the collection fs.chunks, but this can be overridden.
Within the chunks collection the structure of the individual documents is pretty simple:
Like any other MongoDB document, the chunk has its own unique "_id"
In addition, it has a couple of other keys: "files_id"
The "_id" of the file document that contains the metadata for this chunk.
The chunk’s position in the file, relative to the other chunks.
The metadata for each file is stored in a separate collection, which defaults to fs.files.
Each document in the files collection represents a single file in GridFS and can contain.
In addition to any userdefined keys, there are a couple of keys that are mandated by the GridFS specification: "_id"
A unique id for the file—this is what will be stored in each chunk as the value for the "files_id" key.
The total number of bytes making up the content of the file.
The size of each chunk comprising the file, in bytes.
The default is 256K, but this can be adjusted if needed.
A timestamp representing when this file was stored in GridFS.
An md5 checksum of this file’s contents, generated on the server side.
Of all of the required keys, perhaps the most interesting (or least self-explanatory) is "md5"
This means that users can check the value of the "md5" key to ensure that a file was uploaded correctly.
As mentioned above, you are not limited to the required fields in fs.files: feel free to keep any other file metadata in this collection as well.
You might want to keep information such as download count, mimetype, or user rating with a file’s metadata.
Once you understand the underlying GridFS specification, it becomes trivial to implement features that the driver you’re using might not provide helpers for.
For example, you can use the distinct command to get a list of unique filenames stored in GridFS:
This allows your application a great deal of flexibility in loading and collecting information about files.
Once you have data stored in MongoDB, you may want to do more than just retrieve it; you may want to analyze and crunch it in interesting ways.
The Aggregation Framework The aggregation framework lets you transform and combine documents in a collection.
Basically, you build a pipeline that processes a stream of documents through several building blocks: filtering, projecting, grouping, sorting, limiting, and skipping.
For example, if you had a collection of magazine articles, you might want find out who your most prolific authors were.
Assuming that each article is stored as a document in MongoDB, you could create a pipeline with several steps:
Group the authors by name, counting the number of occurrences.
Each of these steps maps to a aggregation framework operator:
To debug a pipeline that’s giving unexpected results, run the aggregation with just the first pipeline operator.
This can help you narrow down which operator is causing issues.
As of this writing, the aggregation framework cannot write to collections, so all results must be returned to the client.
Thus, aggregation results are limited to 16 MB of data (the maximum response size)
Pipeline Operations Each operator receives a stream of documents, does some type of transformation on these documents, and then passes on the results of the transformation.
If it is the last pipeline operator, these results are returned to the client.
Otherwise, the results are streamed to the next operator as input.
Operators can be combined in any order and repeated as many times as necessary.
Generally, good practice is to put "$match" expressions as early as possible in the pipeline.
This has two benefits: it allows you to filter out unneeded documents quickly.
The simplest operation "$project" can perform is simply selecting fields from your incoming documents.
To include or exclude a field, use the same syntax you would in the second argument of a query.
The following would return a result document containing one field, "author", for each document in the original collection:
Inclusion and exclusion rules in general work the same way that they do for “normal” queries.
For example, if you wanted to return the "_id" of each user as "userId", you could do:
The "$fieldname" syntax is used to refer to fieldname’s value in the aggregation framework.
You can use this technique to make multiple copies of a field for later use in a "$group", say.
MongoDB does not track field name history when fields are renamed.
Thus, try to utilize indexes before changing the names of fields.
You can also use expressions, which allow you to combine multiple literals and variables into a single value.
There are several expressions available with aggregation which you can combine and nest to any depth to create more complex expressions.
You generally use these expressions by specifying an array of numbers to operate on.
For example, the following expression would sum the "salary" and "bonus" fields:
If we have a more complex expression, it can be nested.
Suppose that we want to subtract 401k contributions from this total.
You can only use date operations on fields stored with the date type, not numeric types.
Each of these date types is basically the same: it takes a date expression and returns a number.
This would return the month that each employee was hired in:
This would calculate many years each employee had worked at the company:
There are a few basic string operations you can do as well.
This returns a substring of the first argument, starting at the startOffset-th byte and including the next numToReturn bytes (note that this is measured in bytes, not characters, so multibytes encodings will have to be careful of this)
Case-affecting operations are only guaranteed to work on characters from the Roman alphabet.
It extracts the first initial and concatenates it with several constant strings and the "last Name" field:
There are several operators that you can use for control statements.
Finally, there are two control statements: "$cond" : [booleanExpr, trueExpr, falseExpr]
If booleanExpr evaluates to true, trueExpr is returned; otherwise falseExpr is returned.
If expr is null this returns replacementExpr; otherwise it returns expr.
These operators let you include more complex logic in your aggregations by following different “code paths” depending on the shape of your data.
Pipelines are particular about getting properly formed input, so these operators can be invaluable in filling in default values.
The arithmetic operators will complain about nonnumeric values, date operators will complain about non-dates, string operators will complain about non-strings, and everything will complain about missing fields.
If your data set is inconsistent, you can use this conditionals to detect missing values and populate them.
If we had per-minute measurements and we wanted to find the average humidity per day, we would group by the "day" field.
If we had a collection of students and we wanted to organize student into groups based on grade, we could group by their "grade" field.
If we had a collection of users and we wanted to see how many users we had by city, we could group by both the "state" and "city" fields, creating one group per city/ state pair.
We wouldn’t want to just group by city, as there are many cities with identical names in different states.
These grouping operators allow you to compute results for each group.
This adds value for each document and returns the result.
Note that, although the example above used a literal (1), this can also take more complex values.
For example, if we had a collection of sales made in a variety of countries, this would find the total revenue by country:
For example, this would return the average revenue per country, plus the number of sales made:
There are four operators to get the “edges” of a data set: "$max" : expr.
This is only sensible to use when you know the order that the data is being processed in: that is, after a sort.
Thus, these operators work well when you do not have sorted data and are a bit wasteful when data is sorted.
For example, suppose that we had a set of student scores on a test.
We could find the outliers in each grade as follows:
For example, to get the same result as before we could do:
There are two operators available for array manipulation: "$addToSet" : expr.
Keeps an array of values seen so far and, if expr is not present in the array, adds it.
Each value appears at most once in the resulting array and ordering is not guaranteed.
While most operators can continuously process documents as they arrive, "$group" must collect all documents, split them into groups, then send them to the next operator in the pipeline.
This means that, with sharding, "$group" will first be run on each shard and then the individual shards’ groups will be sent to the mongos to do the final grouping and the remainder of the pipeline will be run on the mongos (not the shards)
For example, if we had a blog with comments, we could use unwind to turn each comment into its own “document”:
For example, it is impossible in the normal query language to return all comments by a certain user and only those comments, not the posts they commented on.
You might want to do a final projection to format the output more nicely, as all of the comments will still be in a "comments" subdocument.
If you are sorting a non-trivial number of documents, it is highly recommended that you do the sort at the beginning of the pipeline and have an index it can use.
Otherwise, the sort may be slow and take a lot of memory.
This example would sort employees by compensation, from highest to lowest, and then name from A-Z.
As with “normal” querying, it isn’t efficient for large skips, as it must find all of the matches that must be skipped and then discard them.
Once the pipeline isn’t using the data directly from the collection, indexes can no longer be used to help filter and sort.
The aggregation pipeline will attempt to reorder operations for you, if possible, to be able to use indexes.
MongoDB won’t allow a single aggregation to use more than a fraction of the system’s memory: if it calculates that an aggregation has used more than 20% of the memory, the aggregation will simply error out.
Allowing output to be piped to a collection (which would minimize the amount of memory required) is planned for the future.
If you can quickly whittle down the result set size with a selective "$match", you can use the pipeline for real-time aggregations.
As pipelines need to include more documents and become more complex, it is less likely that you’ll be able to get realtime results from them.
MapReduce MapReduce is a powerful and flexible tool for aggregating data.
It can solve some problems that are too complex to express using the aggregation framework’s query language.
MapReduce uses JavaScript as its “query language” so it can express arbitrarily complex logic.
However, this power comes at a price: MapReduce tends to be fairly slow and should not be used for real-time data analysis.
It splits up a problem, sends chunks of it to different machines, and lets each machine solve its part of the problem.
When all the machines are finished, they merge all the pieces of the solution back into a full solution.
It starts with the map step, which maps an operation onto every document in a collection.
That operation could be either “do nothing” or “emit these keys with X values.” There is then an intermediary stage called the shuffle step: keys are grouped and lists of emitted values are created for each key.
The reduce takes this list of values and reduces it to a single element.
This element is returned to the shuffle step until each key has a list containing a single value: the result.
We’ll go through a couple examples because MapReduce is an incredibly useful and powerful, but also somewhat complex, tool.
Example 1: Finding All Keys in a Collection Using MapReduce for this problem might be overkill, but it is a good way to get familiar with how MapReduce works.
MongoDB assumes that your schema is dynamic, so it does not keep track of the keys in each document.
The best way, in general, to find all the keys across all the documents in a collection is to use MapReduce.
In this example, we’ll also get a count of how many times each key appears in the collection.
This example doesn’t include keys for embedded documents, but it would be a simple addition to the map function to do so.
This is the name of the collection the MapReduce results were stored in.
This is a temporary collection that will be deleted when the connection that did the MapReduce is closed.
We will go over how to specify a nicer name and make the collection permanent in a later part of this chapter.
The number of times emit was called in the map function.
If we do a find on the resulting collection, we can see all the keys and their counts from our original collection:
Each of the key values becomes an "_id", and the final result of the reduce step(s) becomes the "value"
Example 2: Categorizing Web Pages Suppose we have a site where people can submit links to other pages, such as reddit.
First, we need a map function that emits tags with a value based on the popularity and recency of a document:
The final collection will end up with a full list of URLs for each tag and a score showing how popular that particular tag is.
MongoDB and MapReduce Both of the previous examples used only the mapreduce, map, and reduce keys.
These three keys are required, but there are many optional keys that can be passed to the MapReduce command: "finalize" : function.
Query to filter documents by before sending to the map function.
Maximum number of documents to send to the map function.
As with the previous group command, MapReduce can be passed a finalize function that will be run on the last reduce’s output before it is saved to a temporary collection.
Returning large result sets is less critical with MapReduce than group because the whole result doesn’t have to fit in 4 MB.
However, the information will be passed over the wire eventually, so finalize is a good chance to take averages, chomp arrays, and remove extra information in general.
By default, Mongo creates a temporary collection while it is processing the MapReduce with a name that you are unlikely to choose for a collection: a dot-separated string containing mr, the name of the collection you’re MapReducing, a timestamp, and the job’s ID with the database.
MongoDB will automatically destroy this collection when the connection that did the MapReduce is closed.
You can also drop it manually when you’re done with it.
If you want to persist this collection even after disconnecting, you can specify keeptemp : true as an option.
If you’ll be using the temporary collection regularly, you may want to give it a better name.
You can specify a more human-readable name with the out option, which takes a string.
If you specify out, you need not specify keeptemp : true, since it is implied.
Even if you specify a “pretty” name for the collection, MongoDB will use the autogenerated collection name for intermediate steps of the MapReduce.
When it has finished, it will automatically and atomically rename the collection from the autogenerated name.
This means that if you run MapReduce multiple times with the same target collection, you will never be using an incomplete collection for operations.
The output collection created by MapReduce is a normal collection, which means that there is no problem with doing a MapReduce on it or a MapReduce on the results from that MapReduce, ad infinitum!
Sometimes you need to run MapReduce on only part of a collection.
You can add a query to filter the documents before they are passed to the map function.
Every document passed to the map function needs to be deserialized from BSON into a JavaScript object, which is a fairly expensive operation.
If you know that you will need to run MapReduce only on a subset of the documents in the collection, adding a filter can greatly speed up the command.
The filter is specified by the "query", "limit", and "sort" keys.
The "query" key takes a query document as a value.
Any documents that would ordinarily be returned by that query will be passed to the map function.
For example, if we have an application tracking analytics and want a summary for the last week, we can use MapReduce on only the most recent week’s documents with the following command:
The sort option is mostly useful in conjunction with limit.
If, in the previous example, we wanted an analysis of the last 10,000 page views (instead of the last week), we could use limit and sort:
MapReduce can take a code type for the map, reduce, and finalize functions, and, in most languages, you can specify a scope to be passed with code.
It has its own scope key, "scope", and you must use that if there are client-side values you want to use in your MapReduce.
You can set them using a plain document of the form variable_name : value, and they will be available in your map, reduce, and finalize functions.
For instance, in the example in the previous section, we calculated the recency of a page using 1/(new Date() - this.date)
We could, instead, pass in the current date as part of the scope with the following code:
Then, in the map function, we could say 1/(now - this.date)
If you would like to see the progress of your MapReduce as it runs, you can specify "verbose" : true.
You can also use print to see what’s happening in the map, reduce, and finalize functions.
Aggregation Commands There are several commands that MongoDB provides for basic aggregation tasks over a collection.
These commands were added before the aggregation framework and have been superceded by it, for the most part.
However, complex groups may still require JavaScript and counts and distincts can be simpler to run as non-framework commands.
Counting the total number of documents in a collection is fast regardless of collection size.
Counts can use indexes, but indexes do not contain enough metadata to make counting any more efficient than actually doing a query for the criteria.
A common question at this point is if there’s a way to get all of the distinct keys in a collection.
There is no built-in way of doing this, although you can write something to do it yourself using MapReduce (described in “MapReduce” on page 140)
You choose a key to group by, and MongoDB divides the collection into separate groups for each value of the chosen key.
For each group, you can create a result document by aggregating the documents that are members of that group.
If you are familiar with SQL, group is similar to SQL’s GROUP BY.
Suppose we have a site that keeps track of stock prices.
Now, as part of a reporting application, we want to find the closing price for the past 30 days.
You should never store money amounts as floating-point numbers because of inexactness concerns, but for simplicity we’ll do it in this example.
We want our results to be a list of the latest time and price for each day, something like this:
We can accomplish this by splitting the collection into sets of documents grouped by day then finding the document with the latest timestamp for each day and adding it to the result set.
Let’s break this command down into its component keys: "ns" : "stocks"
This determines which collection we’ll be running the group on.
This specifies the key on which to group the documents in the collection.
All the documents with a "day" key of a given value will be grouped together.
The first time the reduce function is called for a given group, it will be passed the initialization document.
This same accumulator will be used for each member of a given group, so any changes made to it can be persisted.
This will be called once for each document in the collection.
It is passed the current document and an accumulator document: the result so far for that group.
In this example, we want the reduce function to compare the current document’s time with the accumulator’s time.
If the current document has a later time, we’ll set the accumulator’s day and price to be the current document’s values.
In the initial statement of the problem, we said that we wanted only the last 30 days worth of prices.
Our current solution is iterating over the entire collection, however.
This is why you can include a "condition" that documents must satisfy in order to be processed by the group command at all:
Some documentation refers to a "cond" or "q" key, both of which are identical to the "condition" key (just less descriptive)
We explicitly set the "price" for each group, and the "time" was set by the initializer and then updated.
The "day" is included because the key being grouped by is included by default in each "retval" embedded document.
If you don’t want to return this key, you can use a finalizer to change the final accumulator document into anything, even a nondocument (e.g., a number or string)
Finalizers can be used to minimize the amount of data that needs to be transferred from the database to the user, which is important because the group command’s output needs to fit in a single database response.
To demonstrate this, we’ll take the example of a blog where each post has tags.
We want to find the most popular tag for each day.
We can group by day (again) and keep a count for each tag.
Then we could find the largest value in the "tags" document on the client side.
However, sending the entire tags document for every day is a lot of extra overhead to send to the client: an entire set of key/value pairs for each day, when all we want is a single string.
We can use a "finalize" function to trim out all of the cruft from our results:
Now, we’re only getting the information we want; the server will send back something like this:
Sometimes you may have more complicated criteria that you want to group by, not just a single key.
Suppose you are using group to count how many blog posts are in each category.
Post authors were inconsistent, though, and categorized posts with haphazard capitalization.
So, if you group by category name, you’ll end up with separate groups for “MongoDB” and “mongodb.” To make sure any variation of capitalization is treated as the same key, you can define a function to determine documents’ grouping key.
To define a grouping function, you must use a $keyf key (instead of "key")
Using "$keyf" makes the group command look something like this:
This chapter covers designing applications to work effectively with MongoDB.
Trade-offs when deciding whether to embed data or to reference it.
Normalization versus Denormalization There are many ways of representing data and one of the most important issues is how much you should normalize your data.
Normalization is dividing up data into multiple collections with references between collections.
Each piece of data lives in one collection although multiple documents may reference it.
Thus, to change the data, only one document must be updated.
However, MongoDB has no joining facilities, so gathering documents from multiple collections will require multiple queries.
Denormalization is the opposite of normalization: embedding all of the data in a single document.
Instead of documents containing references to one definitive copy of the data, many documents may have copies of the data.
This means that multiple documents need to be updated if the information changes but that all related data can be fetched with a single query.
Deciding when to normalize and when to denormalize can be difficult: typically, normalizing makes writes faster and denormalizing makes reads faster.
Thus, you need to find what trade-offs make sense for your application.
Examples of Data Representations Suppose we are storing information about students and the classes that they are taking.
One way to represent this would be to have a students collection (each student is one document) and a classes collection (each class is one document)
Then we could have a third collection (studentClasses) that contains references to the student and classes he is taking:
If you are familiar with relational databases, you may have seen this type of join table before, although typically you’d have one student and one class per document (instead of a list of class "_id"s)
It’s a bit more MongoDB-ish to put the classes in an array, but you usually wouldn’t want to store the data this way because it requires a lot of querying to get to the actual information.
Suppose we wanted to find the classes a student was taking.
We’d query for the student in the students collection, query studentClasses for the course "_id"s, and then query the classes collection for the class information.
Thus, finding this information would take three trips to the server.
This is generally not the way you want to structure data in MongoDB, unless the classes and students are changing constantly and reading the data does not need to be done quickly.
We can remove one of the dereferencing queries by embedding class references in the student’s document:
The "classes" field keeps an array of "_id"s of classes that John Doe is taking.
When we want to find out information about those classes, we can query the classes collection.
This is fairly popular way to structure data that does not need to be instantly accessible and changes, but not constantly.
If we need to optimize reads further, we can get all of the information in a single query by fully denormalizing the data and storing each class as an embedded document in the "classes" field:
The upside of this is that it only takes one query to get the information.
The downsides are that it takes up more space and is more difficult to keep in sync.
For example, if it turns out that physics was supposed to be four credits (not three) every student in the physics class would need to have her document updated (instead of just updating a central “Physics” document)
Finally, you can use a hybrid of embedding and referencing: create an array of subdocuments with the frequently used information, but with a reference to the actual document for more information:
This approach is also a nice option because the amount of information embedded can change over time as your requirements changes: if you want to include more or less information on a page, you could embed more or less of it in the document.
Another important consideration is how often this information will change versus how often it’s read.
If it will be updated regularly, then normalizing it is a good idea.
However, if it changes infrequently, then there is little benefit to optimize the update process at the expense of every read your application performs.
For example, a textbook normalization use case is to store a user and his address in separate collections.
However, people almost never change their address, so you generally shouldn’t penalize every read on the off chance that someone’s moved.
Your application should embed the address in the user document.
If you decide to use embedded documents and you need to update them, you should set up a cron job to ensure that any updates you do are successfully propagated to every document.
For example, you might attempt to do a multiupdate but the server crashes before all of the documents have been updated.
You need a way to detect this and retry the update.
To some extent, the more information you are generating the less of it you should embed.
If the embedded fields or number of embedded fields is supposed to grow without bound then they should generally be referenced, not embedded.
Things like comment trees or activity lists should be stored as their own documents, not embedded.
Finally, fields should be included that are integral to the data in the document.
If a field is almost always excluded from your results when you query for this document, it’s a good sign that it may belong in another collection.
When eventual consistency is acceptable When immediate consistency is necessary.
Documents that grow by a small amount Documents that grow a large amount.
Data that you’ll often need to perform a second query to fetch Data that you’ll often exclude from the results.
Here are some example fields we might have and whether or not they should be embedded: Account preferences.
They are only relevant to this user document, and will probably be exposed with other user information in this document.
Recent activity This one depends on how much recent activity grows and changes.
If it is a fixedsize field (last 10 things), it might be useful to embed.
Friends Generally this should not be embedded, or at least not fully.
Cardinality Cardinality is how many references a collection has to another collection.
Each post has a title, so that’s a one-to-one relationship.
Each author has many posts, so that’s a one-to-many relationship.
And posts have many tags and tags refer to many posts, so that’s a many-to-many relationship.
When using MongoDB, it can be conceptually useful to split “many” into subcategories: “many” and “few.” For example, you might have a one-to-few cardinality between authors and posts: each author only writes a few posts.
You might have many-to-few relation between blog posts and tags: your probably have many more blog posts than you have tags.
However, you’d have a one-to-many relationship between blog posts and comments: each post has many comments.
When you’ve determined few versus many relations, it can help you decide what to embed versus what to reference.
Generally, “few” relationships will work better with embedding, and “many” relationships will work better as references.
Friends, Followers, and Other Inconveniences Keep your friends close and your enemies embedded.
Many social applications need to link people, content, followers, friends, and so on.
Figuring out how to balance embedding and referencing this highly connected information can be tricky.
Thus, there are two basic operations that need to be efficient: how to store subscribers and how to notify all interested parties of an event.
The first option is that you can put the producer in the subscriber’s document, which looks something like this:
Whenever this user does something, all the users we need to notify are right there.
The downside is that now you need to query the whole users collection to find everyone a user follows (the opposite limitation as above)
Either of these options comes with an additional downside: they make your user document larger and more volatile.
The "following" (or "followers") field often won’t even need to be returned: how often do you want to list every follower? If users are frequently followed or unfollowed, this can result in a lot of fragmentation, as well.
Thus, the final option neutralizes these downsides by normalizing even further and storing subscriptions in another collection.
Normalizing this far is often overkill, but it can be useful for an extremely volatile field that often isn’t returned with the rest of the document.
Keep a collection that matches publishers to subscribers, with documents that look something like this:
This keeps your user documents svelte but takes an extra query to get the followers.
As "followers" arrays will generally change size a lot, this allows you to enable the usePo werOf2Sizes on this collection while keeping the users collection as small as possible.
If you put this followers collection in another database, you can also compact it without affecting the users collection too much.
Regardless of which strategy you use, embedding only works with a limited number of subdocuments or references.
If you have celebrity users, they may overflow any document that you’re storing followers in.
The typical way of compensating this is to have a “continuation” document, if necessary.
Then add application logic to support fetching the documents in the “to be continued” ("tbc") array.
Optimizations for Data Manipulation To optimize your application, you must first know what its bottleneck is by evaluating its read and write performance.
Optimizing reads generally involves having the correct indexes and returning as much of the information as possible in a single document.
Optimizing writes usually involves minimizing the number of indexes you have and making updates as efficient as possible.
There is often a trade-off between schemas that are optimized for writing quickly and those that are optimized for reading quickly, so you may have to decide which is a more important for your application.
Factor in not only the importance of reads versus writes, but also their proportions: if writes are more important but you’re doing a thousand reads to every write, you may still want to optimize reads first.
Optimizing for Document Growth If you’re going to need to update data, determine whether or not your documents are going to grow and by how much.
If it is by a predictable amount, manually padding your documents will prevent moves, making writes faster.
Check your padding factor: if it is about 1.2 or greater, consider using manual padding.
When you manually pad a document, you create the document with a large field that will later be removed.
This preallocates the space that the document will eventually need.
For example, suppose you had a collection of restaurant reviews and your documents looked like this:
The "tags" field will grow as users add tags, so the application will often have to perform an update like this:
If "tags" generally doesn’t grow to more than 100 bytes, you could manually pad the document to prevent any unwanted moves.
If you leave the document without padding, moves will definitely occur as "tags" grows.
To pad, add a final field to the document with whatever field name you’d like:
You can either do this on insert or, if the document is created with an upsert, use "$setOnInsert" to create the field when the document is first inserted.
The "$unset" will remove the "garbage" field if it exists and be a no-op if it does not.
If your document has one field that grows, try to keep is as the last field in the document (but before "garbage")
It is slightly more efficient for MongoDB not to have to rewrite fields after "tags" if it grows.
Removing Old Data Some data is only important for a brief time: after a few weeks or months it is just wasting storage space.
There are three popular options for removing old data: capped collections, TTL collections, and dropping collections per time period.
The easiest option is to use a capped collection: set it to a large size and let old data “fall off ” the end.
However, capped collections pose certain limitations on the operations you can do and are vulnerable to spikes in traffic, temporarily lowering the length of time that they can hold.
The second option is TTL collections: this gives you a finer-grain control over when documents are removed.
However, it may not be fast enough for very high-write-volume collections: it removes documents by traversing the TTL index the same way a userrequested remove would.
If TTL collections can keep up, though, they are probably the easiest solution.
See “Time-To-Live Indexes” on page 114 for more information about TTL indexes.
The final option is to use multiple collections: for example, one collection per month.
Every time the month changes, your application starts using this month’s (empty) collection and searching for data in both the current and previous months’ collections.
Once a collection is older than, say, six months, you can drop it.
This can keep up with nearly any volume of traffic, but it is more complex to build an application around, since it has to use dynamic collection (or database) names and possibly query multiple databases.
Planning Out Databases and Collections Once you have sketched out what your documents look like, you must decide what collections or databases to put them in.
This is often a fairly intuitive process, but there are some guidelines to keep in mind.
In general, documents with a similar schema should be kept in the same collection.
MongoDB generally disallows combining data from multiple collections, so if there are documents that need to be queried or aggregated together, those are good candidates for putting in one big collection.
For example, you might have documents that are fairly different “shapes,” but if you’re going to be aggregating them, they all need to live in the same collection.
For databases, the big issues to consider are locking (you get a read/write lock per database) and storage.
Each database resides in its own files and often its own directory on disk, which means that you could mount different databases to different volumes.
Thus, you may want all items within a database to be of similar “quality,” similar access pattern, or similar traffic levels.
For example, suppose we have an application with several components: a logging component that creates a huge amount of not-very-valuable data, a user collection, and a couple of collections for user-generated data.
The user collections are high-value: it is important that user data is safe.
There is also a high-traffic collection for social activities, which is of lower importance but not quite as unimportant as the logs.
This collection is mainly used for user notifications, so it is almost an append-only collection.
Splitting these up by importance, we might end up with three databases: logs, activi ties, and users.
The nice thing about this strategy is that you may find that your highestvalue data is also your smallest (for instance, users probably don’t generate as much data as your logging does)
You might not be able to afford an SSD for your entire data set, but you might be able to get one for your users.
Be aware that there are some limitations when using multiple databases: MongoDB generally does not allow you to move data directly from one database to another.
For example, you cannot store the results of a MapReduce in a different database than you ran the MapReduce on and you cannot move a collection from one database to another with the renameCollection command (e.g., you can rename foo.bar as foo.baz, but not foo2.baz)
Managing Consistency You must figure out how consistent your application’s reads need to be.
MongoDB supports a huge variety in consistency levels, from always reading your own writes to reading data of unknown oldness.
If you’re reporting on the last year of activity, you might only need data that’s correct to the last couple of days.
Conversely, if you’re doing realtime trading, you might need to immediately read the latest writes.
To understand how to achieve these varying levels of consistency, it is important to understand what MongoDB is doing under the hood.
The server keeps a queue of requests for each connection.
When the client sends a request, it will be placed at the end of its connection’s queue.
Any subsequent requests on the connection will occur after the enqueued operation is processed.
Thus, a single connection has a consistent view of the database and can always read its own writes.
Note that this is a per-connection queue: if we open two shells, we will have two connections to the database.
If we perform an insert in one shell, a subsequent query in the other shell might not return the inserted document.
However, within a single shell, if we query for the document after inserting, the document will be returned.
This behavior can be difficult to duplicate by hand, but on a busy server interleaved inserts and queries are likely to occur.
Often developers run into this when they insert data in one thread and then check that it was successfully inserted in another.
For a moment or two, it looks like the data was not inserted, and then it suddenly appears.
This behavior is especially worth keeping in mind when using the Ruby, Python, and Java drivers, because all three use connection pooling.
For efficiency, these drivers open multiple connections (a pool) to the server and distribute requests across them.
They all, however, have mechanisms to guarantee that a series of requests is processed by a single connection.
There is detailed documentation on connection pooling in various languages on the MongoDB wiki.
When you send reads to a replica set secondary (see Chapter 11), this becomes an even larger issue.
Secondaries may lag behind the primary, leading to reading data from seconds, minutes, or even hours ago.
There are several ways of dealing with this, the easiest being to simply send all reads to the primary if you care about staleness.
You could also set up an automatic script to detect lag on a secondary and put it into maintenance mode if it lags too far behind.
If you have a small set, it might be worth using "w" : setSize as a write concern and sending subsequent reads to the primary if getLastError does not return successfully.
Migrating Schemas As your application grows and your needs change, your schema may have to grow and change as well.
There are a couple of ways of accomplishing this, and regardless of the method you chose, you should carefully document each schema that your application has used.
The simplest method is to simply have your schema evolve as your application requires, making sure that your application supports all old versions of the schema (e.g., accepting the existence or non-existence of fields or dealing with multiple possible field types gracefully)
This technique can become messy, particularly if you have conflicting versions.
For instance, one version might require a "mobile" field and one version might require not having a "mobile" field but does require another field, and yet another version thinks that the "mobile" field is optional.
Keeping track of these requirements can gradually turn code into spaghetti.
To handle changing requirements in a slightly more structured way you can include a "version" field (or just "v") in each document and use that to determine what your application will accept for document structure.
This enforces your schema more rigorously: a document has to be valid for some version of the schema, if not the current one.
The final option is to migrate all of your data when the schema changes.
Generally this is not a good idea: MongoDB allows you to have a dynamic schema in order to avoid migrates because they put a lot of pressure on your system.
However, if you do decide to change every document, you will need to ensure that all documents were successfully updated.
If MongoDB crashes in the middle of a migrate, you could end up with some updated and some non-updated documents.
When Not to Use MongoDB While MongoDB is a general-purpose database that works well for most applications, it isn’t good at everything.
Here are some tasks that MongoDB is not designed to do:
MongoDB does not support transactions, so systems that require transactions should use another data store.
There are a couple of ways to hack in simple transaction-like semantics, particularly on a single document, but there is no database enforcement.
Thus, you can make all of your clients agree to obey whatever semantics you come up with (e.g., “Check the "locks" field before doing any operation”) but there is nothing stopping an ignorant or malicious client from messing things up.
Joining many different types of data across many different dimensions is something relational databases are fantastic at.
MongoDB isn’t supposed to do this well and most likely never will.
Finally, one of the big (if hopefully temporary) reasons to use a relational database over MongoDB is if you’re using tools that don’t support MongoDB.
From SQLAlchemy to Wordpress, there are thousands of tools that just weren’t built to support MongoDB.
The pool of tools that support MongoDB is growing but is hardly the size of relational databases’ ecosystem, yet.
Introduction to Replication Since the first chapter, we’ve been using a standalone server, a single mongod server.
It’s an easy way to get started but a dangerous way to run in production: what if your server crashes or becomes unavailable? Your database will at least be unavailable for a little while.
If there are problems with the hardware, you may have to move your data to another machine.
In the worst case, disk or network issues could leave you with corrupt or inaccessible data.
Replication is a way of keeping identical copies of your data on multiple servers and is recommended for all production deployments.
Replication keeps your application running and your data safe, even if something happens to one or more of your servers.
With MongoDB, you set up replication by creating a replica set.
A replica set is a group of servers with one primary, the server taking client requests, and multiple secondaries, servers that keep copies of the primary’s data.
If the primary crashes, the secondaries can elect a new primary from amongst themselves.
If you are using replication and a server goes down, you can still access your data from the other servers in the set.
If the data on a server is damaged or inaccessible, you can make a new copy of the data from one the other members of the set.
This chapter introduces replica sets and covers how to set up replication on your system.
A One-Minute Test Setup This section will get you started quickly by setting up a three-member replica set on your local machine.
This setup is obviously not suitable for production, but it’s a nice way to familiarize yourself with replication and play around with configuration.
This quick-start method stores data in /data/db, so make sure that directory exists and is writable by your user before running this code.
Start up a mongo shell with the --nodb option, which allows you to start a shell that is not connected to any mongod:
This tells the shell to create a new replica set with three servers: one primary and two secondaries.
However, it doesn’t actually start the mongod servers until you run the following two commands:
They will all be dumping their logs into the current shell, which is very noisy, so put this shell aside and open up a new one.
You’ll learn how to choose your own identifier later; testRepl Set is the default name ReplSetTest uses.
Use your connection to the primary to run the isMaster command.
There are a bunch of fields in the output from isMaster, but the important ones indicate that you can see that this node is primary (the "ismaster" : true field) and that there is a list of hosts in the set.
Look at the "pri mary" field to see which node is primary and then repeat the connection steps above for that host/port.
Now that you’re connected to the primary, let’s try doing some writes and see what happens.
Now check one of the secondaries and verify that they have a copy of all of these documents.
Secondaries may fall behind the primary (or lag) and not have the most current writes, so secondaries will refuse read requests by default to prevent applications from accidentally reading stale data.
Thus, if you attempt to query a secondary, you’ll get an error that it’s not primary:
This is to protect your application from accidentally connecting to a secondary and reading stale data.
To allow queries on the secondary, we set an “I’m okay with reading from secondaries” flag, like so:
Note that slaveOk is set on the connection (conn2), not the database (secondaryDB)
You can see that all of our documents are there.
You can see that the secondary does not accept the write.
The secondary will only perform writes that it gets through replication, not from clients.
There is one other interesting feature that you should try out: automatic failover.
If the primary goes down, one of the secondaries will automatically be elected primary.
Your primary may be the other server; whichever secondary noticed that the primary was down first will be elected.
Thus, it does not use the replica set terminology consistently: it still calls the primary a “master.” You can generally think of “master” as equivalent to “primary” and “slave” as equivalent to “secondary.”
When you’re done working with the set, shut down the servers from your first shell.
This shell will be full of log output from the members of the set, so hit Enter a few times to get back to a prompt.
Congratulations! You just set up, used, and tore down replication.
Now that you understand the basics, the rest of this chapter focuses on configuring a replica set under more realistic circumstances.
Remember that you can always go back to ReplSetTest if you want to quickly try out a configuration or option.
Configuring a Replica Set For actual deployments, you’ll need to set up replication across multiple machines.
This section takes you through setting up a real replica set that could be used by your application.
Let’s say that you already have a standalone mongod on server-1:27017 with some data on it.
If you do not have any pre-existing data, this will work the same way, just with an empty data directory.
The first thing you need to do is choose a name for your set.
Any string whatsoever will do, so long as it’s UTF-8
Once you have a name for your replica set, restart server-1 with the --replSet name option.
Now start up two more mongod servers with the replSet option and the same identifier (spock): these will be the other members of the set:
Each of the other members should have an empty data directory, even if the first member had data.
They will automatically clone the first member’s data to their machines once they have been added to the set.
For each member, add the replSet option to its mongod.conf file so that it will be used on startup from now on.
Once you’ve started the mongods, you should have three mongods running on three separate servers.
However, each mongod does not yet know that the others exist.
To tell them about one another, you have to create a configuration that lists each of the members and send this configuration to server-1
It will take care of propagating it to the other members.
The config’s "_id" is the name of the set that you passed in on the command line (in this example, "spock")
The next part of the document is an array of members of the set.
Each of these needs two fields: a unique "_id" that is an integer and a hostname (replace the hostnames with whatever your servers are called)
This config object is your replica set configuration, so now you have to send it to a member of the set.
To do so, connect to the server with data on it (server-1:27017) and initiate the set with this configuration:
Once they have all loaded the configuration, they will elect a primary and start handling reads and writes.
Unfortunately, you cannot convert a standalone server to a replica set without some downtime for restarting it and initializing the set.
Thus, even if you only have one server to start out with, you may want to configure it as a one-member replica set.
That way, if you want to add more members later, you can do so without downtime.
If you are starting a brand-new set, you can send the configuration to any member in the set.
If you are starting with data on one of the members, you must send the configuration to the member with data.
You cannot initiate a set with data on more than one member.
You must use the mongo shell to configure replica sets.
There is no way to do file-based replica set configuration.
It is good to have a passing familiarity with both the helpers and the underlying commands, as it may sometimes be easier to use the command form instead of the helper.
Networking Considerations Every member of a set must be able to make connections to every other member of the set (including itself)
If you get errors about members not being able to reach other members that you know are running, you may have to change your network configuration to allow connections between them.
Also, replica sets configurations shouldn’t use localhost as a hostname.
There isn’t much point to running a replica set on one machine and localhost won’t resolve correctly from a foreign machine.
MongoDB allows all-localhost replica sets for testing locally but will protest if you try to mix localhost and non-localhost servers in a config.
Changing Your Replica Set Configuration Replica set configurations can be changed at any time: members can be added, removed, or modified.
There are shell helpers for some common operations; for example, to add a new member to the set, you can use rs.add:
Note that when you remove a member (or do almost any configuration change other than adding a member), you will get a big, ugly error about not being able to connect to the database in the shell.
This is okay; it actually means the reconfiguration succeeded! When you reconfigure a set, the primary closes all connections as the last step in the reconfiguration process.
Thus, the shell will briefly be disconnected but will automatically reconnect on your next operation.
The reason that the primary closes all connections is that it briefly steps down whenever you reconfigure the set.
It should step up again immediately, but be aware that your set will not have a primary for a moment or two after reconfiguring.
You can check that a reconfiguration succeeded by run rs.config() in the shell.
Each time you change the configuration, the "version" field will increase.
You can also modify existing members, not just add and remove them.
To make modifications, create the configuration document that you want in the shell and call rs.re config.
For example, suppose we have a configuration such as the one shown here:
Someone accidentally added member 1 by IP, instead of its hostname.
To change that, first we load the current configuration in the shell and then we change the relevant fields:
Now that the config document is correct, we need to send it to the database using the rs.reconfig helper:
You can use it to make any legal configuration change you need: simply create the config document that represents your desired configuration and pass it to rs.reconfig.
How to Design a Set To plan out your set, there are certain replica set concepts that you must be familiar with.
The next chapter goes into more detail about these, but the most important is that replica sets are all about majorities: you need a majority of members to elect a primary, a primary can only stay primary so long as it can reach a majority, and a write is safe when it’s been replicated to a majority.
This majority is defined to be “more than half of all members in the set,” as shown in Table 9-1
What is a majority? Number of members in the set Majority of the set.
Note that it doesn’t matter how many members are down or unavailable, as majority is based on the set’s configuration.
For example, suppose that we have a five-member set and three members go down, as shown in Figure 9-1
These two members cannot reach a majority of the set (at least three members), so they cannot elect a primary.
If one of them were primary, it would step down as soon as it noticed that it could not reach a.
After a few seconds, your set would consist of two secondaries and three unreachable members.
With a minority of the set available, all members will be secondaries.
Many users find this frustrating: why can’t the two remaining members elect a primary? The problem is that it’s possible that the other three members didn’t go down, and that it was the network that went down, as shown in Figure 9-2
In this case, the three members on the left will elect a primary, since they can reach a majority of the set (three members out of five)
In the case of a network partition, we do not want both sides of the partition to elect a primary: otherwise the set would have two primaries.
Then both primaries would be writing to the data and the data sets would diverge.
Requiring a majority to elect or stay primary is a neat way of avoiding ending up with more than one primary.
For the members, a network partition looks identical to servers on the other side of the partition going down.
It is important to configure your set in such a way that you’ll usually be able to have one primary.
One common setup that usually isn’t what you want is a two member set: one primary and one secondary.
Suppose one member becomes unavailable: the other member cannot see it, as shown in Figure 9-3
In this situation, neither side of the network partition has a majority so you’ll end up with two secondaries.
For this reason, this type of configuration is not generally recommended.
With an even number of members, neither side of a partition has a majority.
A majority of the set in one data center, as in Figure 9-2
This is a good design if you have a primary data center where you always want your replica set’s primary to be located.
So long as your primary data center is healthy, you will have a primary.
However, if that data center becomes unavailable, your secondary data center will not be able to elect a new primary.
An equal number of servers in each data center, plus a tie-breaking server in a third location.
This is a good design if your data centers are “equal” in preference, since generally servers from either data center will be able to see a majority of the set.
More complex requirements might require different configurations, but you should keep in mind how your set will acquire a majority under adverse conditions.
All of these complexities would disappear if MongoDB supported having more than one primary.
With two primaries, you would have to handle conflicting writes (for example, someone updates a document on one primary and someone deletes it on another primary)
There are two popular ways of handling conflicts in systems that support multiple writers: manual reconciliation or having the system arbitrarily pick a “winner.” Neither of these options is a very easy model for developers to code against, seeing that you can’t be sure that the data you’ve written won’t change out from under you.
Thus, MongoDB chose to only support having a single primary.
This makes development easier but can result in periods when the replica set is read-only.
How Elections Work When a secondary cannot reach a primary, it will contact all the other members and request that it be elected primary.
These other members do several sanity checks: Can they reach a primary that the member seeking election cannot? Is the member seeking.
If a member seeking election receives “ayes” from a majority of the set, it becomes primary.
If even one server vetoes the election, the election is canceled.
A member vetoes an election when it knows any reason that the member seeking election shouldn’t become primary.
You may see a very large negative number in the logs, since a veto is registered as 10,000 votes.
Often you’ll see messages about election results being 9,999 or similar if one member voted for a member and another member vetoed the election:
If two members vetoed and one voted for, the election results would be 19,999, and so on.
The member seeking election (the candidate) must be up to date with replication, as far as the members it can reach know.
All replicated operations are strictly ordered by ascending timestamp, so the candidate must have operations later than or equal to any member it can reach.
The candidate will continue syncing and once it has synced operation 124, it will call for an election again (if no one else has become primary in that time)
This time around, assuming nothing else is wrong with candidate, the member that previously vetoed the election will vote for the candidate.
Assuming that the candidate receives “ayes” from a majority of voters, it will transition into primary state.
A common point of confusion is that members always seek election for themselves.
For simplicity’s sake, neighbors cannot “nominate” another server to be primary, they can only vote for it if it is seeking election.
Member Configuration Options The replica sets we have set up so far have been fairly uniform in that every member has the same configuration as every other member.
However, there are many situations when you don’t want members to be identical: you might want one member to preferentially be primary or make a member invisible to clients so that no read requests can be routed to it.
These and many other configuration options can be specified in the member subdocuments of the replica set configuration.
This section outlines the member options that you can set.
Creating Election Arbiters The example above shows the disadvantages two-member sets have for majority requirements.
However, many people with small deployments do not want to keep three copies of their data, feeling that two is enough and keeping a third copy is not worth the administrative, operational, and financial costs.
For these deployments, MongoDB supports a special type of member called an arbiter, whose only purpose is to participate in elections.
Arbiters hold no data and aren’t used by clients: they just provide a majority for two-member sets.
As arbiters don’t have any of the traditional responsibilities of a mongod server, you can run an arbiter as a lightweight process on a wimpier server than you’d generally use for MongoDB.
You start up an arbiter in the same way that you start a normal mongod, using the -replSet name option and an empty data directory.
You can add it to the set using the rs.addArb() helper:
An arbiter, once added to the set, is an arbiter forever: you cannot reconfigure an arbiter to become a nonarbiter, or vice versa.
One other thing that arbiters are good for is breaking ties in larger clusters.
If you have an even number of nodes, you may have half the nodes vote for one member and half for another.
Note that, in both of the use cases above, you need at most one arbiter.
You do not need an arbiter if you have an odd number of nodes.
A common misconception seems to be that you should add extra arbiters “just in case.” However, it doesn’t help elections go any faster or provide any data safety to add extra arbiters.
If you add an arbiter, you’ll have a four member set, so three members will be required to choose a primary.
If you have an even number of nodes because you added an arbiter, your arbiters can cause ties, not prevent them.
If you have a choice between a data node and an arbiter, choose a data node.
Using an arbiter instead of a data node in a small set can make some operational tasks more difficult.
For example, suppose you are running a replica set with two “normal” members and one arbiter, and one of the data-holding members goes down.
If that member is well and truly dead (the data is unrecoverable), you will have to get a copy of the data from the current primary to the new server you’ll be using as a secondary.
Copying data can put a lot of stress on a server and, thus, slow down your application.
Generally, copying a few gigabytes to a new server is trivial but more than a hundred starts becoming impractical.
Conversely, if you have three data-holding members, there’s more “breathing room” if a server completely dies.
You can use the remaining secondary to bootstrap a new server instead of depending on your primary.
Thus, if possible, use an odd number of “normal” members instead of an arbiter.
Priority Priority is how strongly this member “wants” to become primary.
The highest-priority member will always be elected primary (so long as they can reach a majority of the set and have the most up-to-date data)
For example, suppose you add a member with priority of 1.5 to the set, like so:
If server-4 was, for some reason, unable to catch up, the current primary would stay primary.
Setting priorities will never cause your set to go primary-less.
It will also never cause a member who is behind to become primary (until it has caught up)
One interesting wrinkle with priority is that reconfigurations must always be sent to a member that could be primary with the new configuration.
Hidden Clients do not route requests to hidden members and hidden members are not preferred as replication sources (although they will be used if more desirable sources are not available)
Thus, many people will hide less powerful or backup servers.
To hide server-3, add the hidden: true field to its configuration.
A member must have a priority of 0 to be hidden (you can’t have a hidden primary):
When clients connect to a replica set, they call isMaster() to determine the members of the set.
Thus, hidden members will never be used for read requests.
To unhide a member, change the hidden option to false or remove the option entirely.
Slave Delay It’s always possible for your data to be nuked by human error: someone might accidentally drop your main database or a newly deployed version of your application might have a bug that replaces all of your data with garbage.
To defend against that type of problem, you can set up a delayed secondary using the slaveDelay setting.
A delayed secondary purposely lags by the specified number of seconds.
This way, if someone fat-fingers away your main collection, you can restore it from an identical copy of the data from earlier.
If your application is routing reads to secondaries, you should make slave delayed members hidden so that reads are not routed to them.
Building Indexes Sometimes a secondary does not need to have the same (or any) indexes that exist on the primary.
If you are using a secondary only for backup data or offline batch jobs, you might want to specify "buildIndexes" : false in the member’s configuration.
This is a permanent setting: members that have "buildIndexes" : false specified can never be reconfigured to be “normal” index-building members again.
If you want to change a non-index-building member to an index-building one, you must remove it from the set, delete all of its data, re-add it to the set, and allow it to resync from scratch.
This chapter covers how the pieces of a replica set fit together, including:
Syncing Replication is concerned with keeping an identical copy of data on multiple servers.
The way MongoDB accomplishes this is by keeping a log of operations, or oplog, containing every write that a primary performs.
This is a capped collection that lives in the local database on the primary.
Each secondary maintains its own oplog, recording each operation it replicates from the primary.
This allows any member to be used as a sync source for any other member, as shown in Figure 10-1
Secondaries fetch operations from the member they are syncing from, apply the operations to their data set, and then write the operations to the oplog.
If applying an operation fails (which should only happen if the underlying data has been corrupted or in some way differs from the primary), the secondary will exit.
Oplog keep an ordered list of write operations that have occurred.
Each member has its own copy of the oplog, which should be identical to the primary’s (modulo some lag)
If a secondary goes down for any reason, when it restarts it will start syncing from the last operation in its oplog.
As operations are applied to data and then written to the oplog, the secondary may replay operations that it has already applied to its data.
MongoDB is designed for it to handle this correctly: replaying oplog ops multiple times yields the same result as replaying them once.
As the oplog is a fixed size, it can only hold a certain number of operations.
However, there are a few exceptions: operations that effect multiple documents, such as removes or a multi-updates, that will be exploded into many oplog entries.
The single operation on the primary will be split into one oplog op per document affected.
If you are doing lots of bulk operations, this can fill up your oplog more quickly than you might expect.
Initial Sync When a member of the set starts up, it will check if it is in a valid state to begin syncing from someone.
If not, it will attempt to make a full copy of data from another member of the set.
This is called initial syncing and there are several steps to the process, which you can follow in the mongod’s log:
First, the member does some preliminary bookkeeping: it chooses a member to sync from, creates an identifier for itself in local.me, and drops all existing databases to start with a clean slate:
Note that any existing data will be dropped at this point.
Only do an initial sync if you do not want the data in your data directory or have moved it elsewhere, as mongod’s first action is to delete it all.
Cloning is the initial data copy of all records from the sync source.
This is usually the most time-consuming part of the process:
Then the first oplog application occurs, which applies any operations that happened during the clone.
This may have to reclone certain documents that were moved and, therefore, missed by the cloner:
This is roughly what the logs will look like if some documents had to be recloned.
Depending on the level of traffic and the types of operations that where happening on the sync source, you may or may not have missing objects.
Then the second oplog application occurs, which applies operations that happened during the first oplog application:
It is only distinct from the first application in that there should no longer be anything to reclone.
At this point, the data should exactly match the data set as it existed at some point on the primary so that the secondary can start building indexes.
This can be quite time-consuming if you have large collections or lots of indexes:
Then the final oplog application occurs; this final step is merely to prevent the member from becoming a secondary while it is still far behind the sync source.
It applies all of the operations that happened while indexes were building:
At this point, the member finishes the initial sync process and transitions to normal syncing, which allows it to become a secondary:
The best way to track an initial sync’s progress is by watching the server’s log.
Doing an initial sync is very easy from an operator perspective: start up a mongod with a clean data directory.
Restoring from backup is often faster than copying all of your data through mongod.
Many deployments end up with a subset of their data that’s frequently accessed and always in memory (because the OS is accessing it frequently)
Performing an initial sync forces the member to page all of its data into memory, evicting the frequently-used data.
This can slow down a member dramatically as requests that were being handled by data in RAM are suddenly forced to go to disk.
However, for small data sets and servers with some breathing room, initial syncing is a good, easy option.
In these cases, the new member can “fall off ” the end of sync source’s oplog: the new member gets so far behind the sync source that it can no longer catch up because the sync source’s oplog has overwritten the data the member would need to use to continue replicating.
There is no way to fix this other than attempting the initial sync at a less-busy time or restoring from a backup.
The initial sync cannot proceed if the member has fallen off of the sync source’s oplog.
Handling Staleness If a secondary falls too far behind the actual operations being performed on the sync source, the secondary will go stale.
A stale secondary is unable to continue catch up because every operation in the sync source’s oplog is too far ahead: it would be skipping operations if it continued to sync.
This could happen if the slave has had downtime, has more writes than it can handle, or is too busy handling reads.
When a secondary goes stale, it will attempt to replicate from each member of the set in turn to see if there’s anyone with a longer oplog that it can bootstrap from.
If there is no one with a long-enough oplog, replication on that member will halt and it will need to be fully resynced (or restored from a more recent backup)
To avoid out-of-sync secondaries, it’s important to have a large oplog so that the primary can store a long history of operations.
But in general this is a good trade-off to make because the disk space tends to be cheap and little of the oplog is usually in use, and therefore it doesn’t take up much RAM.
Heartbeats Members need to know about the other members’ states: who’s primary, who they can sync from, and who’s down.
To keep an up-to-date view of the set a member sends out a heartbeat request to every other member of the set every two seconds.
A heartbeat request is a short message that checks everyone’s state.
One of the most important functions of heartbeats is to let the primary know if it can reach a majority of the set.
If a primary can no longer reach a majority of the servers, it will demote itself and become a secondary.
Member States Members also communicate what state they are in via heartbeats.
There are several other normal states that you’ll often see members be in: STARTUP.
This is the state MongoDB goes into when you first start a member.
It’s the state when MongoDB is attempting to load a member’s replica set configuration.
Once the configuration has been loaded, it transitions to STARTUP2
STARTUP2 This state will last throughout the initial sync process but on a normal member, it should only ever last a few seconds.
It just forks off a couple of threads to handle replication and elections and then transitions into the next state: RECOVERING.
This state is a bit overloaded: you may see it in a variety of situations.
On startup, a member has to make a couple checks to make sure it’s in a valid state before accepting reads; therefore, all members will go through recovering state briefly on startup before becoming secondaries.
A member will also go into RECOVERING state if it has fallen too far behind the other members to catch up.
This is, generally, a failure state that requires resyncing the member.
The member does not go into an error state at this point because it lives in hope that someone will come online with a long-enough oplog that it can bootstrap itself back to non-staleness.
There are also a few states that indicate a problem with the system.
Note that a member reported as “down” might, in fact, still be up, just unreachable due to network issues.
This generally indicates that the unknown member is down or that there are network problems between the two members.
If a removed member is added back into the set, it will transition back into its “normal” state.
At the end of the rollback process, a server will transition back into the recovering state and then become a secondary.
You should take a look at the log to figure out what has caused it to go into this state (grep for "replSet FATAL" to find the point where it went into the FATAL state)
You generally will have to shut down the server and resync it or restore from backup once it’s in this state.
Elections A member will seek election if it cannot reach a primary (and is itself eligible to become primary)
A member seeking election will send out a notice to all of the members it can reach.
These members may know of reasons that this member is an unsuitable primary: it may be behind in replication or there may already be a primary that the member.
In these cases, the other members will not allow the election to proceed.
Assuming that there is no reason to object, the other members will vote for the member seeking election.
If the member seeking election receives votes from a majority of the set, the election was successful and will transition into primary state.
If it did not receive a majority if votes, it will remain a secondary and may try to become a primary again later.
A primary will remain primary until it cannot reach a majority of members, goes down, is stepped down, or the set is reconfigured.
Assuming that the network is healthy and a majority of the servers are up, elections should be fast.
It will take a member up to two seconds to notice that a primary has gone down (due to the heartbeats mentioned earlier) and it will immediately start an election, which should only take a few milliseconds.
However, the situation is often non-optimal: an election may be triggered due to networking issues or overloaded servers responding too slowly.
In these cases, heartbeats will take up to 20 seconds to timeout.
If, at that point, the election results in a tie, everyone will have to wait 30 seconds to attempt another election.
Thus, if everything goes wrong, an election may take a few minutes.
Rollbacks The election process described in the previous section means that if a primary does a write and goes down before the secondaries have a chance to replicate it, the next primary elected may not have the write.
For example, suppose we have two data centers, one with the primary and a secondary, and the other with three secondaries, as shown in Figure 10-2
Suppose that there is a network partition between the two data centers, as shown in Figure 10-3
The servers in the first data center are up to operation 126, but that data center hasn’t yet replicated to the servers in the other data center.
Replication across data centers can be slower than within a single data center.
The servers in the other data center can still reach a majority of the set (three out of five servers)
This new primary begins taking its own writes, as shown in Figure 10-4
Unreplicated writes won’t match writes on the other side of a network partition.
When the network is repaired, the servers in the first data center will look for operation 126 to start syncing from the other servers but will not be able to find it.
When this happens, A and B will begin a process called rollback.
Rollback is used to undo ops that were not replicated before failover.
The servers with 126 in their oplogs will look back through the oplogs of the servers in the other data center for a common point.
They’ll find that operation 125 is the latest operation that matches.
At this point, the server will go through the ops it has and write its version of each document affected by those ops to a .bson file in a rollback directory of your data directory.
Then it will copy the version of that document from the current primary.
The server begins syncing from another member (server-1, in this case) and realizes that it cannot find its latest operation on the sync source.
At that point, it starts the rollback process by going into rollback state ("replSet ROLLBACK")
It then begins undoing the operations from the last 26 seconds from its oplog.
Once the rollback is complete, it transitions into recovering state and begins syncing normally again.
To apply operations that have been rolled back to the current primary, first use mongor estore to load them into a temporary collection:
Now you should examine the documents (using the shell) and compare them to the current contents of the collection from whence they came.
For example, if someone had created a “normal” index on the rollback member and a unique index on current primary, you’d want to make sure that there weren’t any duplicates in the rolled-back data and resolve them if there were.
Once you have a version of the documents that you like in your staging collection, load it into your main collection:
If you have any insert-only collections, you can directly load the rollback documents into the collection.
However, if you are doing updates on the collection you will need to be more careful about how you merge rollback data.
One often-misused member configuration option is the number of votes each member has.
Manipulating the number of votes is almost always not what you want and causes a lot of rollbacks (which is why it was not included in the list of member properties in the last chapter)
Do not change the number of votes unless you are prepared to deal with regular rollbacks.
When Rollbacks Fail In some cases, MongoDB decides that the rollback is too large to undertake.
In these cases, you must resync the node that is stuck in rollback.
The most common cause of this is when secondaries are lagging and the primary goes down.
If one of the secondaries becomes primary, it will be missing a lot of operations from the old primary.
The best way to make sure you don’t get a member stuck in rollback is to keep your secondaries as up to date as possible.
This chapter covers how applications interact with replica sets, including:
By default, client libraries will connect to the primary and route all traffic to it.
Your application can perform reads and writes as though it were talking to a standalone server while your replica set quietly keeps hot standbys ready in the background.
Connections to a replica set are similar to connections to a single server.
You do not have to list all members (although you can): when the driver connects to the seeds, it will discover the other members from them.
When a primary goes down, the driver will automatically find the new primary (once one is elected) and will route requests to it as soon as possible.
However, while there is no reachable primary your application will be unable to perform writes.
There may be no primary available for a brief time (during an election) or for an extended period of time (if no reachable member can become primary)
By default, the driver will not service any requests—read or write—during this period.
A common desire is to have the driver hide the entire election process (the primary going away and a new primary being elected) from the user.
However, this is not possible or desirable in many cases, so no driver handles failover this way.
First, a driver can only hide a lack of primary for so long: a set could exist forever with no primary.
Second, a driver often finds out that the primary went down because an operation failed, which means that the driver doesn’t know whether or not the primary processed the operation before going down.
Thus, the driver leaves it to the user: Do you want to retry the operation on the new primary, if one is elected quickly? Assume it got through on the old primary? Check and see if the new primary has the operation? The strategy that makes sense will depend on your application.
Waiting for Replication on Writes To ensure that writes will be persisted no matter what happens to the set, you must ensure that the write propagates to a majority of the members of the set, as mentioned in the previous chapter.
Earlier, we used the getLastError command to check that writes were successful.
We can use that same command to ensure that a write has been replicated to secondaries.
The “w” parameter forces getLastError to wait until the given number of members has the last write.
MongoDB has a special keyword that you can pass to "w" for this: "majori ty"
It is only present if you use the "w" option and it is a list of servers that the last operation was replicated to.
Suppose that we run the command above, but only the primary and an arbiter are up: the primary cannot replicate the write to any other member of the set.
Thus, you should always set wtimeout to a reasonable value.
This may fail for a variety of reasons: the other members may be down or lagging or unavailable due to network issues.
If getLastError times out, your application has to decide what to do next.
Note that getLastError timing out doesn’t mean that the write failed.
It merely means that it failed to replicate far enough in the time specified.
The write is still present on any servers it made it to and will continue to propagate to the other members of the set as quickly as possible.
MongoDB lets you write “too fast.” It will let you write to the primary so quickly that the secondaries cannot keep up.
This forces writes on that particular connection to wait for replication.
Note that it only blocks writes on that connection: writes can still occur on any other connection.
If you wish to make your application behave sensibly and robustly, regularly call getLastError with "majority" and a reasonable timeout.
If this begins timing out, look into what’s wrong with your set.
What Can Go Wrong? Suppose your application sends a write to the primary.
It calls getLastError (without the "majority" option) and receives confirmation that the write was written, but the primary crashes before any secondaries have had a chance to replicate that write.
Now your application thinks that it’ll be able to access that write (getLastError confirmed that the write succeeded) and the current members of the replica set don’t have a copy of it.
At some point, a secondary may be elected primary and start taking new writes.
When the former primary comes back up, it will discover that it has writes that the current primary does not.
To correct this, it will undo any writes that do not match the sequence.
These operations are not lost, but they are written to special rollback files that have to be manually applied to the current primary.
MongoDB cannot automatically apply these writes, since they may conflict with other writes that have happened since the crash.
Thus, the write essentially disappears until an admin gets a chance to apply the rollback files to the current primary.
Writing to a majority prevents this situation: if the application initially used "w" : "majority" and gets a confirmation that the write succeeded, then the new primary would have to have a copy of the write to be elected (a member must be up to date to be elected primary)
If getLastError failed, then the application would know to try again, given that the write had not been propagated to a majority of the set before the primary crashed.
Other Options for “w” "majority" is not the only option that you can pass to getLastError, MongoDB also lets you specify an arbitrary number of servers to replicate to by passing "w" a number, as below:
This would wait until two members (the primary and one secondary) had the write.
If you want the write propagated to n secondaries, you should set "w" to n+1 (to include the primary)
Setting "w" : 1 is the same as not passing the "w" option at all because it just checks that the write was successful on the primary, which is what getLastError does anyway.
The downside to using a literal number is that you have to change your application if your replica set configuration changes.
Custom Replication Guarantees Writing to a majority of a set is considered “safe.” However, some sets may have more complex requirements: you may want to make sure that a write makes it to at least one server in each data center or a majority of the nonhidden nodes.
Replica sets allows you to create custom rules that you can pass to getLastError to guarantee replication to whatever combination of servers you need.
Guaranteeing One Server per Data Center Network issues between data centers are much more common than within data centers and it is more likely for an entire data center to go dark than an equivalent smattering of servers across multiple data centers.
Guaranteeing a write to every data center before confirming success means that, in the case of a write followed by the data center going offline, every other data center will have at least one local copy.
To set this up, first classify the members by data center.
You do this by adding a "tags" field to their replica set configuration:
The "key" field is the key field from the tags, so in this example it will be "dc"
The number is the number groups that are needed to fulfil this rule.
In this case, number is 2 (because we want at least one server from "us-east" and one from "us-west")
Note that rules are somewhat abstracted away from the application developer: they don’t have to know which servers are in “eachDC” to use the rule, and the rule can change without their application having to change.
We could add a datacenter or change set members and the application would not have to know.
Guaranteeing a Majority of Nonhidden Members Often, hidden members are somewhat second-class citizens: you’re never going to fail over to them and they certainly aren’t taking any reads.
Thus, you may only care that nonhidden members received a write and let the hidden members sort it out for themselves.
To create a rule for this, first we tag each of the nonhidden members with its own tag:
This will wait until at least three of the nonhidden member have the write.
Creating Other Guarantees The rules you can create are limitless.
Remember that there are two steps to creating a custom replication rule:
The keys describe classifications; for example, you might have keys such as "data_center" or "region" or "server Quality"
Values determine which group a server belongs to within a classification.
For example, for the key "data_center", you might have some servers tagged "useast", some "us-west", and others "aust"
Rules are immensely powerful ways to configure replication, although they are complex to understand and set up.
Unless you have fairly involved replication requirements, you should be perfectly safe sticking with "w" : "majority"
Sending Reads to Secondaries By default, drivers will route all requests to the primary.
This is generally what you want, but you can configure other options by setting read preferences in your driver.
Read preferences let you specify the types of servers queries should be sent to.
Sending read requests to secondaries is generally a bad idea.
There are some specific situations in which it makes sense, but you should generally send all traffic to the primary.
If you are considering sending reads to secondaries, make sure to weigh the pros and cons very carefully before allowing it.
This section covers why it’s a bad idea and the specific conditions when it makes sense to do so.
Consistency Considerations Applications that require strongly consistent reads should not read from secondaries.
Secondaries should usually be within a few milliseconds of the primary.
Sometimes secondaries can fall behind by minutes, hours, or even days due to load, misconfiguration, network errors, or other issues.
Client libraries cannot tell how up to date a secondary is, so clients will cheerfully send queries to secondaries that are far behind.
Hiding a secondary from client reads can be done but is a manual process.
Thus, if your application needs data that is predictably up to date, it should not read from secondaries.
If your application needs to read its own writes (e.g., insert a document and then query for it and find it) you should not send the read to a secondary (unless the write waits for replication to all secondaries using "w" as shown earlier)
Otherwise, an application may perform a successful write, attempt to read the value, and not be able to find it (because it sent the read to a secondary, which hadn’t replicated yet)
Clients can issue requests faster than replication can copy operations.
To always send read requests to the primary, set your read preference to Primary (or leave it alone, since Primary is the default)
This means that your application cannot perform queries if the primary goes down.
However, it is certainly an acceptable option if your application can deal with downtime during failovers or network partitions or if getting stale data is unacceptable.
Load Considerations Many users send reads to secondaries to distribute load.
However, this is a dangerous way to scale because it’s easy to accidentally overload your system and difficult to recover from once you do.
For example, suppose that you have the situation above: 30,000 reads per second.
You decide to create a replica set with four members to handle this: each secondary is well below it’s maximum load and the system works perfectly.
Now each of the remaining members are handling 100% of their possible load.
If you need to rebuild the member that crashed, it may need to copy data from one of the other servers, overwhelming the remaining servers.
Overloading a server often makes it perform slower, lowering the set’s capacity even further and forcing other members to take more load, causing them to slow down in a death spiral.
Overloading can also cause replication to slow down, making the remaining secondaries fall behind.
Suddenly you have a member down, a member lagging, and everything is too overloaded to have any wiggle room.
If you have a good idea of how much load a server can take, you might feel like you can plan this out better: use five servers instead of four and the set won’t be overloaded if one goes down.
However, even if you plan it out perfectly (and only lose the number of servers you expected), you still have to fix the situation with the other servers under more stress than they would be otherwise.
A better choice is to use sharding to distribute load.
Reasons to Read from Secondaries There are a few cases in which it’s reasonable to send application reads to secondaries.
For instance, you may want your application to still be able to perform reads if the primary goes down (and you do not care if those reads are somewhat stale)
The is the most common case for distributing reads to secondaries: you’d like a temporary readonly mode when your set loses a primary.
One common argument for reading from secondaries is to get low-latency reads.
You can specify Nearest as your read preference to route requests to the lowest-latency member based on average ping time from the driver to the replica set member.
If your application needs to access the same document with low latency in multiple data centers, this is the only way to do it.
If, however, your documents are more location-based (application servers in this data center need low-latency access to some of your data, or application servers in another data center need low-latency access to other data), this should be done with sharding.
You must be willing to sacrifice consistency if you are reading from members that may not have replicated all the writes yet.
Alternatively, you could sacrifice write speed if you wanted to wait until writes had been replicated to all members.
If your application can truly function acceptably with arbitrarily stale data, you can use Secondary or Secondary preferred read preferences.
If there are no secondaries available, this will error out rather than send reads to the primary.
It can be used for applications that do not care about stale data and want to use the primary for writes only.
If you have any concerns about staleness of data, this is not recommended.
Secondary preferred will send read requests to a secondary, if one is available.
If no secondaries are available, requests will be sent to the primary.
Sometimes, read load is drastically different than write load: you’re reading entirely different data than you’re writing.
You might want dozens of indexes for offline processing that you don’t want to have on the primary.
In this case, you might want to set up a secondary with different indexes than the primary.
If you’d like to use a secondary for this purpose, you’d probably create a connection directly to it from the driver, instead of using a replica set connection.
Consider which of the options makes sense for your application.
You can also combine options: if some read requests must be from the primary, use Primary for those.
If you are OK with other reads not having the most up-to-date data, use Primary prefer red for those.
And if certain requests require low latency over consistency, use Near est for those.
Starting Members in Standalone Mode A lot of maintenance tasks cannot be performed on secondaries (because they involve writes) and shouldn’t be performed on primaries.
Thus, the following sections frequently mention starting up a server in standalone mode.
This means restarting the member so that it is a standalone server, not a member of a replica set (temporarily)
To start up a member in standalone mode, first look at the command line argument.
To perform maintenance on this server we can restart it without the replSet option.
This will allow us to read and write to it as a normal standalone mongod.
We don’t want the other servers in the set to be able to contact it, so we’ll make it listen on a different port (so that the other members won’t be able to find it)
Finally, we want to keep the dbpath the same, as we are presumably starting it up this way to manipulate the server’s data somehow.
Thus, we start up this server with the following arguments:
It will now be a running as a standalone server, listening on port 30000 for connections.
The other members of the set will attempt to connect to it on port 27017 and assume that it is down.
When we have finished performing maintenance on the server, we can shut it down and restart it with its original options.
It will automatically sync up with the rest of the set, replicating any operations that it missed while it was “away.”
This document is the same on all members of the set.
Creating a Replica Set You create a replica set by starting up the mongods that you want to be members and then passing one of them a configuration through rs.initiate:
If you do not, MongoDB will attempt to automatically generate a config for a one-member replica set.
It may not use the hostname that you want or correctly configure the set.
You only call rs.initiate on one member of the set.
The member that receives the initiate will pass the configuration on to the other members.
Changing Set Members When you add a new set member, it should either have nothing in its data directory (in which case it will initial sync) or have a copy of the data from another member.
See Chapter 22 for more information about backing up and restoring replica set members.
There are a few restrictions in changing a member’s settings:
You cannot make the member you’re sending the reconfig to (generally the primary)
You cannot turn an arbiter into a nonarbiter and visa versa.
You cannot change a member with "buildIndexes" : false to "buildIndexes" : true.
Thus, if you incorrectly specify a host (say, use a public IP instead of a private one) you can later go back and simply change the config to use the correct IP.
This same strategy applies to change any other option: fetch the config with rs.con fig(), modify any parts of it that you wish, and reconfigure the set by passing rs.re config() the new configuration.
This is to reduce the amount of network traffic required for everyone to heartbeat everyone else and to limit the amount of time elections take.
You can do this by specifying it in the member’s config:
This prevents these members from casting positive votes in elections, although they can still veto.
Please do not alter votes if you can possibly avoid it.
Votes have weird, non-intuitive implications for elections and consistency guarantees.
Often, developers mistakenly think that a member having more votes will make it more likely to be primary (which it won’t)
If you wish a member to be preferentially chosen as primary, use priorities (see “Priority” on page 183)
Forcing Reconfiguration When you permanently lose a majority of the set, you may want to reconfigure the set while it doesn’t have a primary.
This is a little tricky: usually you’d send the reconfig to the primary.
In this case, you can force reconfigure the set by sending a reconfig command to a secondary.
Connect to a secondary in the shell and pass it a reconfig with the "force" option:
Forced reconfigurations follow the same rules as a normal reconfiguration: you must send a valid, well-formed configuration with the correct options.
The "force" option doesn’t allow invalid configs; it just allows a secondary to accept a reconfig.
Forced reconfigurations bump the replica set "version" number by a large amount.
This is normal: it is to prevent version number collisions (just in case there’s a reconfig on either side of a network partition)
When the secondary receives the reconfig, it will update its configuration and pass the new config along to the other members.
The other members of the set will only pick up on a change of config if they recognize the sending server as a member of their current config.
Thus, if some of your members have changed hostnames, you should force reconfig from a member that kept its old hostname.
Manipulating Member State There are several ways to manually change member state for maintenance or in response to load.
Note that there is no way to force a member to become primary other than configuring the set appropriately.
Turning Primaries into Secondaries You can demote a primary to a secondary using the stepDown function:
This makes the primary step down into secondary state for 60 seconds.
If no other primary has been elected in that time period, it will be able to attempt a reelection.
If you would like it to remain a secondary for a longer or shorter amount of time, you can specify your own number of seconds for it to stay in SECONDARY state:
Preventing Elections If you need to do some maintenance on the primary but don’t want any of the other eligible members to become primary in the interim, you can force them to stay secondaries by running freeze on each of them:
Again, this takes a number of seconds to remain secondary.
When you have finished whatever maintenance you are doing and want to “unfreeze” the other members you can run the command again, giving a timeout of 0 seconds:
This will allow the member to hold an election, if it chooses.
You can also unfreeze primaries that have been stepped down by running rs.freeze(0)
Using Maintenance Mode Maintenance mode occurs when you perform a long-running op on a replica set member: it forces the member into RECOVERING state.
Sometimes, a member will go into maintenance mode automatically, for example, if you run a compact on it.
When the compact begins, the member will go into RECOVERING state so that reads will no longer go to that member.
Clients will stop using it for reads (if they were) and it should no longer be used as a replication source.
You might want to do this if a member begins to fall behind.
For example, you could have a script like this, which detects if a member is behind and then puts it in maintenance mode:
Monitoring Replication It is important to be able to monitor the status of a set: not only that everyone is up, but what states they are in and how up-to-date the replication is.
There are several commands you can use to see replica set information.
Often issues with replication are transient: a server could not reach another server but now it can.
The easiest way to see issues like this is to look at the logs.
Make sure you know where the logs are being stored (and that they are being stored) and that you can access them.
Getting the Status One of the most useful commands you can run is replSetGetStatus, which gets the current information about every member of the set (from the of view of the member you’re running it on)
There is a helper for this command in the shell:
This field is only present in the member rs.status() was run on, in this case, server-2
See “Member States” on page 191 to see descriptions of the various states.
The number of seconds a member has been reachable (or the time since this server was started for the "self" member)
The last optime in each member’s oplog (where that member is synced to)
Note that this is the state of each member as reported by the heartbeat, so the optime reported here may be off by a couple of seconds.
The time this server last received a heartbeat from the member.
If there have been network issues or the server has been busy, this may be longer than two seconds ago.
The running average of how long heartbeats to this server have taken.
This is used in determining which member to sync from.
Any status message that the member chose to return in the heartbeat request.
For example, the "errmsg" field in server-3 indicates that this server is in the process of initial syncing.
The hexadecimal number 507e9a30:851 is the timestamp of the operation this member needs to get to to complete the initial sync.
There are a several fields that give overlapping information: "state" is the same as "stateStr", it’s simply the internal id for the state.
Note that this is report is from the point of view of whichever member of the set you run it on: it may be incorrect or out of date due to network issues.
Visualizing the Replication Graph If you run rs.status() on a secondary, there will be a top-level field called "syncing To"
This gives the host that this member is replicating from.
By running the replSetGetStatus command on each member of the set, you can figure out the replication graph.
MongoDB determines who to sync to based on ping time.
When one member heartbeats another, it times how long that request took.
When it has to choose a member to sync from, it looks for the member that is closest to it and ahead of it in replication (thus, you cannot end up with a replication cycle: members will only replicate from the primary or secondaries that are strictly further ahead)
Thus, if you bring up a new member in a secondary data center, it is more likely to sync from the other members in that data center than a member in your primary data center (thus minimizing WAN traffic), as show in Figure 12-1
New secondaries will generally choose to sync from a member in the same data center.
However, there are some downsides to automatic replication chaining: more replication hops mean that it takes a bit longer to replicate writes to all servers.
For example, let’s say that everything is in one data center but, due to the vagaries of network speeds when you added members, MongoDB ends up replicating in a line, as shown in Figure 12-2
As replication chains get longer, it takes longer for all members to get a copy of the data.
It is, however, probably undesirable: each secondary in the chain will have to be a bit further behind than the secondary “in front” of it.
You can fix this by modifying the replication source for a member using the replSetSyncFrom command (or the rs.syncFrom() helper)
Connect to the secondary whose replication source you want to change and run this command, passing it the server you’d prefer this member to sync from:
It may take a few seconds to switch sync sources, but if you run rs.status() on that member again, you should see that the "syncingTo" field now says "server0:27017"
Replication Loops A replication loop is when members end up replicating from one another, for example, A is syncing from B who is syncing from C who is syncing from A.
As none of the members in a replication loop can be a primary, the members will not receive any new operations to replicate and fall behind.
On the plus side, replication loops should be impossible when members choose who to sync from automatically.
However, you can force replication loops using the replSetSyncFrom command.
Inspect the rs.status() output careful before manually changing sync targets and be careful not to create loops.
The replSetSyncFrom command will warn you if you do not choose to sync from a member who is strictly ahead, but it will allow it.
Disabling Chaining Chaining is when a secondary syncs from another secondary (instead of the primary)
As mentioned earlier, members may decide to sync from other members automatically.
You can disable chaining, forcing everyone to sync from the primary, by changing the "allowChaining" setting to false (if not specified, it defaults to true):
With allowChaining set to false, all members will sync from the primary.
If the primary becomes unavailable, they will fall back to syncing from secondaries.
Calculating Lag One of the most important metrics to track for replication is how well the secondaries are keeping up with the primary.
Lag is how far behind a secondary is, which means the difference in timestamp between the last operation the primary has performed and the timestamp of the last operation the secondary has applied.
Note that these are both functions of db, not rs.
This gives information about the size of the oplog and the date ranges of operations contained in the oplog.
If this were a real deployment, the oplog should be much larger (see “Resizing the Oplog” on page 220 for instructions on changing oplog size)
We want the log length to be at least as long as the time it takes to do a full resync.
That way, we don’t run into a case where a secondary falls off the end of the oplog before finishing its initial sync.
The log length is computed by taking the time difference between the first and last operation in the oplog once the oplog has filled up.
If the server has just started with nothing in the oplog, then the earliest operation will be relatively recent.
In that case, the log length will be small, even though the oplog probably still has free space available.
The length is a more useful metric for servers that have been operating long enough to write through their entire oplog at least once.
In this case, the secondary is 12 seconds behind the primary.
Remember that a replica set member’s lag is calculated relative to the primary, not against “wall time.” This usually is irrelevant, but on very low-write systems, this can cause phantom replication lag “spikes.” For example, suppose you do a write once an hour.
Right after that write, before it’s replicated, the secondary will look like it’s an hour behind the primary.
However, it’ll be able to catch up with that “hour” of operations in a few milliseconds.
This can sometimes cause confusion when monitoring a lowthroughput system.
Resizing the Oplog Your primary’s oplog should be thought of as your maintenance window.
If your primary has an oplog that is an hour long, then you only have one hour to fix anything that goes wrong before your secondaries fall too far behind and must be resynced from scratch.
Thus, you generally want to have an oplog that can hold a couple days to a week’s worth of data, to give yourself some breathing room if something goes wrong.
Unfortunately, there’s no easy way to tell how long your oplog is going to be before it fills up and there’s no way to resize it while your server is running.
However, it is possible to cycle through you servers, taking each one offline, making its oplog larger, and then adding it back into the set.
Remember that each server that could become a primary should have a large enough oplog to give you a sane maintenance window.
To increase the size of your oplog, perform the following steps:
If this is currently the primary, step it down and wait for the other servers to catch up.
Temporarily store the last insert in the oplog in another collection:
We could use the last update or delete, but $-operators cannot be inserted into a collection.
Make sure that the last op was inserted into the oplog.
If it was not, the server will drop all of its data and resync when you add it back into the set.
Finally, restart the server as a member of the replica set.
Remember that it only has one op in the oplog to start out with, so you won’t be able to see its true oplog length (how long it is in time) for a while.
Also, it won’t be a very good sync source if other members are behind.
You generally should not decrease the size of your oplog: although it may be months long, there is usually ample disk space for it and it does not use up any valuable resources like RAM or CPU.
Restoring from a Delayed Secondary Suppose someone accidentally drops a database but, luckily, you had a delayed secondary.
Now you need to get rid of the data on the other members and use the delayed slave as your definitive source of data.
This is certainly easy, but your replica set will essentially be one rather overloaded secondary for however long it takes the other members to initial sync.
The other option may or may not work better, depending on your amount of data:
Copy the delayed secondary’s data files to the other servers.
Note that this will mean all the servers will have the same oplog size as the delayed secondary, which may not be what you want.
Building Indexes If you send an index build to the primary, the primary will build the index normally and then the secondaries will build the index when they replicate the “build index” operation.
Although this is the easiest way to build an index, index builds are resourceintensive operations that can make members unavailable.
If all of your secondaries start building an index at the same time, almost every member of your set will be offline until the index build completes.
Therefore, you may want to build an index on one member at a time to minimize impact on your application.
When the index build is complete, restart the server as a member of the replica set.
You should now have a set where every member other than the primary has the index built.
Now there are two options, and you should choose the one that will impact your production system the least:
If you have an “off ” time when you have less traffic, that would probably be a good time to build it.
You also might want to modify read preferences to temporarily shunt more load onto secondaries while the build is in progress.
The primary will replicate the index build to the secondaries, but they will already have the index so it will be a no-op for them.
After its index build is complete, you can reintroduce it to the set.
Note that you could also use this technique to build different indexes on a secondary than you have on the rest of the set.
If you are building a unique index, make sure that the primary is not inserting duplicates or that you build the index on the primary first.
Otherwise, the primary could be inserting duplicates that would then cause replication errors on secondaries.
You will have to restart it as a stand alone, remove the unique index, and restart it.
Replication on a Budget If it is difficult get more than one high-quality server, consider getting a secondary server that is strictly for disaster recovery, with less RAM, CPU, slower disk IO, etc.
The good server will always be your primary and the cheaper server will never handle any client traffic (configure your clients to send all reads to the primary)
You do not want this server to ever become primary.
You do not want clients ever sending reads to this secondary.
This is optional, but it can decrease the load this server has to handle considerably.
If you ever need to restore from this server, you’ll need to rebuild indexes.
If you only have two machines, set the votes on this secondary to 0 so that the primary can stay primary if this machine goes down.
This will give you the safety and security of having a secondary without having to invest in two high-performance servers.
How the Primary Tracks Lag Each member that has ever been a sync source keeps a collection called local.slaves, which holds information about which servers are syncing from it and how up to date they are.
When you run a query using w, MongoDB looks through this information to decide if enough secondaries are up to date enough to proceed.
The local.slaves collection is actually an “echo” of an in-memory data structure, so it may be a few seconds out of date:
The "_id" of each server is important: it is an identifier for the syncing member.
You can see what a member’s "_id" is by connecting to it and looking at the local.me collection:
Occasionally, due to configuration issues, you may end up with multiple servers with the same "_id"
If this happens, only one will be able to report how far it has replicated to the primary.
This, in turn, can cause issues with your application (if you’re depending on a certain number of servers reporting that they got a write) and sharding (migrates cannot proceed until a majority of secondaries have replicated the migration)
If multiple machines have the same "_id", you can fix it by logging into each machine, dropping the local.me collection, and restarting the mongod.
On startup, mongod will repopulate local.me with a new "_id"
If a server’s address changes, you may get errors in the logs about duplicate key exceptions in the local database, given that the slave’s "_id" will be the same but its hostname will have changed.
If this occurs, you can drop the local.slaves collection and the errors will stop (this is simpler than the previous case because you just need to clear the old data, not resolve conflicting data)
The local.slaves collection is never cleaned up by mongod, so it may list servers that haven’t used the member as a sync source in months or members that aren’t even part of the set anymore.
As the collection is merely a dumping ground for MongoDB to report on replication status, there’s no harm in leaving old entries in there.
However, if you find the old entries confusing or cluttered, you can drop the whole collection.
If you have secondaries that are chained, you may notice that the primary has several local.slaves documents for a certain server.
This is because each secondary will “forward” any replication requests it gets to its sync target so that the primary knows where each secondary has synced to.
These are called “ghost syncs” because the requests don’t actually request any ops back; they just inform the primary of where the secondaries have synced to.
The local database is used for replication information because it is not replicated.
Thus, if you have data that you want to be local to a certain machine, you can load it into collections in the local database.
Master-Slave MongoDB originally supported a more traditional master-slave setup (no automatic failover and you declare who the master and slaves are)
There are exactly two reasons to consider using master-slave instead of replica sets: you need more than 11 secondaries or you need to replicate individual databases.
Masterslave will be deprecated at some point, probably once replica sets support unlimited members.
However, there are still times when you may need more than 11 slaves or need to replicate a single database.
To set up a master, start your server with --master.
To start a slave, use two options: -slave and --source master.
Note that you do not use the --replSet option: you’re not setting up a replica set.
At that point, master-slave is set up, and there is no further configuration necessary.
You can begin writing to the master and the slave will replicate the changes.
Master-slave can also be used to replicate a single database.
You can use --only option to select a database to replicate:
Drivers will not automatically distribute reads to slaves if you set a read preference.
You must make an explicit connection to a slave to read from it.
Converting Master-Slave to a Replica Set Converting from a master-slave setup to replica sets requires some downtime.
This is very important, as your former slave will briefly not have an oplog, so it won’t be able to catch up with any writes it misses during the upgrade.
Restart the master with the --replSet option instead of --master.
Start up the slaves with the --replSet option and the --fastsync option.
Ordinarily, if you added a member to the set without an oplog, it would go through the full initial sync process.
The fastsync option tells the member to not worry that it doesn’t have an oplog, just start syncing from the latest time on the master.
Remove fastsync from your config file, command line alias, and long-term memory.
It is an extremely dangerous option to use because it makes the member skip operations on restart.
Its only use is to convert from master-slave to replica sets.
Mimicking Master-Slave Behavior with Replica Sets You usually want a primary to be available as often as possible, thus you should allow automatic failover if the primary becomes unavailable.
However, for some sets you may want to require an operator to manually promote a new primary and never allow an automatic failover.
This makes replica sets behave the same way as master-slave does (and is preferable to using master-slave to get this behavior)
This way, no member will seek election if the primary.
Also, the current primary can remain primary (as it has the only vote in the system) even if all of the other members go down.
If the primary goes down, an operator must manually intervene to select a new primary.
Introduction to Sharding Sharding refers to the process of splitting data up across machines; the term partitioning is also sometimes used to describe this concept.
By putting a subset of data on each machine, it becomes possible to store more data and handle more load without requiring larger or more powerful machines, just a larger quantity of less-powerful machines.
Manual sharding can be done with almost any database software.
Manual sharding is when an application maintains connections to several different database servers, each of which are completely independent.
The application manages storing different data on different servers and querying against the appropriate server to get data back.
This approach can work well but becomes difficult to maintain when adding or removing nodes from the cluster or in the face of changing data distributions or load patterns.
MongoDB supports autosharding, which tries to both abstract the architecture away from the application and simplify the administration of such a system.
MongoDB allows your application to ignore the fact that it isn’t talking to a standalone MongoDB server, to some extent.
On the operations side, MongoDB automates balancing data across shards and makes it easier to add and remove capacity.
Sharding is the most difficult and complex way of configuring MongoDB, both from a development and operational point of view.
You should be comfortable with standalone servers and replica sets before attempting to deploy or use a sharded cluster.
Understanding the Components of a Cluster MongoDB’s sharding allows you to create a cluster of many machines (shards) and break up your collection across them, putting a subset of data on each shard.
This allows your application to grow beyond the resource limits of a standalone server or replica set.
Many people are confused about the difference between replication and sharding.
Remember that replication creates an exact copy of your data on multiple servers, so every server is a mirror-image of every other server.
To hide these details from the application, we run a routing process called mongos in front of the shards.
This router keeps a “table of contents” that tells it which shard contains which data.
Applications can connect to this router and issue requests normally, as shown in Figure 13-1
The router, knowing what data is on which shard, is able to forward the requests to the appropriate shard(s)
If there are responses to the request, the router collects them, merges them, and sends them back to the application.
As far as the application knows, it’s connected to a standalone mongod, as in Figure 13-2
A One-Minute Test Setup As in the replication section, we will start by setting up a quick cluster on a single machine.
We will to connect to this mongos to play around with the cluster.
Your entire cluster will be dumping its logs to your current shell, so open up a second shell and use that to connect to your cluster’s mongos:
Now you are in the situation show in Figure 13-1: the shell is the client and is connected to a mongos.
You can start passing requests to the mongos and it’ll route them to the.
You don’t really have to know anything about the shards, like how many their are or what their addresses are.
So long as there are some shards out there, you can pass the requests to the mongos and allow it to forward them appropriately.
As you can see, interacting with mongos works the same way as interacting with a standalone server does.
You can get an overall view of your cluster by running sh.status()
It will give you a summary of your shards, databases, and collections:
As you can see from the sh.status() output, you have three shards and two databases (admin is created automatically)
Your test database may have a different primary shard than shown above.
A primary shard is a “home base” shard that is randomly chosen for each database.
All of your data will be on this primary shard.
MongoDB cannot automatically distribute your data yet because it doesn’t know how (or if) you want it to be distributed.
You have to tell it, percollection, how you want it to distribute data.
A primary shard is different than a replica set primary.
A primary shard refers to the entire replica set composing a shard.
A primary in a replica set is the single server in the set that can take writes.
To shard a particular collection, first enable sharding on the collection’s database.
Now sharding is enabled on the test database, which allows you to shard collections within the database.
When you shard a collection, you choose a shard key.
This is a field or two that MongoDB uses to break up data.
Choosing a shard key can be thought of as choosing an ordering for the data in the collection.
This is a similar concept to indexing, and for good reason: the shard key becomes the most important index on your collection as it gets bigger.
To even create a shard key, the field(s) must be indexed.
Although we are choosing a shard key without much thought here, it is an important decision that should be carefully considered in a real system.
See Chapter 15 for more advice on choosing a shard key.
If you wait a few minutes and run sh.status() again, you’ll see that there’s a lot more information displayed than there was before:
Sharding splits this into smaller chunks based on the shard key, as shown in Figure 13-4
These chunks can then be distributed across the cluster, as Figure 13-5 shows.
Before a collection is sharded, it can be thought of as a single chunk from the smallest value of the shard key to the largest.
Sharding splits the collection into many chunks based on shard key ranges.
Similarly, $maxKey is like “positive infinity.” It is greater than any other value.
Thus, you’ll always see these as the “caps” on your chunk ranges.
These values are actually BSON types and should not be used in your application; they are mainly for internal use.
If you wish to refer to them in the shell, use the MinKey and MaxKey constants.
Now that the data is distributed across multiple shards, let’s try doing some queries.
However, let’s run an explain to see what MongoDB is doing under the covers:
There are two parts to this explain: a somewhat usual-looking explain output nested inside of another explain’s output.
The way to read this is that the outer explain is from the mongos: this describes what the mongos had to do to process this query.
The inner explain is from any shards that were used in the query, in this case, localhost:30001
As "username" is the shard key, mongos could send the query directly to the correct shard.
Contrast that with the results for querying for all of the data:
As you can see from this explain, this query has to visit all three shards to find all the data.
In general, if we are not using the shard key in the query, mongos will have to send the query to every shard.
Queries that contain the shard key and can be sent to a single shard or subset of shards are called targeted queries.
Queries that must be sent to all shards are called scattergather queries: mongos scatters the query to all the shards and then gathers up the results.
Switch back to your original shell and hit Enter a few times to get back to the command line.
Then run clus ter.stop() to cleanly shut down all of the servers:
If you are ever unsure of what an operation will do, it can be helpful to use ShardingT est to spin up a quick local cluster and try it out.
In the previous chapter, you set up a “cluster” on one machine.
This chapter covers how to set up a more realistic cluster and how each piece fits, in particular:
How to set up config servers, shards, and mongos processes.
When to Shard Deciding when to shard is a balancing act.
You generally do not want to shard too early because it adds operational complexity to your deployment and forces you to make design decisions that are difficult to change later.
On the other hand, you do not want to wait too long to shard because it is difficult to shard an overloaded system without downtime.
Read or write data with greater throughput than a single mongod can handle.
Thus, good monitoring is important to decide when sharding will be necessary.
Generally people speed toward one of these bottlenecks much faster than the others, so figure out which one your deployment will need to provision for first and make plans well in advance about when and how you plan to convert your replica set.
As you add shards, performance should increase roughly linearly per shard up to hundreds of shards.
However, you will usually experience a performance drop if you move from a non-sharded system to just a few shards.
Due to the overhead of moving data, maintaining metadata, and routing, small numbers of shards will generally have higher latency and may even have lower throughput than a non-sharded system.
Thus, you may want to jump directly to three or more shards.
Starting the Servers The first step in creating a cluster is to start up all of the processes required.
As mentioned in the previous chapter, we need to set up the mongos and the shards.
There’s also a third component, the config servers, which are an important piece.
They are normal mon god servers that store the cluster configuration: who the shards are, what collections are sharded by, and the chunks.
Config Servers Config servers are the brains of your cluster: they hold all of the metadata about which servers hold what data.
Thus, they must be set up first and the data they hold is extremely important: make sure that they are running with journaling enabled and that their data is stored on non-ephemeral drives.
Each config server should be on a separate physical machine, preferable geographically distributed.
The config servers must be started before any of the mongos processes, as mongos pulls its configuration from them.
Config servers are standalone mongod processes, so you can start them up the same way you would a “normal” mongod:
When you start up config servers, do not use the --replSet option: config servers are not members of a replica set.
A common question is why three config servers? The reasoning behind the choice is that one config server is not enough: you need redundancy.
Conversely, you don’t want too many config servers, since confirming actions with all of them would be prohibitively time consuming.
Also, if any of them goes down, you cluster’s metadata becomes read-only.
Thus, three was chosen as enough to give redundancy but not have the downsides of having too many servers.
It will probably be made more flexible in the future.
The --configsvr option indicates to the mongod that you are planning to use it as a config server.
It is not strictly required, as all it does is change the default port mon god listens on to 27019 and the default data directory to /data/configdb (you can override either or both of these settings with --port and --dbpath)
It is recommended that you use this option because it makes it easier to tell, operationally, what these servers are doing.
If you start up your config servers without the -configsvr option, though, it’s not a problem.
In terms of provisioning, config servers do not need much space or many resources.
As they don’t use many resources, you can deploy config servers on machines running other things, like app servers, shard mongods, or mon gos processes.
If all of your config servers are lost, you must dig through the data on your shards to figure out which data is where.
Always take a backup of your config servers before performing any cluster maintenance.
The mongos Processes Once you have three config servers running, start a mongos process for your application to connect to.
Note that it does not need a data directory (mongos holds no data itself, it loads the cluster configuration from the config servers on startup)
Make sure that you set logpath to save the mongos log somewhere safe.
You can start as many mongos processes as you’d like.
A common setup is one mongos process per application server (running on the same machine as the application server)
Each mongos must use the exact same list of config servers, down to the order in which they are listed.
Adding a Shard from a Replica Set Finally, you’re ready to add a shard.
There are two possibilities: you may have an existing replica set or you may be starting from scratch.
If you are starting from scratch, initialize an empty set and follow the steps below.
If you already have a replica set serving your application, that will become your first shard.
To convert it into a shard, you are going to tell the mongos the replica set name and give it a seed list of replica set members.
You can specify all the members of the set, but you do not have to.
The set name, “spock”, is taken on as an identifier for this shard.
If we ever want to remove this shard or migrate data to it, we’ll use “spock” to describe it.
This works better than using a specific server (e.g., server-1), as replica set membership and status can change over time.
Once you’ve added the replica set as a shard you can convert your application from connecting to the replica set to connecting to the mongos.
When you add the shard, mongos registers that all the databases in the replica set are “owned” by that shard, so it will pass through all queries to your new shard.
Test failing over a shard’s primary in a development environment to ensure that your application handles the errors received from mongos correctly (they should be identical to the errors that you receive from talking to the primary directly)
Once you have added a shard, you must set up all clients to send requests to the mongos instead of contacting the replica set.
Sharding will not function correctly if some clients are still making requests to the replica set directly (not through the mongos)
Switch all clients to contacting the mongos immediately after adding the shard and set up a firewall rule to ensure that they are unable to connect directly to the shard.
There is a --shardsvr option, analogous to the --configsvr option mentioned previously.
As before, --shardsvr has little practical effect (it changes the default port to 27018) but can be nice to include operationally.
You can also create stand-alone-mongod shards (instead of replica set shards), but this is not recommend for production (ShardingTest in the previous chapter did this)
To add a single mongod as a shard simply specify the hostname of the standalone server in the call to addShard:
If you plan to switch to replica sets later, start with one-member replica sets instead of standalone servers.
Switching from a stand-alone-server shard to a replica set requires downtime (see “Server Administration” on page 285)
Adding Capacity When you want to add more capacity, you’ll need to add more shards.
To add a new, empty shard, create a replica set.
Make sure it has a distinct name from any of your other shards.
Once it is initialized and has a primary, add it to your cluster by running the addShard command through mongos, specifying the new replica set’s name and its hosts as seeds.
If you have several existing replica sets that are not shards, you can add all of them as new shards in your cluster so long as they do not have any database names in common.
For example, if you had one replica set with a “blog” database, one with a “calendar” database, and one with the “mail”, “tel”, and “music” databases, you could add each replica set as a shard and end up with a cluster with three shards and five databases.
However, if you had a fourth replica set that also had a database named “tel”, mongos would refuse to add it to the cluster.
Sharding Data MongoDB won’t distribute your data automatically until you tell it how to do so.
You must explicitly tell both the database and collection that you want them to be distributed.
For example, suppose we want to shard the artists collection in the music database on the "name" key.
Sharding a database is always prerequisite to sharding one of its collections.
Now the collection will be sharded by the "name" key.
If you are sharding an existing collection there must be an index on the "name" field; otherwise the shardCollec tion call will return an error.
If you get an error, create the index (mongos will return the index it suggests as part of the error message) and retry the shardCollection command.
If the collection you are sharding does not yet exist, mongos will automatically create the shard key index for you.
The shardCollection command splits the collection into chunks, which are the unit MongoDB uses to move data around.
Once the command returns successfully, MongoDB will begin balancing the collection across the shards in your cluster.
For large collections it may take hours to finish this initial balancing.
How MongoDB Tracks Cluster Data Each mongos must always know where to find a document, given its shard key.
Theoretically, MongoDB could track where each and every document lived, but this becomes unwieldy for collections with millions or billions of documents.
Thus, MongoDB groups documents into chunks, which are documents in a given range of the shard key.
A chunk always lives on a single shard, so MongoDB can keep a small table of chunks mapped to shards.
A document always belongs to one and only one chunk.
One consequence to this rule is that you cannot use an array field as your shard key, since MongoDB creates multiple index entries for arrays.
A common misconception is that the data in a chunk is physically grouped on disk.
This is incorrect: chunks have no effect on how mon god stores collection data.
Chunk Ranges Each chunk is described by the range it contains.
A newly sharded collection starts off with a single chunk and every document lives in this chunk.
If you looked at the contents of that collection, you’d see documents that looked something like this (some fields have been omitted for clarity):
This document would live on the third chunk, as lower bounds are inclusive.
This document would not be in any of these chunks.
It would be in some chunk with a range lower than the first chunk’s.
If the chunk does need to be split, mongos will update the chunk’s metadata on the config servers.
Chunk splits are just a metadata change (no data is moved)
New chunk documents are created on the config servers and the old chunk’s range ("max") is modified.
Once that process is complete, the mongos resets its tracking for the original chunk and creates new trackers for the new chunks.
When mongos asks a shard if a chunk needs to be split, the shard makes a rough calculation of the chunk size.
If it finds that the chunk is getting large, it finds split points and sends those to the mongos (as shown in Figure 14-3)
A shard may not be able to find any split points though, even for a large chunk, as there are a limited number of ways to legally split a chunk.
Any two documents with the same shard key must live in the same chunk so chunks can only be split between documents where the shard key’s value changes.
For example, if the shard key was "age", the following chunk could be split at the points where the shard key changed, as indicated:
For example, if the chunk contained the following documents, it could not be split (unless the application started inserting fractional ages):
Thus, having a variety of values for your shard key is important.
Other important properties will be covered in the next chapter.
If one of the config servers is down when a mongos tries to do a split, the mongos won’t be able to update the metadata (as shown in Figure 14-4)
All config servers must be up and reachable for splits to happen.
If the mongos continues to receive write requests for the chunk, it will keep trying to split the chunk and fail.
This process of mongos repeatedly attempting to split a chunk and being unable to is called a split storm.
The only way to prevent split storms is to ensure that your config servers are up and healthy as much of the time as possible.
You can also restart a mongos to reset its write counter (so that it is no longer at the split threshold)
When a client writes to a chunk, mongos will check its split threshold for the chunk.
If the split threshold has been reached, mongos will send a request for split points to the shard.
The shard calculates split points for the chunk and sends them to the mongos.
The mongos chooses a split point and attempts to inform the config server but cannot reach it.
Thus, it is still over its split threshold for the chunk and any subsequent writes will trigger this process again.
Another issue is that mongos might never realize that it needs to split a large chunk.
There is no global counter of how big each chunk is.
This means that if your mongos processes go up and down frequently a mongos might never receive enough writes to hit the split threshold before it is shut down again and your chunks will get larger and larger (as shown in Figure 14-6)
As mongos processes perform writes, their counters increase toward the split threshold.
If mongos processes are regularly restarted their counters may never hit the threshold, making chunks grow without bound.
The first way to prevent this is to have fewer mongos churn.
Leave mongos processes up, when possible, instead of spinning them up when they are needed and then turning them off when they are not.
However, some deployments may find it too expensive to run mongos processes that aren’t being used.
If you are in this situation, another way of getting more splits is to make the chunk size smaller than you actually want it to be.
This will prompt splits to happen at a lower threshold.
You can turn off chunk splitting by starting every mongos with --nosplit.
It regularly checks for imbalances between shards and, if it finds an imbalance, will begin migrating chunks.
Although the balancer is often referred to as a single entity, each mongos plays the part of “the balancer” occasionally.
Every few seconds, a mongos will attempt to become the balancer.
If there are no other balancers active, the mongos will take a cluster-wide lock from the config servers and do a balancing round.
Balancing doesn’t affect a mongos’s normal routing operations, so clients using that mongos should be unaffected.
The balancer is the document with the "_id" of "balancer"
The lock’s "who" field tells you which mon gos is—or was—balancing: router-23:27017 in this case.
Once a mongos has become the balancer, it checks its table of chunks for each collection to see if any shards have hit the balancing threshold.
This is when one shard has significantly more chunks than the other shards (the exact threshold varies: larger collections tolerate larger imbalances than smaller ones)
If an imbalance is detected, the balancer will redistribute chunks until all shards are within one chunk of one another.
Assuming that some collections have hit the threshold, the balancer will begin migrating chunks.
It chooses a chunk from the overloaded shard and asks the shard if it should split the chunk before migrating.
Once it does any necessary splits, it migrates the chunk to a machine with fewer chunks.
An application using the cluster does not need be aware that the data is moving: all reads and writes are routed to the old chunk until the move is complete.
Once the metadata is updated, all mongos processes attempting to access the data in the old location will get an error.
These errors should not be visible to the client: the mongos will silently handle the error and retry the operation on the new shard.
If it successfully retrieves the data from the new location, it will return it.
If the mongos is unable to retrieve the new chunk location because the config servers are unavailable, it will return an error to the client.
This is another reason why it is important to always have config servers up and healthy.
The most important and difficult task when using sharding is choosing how your data will be distributed.
To make intelligent choices about this, you have to understand how MongoDB distributes data.
This chapter helps you make a good choice of shard key by covering:
Some alternative strategies if you want to customize how data is distributed.
This chapter assumes that you understand the basic components of sharding as covered in the previous chapters.
Taking Stock of Your Usage When you shard a collection you choose a field or two to use to split up the data.
Once you have more than a few shards, it’s almost impossible to change your shard key, so it is important to choose correctly (or at least notice any issues quickly)
To choose a good shard key, you need to understand your workload and how your shard key is going to distribute your application’s requests.
This can be difficult to picture, so try to work out some examples or, even better, try it out on a backup data set with sample traffic.
This section has lots of diagrams and explanations, but there is no substitute for trying it on your own data set.
For each collection that you’re planning to shard, start by answering the following questions:
How many shards are you planning to grow to? A three-shard cluster has a great deal more flexibility than a thousand-shard cluster.
As a cluster gets larger, you should not plan to fire off queries that can hit all shards, so almost all queries must include the shard key.
Decreasing write latency usually involves sending requests to geographically closer or more powerful machines.
Increasing throughput usually involves adding more parallelization and making sure that requests are distributed evenly across the cluster.
Are you sharding to increase system resources (e.g., give MongoDB more RAM per GB of data)? If so, you want to keep working set size as small possible.
Use these answers to evaluate the following shard key descriptions and decide whether the shard key you choose would work well in your situation.
Does it give you the targeted queries that you need? Does it change the throughput or latency of your system in the ways you need? If you need a compact working set, does it provide that?
Picturing Distributions There are three basic distributions that are the most common ways people choose to split their data: ascending key, random, and location-based.
There are other types of keys that could be used, but most use cases fall into one of these categories.
Ascending Shard Keys Ascending shard keys are generally something like a "date" field or ObjectId—anything that steadily increases over time.
An autoincrementing primary key is another example of an ascending field, albeit one that doesn’t show up in MongoDB much (unless you’re importing from another database)
Suppose that we shard on an ascending field, like "_id" on a collection using ObjectIds.
These chunks will be distributed across our sharded cluster of, let’s say, three shards, as shown in Figure 15-2
This is called the max chunk, as it is the chunk containing $maxKey.
If we insert another document, it will also be in the max chunk.
In fact, every subsequent insert will be into the max chunk! Every insert’s "_id" field will be closer to infinity than the previous (because ObjectIds are always ascending), so they will all go to into the max chunk.
This has a couple of interesting (and often undesirable) properties.
First, all of your writes will be routed to one shard (shard0002, in this case)
This chunk will be the only one growing and splitting, as it is the only one that receives inserts.
As you insert data, new chunks will “fall off ” of this chunk’s butt, as shown in Figure 15-3
The max chunk continues growing and being split into multiple chunks.
This pattern often makes it more difficult for MongoDB to keep chunks evenly balanced because all the chunks are being created by one shard.
Therefore, MongoDB must constantly move chunks to other shards instead of correcting small imbalances that might occur in a more evenly distributed systems.
Randomly Distributed Shard Keys On the other end of the spectrum are randomly distributed shard keys.
Randomly distributed keys could be usernames, email addresses, UUIDs, MD5 hashes, or any other key that has no identifiable pattern in your dataset.
We’ll end up with a random distribution of chunks on the various shards, as shown in Figure 15-4
As in the previous section, chunks are distributed randomly around the cluster.
As more data is inserted, the data’s random nature means that inserts should hit every chunk fairly evenly.
You can prove this to yourself by inserting 10,000 documents and seeing where they end up:
As writes are randomly distributed, shards should grow at roughly the same rate, limiting the number of migrates that need to occur.
The only downside to randomly distributed shard keys is that MongoDB isn’t efficient at randomly accessing data beyond the size of RAM.
However, if you have the capacity or don’t mind the performance hit, random keys nicely distribute load across your cluster.
Location-Based Shard Keys Location-based shard keys may be things like a user’s IP, latitude and longitude, or address.
Location shard keys are not necessarily related to a physical location field: the “location” might be a more abstract way that data should be grouped together.
In any case, it is a key where documents with some similarity fall into a range based on this field.
This can be handy for both putting data close to its users and keeping related data together on disk.
For example, suppose we have a collection of documents that are sharded on an IP address.
Documents will be organized into chunks based on their addresses and randomly spread across the cluster, as shown in Figure 15-5
A sample distribution of chunks in the IP address collection.
If we wanted certain chunk ranges to be attached to certain shards, we could tag these shards and then assign chunk ranges to tags.
We could request that the balancer do this by setting up tag ranges:
When the balancer moves chunks, it will attempt to move chunks with those ranges to those shards.
Chunks that were not covered by a tag range will be moved around normally.
The balancer will continue attempting to distribute chunks evenly among shards.
Shard Key Strategies This section presents a number of shard key options for various types of applications.
Hashed Shard Key For loading data as fast as possible, hashed shard keys are the best option.
A hashed shard key can make any field randomly distributed, so it is a good choice if you’re going.
The trade-off is that you can never do a targeted range query with a hashed shard key.
If you will not be doing range queries, though, hashed shard keys are a good option.
If you create a hashed shard key on a nonexistent collection, shardCollection behaves interestingly: it assumes that you want evenly distributed chunks, so it immediately creates a bunch of empty chunks and distributes them around your cluster.
For example, suppose our cluster looked like this before creating the hashed shard key:
Immediately after shardCollection returns there are two chunks on each shard, evenly distributing the key space across the cluster:
Note that there are no documents in the collection yet, but when you start inserting them, writes should be evenly distributed across the shards from the get-go.
Ordinarily, you would have to wait for chunks to grow, split, and move to start writing to other shards.
With this automatic priming, you’ll immediately have chunk ranges on all shards.
There are some limitations on what your shard key can be if you’re using a hashed shard key.
As with other shard keys, you cannot use array fields.
Hashed Shard Keys for GridFS Before attempting to shard GridFS collections, make sure that you understand how GridFS stores data (see Chapter 6 for an explanation)
In the following explanation, the term “chunks” is overloaded since GridFS splits files into chunks and sharding splits collections into chunks.
Thus, the two types of chunks are referred to as “GridFS chunks” and “sharding chunks” later in the chapter.
However, if you create a hashed index on the "files_id" field, each file will be randomly distributed across the cluster.
But a file will always be contained in a single chunk.
This is the best of both worlds: writes will go to all shards evenly and reading a file’s data will only ever have to hit a single shard.
As a side note, the fs.files collection may or may not need to be sharded, as it will be much smaller than fs.chunks.
You can shard it if you would like, but it less likely to be necessary.
The Firehose Strategy If you have some servers that are more powerful than others, you might want to let them handle proportionally more load than your less-powerful servers.
For example, suppose you have one shard that is composed of SSDs that can handle 10 times the load of your other machines (backed by spinning disks)
You could force all inserts to go to the SSD, and then allow the balancer to move older chunks to the other shards.
This would give lower-latency writes than the spinning disks would.
To use this strategy, we have to pin the highest chunk to the SSD.
Now, pin the current value of the ascending key through infinity to that shard, so all new writes go to it:
Now all inserts will be routed to this last chunk, which will always live on the shard tagged "ssd"
However, ranges from now through infinity will be trapped on this shard unless we modify the tag range.
We could set up a cron job to update the tag range once a day, like this:
Then all of the previous day’s chunks would be able to move to other shards.
Another downside of this strategy is that it requires some changes to scale.
If your SSD can no longer handle the number of writes coming in, there is no trivial way to split the load between this server and another.
If you do not have a high-performance server to firehose into or you are not using tagging, do not use an ascending key as the shard key.
If you do, all writes will go to a single shard.
Multi-Hotspot Standalone mongod servers are most efficient when doing ascending writes.
This conflicts with sharding, in that sharding is most efficient when writes are spread over the cluster.
This technique basically creates multiple hotspots—optimally several on each shard—so that writes are evenly balanced across the cluster but, within a shard, ascending.
The first value in the compound key is a rough, random value with low-ish cardinality.
You can picture each value in the first part of the shard key as a chunk, as shown in Figure 15-6
This will eventually work itself out as you insert more data, although it will probably never be divided up this neatly (right on the $minKey lines)
However, if you insert enough data, you should eventually have approximately one chunk per random value.
As you continue to insert data, you’ll end up with multiple chunks with the same random value, which brings us to the second part of the shard key.
Each chunk contains a single state and a range of _ids.
The second part of the shard key is an ascending key.
This means that, within a chunk, values are always increasing, as shown in the sample documents in Figure 15-7
Of course having n chunks with n hotspots spread across n shards isn’t very extensible: add a new shard and it won’t get any writes because there’s no hot chunk to put on it.
Thus, you want a few hotspot chunks per shard (to give you room to grow)
A few hotspot chunks will keep the effectiveness of ascending writes.
But having, say, a thousand “hotspots” on a shard will end up being equivalent to random writes.
You can picture this setup as each chunk being a stack of ascending documents.
There are multiple stacks on each shard, each ascending until the chunk is split.
Once a chunk is split, only one of the new chunks will be a hotspot chunk: the other chunk will essentially be “dead” and never grow again.
If the stacks are evenly distributed across the shards, writes will be evenly distributed.
Shard Key Rules and Guidelines There are several practical restrictions to be aware of before choosing a shard key.
Determining which key to shard on and creating shard keys should be reminiscent of indexing because the two concepts are similar.
In fact, often your shard key may just be the index you use most often (or some variation on it)
Once inserted, a document’s shard key value cannot be modified.
To change a document’s shard key, you must remove the document, change the key, and reinsert it.
Thus, you should choose a field that is unchangeable or changes infrequently.
Most special types of index cannot be used for shard keys.
Using a hashed index for a shard key is allowed, as covered previously.
Shard Key Cardinality Whether your shard key jumps around or increases steadily, it is important to choose a key with values that will vary.
As with indexes, sharding performs better on high cardinality fields.
If, for example, we had a "logLevel" key that had only values "DE BUG", "WARN", or "ERROR", MongoDB wouldn’t be able to break up your data into more than three chunks (because there would be only three different values for the shard key)
If you have a key with little variation and want to use it as a shard key anyway, you can do so by creating a compound shard key on that key and a key that varies more, like "logLevel" and "timestamp"
It is important that the combination of keys has high cardinality.
Controlling Data Distribution Sometimes, automatic data distribution will not fit your requirements.
This section gives you some options beyond choosing a shard key and allowing MongoDB to do everything automatically.
As your cluster gets larger or busier, these solutions become less practical.
Using a Cluster for Multiple Databases and Collections MongoDB evenly distributes collections across every shard your cluster, which works well if you’re storing homogeneous data.
However, if you have a log collection that is “lower-value” than your other data, you might not want it taking up space on your more expensive servers.
Or, if you have one powerful shard, you might want to use it for only a realtime collection and not allow other collections to use it.
You can set up separate clusters, but you can also give MongoDB specific directions about where you want it to put certain data.
This says, “for negative infinity to infinity for this collection, store it on shards tagged ‘high’.” This means that no data from the important collection will be stored on any other server.
Note that this does not effect how other collections are distributed: other collections will still be evenly distributed between this shard and the others.
Assigning a tag range to a collection does not affect it instantly.
It is an instruction to the balancer that, when it runs, these are the viable targets to move the collection to.
As another example, perhaps we have a collection where we don’t want it on the shard tagged “high” but do want it on any other one.
Now we can specify that we want this collection (call it "normal.coll") distributed across these five shards:
You cannot assign collections dynamically, i.e., “when a collection is created, randomly home it to a shard.” However, you could have a cron job that went through and did it for you.
If you make a mistake or change your mind, you can remove shard tags with sh.remov eShardTag():
If you remove all tags described by a tag range (for example, if untagging the shard marked "high") the balancer won’t distribute the data anywhere because there aren’t any valid locations listed.
All the data will still be readable and writable; it just won’t be able to migrate until you modify your tags or tag ranges.
There is no helper for removing tag ranges, but you can do so manually.
To deal with tag ranges manually, access the config.tags namespace through the mongos.
Similarly, shard tag information is kept in the config.shards namespace in the "tags" field of each shard document.
If a shard has no "tags" field, then it has no tags.
Manual Sharding Sometimes, for complex requirements or special situations, you may prefer to have complete control over which data is distributed where.
You can turn off the balancer if you don’t want data to be automatically distributed and use the moveChunk command to manually distribute data.
To turn off the balancer, connect to a mongos (any mongos is fine) and update the config.settings namespace with the following:
Note that this is an upsert: it creates the balancer setting if one does not exist.
If there is currently a migrate in progress, this setting will not take effect until the migrate has completed.
However, once any in-flight migrations have finished, the balancer will stop moving data around.
Once the balancer is off, you can move data around manually (if necessary)
First, find out which chunks are where by looking at config.chunks:
Now, use the moveChunk command to migrate chunks to other shards.
However, unless you are in an exceptional situation, you should use MongoDB’s automatic sharding instead of doing it manually.
If you end up with a hotspot on a shard that you weren’t expecting, you might end up with most of your data on that shard.
In particular, do not combine setting up unusual distributions manually with running the balancer.
If the balancer detects an uneven number of chunks it will simply reshuffle all of your work to get the collection evenly balanced again.
A sharded cluster is the most difficult type of deployment to administer.
This chapter gives advice on performing administrative tasks on all parts of a cluster, including:
Inspecting what the cluster’s state is: who its members are, where data is held, and what connections are open.
How to add, remove, and change members of a cluster.
Seeing the Current State There are several helpers available to find out what data is where, what the shards are, and what the cluster is doing.
Getting a Summary with sh.status sh.status() gives you an overview of your shards, databases, and sharded collections.
If you have a small number of chunks, it will print a breakdown of which chunks are where as well.
Otherwise it will simply give the collection’s shard key and how many chunks each shard has:
Once there are more than a few chunks, sh.status() will summarize the chunk stats instead of pinting each chunk.
To see all chunks, run sh.status(true) (the true tells sh.status() to be verbose)
All the information sh.status() shows is gathered from your config database.
Seeing Configuration Information All of the configuration information about your cluster is kept in collections in the config database on the config servers.
You can access it directly, but the shell has several helpers for exposing this information in a more readable way.
However, you can always directly query the config database for metadata about your cluster.
Never connect directly to your config servers, as you do not want to take the chance of accidentally changing or removing config server data.
Instead, connect to the mongos and use the config database to see its data, as you would for any other database:
If you manipulate config data through mongos (instead of connecting directly to the config servers), mongos will ensure that all of your config servers stay in sync and prevent various dangerous actions like accidentally dropping the config database.
In general, you should not directly change any data in the config database (exceptions are noted below)
If you do change anything, you will generally have to restart all of your mongos servers to see its effect.
This section covers what each one contains and how it can be used.
The shards collection keeps track of all the shards in the cluster.
A typical document in the shards collection might looks something like this:
The shard’s "_id" is picked up from the replica set name, so each replica set in your cluster must have a unique name.
When you update your replica set configuration (e.g., adding or removing members), the "host" field will be updated automatically.
The databases collection keeps track of all of the databases, sharded and non, that the cluster knows about:
If enableSharding has been run on a database, "partitioned" will be true.
The "pri mary" is the database’s “home base.” By default, all new collections in that database will be created on the database’s primary shard.
The collections collection keeps track of all sharded collections (non-sharded collections are not shown)
In this case, it is a compound key on "x" and "y"
This field is not displayed unless it is true (the shard key is unique)
The chunks collection keeps a record of each chunk in all the collections.
A typical document in the chunks collection might look something like this:
Generally this is the namespace, shard key, and lower chunk boundary.
All values in the chunk are smaller than this value.
The "lastmod" and "lastmodEpoch" fields are used to track chunk versioning.
Thus, the "t" and "i" fields are the major and minor chunk versions: major versions change when a chunk is migrated to a new shard and minor versions change when a chunk is split.
The changelog collection is useful for keeping track of what a cluster is doing, since it records all of the splits and migrates that have occurred.
The "details" give information about what the original document looked like and what it split into.
This output is what the first chunk split of a collection looks like.
Migrates are a bit more complicated and actually create four separate changelog documents: one noting the start of the migrate, one for the "from" shard, one for the "to" shard, and one for the migrate’s commit (when the migration is finalized)
The middle two documents are of interest because these give a breakdown of how long each step in the process took.
This can give you an idea whether it’s the disk, network, or something else that is causing a bottleneck on migrates.
Each of the steps listed in "details" is timed and the "stepN of 5" messages show how long the step took, in milliseconds.
When the "from" shard receives a moveChunk command from the mongos, it:
Coordinates with the "to" shard and config servers to confirm the migrate.
If the "from" server has flaky network connectivity during the final steps, it may end up in a state where it cannot undo the migrate and cannot move forward with it.
The "to" shard’s changelog document is similar to the "from" shard’s, but the steps are a bit different.
When the "to" shard receives a command from the "from" shard, it:
If this shard has never held chunks from the migrated collection before, it needs to know what fields are indexed.
If this isn’t the first time a chunk from this collection is being moved to this shard, then this should be a no-op.
There might be data left over from a failed migration or restore procedure which we wouldn’t want to interfere with the current data.
Copy all documents in the chunk to the "to" shard.
This collection is created if you configure shard tags for your system.
This collection contains documents representing the current balancer settings and chunk size.
By changing the documents in this collection, you can turn the balancer on.
Note that you should always connect to mongos, not the config servers directly, to change values in this collection.
Tracking Network Connections There are a lot of connections between the components of a cluster.
Getting Connection Statistics There is a command, connPoolStats, for finding out connection information about mongoses and mongods.
This gives you information about how many connections a server has open, and to what:
The "available" counts are how many connections are currently available in the connection pools on this instance.
Note that this command only works on mongos processes and mongods that are members of a shard.
You may see connections to other shards in the output of connPoolStats from a shard, as shards connect to other shards to migrate data.
The primary of one shard will connect directly to the primary of another shard and “suck” its data.
When a migrate occurs, a shard sets up a ReplicaSetMonitor (a process that monitors replica set health) to track the health of the shard on the other side of the migrate.
This is totally normal and should have no effect on your application.
Limiting the Number of Connections When a client connects to a mongos, mongos creates a connection to at least one shard to pass along the client’s request.
Thus, every client connection into a mongos yields at least one outgoing connection from mongos to the shards.
To prevent this, you can use the maxConns option to your command line configuration for mongos to limit the number of connections it can create.
The following formula can be used to calculate the maximum number of connections a shard can handle from a single mongos:
Each mongos creates three connections per mongod: a connection to forward client requests, an error-tracking connection (the writeback listener), and a connection to monitor the replica set’s status.
Note that maxConns only prevents mongos from creating more than this many connections.
It doesn’t mean that it does anything particularly helpful when it runs out of connections: it will block requests, waiting for connections to be “freed.” Thus, you must.
When a MongoDB instance exits cleanly it closes all connections before stopping.
The members who were connected to it will immediately get socket errors on those connections and be able to refresh them.
However, if a MongoDB instance suddenly goes offline due to a power loss, crash, or network problems, it probably won’t cleanly close all of its sockets.
In this case, other servers in the cluster may be under the impression that their connection is healthy until they try to perform an operation on it.
At that point, they will get an error and refresh the connection (if the member is up again at that point)
This is a quick process when there are only a few connections.
However, when there are thousands of connections that must be refreshed one by one you can get a lot of errors because each connection to the downed member must be tried, determined to be bad, and re-established.
There isn’t a particularly good way of preventing this aside from restarting processes that get bogged down in a reconnection storm.
Server Administration As your cluster grows, you’ll need to add capacity or change configurations.
This section covers how to add and remove servers from your cluster.
Adding Servers You can add new mongos processes at any time.
Make sure their --configdb option specifies the correct set of config servers and they should be immediately available for clients to connect to.
Changing Servers in a Shard As you use your sharded cluster, you may want to change the servers in individual shards.
To change a shard’s membership, connect directly to the shard’s primary (not through the mongos) and issue a replica set reconfig.
The cluster configuration will pick up the change and update config.shards automatically.
The only exception to this is if you started your cluster with standalone servers as shards, not replica sets.
Changing a shard from a standalone server to replica set.
The easiest way to do this is to add a new, empty replica set shard and then remove the standalone server shard (see “Removing a Shard” on page 286)
If you wish to turn the standalone server into a replica set the process is fairly complex and involves downtime:
Shut down the standalone server (call it server-1) and all mongos processes.
Restart the server-1 in replica set mode (with the --replSet option)
Connect to server-1 and initiate the set as a one-member replica set.
It is risky to manually edit config servers! A good way of ensuring that they are identical is to run the dbhash command on each config server:
This comes up with an MD5 sum for each collection.
Some collections in the config database will be different on different config servers, but config.shards should not be.
They will read the shard data off of the config servers at start up and treat the replica set as a shard.
If at all possible, just add a new shard that’s an empty replica set and let migrations take care of moving your data to it.
Removing a Shard In general, shards should not be removed from a cluster.
If you are regularly adding and removing shards, you are putting a lot more stress on the system than necessary.
If you add too many shards it is better to let your system grow into it, not remove them and add them back later.
The balancer will be tasked with moving all the data on this shard to other shards in a process called draining.
Draining can take a long time if there are a lot of chunks or large chunks to move.
If you have jumbo chunks (see “Jumbo Chunks” on page 292), you may have to temporarily raise the chunk size to allow draining to move them.
If you want to keep tabs on how much has been moved, run removeShard again to give you the current status:
You can run removeShard as many times as you want.
Chunks may have to split to be moved, so you may see the number of chunks increase in the system during the drain.
For example, suppose we have a 5-shard cluster with the following chunk distributions:
Once all the chunks have been moved, if there are still databases “homed” on the shard, you’ll need to remove them before the shard can be removed.
This is not strictly necessary, but it confirms that you have completed the process.
If there are no databases homed on this shard, you will get this response as soon as all chunks have been migrated off.
Once you have started a shard draining, there is no built-in way to stop it.
Changing Config Servers Changing anything about your config servers is difficult, dangerous, and generally involves downtime.
Before doing any maintenance on config servers, take a backup.
All mongos processes need to have the same value for --configdb whenever they are running.
Thus, to change the config servers, you must shut down all your mongos processes, make sure they are all down (no mongos process can still be running with the old --configdb argument), and then restart them with the new --configdb argument.
For example, one of the most common tasks is to move from one config server to three.
To accomplish this, shut down your mongos processes, your config server, and all your shards.
Copy the data directory of your config servers to the two new config servers (so that there is an identical data directory on all three servers)
Now, start up all three config servers and the shards.
Then start each of the mongos processes with --configdb pointing to all three config servers.
Balancing Data In general, MongoDB automatically takes care of balancing data.
This section covers how to enable and disable this automatic balancing as well as how to intervene in the balancing process.
The Balancer Turning off the balancer is a prerequisite to nearly any administrative activity.
With the balancer off a new balancing round will not begin, but it will not force an ongoing balancing round to stop immediately: migrations generally cannot stop on a dime.
Thus, you should check the config.locks collection to see whether or not a balancing round is still in progress:
See “The Balancer” on page 253 for an explanation of the balancer states.
Balancing puts load on your system: the destination shard must query the source shard for all the documents in a chunk, insert them, and then the source shard must delete them.
There are two circumstances in particular where migrations can cause performance problems:
Using a hotspot shard key will force constant migrations (as all new chunks will be created on the hotspot)
Your system must have the capacity to handle the flow of data coming off of your hotspot shard.
Adding a new shard will trigger a stream of migrations as the balancer attempts to populate it.
If you find that migrations are affecting your application’s performance, you can schedule a window for balancing in the config.settings collection.
Run the following update to only allow balancing between 1 p.m.
If you set a balancing window, monitor it closely to ensure that mongos can actually keep your cluster balanced in the time that you have allotted it.
You must be careful if you plan to combine manual balancing with the automatic balancer, since the automatic balancer always determines what to move based on the current state of the set and does not take into account the set’s history.
For example, suppose you have shardA and shardB, each holding 500 chunks.
To prevent this, move 30 quiescent chunks from shardB to shardA before starting the balancer.
That way there will be no imbalance between the shards and the balancer will be happy to leave things as they are.
Alternatively, you could perform 30 splits on shardA’s chunks to even out the chunk counts.
Note that the balancer only uses number of chunks as a metric, not size of data.
Thus, a shard with a few large chunks may end up as the target of a migration from a shard with many small chunks (but a smaller data size)
Changing Chunk Size There can be anywhere from zero to millions of documents in a chunk.
Generally, the larger a chunk is, the longer it takes to migrate to another shard.
MongoDB would be doing a lot of unnecessary work to keep shards within a few megabytes of each other in size.
By default, chunks are 64 MB, which is generally a good balance between ease of migration and migratory churn.
Sometimes you may find that migrations are taking too long with 64 MB chunks.
To speed them up, you can decrease your chunk size.
To do this, connect to mongos through the shell and update the config.settings collection:
The previous update would change your chunk size to 32 MB.
Existing chunks would not be changed immediately, but as splits occurred chunks would trend toward that size.
Note that this is a cluster-wide setting: it affects all collections and databases.
Thus, if you need a small chunk size for one collection and a large chunk size for another, you may have to compromise with a chunk size in between the two ideals (or put the collections in different clusters)
If MongoDB is doing too many migrations or your documents are large, you may want to increase your chunk size.
Moving Chunks As mentioned earlier, all the data in a chunk lives on a certain shard.
If that shard ends up with more chunks than the other shards, MongoDB will move some chunks off it.
Moving a chunk is called a migration and is how MongoDB balances data across your cluster.
You must use the shard key to find which chunk to move ("user_id", in this case)
Generally, the easiest way to specify a chunk is by its lower bound, although any value in the chunk will work (the upper bound will not, as it is not actually in the chunk)
This command will move the chunk before returning, so it may take a while to run.
The logs are the best place to see what it is doing if it takes a long time.
In this case, you must manually split the chunk before moving it, using the splitAt command:
Once the chunk has been split into smaller pieces, it should be movable.
Alternatively, you can raise the max chunk size and then move it, but you should break up large chunks whenever possible.
Sometimes, though, they cannot be broken up: these are called jumbo chunks.
Jumbo Chunks Suppose you choose the "date" field as your shard key.
The "date" field in this collection is a string that looks like "year/month/day", which means that mongos can create at most one chunk per day.
This works fine for a while, until your application suddenly goes viral and gets a thousand times its typical traffic for one day.
This day’s chunk is going to be much larger than any other day’s, but it is also completely unsplittable because every document has the same value for the shard key.
Once a chunk is larger than the max chunk size set in config.settings, the balancer will not be allowed to move the chunk.
These unsplittable, unmovable chunks are called jumbo chunks and they are inconvenient to deal with.
But the only chunks that the balancer can move are the non-jumbo chunks, so it will migrate all the small chunks off the hotspot shard.
Thus, shard1 will fill up a lot faster than the other two shards, even though the number of chunks is perfectly balanced between the three.
Thus, one of the indicators that you have jumbo chunk problems is that one shard’s size is growing much faster than the others.
You can also look at sh.status() to see if you have jumbo chunks: they will be marked with a "jumbo" attribute:
You can use the dataSize command to check chunk sizes.
Be careful—the dataSize command does have to scan the chunk’s data to figure out how big it is.
If you can, narrow down your search by using your knowledge of your data: were jumbo chunks created on certain date? For example, if there was a really busy day on November 1, look for chunks with that day in their shard key range.
If you’re using GridFS and sharding by "files_id", you can look at the fs.files collection to find a file’s size.
To fix a cluster thrown off-balance by jumbo chunks, you must evenly distribute them among the shards.
This is a complex manual process, but should not cause any downtime (it may cause slowness, as you’ll be migrating a lot of data)
In the description below, the shard with the jumbo chunks is referred to as the “from” shard.
The shards that the jumbo chunks are migrated to are called the “to” shards.
Note that you may have multiple “from” shards that you wish to move chunks off of.
You don’t want to the balancer trying to “help” during this process:
MongoDB will not allow you to move chunks larger than the max chunk size, so temporarily raise the chunk size.
Make a note of what your original chunk size is and then change it to something large, like 10,000
Use the moveChunk command to move jumbo chunks off the “from” shard.
If you are concerned about the impact migrations will have on your application’s performance, use the secondaryThrottle option to prevent them from happening too quickly:
It only works if you are running with shards that are replica sets (not standalone servers)
Run splitChunk on the remaining chunks on the donor shard until it has a roughly even number of chunks as the other shards.
When the balancer is turned on again it cannot move the jumbo chunks again, as they are essentially held in place by their size.
As the amount of data you are storing grows, the manual process described in the previous section becomes unsustainable.
Thus, if you’re having problems with jumbo chunks, you should make it a priority to prevent them from forming.
To prevent jumbo chunks, modify your shard key to have more granularity.
You want almost every document to have a unique value for the shard key, or at least never have more than chunksize-worth of data with a single shard key value.
For example, if you were using the year/month/day key described earlier it can quickly be made finer-grained by adding hours, minutes, and seconds.
Similarly, if you’re sharding on something coarsely-grained key like log level, add a second field to your shard key with a lot of granularity, such as an MD5 hash or UUID.
Then you can always split a chunk, even if the first field is the same for many documents.
Refreshing Configurations As a final tip, sometimes mongos will not update its configuration correctly from the config servers.
If you ever get a configuration that you don’t expect or a mongos seems to be out of date or cannot find data that you know is there, use the flushRouterConfig command to manually clear all caches:
If flushRouterConfig does not work, restarting all your mongos or mongod processes clears any possible cache.
Once you have an application up and running, how do you know what it’s doing? This chapter covers how to figure out what kind of queries MongoDB is running, how much data is being written, and how to investigate what MongoDB is actually doing.
Using command-line tools to give you a picture of what MongoDB is doing.
Seeing the Current Operations An easy way to find slow operations is to see what is running.
Anything slow is more likely to show up and have been running for longer.
It’s not guaranteed, but it’s a good first step to see what might be slowing down an application.
This displays a list of operations that the database is performing.
Here are some of the more important fields in the output: "opid"
You can use this number to kill an operation (see “Killing Operations” on page 301)
If this field is false, it means the operation has yielded or is waiting for a lock.
You can use this to find queries that are taking too long or sucking up database resources.
Every log message related to this connection will be prefixed with [conn3], so you can use this to grep the logs for relevant information.
This describes the types of locks taken by this operation.
Whether this operation is currently blocking, waiting to acquire a lock.
The number of times this operation has yielded, releasing its lock to allow other operations to go.
Generally, any operation that searches for documents (queries, updates, and removes) can yield.
An operation will only yield if there are other operations enqueued and waiting to take its lock.
Basically, if there are no operations in "waitingForLock" state, the current operations will not yield.
This shows how long it took this operation to acquire the locks it needed.
You can filter currentOp() to only look for operations fulfilling certain criteria, such as operations on a certain namespace or ones that have been running for a certain length of time.
You can query on any field in currentOp, using all the normal query operators.
Finding Problematic Operations The most common use for db.currentOp() is looking for slow operations.
You can use the filtering technique described in the previous section to find all queries that take longer than a certain amount of time, which may suggest a missing index or improper field filtering.
Sometimes people will find that unexpected queries are running, generally because there’s an app server running an old or buggy version of software.
The "client" field can help track down where unexpected operations are coming from.
Killing Operations If you find an operation that you want to stop, you can kill it by passing db.killOp() its "opid":
In general, operations can only be killed when they yield, so updates, finds, and removes can all be killed.
Operations holding or waiting for a lock usually cannot be killed.
Once you have sent a “kill” message to an operation, it will have a "killed" field in the db.currentOp output.
However, it won’t actually be dead until it disappears from list of current opertations.
False Positives If you look for slow operations, you may see some long-running internal operations listed.
There are several long-running requests MongoDB may have running, depending on your setup.
The most common are the replication thread (which will continue fetching more operations from the sync source for as long as possible) and the writeback listener for sharding.
Any long-running query on local.oplog.rs can be ignored as well as any writebacklistener commands.
If you kill either of these operations, MongoDB will just restart them.
Killing the replication thread will briefly halt replication and killing the writeback listener may cause mongos to miss legitimate write errors.
Preventing Phantom Operations There is an odd, MongoDB-specific issue that you may run into, particularly if you’re bulk-loading data into a collection.
Suppose you have a job that is firing thousands of update operations at MongoDB and MongoDB is grinding to a halt.
You quickly stop the job and kill off all the updates that are currently occurring.
However, you continue to see new updates appearing as soon as you kill the old ones, even though the job is no longer running!
If you are loading data using unacknowledged writes, your application will fire writes at MongoDB, potentially faster than MongoDB can process them.
If MongoDB gets backed up, these writes will pile up in the operating system’s socket buffer.
When you kill the writes MongoDB is working on, this allows MongoDB to start processing the writes in the buffer.
Even if you stop the client sending the writes, any writes that made it into the buffer will get processed by MongoDB, since they’ve already been “received” (just not processed)
The best way to prevent these phantom writes is to do acknowledged writes: make each write wait until the previous write is complete, not just until the previous write is sitting in a buffer on the database server.
Using the System Profiler To find slow operations you can use the system profiler, which records operations in a special system.profile collection.
The profiler can give you tons of information about operations that are taking a long time, but at a cost: it slows down mongod’s overall performance.
Thus, you may only want to turn on the profiler periodically to capture.
If your system is already heavily loaded, you may wish to use another technique described in this chapter to diagnose issues.
By default, the profiler is off and does not record anything.
Level 2 means “profile everything.” Every read and write request received by the database will be recorded in the system.profile collection of the current database.
Profiling is enabled per-database and incurs a heavy performance penalty: every write has to be written an extra time and every read has to take a write lock (because it must write an entry to the system.profile collection)
However, it will give you an exhaustive listing of what your system is doing:
You can use the "client" field to see which users are sending which operations to the database.
If we were using authentication, we could see which user was doing each operation, too.
Often, you do not care about most of the operations that your database is doing, just the slow ones.
For this, you can set the profiling level to 1: profile only slow operations.
You can also specify a second argument, which defines what “slow” means to you.
This would record all operations that took longer than 500 ms:
Even with profiling off, slowms has an effect on mongod: it sets the threshold printing slow operation in the log.
Thus, if you lower slowms to profile something, you might want to raise it again before turning off profiling.
The profiling level is not persistent: restarting the database clears the level.
There are command-line options for configuring the profiling level, --profile lev el and --slowms time, but bumping the profiling level is generally a temporary debugging measure, not something you want to add to your configuration long-term.
If you turn on profiling and the system.profile collection does not already exist, MongoDB creates a small capped collection for it (a few megabytes in size)
If you want to run the profiler for an extended period of time, this may not be enough space for the number of operations you need to record.
You can make a larger system.profile collection by turning off profiling, dropping the system.profile collection, and creating a new system.profile capped collection that is the size you desire.
Calculating Sizes In order to provision the correct amount of disk and RAM, it is useful to know how much space documents, indexes, collections, and databases are taking up.
See “Calculating the Working Set” on page 348 for information on calculating your working set.
Pass in any document to get the size it would be when stored in MongoDB.
For example, you can see that storing _ids as ObjectIds is more efficient than storing them as strings:
This shows you exactly how many bytes a document is taking up on disk.
However, this does not count padding or indexes, which can often be significant factors in the size of a collection.
Collections For seeing information about a whole collection, there is a stats function:
The next couple of fields have to do with the size of the collection.
Equivalently, if you take the "avgObjSize" and multiply it by "count", you’ll get "size"
As mentioned above, a total count of the documents’ bytes leaves out some important space a collection uses: the padding around each document and the indexes.
Collections always have empty space at the “end” so that new documents can be added quickly.
An index is not counted in "nindexes" until it finishes being built and cannot be used until it appears in this list.
Each index is currently one “bucket” (8 KB), since the collection is so small.
In general, indexes will be a lot larger than the amount of data they store, as there is a lot of free space to optimize adding new entries.
You can minimize this free space by having rightbalanced indexes (as described in “Introduction to Compound Indexes” on page 84)
As your collections get bigger, it may become difficult to read stats() output with sizes in the billions of bytes or beyond.
For example, this would get the collection stats in terabytes:
Databases Databases have a stats function that’s similar to collections’:
First, we have the name of the database and the number of collections it contains.
The bulk of the document contains information about the size of your data.
This number should be equivalent to adding the sizes of all of the brains.* files in your data directory.
The next largest field is generally going to be "storageSize", which is the total amount of space the database is using.
You should always have an empty (full of zeros) file for each database: as soon as it’s written to, the next file will be preallocated.
Thus, this empty file (plus anything not being used yet in the previous files) is the difference between "fileSize" and "storageSize"
Note that this does not include space on the free list, but it does include documents’ padding.
Thus, the difference between this and the "storageSize" should be the size of documents deleted.
This is the size of the .ns file, which is essentially a database’s table of contents.
Keep in mind that listing databases on a system with a high lock percent can be very slow and block other operations.
Using mongotop and mongostat MongoDB comes with a few command-line tools that can help you determine what MongoDB is doing by printing stats every few seconds.
You can also run mongotop --locks to give you locking statistics for each database.
By default, mongostat prints out a list of statistics once per second, although this is configurable by passing a different number of seconds on the command line.
Each of the fields gives a count of how many times the activity has happened since the field was last printed.
This is generally roughly the size of your data directory.
This is generally twice the size of your data directory (once for the mapped files, once again for journaling)
This should generally be as close as possible to all the memory on the machine.
It reports the percent of time the database was locked combined with how long the global lock was held, meaning that this value might be over 100%
It is how many index accesses had to page fault: the index entry (or section of index being searched) was not in memory, so mongod had to go to disk.
You can run mongostat on a replica set or sharded cluster.
If you use the --discover option, mongostat will try to find all the members of the set or cluster from the member it initially connects to and will print one line per server per second for each.
For a large cluster, this can get unmanageable fast, but it can be useful for small clusters and tools that can consume the data and present it in a more readable form.
This chapter covers how to administrate your collections and databases.
Generally the things covered in this section are not daily tasks but can be critically important for your application’s performance, for instance:
Setting Up Authentication One of the first priorities for systems administrators is to ensure their system is secure.
The best way to handle security with MongoDB is to run it in a trusted environment, ensuring that only trusted machines are able to connect to the server.
That said, MongoDB supports per-connection authentication, albeit with a fairly coarse-grained permissions scheme.
See http://docs.mongodb.org/manual/security for the most up-todate information about authentication and authorization.
Authentication Basics Each database in a MongoDB instance can have any number of users.
When security is enabled, only authenticated users of a database are able to perform read or write operations.
There are two special databases: users in the admin and local databases can perform operations on any database.
A user that belongs to either one of these databases can be thought of as a superuser.
After authenticating, admin users are able to read or write from any database and are able to perform certain admin-only commands, such as listDatabases or shutdown.
Before starting the database with security turned on, it’s important that at least one admin user has been added.
Let’s run through a quick example, starting from a shell connected to a server without authentication turned on:
Here we’ve added an admin user, root, and two users on the test database.
One of those users, "read_only", has read permissions only and cannot write to the database.
From the shell, a read-only user is created by passing true as the third argument to addUser.
To call addUser, you must have write permissions for the database in question; in this case we can call addUser on any database because we have not enabled security yet.
The addUser method is useful for more than just adding new users: it can be used to change a user’s password or read-only status.
Just call addUser with the username and a new password or read-only setting for the user.
Now let’s restart the server, this time adding the --auth command-line option to enable security.
After enabling security, we can reconnect from the shell and try it:
When we first connect, we are unable to perform any operations (read or write) on the test database.
After authenticating as the read_user user, however, we are able to perform a simple find.
When we try to insert data, we are again met with a failure because of the lack of authorization.
As a non-admin user, though, test_user is not able to list all the available databases using the show dbs helper.
The final step is to authenticate as an admin user, root, who is able to perform operations on any database.
Setting Up Authentication If authentication is enabled, clients must be logged in to perform any reads or writes.
However, there is one oddity in MongoDB’s authentication scheme: before you create a user in the admin database, clients that are “local” to the server can perform reads and writes on the database.
Generally, this is not an issue: create your admin user and use authentication normally.
With sharding, the admin database is kept on the config servers, so shard mongods have no idea it even exists.
Therefore, as far as they know, they are running with authentication enabled but no admin user.
Thus, shards will allow a local client to read and write from them without authentication.
Hopefully this wouldn’t be an issue: optimally your network will be configured so that only mongos processes are accessible to clients.
However, if you are worried about clients running locally on shards and connecting directly to them instead of going through the mongos, you may wish to add admin users to your shards.
Note that you do not want the sharded cluster to know about these admin users: it already has an admin database.
The admin databases you’re creating on the shards are for your use only.
To do this, connect to the primary of each shard and run the addUs er() function:
Make sure that the replica sets you create users on are already shards in the cluster.
If you create an admin user and then try to add the mongods as a shard the addShard command will not work (because the cluster already contains an admin database)
Knowing where and how user information is stored makes performing some common administration tasks trivial.
For example, to remove a user, simply remove the user document from the system.users collection:
When a user authenticates, the server keeps track of that authentication by tying it to the connection used for the authenticate command.
Creating and Deleting Indexes Chapter 5 covered what commands to run to create an index, but it didn’t go into the operational aspects of doing so.
Creating an index is one of the most resource-intensive operations you can do on a database, so schedule index creations carefully.
Building an index requires MongoDB to find the indexed field (or lack thereof) in every document in the collection, then sort all the values found.
As you might imagine, this becomes a very expensive task as your collection gets bigger.
Thus, indexing should be done in a way that affects your production server as little as possible.
Creating an Index on a Standalone Server On a standalone server, build the index in the background during an off-time.
There isn’t much else you can do to minimize impact.
To build an index in the background, run ensureIndex with the "background" : true option:
Any type of index can be built in the background.
A foreground index build takes less time than a background index build but locks the database for the duration of the process.
Thus, no other operations can read or write to the database while a foreground index is building.
Background indexes yield the write lock regularly to allow other operations to go.
This means that they can take longermuch longer on write-heavy servers—but the server can serve other clients while building an index.
Creating an Index on a Replica Set The easiest way to create an index on a replica set is to create it on the primary and wait for it to be replicated to the secondaries.
If you have a large collection, this can lead to the situation where all of your secondaries start building the index at the same time.
Suddenly all of your secondaries will be unavailable for client reads and may fall behind in replication.
When you have finished this process, only the primary should be left without an index.
Now there are two options: you can either build the index on the primary in the background (which will put more strain on the primary) or you can step down the primary and then follow steps 1 through 4 to build the index on it as you did with the other members of the set.
This involves a failover, which may be more or less preferable than adding load to the primary.
You can also use this isolate-and-build technique to build an index on a member of the set that isn’t configured to build indexes (one that has the "buildIndexes" : false option set): start it as a standalone member, build the index, and add it back into the set.
Creating an Index on a Sharded Cluster To create indexes on a sharded cluster, we want to follow the same procedure as described for replica sets and build the index on one shard at a time.
Then follow the procedure outlined in the previous section for each shard, treating it as a individual replica set.
Finally, run ensureIndex through the mongos and turn the balancer back on again.
This procedure is only required for adding an index to existing shards: new shards will pick up on the index when they start receiving chunks from a collection.
Removing Indexes If an index is no longer necessary, you can remove it with the dropIndexes command and the index name.
Query the system.indexes collection to figure out what the index name is, as even the autogenerated names vary somewhat from driver to driver:
You can drop all indexes on a collection by passing in "*" as the value for the "index" key:
The only way to get rid of that one is to drop the whole collection.
Removing all the documents in a collection (with remove) does not affect the indexes; they will be repopulated when new documents are inserted.
Beware of the OOM Killer The Linux out-of-memory killer will kill processes that are using a lot of memory.
Because of the way MongoDB uses memory, it is not usually an issue, but index creations are the one time it can be.
If you are building an index and your mongod suddenly disappears, check /var/log/messages for notices from the OOM killer.
Running a background index build or adding some swap space can prevent this.
If you have administrative permissions on the machine, you may want to make MongoDB unkillable.
See “The OOM Killer” on page 383 for more information.
Preheating Data When you restart a machine or bring up a new server, it can take a while for MongoDB to get all the right data from disk into memory.
If your performance constraints require that data be in RAM, it can be disastrous to bring a new server online and then let your application hammer it while it gradually pages in the data it needs.
There are a couple of ways to get your data into RAM before officially bringing the server online, to prevent it from messing up your application.
Thus, if the mongod process needs to be restarted it should not affect what data is in memory.
However, mon god will report low resident memory until it has a chance to ask the OS for the data it needs.
Moving Databases into RAM If you need a database in RAM, you can use the UNIX dd tool to load it before starting the mongod:
Replace brains with the name of the database you want to load.
If you load a database or set of databases into memory and it takes up more memory than you have, some of its data will fall back out of memory immediately.
When you start the mongod, it will ask the operating system for the data files and the operating system, knowing that the data files are in memory, will be able to quickly access it.
However, this technique is only helpful if your entire database fits in memory.
If it does not, you can do more fine-grained preheating using the following techniques.
Moving Collections into RAM MongoDB has a command for preheating data called touch.
Start mongod (perhaps on a different port or firewalled off from your application) and touch a collection to load it into memory:
This will load all the documents and indexes into memory for the logs collection.
You can specify to only load documents or only load indexes.
Once touch completes, you can allow your application to access MongoDB.
However, an entire collection (even just the indexes) can still be too much data.
For example, your application might only require one index to be in memory, or only a small fraction of the documents.
In that case, you’ll have to custom preheat the data.
Custom-Preheating When you have more complex preheating needs, you may have to roll your own preheating scripts.
Here are some common preheating requirements and how to deal with them: Load a specific index.
The explain forces mongod to iterate through all of the results for you.
You must specify that you only want to return the indexed fields (the second argument to find) or the query will end up loading all the documents into memory, too (which you may want, but it’s good to be aware of)
Note that this will always load the index and documents into memory for indexes that cannot be covered, such as multikey indexes.
Load recently updated documents If you have an index on a date field that you update when you update the document, you can query on recent dates to load recent documents.
If you don’t have an index on the date field, this query will end up loading all documents in the collection into memory, so don’t bother.
If you don’t have a date field, you might be able to use the "_id" field if you’re mostly concerned with recent inserts (see below)
Load recently created documents If you are using ObjectIds for your "_id" field, you can use the fact that recently created documents contain a timestamp to query for them.
For example, suppose we wanted to find all documents created in the last week.
We could create an "_id" that was less than every ObjectId created in the last week:
Replace year, month, and date appropriately and this gives you the date, in seconds.
Now we need to get an ObjectId from this time.
First, convert it into a hex string, then append 16 zeros to it:
This will load all the docs (and some right-hand branches of the "_id" index) from the last week.
Replay application usage MongoDB has a facility for recording and replaying traffic called the diaglog.
Enabling the diaglog incurs a performance penalty, so it is best to use it temporarily to gather a “representative” slice of traffic.
To gather traffic, run the following command in the mongo shell:
You probably don’t want to capture writes because you don’t want extra writes applied to your new member when you replay the diaglog.
Now let the diaglog record operations by letting mongod run for however long you want while sending it traffic.
Reads will be stored in the diaglog file in the data directory.
To use your diaglog files, start up your new server and, from the server where the diaglog files live, run:
This sends the recorded operations to hostname:27017 as a series of normal queries.
Note that the diaglog will capture the command turning on the diaglog, so you’ll have to log into the server and turn it off when you’re done replaying the diaglog (you also might want to delete the diaglog files it generates from the replay)
These techniques can be combined: you could load a couple of indexes while replaying the diaglog, for example.
You can also run them all at the same time if you aren’t bottlenecked on disk IO, either through multiple shells or the startParallelShell command (if the shell is local to the mongod):
Replace port with the port on which mongod is running.
Sometimes, if you have deleted or updated a lot of data, you’ll end up with collection fragmention.
Fragmentation occurs when your data files have a lot of empty space that MongoDB can’t reuse because the individual chunks of free space are too small.
In this case, you’ll see messages like this in the log:
However, it means that an entire extent had no documents in it.
To get rid of empty extents and repack collections efficiently, use the compact command:
Thus, the recommended procedure is similar to that of building indexes: compact data on each of the secondaries, then step down the primary and run the final compaction on it.
When you run a compact on a secondary it will drop into recovering state, which means that it will return errors if sent read requests and it cannot be used as a sync source.
When the compaction is finished, it will return to secondary state.
If you need a higher padding factor for the collection, you can specify it as an argument to compact:
This does not permanently affect the padding factor, just how MongoDB rearranges documents.
Once the compaction is finished, the padding factor will go back to whatever it was originally.
Compacting does not decrease the amount of disk space a collection uses: it just puts all of the documents at the “beginning” of a collection, on the assumption that the collection will expand again to use the available space.
Thus, compaction is only a brief reprieve if you are running out of disk space: it will not decrease the amount of disk space MongoDB is using, although it may make MongoDB not need to allocate new space for longer.
Repair makes a full copy of the data so you must have free space equal to the size of your current data files.
This is often annoying, as the most common reason to do a repair is that their machine is running low on disk space.
However, if you can mount another disk, you can specify a repair path, which is the directory (your newly-mounted drive) that repair should use for the new copy of the data.
Since it makes a totally clean copy of your data you can interrupt a repair at any time with no effect on your original data set.
If you run into the problem in the middle of a repair, you can delete the temporary repair files without affecting your actual data files.
To repair, start mongod with the --repair option (and --repairpath, if desired)
Moving Collections You can rename a collection using the renameCollection command.
This cannot move collections between databases, but it can change the collection’s name.
This operation is almost instantaneous, regardless of the size of the collection being renamed.
On busy systems, it can take a few nerve-wracking seconds, but it can be performed in production with no performance penalty:
You can optionally pass a second argument: what to do with the newName collection if it already exists.
The options are true (drop it) or false (the default: error out if it exists)
To move a collection to another database, you must either dump/restore it or manually copy the documents (do a find and iterate over the results, inserting them into the new database)
You cannot use cloneCollection move a collection within a mongod: it can only move collections between servers.
Preallocating Data Files If you know that your mongod will need certain data files, you can run the following script to preallocate them before your application goes online.
This can be especially helpful for the oplog (which you know will be a certain size) and any databases that you know will be a given size, at least for a while:
Store this in a file (say, preallocate), and make the file executable.
Go to your data directory and allocate the files that you need:
Once you start the database and it accesses the datafiles for the first time, you cannot delete any of the data files.
For example, if you allocated data files test.0 through test.
Once MongoDB is aware of them, it will be unhappy if they go away.
Durability is the guarantee that an operation that is committed will survive permanently.
MongoDB has highly configurable durability settings, from absolutely no guarantees to completely durable.
How to configure your application and server to give you the level of durability you.
MongoDB can ensure data integrity after crashes and hard shutdowns, assuming disk and software are behaving correctly.
Note that relational databases usually use durability to describe transaction persistence.
As MongoDB does not support transactions, it is used a bit differently here.
What Journaling Does As you perform writes, MongoDB creates a journal that contains the exact disk location and bytes changed for that write.
Thus, if the server suddenly stops, on startup the journal can be used to replay any writes that were not flushed to disk before the shutdown.
The numbers will continue to increase the longer MongoDB is running.
On clean shutdown, the journal files will be removed (as there is no need for them after a clean shutdown)
If there is a crash (or kill -9), mongod will replay its journal files on startup, spitting out a lot of lines of checksumming.
These lines are verbose and indecipherable, but they are an indication that everything is working as it should.
You may want to try running kill -9 on mongod in development so you know what to expect on restart if it happens in production.
Planning Commit Batches By default, MongoDB writes to the journal every 100 ms once a few megabytes of data have been written, whichever comes sooner.
This means that MongoDB commits changes in batches: every write isn’t flushed to disk immediately, but with the default settings you are unlikely to lose more than 100 ms of writes in the event of a crash.
However, this guarantee is not strong enough for some applications, so there are several ways to get stronger durability guarantees.
You can ensure a write has been durably written by passing the j option to getLastError.
Note that this means that, if you use "j" : true as an option on every write, your write speed will essentially be throttled to 33 writes/sec:
It generally doesn’t take this long to flush writes to disk, so you’ll find your write performance improves if you allow MongoDB to batch most writes instead of committing after every one.
Thus, if you have 50 important writes, you could use “normal” getLastError (without the j option) and then call it with the j option after the final write.
If that succeeds, you know that all 50 writes are flushed safely to disk.
If you have many connections with incoming writes, you can mitigate the speed penalty of using j by having many writes happen in parallel.
Setting Commit Intervals Another option for making journaling less intrusive is that you can shorten (or lengthen) the amount of time between journal commits.
Regardless of the interval set, calling getLastError with "j" : true will cut the time to a third of the time set.
If clients try to write faster than the journal can flush, mongod will block writes until the journal finishes writing to disk.
This is the only time that mongod will throttle writes.
Turning Off Journaling Journaling is recommended for all production deployments, but in some cases you may wish to turn it off.
Journaling impacts the speed with which MongoDB can write, even without the j option.
If value of the data is not worth the speed penalty, you may wish to disable journaling.
The downside to disabling journaling is that MongoDB has no way of ensuring data integrity after a crash.
After a crash without journaling, you should assume that your data is corrupt and must either be repaired or replaced.
You should not use data after a crash without journaling unless you don’t care if your database suddenly stops working.
Assuming that you would prefer your database to continue to work after a crash, there are a few options.
Delete all of the files in the data directory and get new ones: restore from a backup, take a snapshot of a known-clean member, or initial sync the server if it’s a member of a replica set.
If you have a replica set with a small amount of data, resyncing is probably your best option: stop the member (if it’s not already down), delete everything in its data directory, and start it back up again.
Repairing Data Files If you have no backups, no copies, and no other members of the set, you’ll need to make due with whatever data can be salvaged.
There are two repair tools that come with mongod: the repair built into mongod itself and a more hardcore repair built into mongodump.
The mongodump repair may find more of your data, but it takes a long time to do it (and the built-in repair is not exactly speedy)
Additionally, if you use mongodump’s repair, you’ll still need to restore the data before you’re ready to start up again.
Thus, you should judge how much time you’re willing to devote to data recovery and choose accordingly.
MongoDB will not start listening on 27017 when running a repair, but you can watch the log to see what it is doing.
To help with this (a bit), repair supports a --repairpath option.
This lets you mount an “emergency drive” and repair your data onto that if you don’t have enough space left on your primary disk.
If repair gets killed or errors out (say, runs out of disk space), you won’t be in any worse of a situation.
Repair writes all of its output to new files, not changing your original files until the very end, so your original data files will be in no worse shape than when you started the repair.
The other option is using the --repair option on mongodump.
Neither of these are particularly great options, but they should let you get mongod running with a clean dataset again.
The mongod.lock File There is a special file called mongod.lock in your data directory that is important when you’re running without journaling (if you are running with journaling, it should never come up)
When mongod exits it clears mongod.lock so that mongod knows on startup that it shut down cleanly.
Conversely, if the lock file was not cleaned out, mongod knows that it existed uncleanly.
If mongod detects that it previously exited uncleanly, it will not allow you to start up again so that you know you have to get a clean copy of your data.
However, some people have realized that you can get around this check by deleting the lock file before starting mongod.
Routinely deleting the lock file on startup means that you.
If it prevents mongod from starting, fix your data, and don’t delete mon god.lock.
Sneaky Unclean Shutdowns One important reason to not delete the lock file is that you may not even notice a hard crash.
Suppose that you reboot a machine for some routine maintenance.
The init scripts should take care of shutting down mongod before the server shuts down.
However, init systems will generally try shutting down a process gently and, if it doesn’t shut down after a couple of seconds, hard-kill it.
On a busy system, MongoDB can easily take 30 seconds to shut down: it is the unusual init script that will wait for it.
Thus, you may have more hard shutdowns than you’re aware of.
What MongoDB Does Not Guarantee There are a couple of situations where MongoDB cannot guarantee durability, such as if there are hardware issues or filesystem bugs.
In particular, if a hard disk is corrupt, there is nothing MongoDB can do to protect your data.
Also, different varieties of hardware and software may have different durability guarantees.
For example, some cheaper or older hard disks report a write’s success while the write is queued up to be written, not when it has actually been written.
MongoDB cannot defend against misreporting at this level: if the system crashes, data may be lost.
Basically, MongoDB is only as safe as the underlying system: if the hardware or filesystem destroys the data, MongoDB cannot prevent it.
If one machine fails, hopefully another will still be functioning correctly.
Checking for Corruption The validate command can be used to check a collection for corruption.
The main field you’re looking for is "valid", near the end, which will hopefully be true.
If it is not, validate will give some details about the corruption it found.
Most of the output from validate describes internal structures of the collection, which are not particularly useful for debugging.
Extents get larger as a collection expands, growing up to 2 GB in size.
It contains pointers to the previous and next extents ("xnext" and "xprev"), the extent’s size (note how much smaller it is than the last extent in the collection: the first extent will generally be quite small), and pointers to the first and last record in the extent.
The number of documents that have been removed from this collection during its lifetime.
The size of the free list (all of the free space available) for this collection.
This is not just documents that have been deleted but all the space that has been preallocated, as well.
You can only run validate on collections, not indexes, so you generally cannot tell if an index is corrupt unless you walk it.
Walk an index by running a query for every document in the collection hinting the desired index.
If you get an assertion about invalid BSONObj, this is usually corruption.
If the first element shown is garbage, there isn’t much you can do.
If the first element is viewable (as the ObjectId in this example is), you may be able to remove the corrupt document.
Note that this technique may not work if the corruption is not limited to that document: you may still have to repair.
Durability with Replication Due to the majority issues discussed in Chapter 10, a write to a replica set may be rolled back until it has been written to a majority of the set.
To put together the options here with the journaling ones above, you can say:
As of this writing, this only guarantees that the write has been written durably to the primary and has been written (not necessarily durably) to the secondaries.
Theoretically, it is possible for a majority of the servers to crash in the 100 ms between the write being written and it being journaled, in which case it would be rolled back on the current primary.
Unfortunately it is not trivial to fix, but there are a few open bugs to change this behavior.
In Chapter 2, we covered the basics of starting MongoDB.
This chapter will go into more detail about which options are important for setting up MongoDB in production, including:
Starting from the Command Line The MongoDB server is started with the mongod executable.
A couple of the options are widely used and important to be aware of: --dbpath.
Specify an alternate directory to use as the data directory; the default is /data/db/ (or, on Windows, \data\db\ on the MongoDB binary’s volume)
Each mongod process on a machine needs its own data directory, so if you are running three instances of mongod on one machine, you’ll need three separate data directories.
When mon god starts up, it creates a mongod.lock file in its data directory, which prevents any other mongod process from using that directory.
If you attempt to start another MongoDB server using the same data directory, it will give an error:
Specify the port number for the server to listen on.
By default, mongod uses port 27017, which is unlikely to be used by another process (besides other mongod processes)
If you would like to run more than one mongod process on a single machine, you’ll need to specify different ports for each one.
If you try to start mongod on a port that is already being used, it will give an error:
If you are starting up mongod for the first time (with an empty data directory), it can take the filesystem a few minutes to allocate database files.
The parent process will not return from forking until the preallocation is done and mongod is ready to start accepting connections.
You can tail the log to see what it is doing.
This option sends all output to the specified file rather than outputting on the command line.
This will create the file if it does not exist, assuming you have write permissions to the directory.
It will also overwrite the log file if it already exists, erasing any older log entries.
If you’d like to keep old logs around, use the -logappend option in addition to --logpath (highly recommended)
This allows you to mount different databases on different disks, if necessary or desired.
Common uses for this are putting a local database (replication) on its own disk or moving a database to a different disk if the original one fills up.
You could also put databases that handle more load on faster disks and lower on slower.
It basically gives you more flexibility to move things around later.
Use a configuration file for additional options not specified on the command line.
This is typically used to make sure options are the same between restarts.
For example, to start the server as a daemon listening on port 5586 and sending all output to mongodb.log, we could run this:
Note that mongod may decide to preallocate journal files before it considers itself “started.” If it does, fork will not return to the command prompt until the preallocation has.
You can tail mongodb.log (or wherever you redirected the log file) to watch its progress.
When you first install and start MongoDB, it is a good idea to look at the log.
This might be an easy thing to miss, especially if MongoDB is being started from an init script, but the log often contains important warnings that prevent later errors from occurring.
If you don’t see any warnings in the MongoDB log on startup, then you are all set.
If there are any warnings in the startup banner, take note of them.
MongoDB will warn you about a variety of issues: that you’re running on a 32-bit machine (which MongoDB is not designed for), that you have NUMA enabled (which can slow your application to a crawl), or that your system does not allow enough open file descriptors (MongoDB uses a lot of file descriptors)
The log preamble won’t change when you restart the database, so feel free to run MongoDB from an init script and ignore the logs, once you know what they say.
However, its a good idea to check again each time you do an install, upgrade, or recover from a crash, just to make sure MongoDB and your system are on the same page.
This collection can be useful for tracking upgrades and changes in behavior.
File-Based Configuration MongoDB supports reading configuration information from a file.
This can be useful if you have a large set of options you want to use or are automating the task of starting up MongoDB.
To tell the server to get options from a configuration file, use the -f or -config flags.
The options supported in a configuration file are exactly the same as those accepted at the command line.
This configuration file specifies the same options we used earlier when starting with regular command-line arguments.
It also highlights most of the interesting aspects of MongoDB configuration files:
Any text on a line that follows the # character is ignored as a comment.
The syntax for specifying options is option = value, where option is case-sensitive.
For command-line switches like --fork, the value true should be used.
Stopping MongoDB Being able to safely stop a running MongoDB server is at least as important as being able to start one.
There are a couple of different options for doing this effectively.
The shutdown command, when run on a primary, steps down the primary and waits for a secondary to catch up before shutting down the server.
This minimizes the chance of rollback, but the shutdown isn’t guaranteed to succeed.
If there is no secondary available that can catch up within a few seconds, the shutdown command will fail and the (former) primary will not shut down:
This is equivalent to sending a SIGINT or SIGTERM signal (all three of these options result in a clean shutdown, but there may be unreplicated data)
If the server is running as the foreground process in a terminal, a SIGINT can be sent by pressing Ctrl-C.
Otherwise, a command like kill can be used to send the signal.
When mongod receives a SIGINT or SIGTERM, it will do a clean shutdown.
This means it will wait for any running operations or file preallocations to finish (this could take a moment), close all open connections, flush all data to disk, and halt.
You should restrict access as tightly as possible between the outside world and MongoDB.
The best way to do this is to set up firewalls and only allow MongoDB to be reachable on internal network addresses.
Chapter 23 covers what connections are necessary to allow between MongoDB servers and clients.
Beyond firewalls, there are a few options you can add to your config file to make it more secure: --bind_ip.
Specify the interfaces that you want MongoDB to listen on.
Generally you want this to be an internal IP: something application servers and other members of your cluster can access but is inaccessible to the outside world.
For config servers and shards, they’ll need to be addressable from other machines, so stick with non-localhost addresses.
By default, MongoDB starts a tiny HTTP server on a port 1000 above wherever you started MongoDB.
This gives you some information about your system, but nothing you can’t get elsewhere and is somewhat useless on a machine you probably only access via SSH and exposes information that should be inaccessable to the outside world.
If you’re not planning to connect via file system socket, you might as well disable this option.
You would only connect via file system socket on a machine that is also running an application server: you must be local to use a file system socket.
Most security issues that have been reported with MongoDB have been JavaScript-related and it is generally safer to disallow it, if your application allows.
Several shell helpers assume that JavaScript is available on the server, notably sh.status()
You will see errors if you attempt to run any of these helpers with JavaScript disabled.
It is disabled by default and allows running many commands on the server.
Data Encryption As of this writing, MongoDB provides no built-in mechanism for encrypting data stored.
If you require data to be encrypted, use filesystem encryption.
Another possibility is manually encrypting certain fields (although MongoDB has no special ability to query for encrypted values)
Due to licensing issues the default builds do not have SSL, but you can download a subscriber build at http://www.10gen.com, which supports SSL.
You can also compile MongoDB from source to enable SSL support.
Consult your driver’s documentation on how to create SSL connections using your language.
Most init scripts use the --logpath option to send logs to a file.
If you have multiple MongoDB instances on a single machine (say, a mongod and a mongos), make sure that their logs are stored in separate files.
Make sure that you know where the logs are and have read access to the files.
MongoDB spits out a lot of log messages, but please do not run with the --quiet option (which suppresses some of them)
If you are debugging a specific issue with your application, there are a couple options for getting more info from the logs.
First, you can change the log level, either by restarting MongoDB with more v’s or running the setParameter command:
Remember to turn log level back down to 0, or your logs may be needlessly noisy.
You can turn log level up to 5, at which point mongod will print out almost every action it takes, including the contents of every request handled.
This can cause a lot of IO as mongod writes everything to the log file, which can slow down a busy system.
Turning on profiling is a better option if you need to see every operation as it’s happening.
By default, MongoDB logs information about queries that take longer than 100 ms to run.
If 100 ms it too short or too long for your application, you can change the threshold with setProfilingLevel:
The second line will turn off profiling, but the value in milliseconds given in the first line will continue to be used as a threshold for the log (across all databases)
You can also set this parameter by restarting MongoDB with the --slowms option.
Finally, set up a cron job that rotates your log every day or week.
If MongoDB was started with --logpath, sending the process a SIGUSR1 signal will make it rotate the log.
There is also a logRotate command that does the same thing:
You cannot rotate logs if MongoDB was not started with --logpath.
Before you deploy, it is important to set up some type of monitoring.
Monitoring should allow you to track what your server is doing and alert you if something goes wrong.
Examples use chapters from the Mongo Monitoring Service (MMS) to demonstrate what to look for when monitoring.
If you do not want to use MMS, please use some type of monitoring.
It will help you detect potential issues before they cause problems and let you diagnose issues when they occur.
Monitoring Memory Usage Accessing data in memory is fast and accessing data on disk is slow.
Unfortunately, memory is expensive (and disk is cheap) and typically MongoDB uses up memory before any other resource.
This section covers how to monitor MongoDB’s interactions with disk and memory, and what to watch for.
Introduction to Computer Memory Computers tend to have a small amount of fast-to-access memory and a large amount of slow-to-access disk.
When you request a page of data that is stored on disk (and not yet in memory), your system page faults and copies the page from disk into memory.
It can then access the page in memory extremely quickly.
Copying a page from disk into memory takes a lot longer than reading a page from memory.
Thus, the less MongoDB has to copy data from disk, the better.
If MongoDB can operate almost entirely in memory, it will be able to access data much faster.
Thus, MongoDB’s memory usage is one of the most important stats to track.
First is resident memory: this is the memory that MongoDB explicitly owns in RAM.
For example, if we query for a document and it is paged into memory, that page is added to MongoDB’s resident memory.
This address isn’t the literal address of the page in RAM.
MongoDB can pass it to the kernel and the kernel will look up where the page really lives.
This way, if the kernel needs to evict the page from memory, MongoDB can still use the address to access it.
MongoDB will request the memory from the kernel, the kernel will look at its page cache, see that the page is not there, page fault to copy the page into memory, and return it to MongoDB.
The pages of data MongoDB has addresses for is how MongoDB’s mapped memory is calculated: it includes all of the data MongoDB has ever accessed.
It will usually be about the size of your data set.
MongoDB keeps an extra virtual address for each page of mapped memory for journaling to use (see Chapter 19)
This doesn’t mean that there are two copies of the data in memory, just two addresses.
Thus, the total virtual memory MongoDB uses will be approximately twice your mapped memory size (or twice your data size)
If journaling is disabled, mapped and virtual memory sizes will be approximately equal.
Note that both virtual memory and mapped memory are not “real” memory allocations: they do not tell you anything about how much RAM is being used.
Theoretically, MongoDB could have a petabyte of memory mapped and only a couple of gigabytes in RAM.
Thus, you do not have to worry if mapped or virtual memory sizes exceed RAM.
Figure 21-1 shows the MMS graph for memory information, which describes how much resident, virtual, and mapped memory MongoDB is using.
On a box dedicated to MongoDB, resident should be a little less than the total memory size (assuming your working set is as large or larger than memory)
Resident memory is that only statistic that actually tracks how much data is in physical RAM, but by itself this stat does not tell you much about how MongoDB is using memory.
From the top line to the bottom: virtual, mapped, and resident memory.
If your data fits entirely in memory, resident should be approximately the size of your data.
When we talk about data being “in memory,” we’re always talking about the data being in RAM.
Tracking Page Faults As you can see from Figure 21-1, memory metrics tend to be fairly steady, but as your data set grows virtual and mapped will grow with it.
Resident will grow to the size of your available RAM and then hold steady.
You can use other statistics to find out how MongoDB is using memory, not just how much of each type it has.
One useful stat is number of page faults, which tells you how often the data MongoDB is looking for is not in RAM.
If the disk in Figure 21-2 can handle that many faults and the application can handle the delay of the disk seeks, there is no particular problem with having so many faults (or more)
On the other hand, if your application cannot handle the increased latency of reading data from disk, you have no choice but to store all of your data in memory (or use SSDs)
A system that is page faulting hundreds of times a minute.
A system that is page faulting a few times a minute.
Regardless of how forgiving the application is, page faults become a problem when the disk is overloaded.
The amount of load a disk can handle isn’t linear: once a disk begins getting overloaded, each operation must queue for a longer and longer period of time, creating a chain reaction.
There is usually a tipping point where disk performance begins degrading quickly.
Thus, it is a good idea to stay away from the maximum load that your disk can handle.
If your application is behaving well with a certain number of page faults, you have a baseline for how many page faults the system can handle.
If page faults begin to creep up and performance deteriorates, you have a threshold to alert on.
You can see page fault stats per-database by looking at "recordStats" field of serverStatus’s output:
Minimizing Btree Misses Accessing index entries that are not in memory is particularly inefficient, as it often causes two page faults.
There is one fault to load the index entry into memory and then another to load the document into memory.
When an index lookup causes a page fault it’s called a btree miss.
MongoDB also tracks btree hits: when an index access does not have to go to disk.
Indexes are so frequently used that they are generally in memory, but if there is too little memory, a lot of indexes, or an unusual access pattern (e.g., a lot of table scans), btree misses may be higher.
They should generally be low so if you’re seeing a lot of them, track down the cause.
Some IO wait is normal (MongoDB has to go to disk sometimes and, although it tries not to block anything when it does, cannot completely avoid it)
Tracking Background Flush Averages One other disk metric to watch is how long it takes MongoDB to write its dirty pages to disk, also known as the background flush average.
If the background flush average starts creeping up, you know that your disk is having trouble keeping up with requests.
At least once a minute (by default), MongoDB will flush all writes that have happened to disk.
Depending on the operating system, MongoDB may flush writes more frequently if there are a lot of dirty pages.
You can also configure the interval by passing a number of seconds to the --syncdelay option when starting mongod.
More frequent syncs will make the amount of data to be synced smaller, but can also be less efficient.
A common misconception is that syncdelay has something to do with data durability.
Generally, you want to see background flush averages of less than a second.
On a slow disk or a busy system, this can creep up, taking longer and longer as the disk gets overloaded.
At some point, the disk will be so overloaded that flushes will take longer than 60 seconds, meaning MongoDB will be trying to flush constantly (which puts even more load on the disk)
What you don’t want to see is a trend towards tens of seconds.
Figure 21-6 shows a graph of background flush averages over time.
This system’s hard drive is working hard: it always takes it more than 5 seconds to write the preceding minute’s data to disk.
If background flush average creeps up beyond what is reasonable for your disks (probably a few seconds) over a broad time period, start thinking about how you’re going to lighten the load on your disks.
MongoDB only has to flush dirty data (that is, data that’s changed) so background flush average will generally reflect write load.
You should always track IO wait and page faults in addition to background flush average.
Calculating the Working Set In general, the more of your data that is in memory, the faster MongoDB will perform.
Thus, in order from fastest to slowest, an application could have:
This is nice to have but is often too expensive or infeasible.
It may be necessary for applications that depend on fast response times.
Your working set is the data and indexes that your application uses.
This may be everything, but generally there’s a core data set (for example, the users collection and the last month of activity) that covers 90% of requests.
If this working set fits in RAM, MongoDB will generally be fast: it only has to go to disk for a few “unusual” requests.
One way to think about this is to track data accessed over time, as shown in Figure 21-7
You can measure for that amount of time to figure out how much your data set grows.
Note that this example uses time, but it’s possible that there’s another access pattern that makes more sense for your application (time being the most common one)
The working set is data used in the requests before the cutoff of “frequent requests”
You can also use MongoDB’s stats to estimate the working set.
MongoDB keeps a map of what it thinks is its memory, which you can see by passing in the "workingSet" : 1 option to serverStatus:
MongoDB does not actually know how many pages are in memory, but it should be close.
The "workingSet" field is not included in serverStatus’s output by default.
Once your application is has accessed the data it usually accesses (a process called preheating), it should never have to go to disk again for the working set.
Obviously, MongoDB will almost always have to go to disk for the nonworking set data.
On the other hand, suppose our working set does not fit in RAM.
Then the working set will generally take up most of RAM.
Thus, there is a constant churn back and forth from disk: accessing the working set does not have predictable performance anymore.
Tracking Performance Performance of queries is often important to track and keep consistent.
There are several ways to track if MongoDB is having trouble with the current request load.
The other possibility is that you are running a lot of MapReduces or other server-side JavaScript.
It is a good idea to track CPU (particularly after deploying a new version of your application) to ensure that all your queries are behaving as they should.
Note that the graph shown in Figure 21-9 is fine: if there is a low number of page faults, IO wait may be dwarfed by other CPU activities.
It is only when the other activities creep up that bad indexes may be a culprit.
The top line is user and the lower line is system.
A similar metric is queuing: how many request are waiting to be processed by MongoDB.
A request is considered queued when it is waiting for the lock it needs to do a read or a write.
Figure 21-10 shows a graph of read and write queues over time.
No queues are preferred (basically an empty graph), but this graph is nothing to be alarmed about.
In a busy system, it isn’t unusual for an operation to have to wait a bit for the correct lock to be available.
You can see if requests are piling up by looking at the number of requests enqueued.
A large and ever-present queue is an indication that mongod cannot keep up with its load.
You should decrease the load on that server as fast as possible.
You can correlate statistics about queuing with lock percentage: the amount of time MongoDB spends locked.
Often disk IO will throttle writes more than locking but locking is still important to track, especially for systems with fast disks or many sequential writes.
Again, one of the most common causes of high lock percentage is that you are missing an index.
Thus, there is an unfortunate cascading nature to high lock percentages making everything slower, causing requests to build up, causing more load on the system and even higher lock percentages.
Figure 21-11 shows an alarmingly high lock percentage, which should be dealt with as soon as possible.
Lock percentage is often spiky depending on traffic levels; but if it trends upwards over time it’s a good indication that your system is under stress and that something needs to change.
Thus, you should alert on lock percentage over a long time (so that a sudden spike in traffic won’t trigger it)
On the other hand, you may want to also trigger an alert if lock percentage suddenly spikes, say 25% over its normal value.
This might be an indication that your system cannot handle load spikes and that you may have to add capacity.
In addition to the global lock percentage, MongoDB tracks locking per database, so you can see if you have a particular database with a lot of contention.
Tracking Free Space One other metric that is basic but important to monitor is disk usage: track free disk space.
Sometimes users wait until their disk runs out of space before they think about how they want to handle it.
By monitoring your disk usage, you can predict how long your current drive will be sufficient and plan in advance what to do when it is not.
As you run out of space, there are several options:
Shut down each member of a replica set (one at a time) and copy its data to a larger.
Replace members of your replica set with members with a larger drive: remove an old member and add a new member, and allow that one to catch up with the rest of the set.
If you are using the directoryperdb option and you have a particularly fastgrowing database, move it to its own drive.
Then mount the volume as a directory in your data directory.
This way the rest of your data doesn’t have to be moved.
Regardless of the technique you choose, plan ahead to minimize the impact on your application.
You need time to take backups, modify each member of your set in turn, and copy your data from place to place.
Monitoring Replication Replication lag and oplog length are important to track.
Lag is when the secondaries cannot keep up with the primary.
Lag is calculated by subtracting the time of the last op applied on a secondary from the time of the last op on the primary.
For example, if a secondary just applied an op with the timestamp 3:26:00 p.m.
You want lag to be as close to 0 as possible, and it is generally on the order of milliseconds.
If a secondary cannot replicate writes as fast as the primary can write, you’ll start seeing nonzero lag.
The most extreme case of this is when replication is stuck: it cannot apply any more operations for some reason.
At this point, lag will grow by one second per second, creating the steep slope shown in Figure 21-13
This could be caused by network issues or a missing "_id" index, which is required on every collection for replication to function properly.
Make sure you create the "_id" index as a unique index.
Once created, the "_id" index cannot be dropped or changed (other than by dropping the whole collection)
Replication getting stuck and, just before February 10, beginning to recover.
If a system is overloaded, a secondary may gradually fall behind.
But you generally won’t see the characteristic “one second per second” slope in the graph, but some replication will still be happening.
Still, it is important to be aware if the secondaries cannot keep up with peak traffic or are gradually falling further behind.
Primaries do not throttle writes to “help” secondaries catch up, so it common for secondaries to fall behind on overloaded systems (particularly as MongoDB tends to prioritize writes over reads, which means replication can be starved on the primary)
You can force throttling of the primary to some extent by using “w” with your write concern.
You also might want to try removing load from the secondary by routing any requests it was handling to another member.
If you are on an extremely underloaded system you may see another interesting pattern: sudden spikes in replication lag, as shown in Figure 21-14
The spikes shown are not actually lag—they are caused by variations in sampling.
The mongod is processing one write every couple of minutes.
Because lag is measured as the difference between timestamps on the primary and secondary, measuring the timestamp of the secondary right before a write on the primary makes it look minutes behind.
If you increase the write rate, these spikes should disappear.
The other important metric to track is the length of each member’s oplog.
Every member that might become primary should have an oplog longer than a day.
If a member may be a sync source for another member, it should have an oplog longer than the time an initial sync takes to complete.
Figure 21-15 shows what a standard oplog-length graph looks like.
This oplog has an excellent length: 1,111 hours is over a month of data! In general, oplogs should be as long as you can afford the disk space to make them.
Given the way they’re used, they take up basically no memory and a long oplog can mean the difference between a painful ops experience and an easy one.
Figure 21-16 shows a slightly unusual variation caused by a fairly short oplog and variable traffic.
The administrator may want to make the oplog longer when she gets a chance.
An oplog length of an application with daily traffic peaks.
It is important to take regular backups of your system.
Backups are good protection against most types of failure, and very little can’t be solved by restoring from a clean backup.
Backups are only useful if you are confident about deploying them in an emergency.
Thus, for any backup technique you choose, be sure to practice both taking backups and restoring from backups until you are comfortable with the restore procedure.
Backing Up a Server There are a variety of ways of taking backups.
Regardless of method, taking a backup can cause strain on a system: it generally requires reading all your data into memory.
Thus, backups should generally be done on replica set secondaries (as opposed to the primary) or, for standalone servers, at an off time.
The techniques in this section apply to any mongod, whether a standalone or a member of a replica set, unless otherwise noted.
Filesystem Snapshot The simplest way to make a backup is to take a filesystem snapshot.
However, this requires your filesystem to support snapshotting and you must be running mongod with journaling enabled.
If your system fulfills these two prerequisites, this method requires no preparation: simply take a snapshot at any time.
The exact command for restoring from a snapshot varies by filesystem, but basically you restore the snapshot and then start mongod.
As you took a snapshot on a live system, the snapshot is essentially what the data files would look like if mongod had been kill -9-ed at the time the snapshot was taken.
Thus, on startup, mongod will replay the journal files and then begin running normally.
Copying Data Files Another way of creating backups is to make a copy of everything in the data directory.
Because you cannot copy all of the files at the same moment without filesystem support, you must prevent the data files from changing while you are making the copy.
This command locks the database against any further writes and then flushes all dirty data to disk (fsync), ensuring that the files in the data directory have the latest consistent information and are not changing.
Once this command has been run, mongod will enqueue all incoming writes.
It will not process any further writes until it has been unlocked.
Note that this command stops writes to all databases (not just the one db is connected to)
Once the fsynclock command returns, copy all of the files in your data directory to a backup location.
On Linux, this can be done with a command such as:
Make sure that you copy absolutely every file and folder from the data directory to the backup location.
Excluding files or directories may make the backup unusable or corrupt.
Once you have finished copying the data, unlock the database to allow it to take writes again:
Note that there are some locking issues with authentication and fsynclock.
If you are using authentication, do not close the shell between calling fsyncLock() and fsyncUn lock()
If you disconnect, you may be unable to reconnect and have to restart mon god.
The fsyncLock() setting does not persist between restarts, mongod will always start up unlocked.
As an alternative to fsynclocking, you can instead shut down mongod, copy the files, and then start mongod back up again.
Shutting down mongod effectively flushes all changes to disk and prevents new writes from occurring during the backup.
To restore from data directory copies, ensure that mongod is not running and that the data directory you want to restore into is empty.
Copy the backed-up data files to the data directory, and then start mongod.
For example, the following command would restore the files backed up with the command shown earlier:
Despite the warnings about partial data directory copies, you can use this method to backup individual databases if you know what to copy.
To back up an individual database (called, say, "myDB"), copy all of the myDB.* files (including the .ns file) to backup.
If you are using the --directoryperdb option, copy the entire myDB directory.
You can restore specific databases by copying just the files with the correct database name into your data directory.
You must be starting from a clean shutdown to restore piecemeal like this.
If you had a crash or a hard shutdown, do not attempt to restore a single database from backup: replace the entire directory from backup and start the mongod to allow the journal files to be replayed.
Depending on what else your database is doing, mongodump may hang forever if the database is locked.
Using mongodump The final way of taking a backup is to use mongodump.
However, it also has some benefits: it is a good way to backup individual databases, collections, and even subsets of collections.
Here, we will focus on the most useful ones to use for backup.
If you are running mongodump on the same machine as the mongod, you can simply specify the port mongod is running on:
The actual data is stored in .bson files, which merely contain every document in a collection in BSON, concatenated together.
You can examine .bson files using the bsondump tool, which comes with MongoDB.
You do not even need to have a server running to use mongodump: you can use the -dbpath option to specify your data directory and mongodump will use the data files to copy data:
One issue with mongodump is that it is not an instantaneous backup: the system may be taking writes while the backup occurs.
Thus, someone might begin a backup that causes mongodump to dump the database A.
However, mongodump has already dumped it, so you’ll end up with a snapshot of the data in a state it never existed in on the original server.
To avoid this, if you are running mongod with --replSet, you can use mongodump’s --oplog option.
This will keep track of all operations that occur on the server while the dump is taking place, so these operations can be replayed when the backup is restored.
This gives you a consistent point-in-time snapshot of data from the source server.
If you used the --oplog option to dump the database, you must use the -oplogReplay option with mongorestore to get the point-in-time snapshot.
If you are replacing data on a running server, you may (or may not) wish to use the -drop option, which drops a collection before restoring it.
The behavior of mongodump and mongorestore has changed over time.
To prevent compatibility issues, try to use the same version of both utilities (you can see their versions by running mongodump --version and mongorestore --version)
You can restore into an entirely different database and collection than you dumped from.
This can be useful if different environments use different database names (say, dev and prod) but the same collection names.
To restore a .bson file into a specific database and collection, specify the targets on the command line:
Specifically, unique indexes require that the data does not change in ways that would violate the unique index constraint during the copy.
The safest way to do this is to choose a method that “freezes” the data, then take a backup as described in either of the previous two sections.
Backing Up a Replica Set Generally, you should take backups from a secondary: this keeps load off of the primary and you can lock a secondary without affecting your application (so long as your application isn’t sending it read requests)
You can use any of the three methods outlined previously to backup a replica set member, but file system snapshot or data file copy are recommended.
Either of these techniques can be applied to replica set secondaries with no modification.
First, if you are using mongodump, you must take your backups using the --oplog option to get a pointin-time snapshot; otherwise the backup’s state won’t match the state of anyone else in the cluster.
You must also create an oplog when you restore from a mongodump backup, or the restored member will not know where it was synced to.
To restore a replica set member from a mongodump backup, start the target replica set member as a standalone server with an empty data directory.
First, run mongorestore (as described in the previous section) with the --oplogReplay option.
Now it should have a complete copy of the data, but it still needs an oplog.
See “Resizing the Oplog” on page 220 for advice on oplog sizing.
The easiest way to do this is to restore the oplog.bson backup file from the dump into the local.oplog.rs collection:
Once this mongorestore is complete, you can restart this server as a replica set member.
Backing Up a Sharded Cluster Sharded clusters are impossible to “perfectly” back up while active: you can’t get a snapshot of the entire state of the cluster at a point in time.
However, this limitation is generally sidestepped by the fact that as your cluster gets bigger, it becomes less and less likely that you’d ever have to restore the whole thing from backup.
Thus, when dealing with a sharded cluster, we focus on backing up pieces: the config servers and the replica sets individually.
Turn off the balancer before performing any of these operations on a sharded cluster (either backup or restore)
You cannot get a consistent snapshot of the world with chunks flying around.
See “Balancing Data” on page 289 for instructions on turning the balancer on and off.
Backing Up and Restoring an Entire Cluster When a cluster is very small or in development, you may want to actually dump and restore the entire thing.
You can accomplish this by turning off the balancer and then running mongodump through the mongos.
This creates a backup of all of the shards on whatever machine mongodump is running on.
To restore from this type of backup, run mongorestore connected to a mongos.
After turning off the balancer, you can alternatively take filesystem or data directory backups of each shard and the config servers.
However, you will inevitably get copies from each at slightly different times, which may or may not be a problem.
Also, as soon as you turn on the balancer and a migrate occurs, some of the data you backed up from one shard will no longer be there.
Backing Up and Restoring a Single Shard Most often, you’ll only need to restore a single shard in a cluster.
If you are not too picky, you can restore from a backup of that shard taken using one of the single-server methods just described.
There is one important issue to be aware of: suppose you take a backup of your cluster on Monday.
On Thursday, your disk melts down and you have to restore from backup.
However, in the intervening days, new chunks may have moved to this shard.
Your backup of the shard from Monday will not contain these new chunks.
You may be able to use a config server backup to figure out where the disappearing chunks lived on.
Monday, but it is a lot more difficult than simply restoring the shard.
In most cases, restoring the shard and losing the data in those chunks is the preferable route.
You can connect directly to a shard to restore from backup (instead of going through mongos)
Creating Incremental Backups with mongooplog All of the backup methods outlined must make a full copy of the data, even if very little of it has changed since the last backup.
If you have data that is very large relative to the amount that is being written, you may want to look into incremental backups.
Instead of making full copies of the data every day or week, you take one backup and then use the oplog to back up all operations that have happened since the backup.
This technique is much more complex than the ones described above, so prefer them unless incremental backups are absolutely necessary.
This technique requires two machines, A and B, running mongod.
A is your main machine (probably a secondary) and B is your backup machine:
Keep this somewhere safe—you’ll need it for a later step.
Take a backup of your data, using one of the techniques above to get a point-intime backup.
Periodically add any operations that have happened on A to B’s copy of the data.
There is a special tool that comes with MongoDB distributions that makes this easy: mongooplog (pronounced mon-goop-log) which copies data from the oplog of one server and applies it to the data set on another.
This technique is sort of like keeping a secondary up-to-date manually, so you may just want to use a slavedelayed secondary instead.
This chapter gives recommendations for setting up a server to go into production.
Choosing what hardware to buy and how to set it up.
Designing the System You generally want to optimize for data safety and as fast access as you can afford.
This section discusses the best way to accomplish these goals when choosing disks, RAID configuration, CPU, and other hardware and low-level software components.
Choosing a Storage Medium In order of preference, we would like to store and retrieve data from:
Unfortunately, most people have limited budgets or enough data that storing everything in RAM is impractical and SSDs are too expensive.
Thus, the typical deployment is a small amount of RAM (relative to total data size) and a lot of space on a spinning disk.
If you are in this camp, the important thing is that your working set is smaller than RAM and you should be ready to scale out if the working set gets bigger.
If you are able to spend what you like on hardware, buy a lot of RAM and/or SSDs.
Reading data from RAM takes a few nanoseconds (say, 100)
Conversely, reading from disk takes a few milliseconds (say, 10)
It can be hard to picture the difference between these two numbers, so suppose we scale them up to more relatable numbers: if accessing RAM took 1 second, accessing disk would take over a day!
Thus, we want to access disk as seldom as possible.
Fast spinning disks do not cut down disk access time that much, so don’t spend lots of money on them.
It is more productive to get more memory or SSDs.
He added a new shard backed by SSDs and, in the graphs, was concurrently running one shard with SSDs and one with spinning disks.
In comparison, the chart in Figure 23-2 plots queries on the SSD drive.
The other interesting point to note about SSDs versus spinning disks is the amount of stress these relative loads placed on the system.
If we take a look at hardware monitoring on the spinning disk server (Figure 23-3), we can see that the disk is quite busy.
The main visible line on the chart is IO wait: the percent of time that the CPU was waiting for disk IO.
This means that the user’s workload was essentially being throttled by his disk (which was why he was adding SSDs)
In contrast, Figure 23-4 shows the CPU usage on the SSD machine.
Thus, the limiting factor on this machine is how fast the CPU can run.
As the numbers are greater than 100%, this graph also shows that multiple processors were being utilized.
Contrast that to the chart in figure Figure 23-3, where not even a single core could be fully utilized due to the lag from disk IO.
Finally, you can see some of the effect this has on MongoDB by looking at the graphs of lock times in Figure 23-5
Contrast this with the lock percentage on the SSD machine in Figure 23-6
The bump at the beginning of the chart was from a data-loading operation that he performed before bringing the SSD online.
As you can see, SSDs can shoulder a lot more load than spinning disks, but unfortunately they aren’t an option for a lot of deployments.
Even if it’s not possible to use SSDs for your entire cluster, consider deploying as many as possible and then using the forced hot spot data pattern in Chapter 15 to take advantage of them.
Note that generally you cannot add an SSD to an existing replica set (where the other members have spinning disks)
If the SSD machine becomes primary and handles anything close to the load it is capable of, the other members will not be able to replicate from it quickly enough and will fall behind.
Thus, it is a good idea to add a new shard to your cluster if you are introducing SSDs.
Note that SSDs are excellent for normal data usage patterns, but spinning disks actually work very well for the journal.
Putting the journal on a spinning disk and your data on SSDs can save you SSD space and should have no impact on performance.
Recommended RAID Configurations RAID is hardware or software that lets you treat multiple disks as though they were a single disk.
There are a number of ways to configure RAID depending on the features you’re looking for, generally some combination of speed and fault-tolerance.
Each disk holds part of the data, similar to MongoDB’s sharding.
Because there are multiple underlying disks, lots of data can be written to disk at the same time.
However, if a disk fails and the data is lost, there are no copies of it.
It also can cause slow reads (we’ve particularly seen this on Amazon’s Elastic Block Store), as some data volumes may be slower than others.
An identical copy of the data is written to each member of the array.
This has lower performance than RAID0, as a single member with a slow disk can slow down all writes.
However, if a disk fails, you will still have a copy of the data on another member of the array.
RAID5 Striping disks, plus keeping an extra piece of data about the other data that’s been stored to prevent data loss on server failure.
Basically, RAID5 can handle one disk going down and hide that failure from the user.
However, to do this, it is slower than any of the other varieties listed here because it needs to calculate this extra piece of information whenever data is written.
This is particularly expensive with MongoDB, as a typical workload does many small writes.
It is a matter of personal preference: how much risk are you willing to trade for performance?
If you have a choice between investing in memory and investing in CPU, go with memory every time.
Theoretically, you could max out multiple cores on reads or in-memory sorts, but in practice that’s rare.
When choosing between speed and number of cores, go with speed.
MongoDB is better at taking advantage of more cycles on a single processor than increased parallelization.
Choosing an Operating System 64-bit Linux is the operating system MongoDB runs best on.
CentOS and RedHat Enterprise Linux are probably the most popular choices, but any flavor should work (Ubuntu and Amazon Linux are also common)
Use the most recent stable versions of operating systems because old, buggy packages or kernels can sometimes cause issues.
Other flavors of Unix are not as well supported: proceed with caution if you’re using Solaris or one of the BSD variants.
Builds for these systems have, at least historically, had a lot of issues.
For example, you could have a mongos process running on Windows and the mongods that are its shards running on Linux.
You can also copy data files from Windows to Linux or visa versa with no compatibility issues.
Arbiters and mon gos processes can be run on 32-bit machines.
Do not run any other type of MongoDB server on a 32-bit machine.
Most drivers support both littleand big- endian systems, so you can run clients on either.
However, the server must always be run on a little-endian machine.
Swap Space You should allocate a small amount of swap in case memory limits are reached to prevent the kernel from killing MongoDB.
The majority of memory MongoDB uses is “slippery”: it’ll be flushed to disk and replaced with other memory as soon as the system requests the space for something else.
Therefore, database data should never be written to swap space: it’ll be flushed back to disk first.
However, occasionally MongoDB will use swap for operations that require ordering data: either building indexes or sorting.
It attempts not to use too much memory for these types of operations, but by performing many of them at the same time you may be able to force swapping.
If your application is managing to make MongoDB use swap space, you should look into redesigning your application or reducing load on the swapping server.
Filesystem For Linux, the ext4 filesystem or XFS are recommended for your data volumes.
It is nice to have a filesystem that can do filesystem snapshots for backups, but that’s a matter of preference.
MongoDB has to regularly allocate and zero-fill 2 GB data files, which can freeze for minutes at a time.
If ext3 is necessary, there are some hacks around this.
However, try to use something else if at all possible.
Some client versions lie about flushing, randomly remount and flush the page cache, and do not support exclusive file locking.
Using NFS can cause journal corruption and should be avoided at all costs.
Virtualization Virtualization is a great way to get cheap hardware and be able to expand fast.
However, there are some downsides, particularly unpredictable network and disk IO.
Turn Off Memory Overcommitting The memory overcommit setting controls what happens when processes request too much memory from the operating system.
Depending on this setting, the kernel may give memory to processes even if that memory is not actually available (in the hopes that it’ll become available by the time the process needs it)
That’s called overcommitting: the kernel promises memory that isn’t actually there.
The value 2 is complicated, but it’s the best option available.
You do not need to restart MongoDB after changing this setting.
Mystery Memory Sometimes the virtualization layer does not handle memory provisioning correctly.
Assuming you don’t end up on the lucky side, there isn’t much you can do.
If your readahead is set appropriately and your virtual machine just won’t use all the memory it should, you may just have to switch virtual machines.
Handling Network Disk IO Issues One of the biggest problems with using virtualized hardware is that you are generally sharing a disk with other tenants, which exacerbates the disk slowness mentioned previous because everyone is competing for disk IO.
Thus, virtualized disks can have very unpredictable performance: they can work fine while your neighbors aren’t busy and suddenly slow down to a crawl if someone else starts hammering the disk.
The other issue is that this storage is often not physically attached to the machine MongoDB is running on, so even when you have a disk all to yourself disk IO will be slower than it would be with a local disk.
Amazon has what is probably the most widely-used networked block store, called Elastic Block Store (EBS)
On the plus side, this makes backups very easy (take a snapshot from a secondary, mount the EBS drive on another instance, and start up mongod)
If you require more predictable performance, there are a couple of options.
The most straightforward way to guarantee the performance you expect is to not host MongoDB in the cloud.
Host it on your own servers and you know no one else is slowing things down.
However, that’s not an option for a lot of people, so the next-best thing is to get an instance that guarantees a certain number of IOPS (IO Operations Per Second)
If you can’t pursue either of these options and you need more disk IO than an overloaded EBS volume can sustain, there is a way to hack around it.
Basically, what you can do is to keep monitoring against the volume MongoDB is using.
If and when that volume slows down, immediately kill that instance and bring up a new one with a different data volume.
Spiking IO utilization (“IO wait” on MMS), for obvious reasons.
The number of lost TCP packets go up (Amazon is particularly bad about this: when performance starts to fall, it drops TCP packets all over the place)
MongoDB’s read and write queues spiking (this can be seen on MMS or in mongo stat’s qr/qw column)
If your load varies over the day or week, make sure your script takes that into account: you don’t want a rogue cron job killing off all of your instances because of an unusuallyheavy Monday morning rush.
This hack relies on you having recent backups or relatively quick-to-sync data sets.
If you have each instance holding terabytes of data, you might want to pursue an alternative approach.
Also, this is only likely to work: if your new volume is also being hammered, it will be just as slow as the old one.
Ephemeral drives are the actual disks attached to the physical machine your VM is running on, so they don’t have a lot of the problems networked storage does.
Local disks can still be overloaded by other users on the same box, but with a large box you can be reasonably sure you’re not sharing disks with too many others.
Even with a smaller instance, often the ephemeral drive will give better performance than a networked drive so long as the other tenants aren’t doing tons of IOPS.
The downside is in the name: these disks are ephemeral.
If your EC2 instance goes down, there’s no guarantee you’ll end up on the same box when you restart the instance and then your data will be gone.
You should make sure that you do not store any important or unreplicated data on these disks.
In particular, do not put the journal on these ephemeral drives or your database on network storage.
In general, think of ephemeral drives as a slow cache rather than a fast disk and use them accordingly.
Configuring System Settings There are several system settings which can help MongoDB run more smoothly and which are mostly related to disk and memory access.
This section covers each of these options and how you should tweak it.
Turning Off NUMA When machines had a single CPU, all RAM was basically the same in terms of access time.
As machines started to have more processors, engineers realized that having all memory be equally far from each CPU (as shown in Figure 23-7) was less efficient than having each CPU have some memory that is especially close to it and fast for that particular CPU to access.
This architecture where each CPU has its own “local” memory is called non-uniform memory architecture (NUMA), shown in Figure 23-8
Uniform memory architecture: all memory has the same access cost for each CPU.
Non-Uniform Memory Architecture: certain memory is attached to a CPU, giving the CPU faster access to that particular memory.
CPUs can still access other CPUs’ memory, but it is more expensive than accessing their own.
For lots of applications, NUMA works well: the processors often need different data because they’re running different programs.
However, this works terribly for databases in general and MongoDB in particular because databases have such different memory access patterns than other types of applications.
MongoDB uses a massive amount of memory and needs to be able to access memory that is “local” to other CPUs.
However, the default NUMA settings on many systems makes this difficult.
CPUs favor using the memory that is attached to them and processes tend to favor one CPU over the others.
It must use its local memory for data that doesn’t have a “home” yet, but its local memory is full.
Thus, it has to evict some of the data in its local memory to make room for the new data, even though there’s plenty of space left on the memory attached to CPU2! This process tends to cause MongoDB to run much slower than expected, as it only has a fraction of the memory available that it should have.
MongoDB vastly prefers semi-efficient access to more data than extremely efficient access to less data.
Turning off NUMA is one of the magic “go faster” buttons that you definitely want to make sure you’ve pressed.
Like using SSDs, disabling NUMA just makes everything work better.
For example, if you’re using grub, you can add the numa=off option to grub.cfg:
If your system cannot turn it off in BIOS, you’ll have to start mongod with: $ numactl --interleave=all mongod [options]
This nasty setting can be thought of as “super NUMA.” If it’s enabled, whenever a page of memory is accessed by a CPU, it will moved to that CPU’s local memory.
Thus, if you have a threadA on one CPU and threadB on another and they’re both hitting a page of memory, that page will be copied from one CPU’s local memory to the other’s on every single access.
You do not have to restart mongod for zone_reclaim_mode changes to take effect.
If you have NUMA enabled, your hosts will show up in yellow on MMS, as shown in Figure 23-10
You can see the actual warning that’s causing it to be yellow by going to the “Last Ping” tab.
Figure 23-11 shows the warning you’ll see if NUMA is enabled.
Once NUMA has been disabled, MMS will display the host in blue again.
There are several other reasons a host may appear in yellow.
Setting a Sane Readahead Readahead is an optimization where the operating system reads more data from disk than was actually requested.
This is useful because most workloads computers handle are sequential: if you load the first 20 MB of a video, you are probably going to want the next couple of megabytes of it.
Thus, the system will read more from disk than you actually request and store it in memory, just in case you need it soon.
However, MongoDB is not a typical workload and readahead is a frequent issue on MongoDB systems.
MongoDB tends to read many small pieces of data from random places on the disk, so the default system settings do not work very well.
If readahead is high, memory gradually fills up with data that MongoDB didn’t request, forcing MongoDB to go to disk more often.
However, if you are accessing data fairly randomly across the disk, all of those prefetched sectors would be wasted.
If memory was contained your working set, 255 sectors of your working set would have to be evicted to make room for these sectors that aren’t going to be used.
Fortunately, there’s a fairly easy way to see if your readahead setting is actively hurting you: check the resident set size of MongoDB and compare it to the system’s total RAM.
If it is much lower, then your readahead is probably too high.
This technique of comparing resident set size with total memory size works because data that’s been “read ahead” from disk is in memory but MongoDB didn’t request it, so it isn’t included in the calculation of MongoDB’s resident memory size.
This shows you the settings for each of your block devices.
You can change this setting for a device by running this command with the --setra option:
You don’t want to set readahead too low, either: you don’t want to have to go to disk multiple times to fetch a single document.
If you have large documents (greater than a megabyte in size), consider a higher readahead.
When using RAID, you have to set readahead on everything: the RAID controller and the individual volumes.
Unintuitively, you must restart MongoDB for readahead settings to take effect.
You’d think you’re setting a disk property, so it should apply to all running programs.
Unfortunately, processes make a copy of the readahead value when they start and continue to use that value until terminated.
You have no plans for it to ever grow beyond memory.
MongoDB needs to page in lots of tiny pieces of memory, so using hugepages can result in more disk IO.
Systems move data from disk to memory and back by the page.
However, using hugepages means that you are keeping megabytes of data from one section of disk in memory.
If your data does not fit in RAM, then swapping in larger pieces from disk will just fill up your memory quickly with data that will need to be swapped out again.
Also, flushing any changes to disk will be slower, as the disk must write megabytes of “dirty” data, instead of a few kilobytes.
Note that on Windows this is called Large Pages, not hugepages.
Some versions of Windows have it enabled by default and some do not, so check and make sure it is turned off.
Hugepages were actually developed to benefit databases, so this may be surprising to experienced database admins.
However, MongoDB tends to do a lot less sequential disk access than relational databases do.
Choosing a Disk Scheduling Algorithm The disk controller receives requests from the operating system and processes them in an order determined by a scheduling algorithm.
For other hardware and workloads, it may not make a difference.
The best way to decide is to test them out yourself on your workload.
Deadline and completely fair queueing (CFQ) both tend to be good choices.
There are a couple of situations where the noop scheduler (a contraction of “no-op” is the best choice): if you’re in a virtualized environment, use the noop scheduler.
The noop scheduler basically passes the operations through to the underlying disk controller as quickly as possible.
It is fastest to do this and let the real disk controller handle any reordering that needs to happen.
Similarly, on SSDs, the noop scheduler is generally the best choice.
SSDs don’t have the same locality issues that spinning disks do.
Finally, if you’re using a RAID controller with caching, use noop.
The cache behaves like an SSD and will take care of propagating the writes to the disk efficiently.
You can change the scheduling algorithm by setting the --elevator option in your boot configuration.
The option is called elevator because the scheduler behaves like an elevator, picking up people (IO requests) from different floors (processes/ times) and dropping them off where they want to go in an arguableoptimal way.
Often all of the algorithms work pretty well; you may not see much of a difference between them.
Don’t Track Access Time By default, the system tracks when files were last accessed.
As the data files used by MongoDB are very high-traffic, you can get a performance boost by disabling this tracking.
You can do this on Linux by changing atime to noatime in /etc/fstab:
You must remount the device for the changes to take effect.
Also, be aware that setting noatime can affect other programs using the partition, such as mutt or backup tools.
Setting this may affect the Remote Storage service, but you probably shouldn’t be using a service that automatically moves your data to other disks anyway.
Modifying Limits There are two limits that MongoDB tends to blow by: the number of threads a process is allowed to spawn and the number of file descriptors a process is allowed to open.
Whenever a MongoDB server accepts a connection, it spawns a thread to handle all activity on that connection.
Depending on your application server configuration, your client may spawn anywhere from a dozen to thousands of connections to MongoDB.
The other limit to modify is the number of file descriptors MongoDB is allowed to open.
Every incoming and outgoing connection uses a file descriptor, so having the client connection storm above would create 20,000 open filehandles (incidentally, the maximum number MongoDB will allow)
Thus, there are a couple adjustments to make: many people purposefully configure mongos processes to only allow a certain number of incoming connections by using the maxConns option.
This is a good way to enforce that your client is behaving well.
You should also increase the limit on the number of file descriptors, as the default (generally 1024) is simply too low.
Set the max number of file descriptors to unlimited or, if you’re nervous about that, 20,000
Each system has a different way of changing these limits, but in general, make sure that you change both the hard and soft limits.
A hard limit is enforced by the kernel and can only be changed by an administrator, versus a soft limit, which is user-configurable.
If the number of connections is left at 1024, MMS will warn you by displaying the host in yellow on the host list (as shown in the NUMA example above)
If low limits are the issue, the “Last Ping” tab should display a message similar to that shown in Figure 23-12
That will stop MongoDB from warning you about them and give you some breathing room, just in case.
Configuring Your Network This section covers which servers should have connectivity to which other servers.
Often, for reasons of network security (and sensibility), you may want to limit the connectivity of MongoDB servers.
Note that multiserver MongoDB deployments should handle networks being partitioned or down, but it isn’t recommended as a general deployment strategy.
For a standalone server, clients must be able to make connections to the mongod.
Members of a replica set must be able to make connections to every other member.
Clients must be able to connect to all nonhidden, nonarbiter members.
Depending on network configuration, members may also attempt to connect to themselves, so you should allow mongods to create connections to themselves.
There are four components: mongos servers, shards, config servers, and clients.
A client must be able to connect to a mongos.
A mongos must be able to connect to the shards and config servers.
A shard must be able to connect to the other shards and the config servers.
There are three possible values in the table: “Required” means that connectivity between these two components is required for sharding to work as designed.
MongoDB will attempt to degrade gracefully if it loses these connections due to network issues, but you shouldn’t purposely configure it that way.
For example, it is recommended that clients only make connections to the mongos, not the shards, so that clients do not inadvertently make requests directly to shards.
Similarly, clients should not be able to directly access config servers so that they cannot accidentally modify config data.
Note that mongos processes and shards talk to config servers, but config servers don’t make connections to anyone, even one another.
Shards must communicate during migrates: shards connect to one another directly to transfer data.
As mentioned earlier, replica set members that compose shards should be able to connect to themselves.
System Housekeeping This section covers some common issues you should be aware of before deploying.
Synchronizing Clocks In general, it’s safest to have your systems’ clocks within a second of each other.
Replica sets should be able to handle nearly any clock skew.
Sharding can handle some skew (if it gets beyond a few minutes, you’ll start seeing warnings in the logs), but it’s best to minimize.
Having in-sync clocks also makes figuring out what’s happening from logs easier.
You can keep clocks synchronized using the w32tm tool on Windows and the ntp daemon on Linux.
The OOM Killer Very occasionally, MongoDB will allocate enough memory that it will be targeted by the OOM killer (out-of-memory killer)
This particularly tends to happen during index builds, as that is one of the only times when MongoDB’s resident memory should put any strain on the system.
If your MongoDB process suddenly dies with no errors or exit messages in the logs, check /var/log/messages (or wherever your kernel logs such things) to see if it has any messages about terminating mongod.
If the kernel has killed MongoDB for memory overuse, you should see something like this in the kernel log:
If you were running with journaling, you can simply restart mongod at this point.
If you were not, restore from a backup or resync the data from a replica.
The OOM killer gets particularly nervous if you have no swap space and start running low on memory, so a good way to prevent it from going on a spree is to configure a modest amount of swap.
MongoDB should never use it, but it makes the OOM killer happy.
If the OOM killer kills a mongos, you can simply restart it.
Turn Off Periodic Tasks Check that there aren’t any cron jobs or daemons that might periodically pop to life and steal resources.
These programs will come to life, consume a ton of RAM and CPU, and then disappear.
This is not something that you want running on your production server.
This means that, on most platforms, you can download an archive from http://www.mongodb.org/ downloads, inflate it, and run the binary.
The MongoDB server requires a directory it can write database files to and a port it can listen for connections on.
This section covers the entire install on the two variants of system: Windows and everything else (Linux, Max, Solaris)
When we speak of “installing MongoDB,” generally what we are talking about is setting up mongod, the core database server.
Most of the time, this will be the MongoDB process you are using.
Choosing a Version MongoDB uses a fairly simple versioning scheme: even-point releases are stable, and odd-point releases are development versions.
Let’s take the 2.4/2.5 release as a sample case to demonstrate how the versioning timeline works:
This is a major release and will have an extensive changelog.
This is the new development branch that is fairly similar to 2.4.0 but probably with an extra feature or two and maybe some bugs.
Developers are conservative about what is backported; few new features are ever added to a stable release.
After extensive testing of 2.6.0-rc0, usually there are a couple minor bugs that need to be fixed.
You can see how close a production release is by browsing the core server roadmap on the MongoDB bug tracker.
If you are running in production, you should use a stable release.
If you are planning to use a development release in production, ask about it first on the mailing list or IRC to get the developers’ advice.
If you are just starting development on a project, using a development release may be a better choice.
By the time you deploy to production, there will probably be a stable release with the features you’re using (MongoDB attempts to stick to a regular cycle of stable releases every six months)
However, you must balance this against the possibility that you would run into server bugs, which can be discouraging to a new user.
Use the advice in the previous section to choose the correct version of MongoDB.
When you click the link, it will download the .zip.
Now you need to make a directory in which MongoDB can write database files.
You can create this directory or any other empty directory anywhere on the filesystem.
If you chose to use a directory other than \data\db, you’ll need to specify the path when you start MongoDB, which is covered in a moment.
Now that you have a data directory, open the command prompt (cmd.exe)
Navigate to the directory where you unzipped the MongoDB binaries and run the following:
If you chose a directory other than C:\data\db, you’ll have to specify it here, with the --dbpath argument:
See Chapter 20 for more common options, or run mongod.exe --help to see all options.
Installing as a Service MongoDB can also be installed as a service on Windows.
To install, simply run with the full path, escape any spaces, and use the --install option.
It can then be started and stopped from the Control Panel.
Go to the MongoDB downloads page, and select the correct version for your OS.
Macs are especially picky that you choose the correct build and will refuse to start MongoDB and give confusing error messages if you choose the wrong build.
You can check what you’re running by clicking the apple in the upper-left corner and selecting the About This Mac option.
You must create a directory for the database to put its files.
By default, the database will use /data/db, although you can specify any other directory.
If you create the default directory, make sure it has the correct write permissions.
You can create the directory and set the permissions by running the following:
Of course, you can also just create a directory in your home folder and specify that MongoDB should use that when you start the database, to avoid any permissions issues.
See section TODO for a summary of the most common options, or run mongod with --help to see all the possible options.
Installing from a Package Manager On these systems, there are many package managers that can also be used to install MongoDB.
If you prefer using one of these, there are official packages for RedHat, Debian, and Ubuntu as well as unofficial packages for many other systems.
If you use an unofficial version, make sure it installs a relatively recent version.
If you go for the MacPorts version, be forewarned: it takes hours to compile all the Boost libraries, which are MongoDB prerequisites.
Regardless of the package manager you use, it is a good idea to figure out where it is putting the MongoDB log files before you have a problem and need to find them.
It’s important to make sure they’re being saved properly in advance of any possible issues.
It is not necessary to understand MongoDB’s internals to use it effectively, but it may be of interest to developers who wish to work on tools, contribute, or simply understand what’s happening under the hood.
Because documents are used extensively for communication in MongoDB, there also needs to be a representation of documents that is shared by all drivers, tools, and processes in the MongoDB ecosystem.
That representation is called Binary JSON, or BSON (no one knows where the J went)
The database understands BSON, and BSON is the format in which documents are saved to disk.
When a driver is given a document to insert, use as a query, and so on, it will encode that document to BSON before sending it to the server.
Likewise, documents being returned to the client from the server are sent as BSON strings.
This BSON data is decoded by the driver to its native document representation before being returned to the client.
In the worst case BSON is slightly less efficient than JSON; and in the best case (e.g., when storing binary data or large numerics), it is much more efficient.
Traversability In some cases, BSON does sacrifice space efficiency to make the format easier to traverse.
For example, string values are prefixed with a length rather than relying on a terminator to signify the end of a string.
This traversability is useful when the MongoDB server needs to introspect documents.
Performance Finally, BSON is designed to be fast to encode to and decode from.
It uses C-style representations for types, which are fast to work with in most programming languages.
Wire Protocol Drivers access the MongoDB server using a lightweight TCP/IP wire protocol.
The protocol is documented on the MongoDB wiki but basically consists of a thin wrapper around BSON data.
For example, an insert message consists of 20 bytes of header data (which includes a code telling the server to perform an insert and the message length), the collection name to insert into, and a list of BSON documents to insert.
Data Files Inside of the MongoDB data directory, which is /data/db/ by default, there are separate files for each database.
Each database has a single .ns file and several data files, which have monotonically increasing numeric extensions.
The numeric data files for a database will double in size for each new file, up to a maximum file size of 2 GB.
This behavior allows small databases to not waste too much space on disk, while keeping large databases in mostly contiguous regions on disk.
Preallocation happens in the background and is initiated every time that a data file is filled.
This means that the MongoDB server will always attempt to keep an extra, empty data file for each database to avoid blocking on file allocation.
Namespaces and Extents Within its data files, each database is organized into namespaces, each storing a specific collection’s data.
The documents for each collection have their own namespace, as does each index.
Metadata for namespaces is stored in the database’s .ns file.
The data for each namespace is grouped on disk into sections of the data files, called extents.
In Figure B-1 the foo database has three data files, the third of which has been preallocated and is empty.
The first two data files have been divided up into extents belonging to several different namespaces.
Figure B-1 shows us several interesting things about namespaces and extents.
Each namespace can have several different extents, which are not (necessarily) contiguous on disk.
Like data files for a database, extents for a namespace grow in size with each new allocation.
This is done to balance wasted space used by a namespace versus the desire to keep data for a namespace mostly contiguous on disk.
The figure also shows a special namespace, $freelist, which keeps track of extents that are no longer in use (e.g., extents from a dropped collection or index)
When a namespace allocates a new extent, it will first search the freelist to see whether an appropriately sized extent is available.
Memory-Mapped Storage Engine The default storage engine (and only supported storage engine at the time of this writing) for MongoDB is a memory-mapped engine.
When the server starts up, it memory maps all its data files.
It is then the responsibility of the operating system to manage flushing data to disk and paging data in and out.
MongoDB’s code for managing memory is small and clean because most of that work is pushed to the operating system.
The virtual size of a MongoDB server process is often very large, exceeding the size of the entire data set.
This is OK because the operating system will handle keeping the amount of data resident in memory contained.
This is because all of the data must be addressable using only 32 bits.
We’d like to hear your suggestions for improving our indexes.
About the Author Kristina Chodorow is a software engineer who worked on the MongoDB core for five years.
She led MongoDB’s replica set development as well as writing the PHP and Perl drivers.
She has given talks on MongoDB at meetups and conferences around the world and maintains a blog on technical topics at http://www.kchodorow.com.
Colophon The animal on the cover of MongoDB: The Definitive Guide, Second Edition is a mongoose lemur, a member of a highly diverse group of primates endemic to Madagascar.
Freed from competition with other African species (such as monkeys and squirrels), lemurs adapted to fill a wide variety of ecological niches, branching into the almost 100 species known today.
These animals’ otherworldly calls, nocturnal activity, and glowing eyes earned them their name, which comes from the lemures (specters) of Roman myth.
Malagasy culture also associates lemurs with the supernatural, variously considering them the souls of ancestors, the source of taboo, or spirits bent on revenge.
Some villages identify a particular species of lemur as the ancestor of their group.
Females and young lemurs have white beards, while males have red beards and cheeks.
Mongoose lemurs eat fruit and flowers and they act as pollinators for some plants; they are particularly fond of the nectar of the kapok tree.
One of the two species of lemur found outside of Madagascar, they also live in the Comoros Islands (where they are believed to have been introduced by humans)
They have the unusual quality of being cathemeral (alternately wakeful during the day and at night), changing their activity patterns to suit the wet and dry seasons.
Mongoose lemurs are threatened by habitat loss and they are classified as a vulnerable species.
