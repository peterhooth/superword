A catalogue record for this book is available from the British Libmry.
Distributed systems and distributed information processing have received considerable attention in the past few years, and almost every university offers at least one course on the design of distributed algorithms.
It has been remarked that algorithms are the backbone of every computer application; therefore a text devoted solely to distributed algorithms seems to be justified.
The aim of this book is to present a large body of theory about distributed algorithms, which has been developed over the past twenty years or so.
This book can be used as a textbook for a one- or two-semester course on distributed algorithms; the teacher of a one-semester course may select topics to his own liking.
The book will also provide useful background and reference information for professional engineers and researchers working with distributed systems.
The projects usually require the reader to develop a small but non-trivial extension or application of the material treated in the chapter, and in most cases I do not have a "solution"
If the reader succeeds in working out one of these small projects, I would be pleased to have a copy of the result.
A list of answers (sometimes partial) to most of the exercises is available for teachers; it can be obtained from the author or by anonymous ftp.
The main topic of this book, however, is not what these systems look like, or how they are used, but how they can be made to work.
And even that topic will be further specialized towards the treatment of the algorithms used in the systems.
Of course, the entire structure and operation of a distributed system is not fully understood by a study of its algorithms alone.
To understand such a system fully one must also study the complete architecture of its hardware and software, that is , the partition of the entire functionality into modules.
Also, there are many important questions related to properties of the programming languages used to build the software of distributed systems.
Distributed computer systems may be preferred over sequential systems, or their use may simply be unavoidable, for various reasons, some of which are discussed below.
The choice of a distributed system may be motivated by more than one of the arguments listed below, and some of the advantages may come as a spin-off after the choice has been made for another reason.
Distributed systems have the potential to be more reliable than stand-alone systems because they have a partial-failure property.
By this it is meant that some nodes of the system may fail ,  while others are still operating correctly and can.
By a computer network we mean a collection of computers , connected by communication mechanisms by means of which the computers can exchange information.
Depending on the distance between the computers and their ownership, computer networks are called either wide-area networks or local-area networks.
In wide-area networks the probability that something will go wrong during the transmission of a message can never be ignored; distributed algorithms for wide-area networks are usually designed to cope with this possibility.
Local-area networks are much more reliable, and algorithms for them can be designed under the assumption that communication is completely reliable.
In this case, however, the unlikely event in which something does go wrong may go undetected and cause the system to operate erroneously.
Even though in local-area networks not all nodes are necessarily equal, it is usually possible to agree on common software and protocols to be used within a single organization.
In wide-area networks a variety of protocols is in use, which poses problems of conversion between different protocols and of designing software that is compatible with different standards.
Within a single organization all users may be trusted, but in a wide-area network this is certainly not the case.
Introduction: Distributed Systems network requires the development of secure algorithms, safe against offensive users at other nodes.
Much pioneering work in the development of wide-area computer networks was done in projects of the Advanced Research Projects Agency (ARPA) of the United States Department of Defense.
The network ARPANET became operational in 1969, and connected four nodes at that time.
This network has grown to several hundreds of nodes, and other networks have been set up using similar technology (MILNET, CYPRESS)
The ARPANET contains special nodes (called interface message processors (IMPs) ) whose only purpose is to process the message traffic.
In more technical language, the structure is represented by a graph, of which the edges represent the communication lines of the network.
A summary of graph theory terminology is given in Appendix B.
This problem occurs not only for two nodes directly connected by a communication line, but also for nodes not directly connected, but communicating with the help of intermediate nodes.
In this case the problem is even more complicated, because in addition messages may arrive in a different order from that in which they were sent , may arrive only after a very long period of time, or may be duplicated.
In a point-to-point network it is usually too expensive to provide a communication line between each pair of nodes.
Consequently, some pairs of nodes must rely on other nodes in order to communicate.
The problem of routing concerns the selection of a path (or paths) between nodes that want to communicate.
The algorithm used to select the path is related to the scheme by which nodes are named, i .e.
Path selection in intermediate nodes is done using the address , and the selection can be done more efficiently if topological information is "coded" in the addresses.
Networks connect computers having different owners , some of whom may attempt to abuse or even disrupt the installations of others.
A local-area network is used by an organization to connect a collection of computers it owns.
Usually, the main purpose of this connection is to share.
Incidentally, networks are also used to speed up computations (by shunting tasks to other nodes) and to allow some nodes to be used as a stand-by for others in the event of their failure.
In the first half of the nineteen-seventies , the Ethernet2 local-area network was developed by the Xerox Corporation.
While the names of wide-area networks , ARPANET, BITNET, etc.
There was one ARPANET, one BITNET, and one UUCP net , and there is one Internet , but each company may set up its own private Ethernet , Token Ring, or SNA network.
The design of SNA was complicated by the requirement of making it compatible with each of the many networking products already offered by IBM.
Some tasks must be carried out by exactly one process of a set , for example, generating output or initializing a data structure.
If, as is sometimes desirable or necessary, no process is assigned this task a priori , a distributed algorithm must be executed to select one of the processes to perform the task.
If the main design objective of the multiprocessor computer is to improve the speed of computation, it is often called a parallel computer.
If its main design objective is to increase the reliability, it is often called a replicated system.
A Transputer chip contains a central processing unit (CPU), a dedicated floating point unit (FPU) , a local memory, and four dedicated communication processors.
Inmos also produces special chips dedicated to communication, called routers.
Each router can simultaneously handle the traffic of 32 Transputer links.
Each incoming message is inspected to see via which link it must be forwarded; it is then sent via that link.
The popularity of these machines peaked in the first half of the nineties.
Another example of a parallel computer is the Connection Machine CM-54
In a parallel computer the computation is divided into subcomputations , each performed by one of the nodes.
In a replicated system each node carries out the entire computation, after which the results are compared in order to detect and correct errors.
The Inmos C104 routing chips use a very simple routing algorithm.
In a replicated system there must be a mechanism to overcome failures in one or more processors.
The assumptions under which replicated systems must remain correct are much more severe since a processor may produce an erroneous answer but otherwise cooperate in the protocols exactly like a correctly operating processor.
Voting mechanisms must be implemented to filter the results of the processors, so that only correct answers are delivered as long as a majority of the processors operates correctly.
A classical example to illustrate this simplification is Conway 's record conversion.
After each input record an additional blank must be inserted, and each pair of asterisks ( "**" ) must be replaced by an exclamation mark ("!")
Each output record must be followed by an end-of-record (EOR) character.
The conversion can be carried out by a single program, but writing this program is very intricate.
A semaphore [Dij68] is a non-negative (integer) variable whose value can be read and written in one atomic operation.
A V operation increments its value, and a P operation decrements its value when it is positive (and suspends the process executing this operation as long as the value is zero)
Semaphores are an appropriate tool to implement mutual exclusion on a shared data structure: the semaphore is initialized to 1 ,  and access to the structure is preceded by a P operation and followed by a V operation.
Semaphores put a large responsibility for correct use onto each process; the integrity of shared data is violated if a process manipulates the data erroneously or does not execute the required P and V operations.
A pipe [Bou83] is a mechanism that moves a data stream from one process to another and synchronizes the two communicating processes; it is a pre-programmed solution to the producer-consumer problem.
The software for implementing computer communication networks is very complicated.
In this section it is explained how this software is usually structured in acyclically dependent modules called layers (Subsection 1.2.1)
Also the languages used for programming distributed systems are briefly discussed (Subsection 1.2.4)
The complexity of the tasks performed by the communication subsystem of a distributed system requires that this subsystem is designed in a highly structured way.
To this end, networks are always organized as a collection of modules, each performing a very specific function and relying on services offered by other modules.
In network organizations there is always a strict hierarchy between these modules, because each module exclusively uses the services offered by the previous module.
The modules are called layers or levels in the context of network implementation; see Figure 1.4
The functionality of each layer must be implemented by a distributed algorithm, such that the algorithm for layer i solves a "problem" defined by the i/ (i + 1) interface, under the "assumptions" defined in the (i - l ) /i interface.
The solution of this problem defines the type of messages exchanged by the layer-i processes and the meaning of these messages, i .e.
The rules and conventions used in the "conversation" between the processes at layer i are referred to as the layer-i protocol.
The same holds at higher layers: the layer interfaces serve to screen the implementation of a layer from other layers, and the implementation can be changed without affecting the other layers.
The OSI reference model consists of seven layers , namely the physical, data-link, network, transport, session, presentation, and application layers.
The reference model specifies the interfaces between the layers and provides, for each layer, one or more standard protocols (distributed algorithms to implement the layer)
The purpose of the data-link layer is to mask the unreliability of the physical layer, that is , to provide a reliable link to the higher layers.
The data-link layer only implements a reliable connection between nodes that are directly connected by a physical link, because it.
To achieve its goal, the layer divides the bitstream into pieces of  fixed length, called frames.
The receiver of a frame can check whether the frame was received correctly by verifying its checksum, which is some redundant information added to each frame.
There is a feedback from the receiver to the sender to inform the sender about correctly or incorrectly received frames; this feedback takes place by means of acknowledgement messages.
The sender will send a frame anew if it turns out that it is received incorrectly or completely lost.
The purpose of the network layer is to provide a means of communication between all pairs of nodes , not just those connected by a physical channel.
This layer must select the routes through the network used for communication between non-adjacent nodes and must control the traffic load of each node and channel.
Although the data-link layer provides reliable service to the network layer, the service offered by the network layer is not reliable.
Messages (called packets in this layer) sent from one node to the other may follow different paths, causing one message to overtake the other.
Owing to node failures messages may be lost (a node may go down while holding a message) , and owing to superfluous retransmissions messages may even be duplicated.
The layer may guarantee a bounded packet lifetime; i .e.
The purpose of the transport layer is to mask the unreliability introduced by the network layer, Le.
The problem would be similar to the one solved by the data-link layer, but it is complicated by the possibility of the duplication and reordering of messages.
This makes it impossible to use cyclic sequence numbers, unless a bound on the packet lifetime is guaranteed by the network layer.
The purpose of the session layer is to provide facilities for maintaining connections between processes at different nodes.
A connection can be opened and closed and between opening and closing the connection can be used for data exchange, using a session address rather than repeating the address of the remote process with each message.
The session layer uses the reliable end-to-end communication offered by the transport layer, but structures the exchanged messages into sessions.
A session can be used for file transfer or remote login.
The session layer can provide the mechanisms for recovery if a node crashes during a session and for mutual exclusion if critical operations may not be performed at both ends simultaneously.
The purpose of the application layer is to fulfill concrete user requirements such as file transmission, electronic mail, bulletin boards, or virtual terminals.
The wide variety of possible applications makes it impossible to standardize the complete functionality of this layer, but for some of the applications listed here standards have been proposed.
In answer to these problems the IEEE has approved additional standards, covering only the lower levels of the OSI hierarchy, to be used in local-area networks (or, to be more precise, all networks that are bus-structured rather than point-to-point)
Because no single standard could be general enough to encompass all networks already widely in use, the IEEE has approved three different , non-compatible standards, namely CSMAjCD, token bus, and token ring.
The data-link layer is replaced by two sublayers, namely the medium access control and the logical link control sublayers.
The purpose of the physical layer in the IEEE standards is similar to that of the original ISO standard, namely to transmit sequences of bits.
The purpose of this sublayer is to resolve conflicts that arise between nodes that want to use the shared communication medium.
A statical approach would once and for all schedule the time intervals during which each node is allowed to use the medium.
This method wastes a lot of bandwidth, however, if only a few nodes have data to transmit , and all other nodes are silent ; the medium remains idle during the times scheduled for the silent nodes.
In token buses and token rings access to the medium is on a round-robin basis: the nodes circulate a privilege, called the token, among them and the node holding this token is allowed to use the medium.
If the node holding the token has no data to transmit , it passes the token to the next node.
In a token ring the cyclic order in which nodes get their turn is determined by.
In the carrier sense multiple access with collision detection (CSMA/CD) standard, nodes observe when the medium is idle, and if so they are allowed to send.
If two or more nodes start to send (approximately) simultaneously, there is a collision, which is detected and causes each node to interrupt its transmission and try again at a later time.
The purpose of this layer is comparable to the purpose of the data-link layer in the OSI model, namely to control the exchange of data between nodes.
The layer provides error control and flow control , using techniques similar to those used in the OSI protocols , namely sequence numbers and acknowledgements.
In this subsection we describe some of the constructs one may observe in actual programming languages designed for distributed systems.
We confine ourselves here to a brief description of these constructs; for more details and examples of actual languages that use the various constructs, see, e.g.
Parallelism is usually expressed by defining several processes, each being a sequential entity with its own state space.
A language may either offer the possibility of statically defining a collection of processes or allow the dynamic creation and termination of processes.
It is also possible to express parallelism by means of parallel statements or in a functional programming language.
Parallelism is not always explicit in a language; the partitioning of code into parallel processes may be performed by a sophisticated compiler.
Introduction: Distributed Systems send operation has been executed, which implements the synchronization.
In some languages a receive operation is not available explicitly; instead, a procedure or operation is activated implicitly when a message is received.
A language may provide synchronous message passing, in which case the send operation is completed only after execution of the receive operation.
In other words, the sender is blocked until the message has been received, and a two-way synchronization between sender and receiver results.
The term multicast is also used to refer to messages that are sent to a collection of (not necessarily all) processes.
A somewhat more structured communication primitive is the remote procedure call (RPC)
To communicate with process b, process a calls a procedure present in process b by sending the parameters of the procedure in a message; a is suspended until the result of the procedure is returned in another message.
The previous sections have given reasons for the use of distributed computer systems and explained the nature of these systems; the need to program these systems arises as a consequence.
The programming of distributed systems must be based on the use of correct, flexible, and efficient algorithms.
Distributed Algorithms 27 In this section it is argued that the development of distributed algorithms is a craft quite different in nature from the craft used in the development of centralized algorithms.
This book is intended to introduce the reader to this research field.
Distributed systems differ from centralized (uniprocessor) computer systems in three essential respects, which we now discuss.
In a centralized algorithm control decisions can be made based upon an observation of the state of the system.
Even though the entire state usually cannot be accessed in a single machine operation, a program may inspect the variables one by one, and make a decision after all relevant information has been considered.
No data is modified between the inspection and the decision, and this guarantees the integrity of the decision.
This information can only be deduced indirectly by comparing information about messages sent and received by the nodes.
The events constituting the execution of a centralized algorithm are totally ordered in a natural way by their temporal occurrence; for each pair of events, one occurs earlier or later than the other.
The temporal relation induced on the events constituting the execution of a distributed algorithm is not total; for.
The combination of lack of knowledge of the global state, lack of a global time-frame, and non-determinism makes the design of distributed algorithms an intricate craft ,  because the three aspects interfere in several ways.
The concepts of time and state are highly related; in centralized systems the notion of time may be defined by considering the sequence of states assumed by the system during its execution.
Those alternative sequences usually consist of different global states; this gives the statement "the system assumed this or that state during its execution" a very dubious meaning.
We shall illustrate the difficulties imposed by the lack of knowledge of the global state and the lack of a global time-frame by an example problem, discussed by Belsnes [BeI76] , namely the reliable exchange of information via an unreliable medium.
Consider two processes , a and b, connected by a data network, which transmits messages from one process to the other.
A message may be received an arbitrarily long time after it is sent , but may also be lost altogether in the network.
The reliability of the communication is increased by the use of network control procedures (NCPs) , via which a and b access the network.
Process a initiates the communication by giving an information unit m to NCP A.
The interaction between the NCPs (via the data network, DN) must ensure that the information m is delivered at process b (by NCP B) , after which a is notified of the delivery (by NCP A)
Even i f  only a single information unit i s  to  be  transported from a to  b, the network's unreliability forces NCP A and NCP B to engage in a conversation consisting of several messages.
They maintain state information about this conversation, but because the number of possible conversation partners for each process is large, it is required that state information is discarded after the message exchange is complete.
The initialization of status information is called opening and its discarding is called closing the conversation.
Introduction: Distributed Systems that , after closing the conversation, an NCP is in exactly the same state as before opening it ; this is called the closed state.
Information unit m is said to be lost if a was notified of its receipt by b, but the unit was never actually delivered to b.
Unit m is said to be duplicated if it was delivered twice.
It is assumed that NCPs may fail (crash) , after which they are restarted in the closed state (effectively losing all information about a currently open conversation)
As a first observation, it can be shown that no matter how intricately the NCPs are designed, it is not possible to achieve completely reliable communication.
This observation can be made independently of the design of the data network or the NCPs and only relies on the assumption that an NCP may lose information about an active conversation.
To see this, assume that after initialization of a communication by a, NCP A and NCP B start a conversation, during which NCP B is supposed to deliver m to b after the receipt of a message M from NCP A.
Consider the case where NCP B crashes and is restarted in the closed state after NCP A has sent message M.
In this situation, neither NCP A nor NCP B can tell whether m was already delivered when NCP B crashed; NCP A because it cannot observe the events in NCP B (lack of knowledge of the global state) and NCP B because it has crashed and was restarted in the closed state.
Regardless of how the NCPs continue their conversation, an error may be introduced.
If NCP A sends the message to NCP B again and NCP B delivers the message, a duplication may arise.
If notification to a is given without delivery, a loss may arise.
We shall now evaluate several possible designs for the NCPs with respect to the possibility of losing or duplicating messages.
We shall try to design the protocols in such a way that losses are avoided in any case.
In the simplest possible design, NCP A sends the data unaltered via the network, notifies a, and closes, in a single action upon initialization.
This protocol introduces a loss whenever the network refuses to deliver a message, but there is no possibility of introducing duplications.
When this message is received, NCP A closes the conversation.
But not only do acknowledgements introduce the possibility of duplicates, they also fail to safeguard against losses, as the following scenario shows.
Process a offers two information units, ml and m2 ,  for transmission.
The slow delivery is not detected due to the lack of a global time.
The problem of reliable interprocess communication can be solved more easily if a weak notion of global time is assumed, namely, that there exists an upper bound T on the transmission delay of any message sent through the network.
This is considered a global timing assumption, because it induces a temporal relation between events in different nodes (namely, the sending by NCP A and a receipt by NCP B)
The receipt of messages from earlier conversations can be prevented in this protocol by closing the conversation in NCP A only 2T after sending the last message.
A loss of the ( data, m ) message causes a timeout in NCP A, in which case NCP A retransmits the message.
A loss of the ( ack ) message also causes a retransmission of ( data, m ) , but this does not lead to a duplication because NCP B has an open conversation and recognizes the message it has already received.
Again the problem has arisen because messages of one conversation have interfered with another conversation.
This can be ruled out by selection of a pair of new conversation identification numbers for each new conversation, one by NCP A and one by NCP B.
The numbers chosen are included in all messages of the conversation, and are used to verify that a received message indeed belongs to the current conversation.
The normal conversation of the three-message protocol is as follows.
However, NCP B does not verify the validity of a ( data, m, x ) before delivering m (in step 2) , which easily leads to duplication of information.
If the message sent in step 1 is delayed and retransmitted, a later-arriving ( data, m, x ) message causes NCP B to deliver information m again.
Of course, NCP B should also verify the validity of messages it receives before delivering the data.
Notification is now given by NCP A before delivery by NCP B, but because NCP B has already received the information this seems justified.
It must be ensured, though, that NCP B will now deliver the data in any case; in particular, when the ( close, x ,  y ) message is lost.
The possibility of a crash of NCP B forces the error handling to be such that a duplicate may still occur, even when no NCP actually crashes.
An error message ( nocon, x, y )  is sent by NCP B when an ( agree, x, y )  message is received and no conversation is open.
Assume that NCP A does not receive an ( ack, x ,  y )  message, even after several retransmissions of ( agree, x, y ) ; only ( nocon, x ,  y )  messages are received.
Because it is possible that NCP B crashed before it received ( agree, x ,  Y ) , NCP A is forced to start a new conversation (by sending ( data, m, x ) ) to prevent loss of m! But it is also.
It is possible to modify the protocol in such a way that NCP A notifies and closes upon receipt of the ( nocon, x, y )  message; this prevents duplicates, but may introduce a loss, which is considered even less desirable.
There are several journals and annual conferences that specialize in results concerning distributed algorithms and distributed computation.
Some other journals and conferences do not specialize solely in this subject but contain.
This book was written with the following three objectives in mind.
To make the reader familiar with techniques that can be used to investigate the properties of a given distributed algorithm, to analyze and solve a problem that arises in the context of distributed systems, or to evaluate the merits of a particular network model.
To provide insight into the inherent possibilities and impossibilities of several system models.
The impact of process failures is studied in Part Three.
Where a subject cannot be treated in full detail, references to the relevant scientific literature are given.
The material collected in the book is divided into three parts: Protocols , Fundamental Algorithms, and Fault Tolerance.
This part deals with the communication protocols used in the implementation of computer communication networks and also introduces the techniques used in later parts.
In Chapter 2 the model that will be used in most of the later chapters is introduced.
The model is both general enough to be suitable for the development and verification of algorithms and tight enough for proving impossibility results.
It is based on the notion of transition systems, for which proof rules for safety and liveness properties can be given easily.
The notion of causality as a partial order on events of a computation is introduced and logical clocks are defined.
Chapter 4 considers the problem of routing in computer networks.
It first presents some general theory about routing and an algorithm by Toueg for the computation of routing tables.
Also treated are the Netchange algorithm of Tajibnapis and a correctness proof for this algorithm given by Lamport.
This chapter ends with compact routing algorithms, including interval and prefix routing.
These algorithms are called compact routing algorithms, because they require only a small amount of storage in each node of the network.
This chapter also defines the time complexity of distributed algorithms and examines the time and message complexity of a number of distributed depth-first search algorithms.
Chapter 9 studies the computational power of systems where processes are not distinguished by unique identities.
It was shown by Angluin that in this case many computations cannot be carried out by a deterministic algorithm.
The chapter introduces probabilistic algorithms, and we investigate what kind of problems can be solved by these algorithms.
Chapter 10 explains how the processes of a system can compute a global "picture" , a snapshot , of the system's state.
Such a snapshot is useful for determining properties of the computation, such as whether a deadlock has occurred, or how far the computation has progressed.
Snapshots are also used to restart a computations after occurrence of an error.
Chapter 16 studies the properties of abstract mechanisms, referred to as failure detectors, by which processes can obtain an estimated view of crashed and correct processes.
We show implementations of such mechanisms and how they can be used to implement specifications in faulty environments.
An algorithm is stabilizing if, regardless of its initial configuration, it converges eventually to its intended behavior.
Some theory about stabilizing algorithms will be developed, and a number of stabilizing algorithms will be presented.
Also, stabilizing algorithms for data transmission (as in Chapter 3) have.
A distributed computation is usually considered as a collection of discrete events, each event being an atomic change in the configuration (the state of the entire system)
What makes a system "distributed" is that each transition is only influenced by, and only influences, part of the configuration, basically the (local) state of a single process.
Or the local states of a subset of interacting processes.
A system whose state changes in discrete steps (transitions or events) can usually be conveniently described by the notion of a transition system.
In the study of distributed algorithms this applies to the distributed system as a whole , as well as to the individual processes that cooperate in the algorithm.
Processes in a distributed system communicate either by accessing shared variables or by message passing.
We shall take a more restrictive point of view and consider only distributed systems where processes communicate by exchanging messages.
A transition system consists of the set of all possible states of the system, the transitions ( "moves" ) the system can make in this set , and a subset of states in which the system is allowed to start.
To avoid confusion between the states of a single process and the states of the entire algorithm (the "global states" ) ,  the latter will from now on be called configurations.
A transition system is a triple S = (C , -----t , 'I) , where C zs a set of configurations, -----t is a binary transition relation on C, and 'I is a subset of C of initial configurations.
A transition relation is a subset of C x C.
Configuration 8 is reachable if it is reachable from an initial configuration.
Let M be a set of possible messages and denote the collection of multisets with elements from M by M(M)
In the sequel we shall denote processes by p, q, r, PI , P2 , etc.
The executions of a process are the executions of the transition system (Z ,  f-, I)
We will however be interested in executions of  the entire system, and in such an execution the executions of the processes are coordinated through the communication subsystem.
To describe the coordination, we shall define a distributed system as a transition system where the configuration set , transition relation, and initial states are constructed from the corresponding components of the processes.
It i s  assumed that for each message there i s  a unique process that can receive the message.
We illustrate this by an elementary example in which two processes are to send each other some information.
In the asynchronous case , each process may first send a message and subsequently receive the message from the other process.
In the synchronous case , no such buffering is possible, and if both processes must send their own message before they can receive a message, no transition can take place.
In the synchronous case, one of the processes must receive the message of the other process before it sends its own message.
Needless to say, if both processes must receive a message before sending their own message, again no transition can take place.
An exchange of two messages can take place in a synchronous system only if one of the following two conditions is satisfied.
It is determined in advance which of the two processes will send first , and which one will receive first.
In many cases it is not possible to make this choice in advance, because it requires the two processes to execute different local algorithms.
The processes have the non-deterministic choice either to send first and receive next , or to receive first and send next.
In each execution one of the possible execution orders will be chosen for each process , i .e.
When we present an algorithm for asynchronous message passing and state that the algorithm can also be used under synchronous message passing, the addition of this non-determinism, which is always possible, is assumed implicitly.
It has been argued that fairness assumptions should not be made; rather, algorithms should be designed so as not to rely on these assumptions.
A discussion of some intricate issues related to fairness assumptions is found in [Fra86]
These requirements may also occur in a weakened form, for example, that they must be satisfied with some fixed probability over the set of possible executions.
The verification methods described in this section are based on the truth of assertions in the configurations reached in an execution.
An assertion is a unary relation on the set of configurations, that is, a predicate that is true for a subset of the configurations and false for others.
A safety property of an algorithm is a property of the form "Assertion P is true in each configuration of each execution of the algorithm"
Informally this is formulated as "Assertion P is always true"
The basic technique for showing that assertion P is always true is to demonstrate that P is an invariant according to the definitions that are to follow.
The definition says that an invariant holds in each initial configuration, and is preserved under each transition.
It follows that it holds in each reachable configuration, as is formulated in the following theorem.
Theorem 2.10 If P is an invariant of S, then P holds for each configuration of each execution of S.
Second, assume P( ,i ) holds and ,i ---t ,HI is a transition that occurs in E.
Then P holds in each configuration of each execution of S.
It is sometimes useful to prove a weak invariant first , and subsequently use this to prove a stronger invariant.
How an invariant can be made stronger is demonstrated by the following definition and theorem.
According to Definition 2.9 ,  it must be shown that.
On the other hand, termination is allowed if the goal has been reached; in this case one speaks of proper termination.
Norm functions rely on the mathematical concept of well-founded sets.
This is a set with an order < ,  for which there are no infinite decreasing sequences.
I f  E i s  finite, its last configuration is a terminated configuration, and as term =? P is always true in S, P holds in this configuration.
By the choice of E' and the property of f, s is a decreasing sequence, and hence, by the well-foundedness of W, s is finite.
It then suffices to show that f never increases, but decreases with each transition of this type.
The view on executions as sequences of transitions naturally induces a notion of time in executions.
A transition a is then said to occur earlier than transition b if a occurs in the sequence before b.
Observe that each execution defines a unique sequence of events in this way.
In such a diagram, a horizontal line is drawn for each process, and each event is drawn as a dot on the line of the process where it takes place.
If a message m is sent in event s and received in event r, an arrow is drawn from s to r;  the events s and r are said to be corresponding events in this case.
Therefore a notion of time as a total order on the events of an execution is not suitable for distributed executions, and instead the notion of causal dependence is introduced.
It has been remarked already that the transitions of a distributed system influence, and are influenced by, only part of the configuration.
This leads to the observation that two consecutive events, influencing disjoint parts of the configuration, are independent and can also occur in reversed order.
For systems with asynchronous message passing this is expressed in the following theorem.
Let ep and eq be two events that occur consecutively in an execution, i .e.
Indeed, the theorem explicitly states that p and q must be different , and if eq receives the message sent in ep , the receive event is not applicable in the starting configuration of ep , as also required.
Thus, if one of these two statements is true, the events cannot occur in the reversed order ; otherwise they can occur in reversed order and yet result in the same configuration.
However, from the point of view of the process these events are indistinguishable.
The relation -<, called the causal order, on the events of the execution is the smallest relation that satisfies.
If s is a send event and r the corresponding receive event, then s -< r.
We write a :::S b to denote (a -< b V a = b)
As :::S is a partial order , there may exist events a and b for which neither a :::S b nor b :::S a holds.
Such events are said to be concurrent, notation a I I  b.
In this subsection it is shown that the events of an execution can be reordered in any order consistent with the causal order, without affecting the result of the execution.
This reordering of the events gives rise to a different sequence of configurations , but this execution will be regarded as equivalent to the original execution.
If this is the case , F is called the implied execution of the sequence f.
We would like F to be uniquely determined by f ,  but this is not always the case ; if for some process p no event in p is included in f ,  the state of p can be an arbitrary initial state.
However, if f contains at least one event of p, then the first event in p, say (c, x ,  y,  d) , defines the initial state of p to be c.
Therefore, if f contains at least one event in each process, 00 is uniquely defined, and this defines the entire execution uniquely.
Then f defines a unique execution F starting in the initial configuration of E.
To show that cp = e two cases are distinguished.
In both cases we note that the causal order of E totally orders the events in process p; this implies that the events in process p occur in exactly the same order in f and E.
Case 1 :  fi is  the first event in p of f ;  then cp is  the initial state of p.
Case 2 :  Ii is not the first event in p of f ;  let the last event in p of f before fi be Ii, = (e' , x' , y' , d') ,  then Cp = d'
But then fi' is also the last event in p before fi in E, which implies that c = d'
We must finally show that the last configurations of F and E coincide if E is finite.
If E contains no event in p, the state of p in I'k equals its initial state.
Otherwise, the state of p in I'k is the state after the last event in p of E;  this is also the last event in p of f ,  so this is also the state of p in 8k.
The messages in transit in I'k are exactly those messages for which the send event is not matched by a corresponding receive event in E.
Executions F and E have the same collection of events, and the causal order of these events is the same for E and F.
It is therefore also the case that E is an permutation of the events of F that is consistent with the causal order of F.
Although the depicted executions are equivalent and contain the same collection of events, they do not contain the same collection of configurations.
A global observer, who has access to the actual sequence of the events, may distinguish between two equivalent executions, i .e.
However, the processes cannot distinguish between two equivalent executions since it is not possible for them to decide which of two equivalent executions takes place.
Assume that it must be decided whether the messages sent in events e and 1 were in transit simultaneously.
There is a boolean variable sim in one of the processes, which must be set to true if the messages were in transit.
An equivalence class under "" is completely characterized by the collection of events and the causal order on these events; the equivalence classes are called the computations of the algorithm.
Definition 2.22 A computation of a distributed algorithm is an equivalence class (under "") of executions of the algorithm.
A result from the theory of partial orders implies that each order can occur for a pair of concurrent events of a computation.
Consequently, i f  a and b are concurrent events of  a computation C, there exist executions Ea and Eb of this computation such that a takes place earlier than b in Ea , and b takes place earlier than a in Eb.
The processes in the execution have no means of deciding which of the two events takes place first.
We have defined executions first (as runs in a transition system) and subsequently introduced computations as equivalence classes of executions.
This order reflects an operational view of what happens in a distributed system.
Many other works take the more symbolic road of first introducing computations as partially ordered sets over events and subsequently introduce executions as linearizations of these posets.
This is done, for example, in the work of Charron-Bost et al.
In the remainder of this section some examples of clocks are discussed.
Thus each event is labeled by its place in the sequence of events.
In an equivalent execution, in which this event is exchanged with the next event, and consequently, has a different value of 8g , the same value i is stored in the process.
To say it in different words, 8g is defined for executions, but not for computations.
It is possible to extend the model that is the subject of this chapter by supplying each process with a hardware clock.
In this way it is possible to record for each event the real time at which it occurs; the numbers obtained satisfy the definition of a clock.
It is left as an exercise to show that with this definition 8 L is a clock.
It is not specified under what conditions a message must be sent or how the state of a process changes.
The clock is a supplementary mechanism that can be added to any distributed algorithm to order its events.
Vectors of length n are naturally ordered by the vector order, defined as follows:
Like Lamport 's clock, this function can be evaluated by a distributed algorithm.
So far, we have modeled the communication subsystem of a distributed system by the collection of messages currently in transit.
Further , it has been assumed that each message can only be received by a single process called the destination of the message.
In general it is not necessarily the case that each process can send messages to every other process.
Instead, for each process a subset of the other processes is defined (called the neighbors of the process) to which it can send messages.
If a process p can send messages to process q, a channel is said to exist from p to q.
Unless stated otherwise it is assumed that channels are bidirectional, that is, the same channel allows q to send messages to p.
A channel that accomodates only one-way traffic from p to q is called a unidirectional (or directed) channel from p to q.
Representation as a graph allows us to speak about the communication system in terms of graph theory; see Appendix B for an introduction to this terminology.
Throughout this book, unless stated otherwise, it is assumed that the topology is connected, that is, a path between any two nodes exists.
A clique is a network in which an edge exists between any two nodes.
Here V is the set of bit strings of length n:
A static topology means that the topology remains fixed during the distributed computation.
A dynamic topology means that channels (sometimes even processes) can be added or removed from the system during the computation.
These changes in the topology can also be modeled by transitions of the configuration, namely, if the process states reflect the set of neighbors of the process (see Chapter 4)
A channel is said to be reliable when every message that is sent in the channel is received exactly once (provided that the destination is able to receive the message)
Unless stated otherwise, it is always assumed in this book that the channels are reliable.
This assumption in fact adds a (weak) fairness condition; indeed, after a message has been sent , the receipt of this message (in a suitable state of the destination) is applicable from then on.
A channel that is not reliable may exhibit communication failures, which can be of several types , e .g.
The loss of a message occurs when the message is sent , but never received; it can be modeled by a transition that removes the message from M.
The garbling of a message occurs when the message received is different from the message sent ; it can be modeled by a transition that changes one message of M.
The duplication of a message occurs when the message is received more often than it is sent ; it is modeled by a transition that copies a message of M.
The creation of a message occurs when a message is received that has never been sent ; it is modeled by a transition that inserts a message in M.
A channel is said to be fifo if it respects the order of the messages sent through it.
Unless stated otherwise, fifo channels will not be assumed in this book.
Fifo channels can be represented in the model of Definition 2.6 by replacing the collection M by a set of queues, one for each channel.
Sending is done by appending a message to the end of this queue, and a receive event deletes a message from the head.
When fifo channels are assumed, a new type of communication failure arises, namely the reordering of messages in a channel; it can be modeled by a transition that exchanges two messages in the queue.
A hierarchy of delivery assumptions , consisting of full asynchronism, causally ordered delivery, fifo, and synchronous communication, was discussed by Charron-Bost et al.
The capacity is the number of messages that can be in transit in the channel at the same time.
The channel is full in each configuration in which it actually contains a number of messages equal to its capacity.
A sending event is applicable only if the channel is not full.
Usually, a bound on the message transmission time (the time between sending and receiving the message) is assumed in conjunction with the availability of real-time clocks.
This bound can also be included in the general model of transition systems.
Unless stated otherwise, real-time assumptions are not made in this book; that is , we consider fully asynchronous systems and algorithms.
Initial process knowledge is the term used to refer to information about the distributed system that is represented in the initial states of the processes.
If an algorithm is said to rely on such information it is assumed that the relevant information is correctly stored in the processes prior to the start.
Information about the topology includes : the number of processes , the diameter of the network graph, and the topology of the graph.
The network is said to have a sense of direction if a consistent edge-labeling with directions in the graph is known to the processes (see Appendix B)
In many algorithms it i s  required that the processes have unique names (identities) ,  and that each process knows its own name initially.
Further assumptions can be made regarding the set from which the names are chosen, such as that the names are linearly ordered or that they are (positive) integers.
Unless stated otherwise , in this book it will always be assumed that processes do have access to their identity; in this case the system is referred to as a named network.
If processes are distinguished by a unique name, it is possible to assume that each process knows initially the names of its neighbors.
This assumption is referred to as neighbor knowledge and, unless stated otherwise, will not be made.
Process names may be useful for purposes of message addressing; the name of the destination of a message is given when sending a message by direct addressing.
A stronger assumption is that each process knows the entire collection of process names.
A weaker assumption is that the processes know of the existence but no of the names of their neighbors.
Direct addressing uses the process ' identity as an address , while in indirect addressing the processes p, r, and s use different names (a, b ,  and c, respectively) to address messages with destination q.
The most important property of a distributed algorithm is its correctness: it must satisfy the requirements posed by the problem that the algorithm is.
To compare different algorithms for the same problem it is useful to measure the consumption of resources by an algorithm.
The resource consumption of distributed algorithms can be measured in several ways.
This is the total number of messages exchanged by the algorithm.
Most of the algorithms in this book use messages that contain o (log N) bits (where N is the number of processes) , so their bit complexity exceeds their message complexity by a logarithmic factor.
In most cases only the message complexity of algorithms will be analyzed, and the bit complexity will be computed only for algorithms using very long or very short messages.
As our model of distributed algorithms does not contain a notion of time it is not obvious how the time complexity of distributed algorithms can be defined.
Different definitions are found in the literature (see Section 6.4 for a comparison)
The space complexity of an algorithm equals the amount of memory needed in a process to execute it.
The space in a process is the logarithm of the number of states of that process.
Assume PI and P2 are Q-derivates of a system s.
Prove that if P is an invariant of Sl and S2 , then P is an invariant of s.
Prove that if P i s  a Q-derivate of Sl and S2 , then P is a Q-derivate of S.
Give an  example where P is always true in  both Sl and S2 , but not in  S.
A set can be  well-founded, even when it has an element smaller than which there exist infinitely many elements in the set.
Define clocks for such systems, and give a distributed algorithm for computing a clock.
This chapter discusses two protocols that are used for the reliable exchange of data between two computing stations.
In the ideal case, data would simply be exchanged by sending and receiving messages.
Unfortunately, the possibility of communication errors cannot always be ignored; the messages must be transported via a physical medium, which may lose , duplicate, reorder, or garble messages transmitted through it.
These errors must be detected and corrected by supplementary mechanisms, traditionally referred to as protocols , implemented in the computing stations.
They are included in this book for different reasons ; the first protocol is completely asynchronous, while the second protocol relies on the correct use of timers.
In both cases the verification of the protocol concentrates on.
The treatment of the first protocol shows that timer-based mechanisms are not essential to achieve the required safety properties of data transmission protocols.
It is widely believed [Wat81] that the correct use of timers and a bound on the time for which a message can be in transit are necessary for a safe connection management.
Thus in order to prove the safety of protocols for connection management the role of timers must be taken into account.
It is realistic to take into account the possibility that messages are garbled during transport.
The contents of a message communicated via a physical connection can be damaged due to atmospheric noise, malfunctioning memory units ,  etc.
The receipt of a garbled message is then treated as if no message were received, and thus the garbling of a message in fact causes the message to be lost.
For this reason garbling is not treated explicitly; instead, the possibility of message loss is always considered.
In this section a symmetric protocol that allows information to be sent reliably in both directions is studied.
As it is used for the exchange of information between stations that are directly connected through a line, the fifo property of channels may be assumed.
The assumptions, requirements, and protocol are completely symmetric w.r .t.
The input of p consists of the information it must send to q, and is modeled by an infinite array inp of words.
The output of p consists of the information it receives from q, and is also modeled by an infinite array of words, outp.
For the time being it is assumed that p has random read-access to inp and random write-access to outp.
Initially the value of outp [i] is undefined and represented by udef for all i.
The input and the output of process q are modeled respectively by arrays inq and outq.
These arrays are all indexed by the natural numbers , i .e.
This is why the protocol is called a "sliding-window" protocol.
Process p contains a variable sp , indicating the lowest numbered word that p still expects to receive from q.
Thus, at any time, p has already written outp [O] through outp [sp - 1]
The required properties of the protocol can now be stated.
The safety property says that each process only outputs the correct data; the liveness property says that all data will eventually be delivered.
Transmission protocols usually rely on the use of acknowledgement messages.
An acknowledgement message is sent by the receiving process to inform the sender about data it has correctly received.
If the sender of the data does not receive an acknowledgement it assumes that the data message (or the acknowledgement) was lost , and retransmits the same data.
In the protocol of this section, however, no explicit acknowledgement messages are used.
In this protocol both stations have messages to send to the other station; the messages of a station serve also as the acknowledgements for the other station's messages.
The messages exchanged by the processes are referred to as packets, and they are of the form ( pack, w, i ) , where w is a data word and i a natural number (called the sequence number of the packet )
This packet , when sent by p (to q) , transmits the word w = inp [i] to q, but also, as mentioned earlier, acknowledges the receipt of a number of packets from q.
Process p can be a fixed number of fp packets "ahead of" q if we postulate that the data packet ( pack, w, i )  sent by p acknowledges the receipt of the words numbered O.
The meaning of the packet is analogous when sent by q.
The constants fp and lq are non-negative and known to both p and q.
The implied meaning of data packets as acknowledgements has two consequences for the transitions of the protocol :
Process p can send the word inp [i] (as the packet ( pack, inp [i] , i ) ) only after storing all the words outp [O] through outp [i - fp] , i .e.
When p receives ( pack, w, i ) , retransmission of  words from inp [O] through inp [i - lq] is no longer necessary.
A variable Op is introduced for process p (and aq for q) to indicate the lowest numbered word for which an acknowledgement has not yet been received by p (or q, respectively)
Process p can send any word of which the index falls within the bounds outlined above.
When a message is received, a check is first made if an identical message has already been received (in case it is a retransmission)
If not , the word contained in it is written to the output, and Up and Sp are updated.
There are also actions Sq, Rq , and Lq ,  with p and q reversed.
The communication subsystem is represented by two queues, Qp for the packets with destination p and Qq for the packets with destination q.
Observe that the renewed computation of sp in Rp never gives a value smaller than the previous one, so sp never decreases.
To show that this algorithm satisfies the requirements given earlier, it will first be demonstrated that the assertion P is an invariant.
In this and other assertions i is a natural number.
In each initial configuration Qp and Qq  are empty, for all i ,  outp [i] and outq [i] are udef ,  and Up, aq, Sq , and sp are 0; these imply that P is true.
The transitions of the protocol will now be considered in turn to show that they preserve P.
First note that the values of inp and of inq never change.
Sp: To see that Sp preserves (Op) , observe that Sp does not increase sp and does not make any outp [i] equal to udef.
To see that Sp preserves (Oq) , observe that Sp does not increase Sq and does not make any outq [i] equal to udef.
To see that Sp preserves (1p) , observe that Sp adds no packets to  Qp and does not decrease Sq.
To see that Rp preserves (3q) , observe that sp can only be increased in an application of Rp.
To prove the liveness of the protocol it is necessary to make some fairness assumptions and an assumption about fp and lq.
Without these assumptions the liveness requirement is not satisfied by the protocol, as can be seen by the following arguments.
The non-negative constants fp and lq are as yet unspecified; if they are both chosen to be 0 the protocol deadlocks in its initial configuration (each initial configuration is terminal)
It is therefore assumed that fp + lq > o.
A configuration of the protocol can be denoted , = (Cp, cq ,  Qp , Qq ) ,  where cp and cq are states of p and q.
Let , be a configuration in which Sp is applicable (for some i )
The protocol satisfies the eventual delivery requirement if the following two fairness assumptions are satisfied.
If the sending of a packet is applicable for an infinitely long time, the packet is sent infinitely often.
If the same packet is sent infinitely often, it is received infinitely often.
Assumption Fl ensures that a packet is retransmitted over and over again if an acknowledgement is never received; F2 excludes computations such as the one above where the retransmission is never received.
Neither of the two processes can be much ahead of the other : the difference between sp and Sq remains bounded.
This is why the protocol is called balanced, and implies that if the eventual delivery requirement is satisfied for sp , then it is also satisfied for Sq , and vice versa.
It also indicates that the protocol should not be used in situations where one process has many more words to send than the other.
It will first be demonstrated that a deadlock of the protocol is not possible.
The invariant implies that one of the two processes can send the packet containing the lowest numbered word that is still missing to the other.
Claim 3.5 P implies that the sending of ( pack, inp [sq] , Sq )  by p or the sending of ( pack, inq [sp] ,  sp ) by q is applicable.
But ,  as the receipt of a packet with sequence number sp by p causes sp to be increased (and vice versa for q) , this contradicts the assumption that neither sp nor Sq is ever increased.
It will now be shown that it suffices to store only a bounded number of words at any moment.
Whenever sp increases , p reads the next words of the sending window from the source that produces the words.
Finally it will be shown that sequence numbers can be bounded if the fifo property of the channels is exploited.
Using the fifo assumption it can be shown that the sequence number of a packet that is received by p is always within a 2L-range around sp.
Note that this is the first time the fifo assumption is used.
It will now be shown that the transitions preserve these assertions.
It follows from the symmetry of the protocol that Sq , Rq , and Lq preserve P' as well.
The values of the constants lp and lq have an important influence on the efficiency of the protocol.
The optimal values depend on a number of system dependent parameters , such as.
The state information of (the data transmission part of) the protocol is kept in a data structure called the connection record.
The transmission of data is only considered in one direction, namely from p to q.
Sometimes p will be referred to as the sender, and q as the receiver.
It should be noted, however, that the protocol uses acknowledgement messages that are sent in the reverse direction, i .e.
It will usually be the case that data must be transmitted in two directions.
To handle this situation a second protocol, in which the roles of p and q are reversed, is executed in addition.
It is then possible to introduce combined datal ack messages, containing both data (with the corresponding sequence number) and the information contained in an acknowledgement packet of the timer-based protocol.
The receiver does not store any data packets with a higher number than the one it expects.
Only if the next packet to arrive is the expected one it is taken into account and immediately delivered.
More intricate versions of the protocol would store early-arriving packets with a higher sequence number and deliver them after all packets with lower sequence numbers have arrived and have been delivered.
Only the mechanism necessary to guarantee the safety of the protocol is included.
The sender may put only a single word in each.
The protocol is timer-based, that is , the processes have access to physical clock devices.
With regard to the time and the timers in the system the following assumptions are made.
A global measure of time extends over all processes of the system, that is, each event is said to take place at a certain time.
Each event itself is assumed to have duration 0, and the time at which an event takes place is not observable by the processes.
The lifetime of a packet is bounded by a constant /-l (the maximum packet lifetime)
If a packet is duplicated in a channel, each of the copies must be received within a time /-l after the sending of the original packet (or become lost)
Again, this array is never stored in p entirely; p can only access a part of it at any time.
The part of inp that can be accessed by p is extended (at the higher end) when p obtains a next word from the process that generates the words.
This operation is referred to as the acceptance of a word by the sender.
Instead of writing an (infinite) array, the receiver hands words to the consuming process by an operation referred to.
Ideally, each word of inp should be delivered exactly once and the words should be delivered in the correct order.
The specification of the protocol , however, is weaker than this , and the reason is that the protocol is only allowed to treat each word of inp during a bounded time interval.
It cannot be guaranteed by any protocol that a word is received within a bounded time, because it is possible that all packets are lost during this time.
Therefore the specification of the protocol allows for the possibility of a reported loss, where the sending protocol generates an error report indicating that the word may be lost.
If, for this reason, a higher-level protocol were to offer this word to p again a duplication could effectively occur ; we will , however, not concern ourselves with this problem here.
Each word of inp is delivered by q or reported by p (as "possibly lost" ) within a bounded time after the acceptance of the word by p.
The words delivered by q occur in strictly increasing order in inp.
Protocol constants: U : real ; R : real ; S : real ;
Connection record of sender: Low : integer ; High : integer ; St : timer ;
Auxiliary variables: B : integer init 0 ; cr : boolean init false ; cs : boolean init false ;
The sender may send a word only during a time interval, of length U, starting from the moment at which the sender accepts the word.
To this end a timer Ut [i] is associated with each word inp [i] ; it is set to U at the moment of acceptance, and must be positive for a word to be transmitted.
Thus the sending window of p consists of those words of the range Low.
High - 1 for which the associated timer is positive.
Rt : =  R ; Exp := i + 1 ; deliver w end.
The start-of-sequence bit is used by the receiver when a packet is received in a closed connection, to decide whether a connection can be opened (and the word in the packet delivered)
The sender sets the bit to true when all previous words have been acknowledged or reported (as possibly lost )
When q receives a packet in an already open connection the contained word is delivered if and only if the sequence number of the packet equals the next expected sequence number (stored in Exp)
The communication subsystem is represented by two multisets, Mp for the packets with destination p and Mq for the packets with destination q.
These transitions represent channel failures and the progress of time.
When the receiver's timer reaches the value 0, its connection is closed.
The required properties of the protocol will be proved in a series of lemmas and theorems.
The assertion Po defined below expresses that the sender 's connection remains open as long as there are any packets in the system, and that the sequence numbers of those packets have the correct meaning in the current connection.
To interpret (3) , the value of High is supposed to be zero in all configurations where no connection exists at the sender.
Lemma 3.10 Po is an invariant of the timer-based protocol.
Initially no connections exist , there are no packets ,  and B = 0, which implies that Po is true.
Ap:  ( 1 )  is preserved because an assignment to St always results in St = S.
Sp : ( 1 )  is preserved because St is always set to S.
Ep :  Action Ep does not change any of the variables mentioned in Po.
Rq: (2) is preserved because Rt is always assigned as R (if at all)
Sq : (4) is preserved because each packet is sent with remaining packet lifetime equal to !-t.
Observe that Time decreases all timers (including p-fields of packets) by the same amount , hence preserves all assertions of the form Xt 2: Yt + C, where Xt and Yt are timers, and C a constant.
The first requirement for the protocol is that each word is eventually delivered or reported as lost.
It can now be shown that the protocol does not lose any words without reporting the fact.
Ep: The value of Low may be incremented when action Ep is applied, but the generation of an error report ensures that ( 10) is preserved.
Rq : First consider the case that q receives ( data, true, I ,  w, p ) while no connection exists ( cr is false)
Now consider the case that Exp is incremented as the result of the receipt of ( data, s , Exp , w, p ) in an open connection.
Loss: An application of Loss can only falsify the premises of the clauses.
Dupl: The insertion of a packet m is applicable only if the premise of the relevant clause (and hence the conclusion) was true even before the insertion.
If the connection is not closed within this period of time and B + Low :::; 1, the reporting of all words from the range B + Low.
To establish the second correctness requirement of the protocol it must be shown that each word that is accepted has a higher index (in inp) than the previously accepted word.
Assume q receives a packet ( data, s ,  i I , W,  p )  and delivers w.
If a connection did exist , il = Exp, hence B + il = B + Exp = pr + l  by ( 18) , which implies that w = inp[pr + l]
The No Loss and Ordering requirements are both safety properties, and they allow an extremely simple solution, namely a protocol that does not send or receive any packets ,  and reports every word as lost.
It goes without saying that such a protocol , which does not achieve any form of data transport from sender to receiver, would not be considered a very "good" solution.
On the receiver's side a mechanism is provided for triggering the sending of an acknowledgement whenever a packet is delivered or received in an open connection.
To this end it must be assumed that the acceptance rate of words (by p) is bounded in a way such that a word can only be accepted if the Lth previous word is at least U + 2/1 + R time units old.
All invariant clauses of P2 regarding packets are of the form.
Assertions having these forms can be falsified by packet loss or duplication, and hence cannot serve in the correctness proof of algorithms that must tolerate these faults.
Similar observations apply to the shape of invariants in the Time action.
It has already been remarked that this action preserves all assertions of the form.
Exercise 3.3 In the timer-based protocol the sender may report a word as possibly lost, when in fact the word was delivered correctly by the receiver.
Describe an execution of the protocol where this phenomenon occurs.
Is it possible to design a protocol in which the sender generates an.
Exercise 3.4 Assume that, due to a failing clock device, the receiver may fail to close its connection in time.
Describe a computation of the timer-based protocol in which a word is lost without being reported by the sender.
Exercise 3.5 Describe a computation of the timer-based protocol in which the receiver opens a connection upon receipt of a packet with a sequence number greater than zero.
Exercise 3.8 A network engineer wants to use the timer-based protocol, but wants to allow possibly lost words to be reported earlier with the following modification of Ep.
A process (a node in a computer network) is in general not connected directly to every other process by a channel.
A node can send packets of information directly only to a subset of the nodes called the neighbors of the node.
Routing is the term used to describe the decision procedure by which a node selects one (or, sometimes, more) of its neighbors to forward a packet on its way to an ultimate destination.
The objective in designing a routing algorithm is to generate (for each node) a decision-making procedure to perform this function and guarantee delivery of each packet.
It will be clear that some information about the topology of the network must be stored in each node as a working basis for the (local) decision procedure ; we shall refer to this information as the routing tables.
With the introduction of these tables the routing problem can be algorithmically divided into two parts; the definition of the table structure is of course related to the algorithmical design.
The routing tables must be computed when the network is initialized and must be brought up to date if the topology of the network changes.
When a packet is to be sent through the network it must be forwarded using the routing tables.
The algorithm must deliver every packet offered to the network to its ultimate destination.
An algorithm is called optimal if it uses the "best" paths.
The algorithm for the computation of the tables must use as few messages, time, and storage as possible.
Other aspects of complexity are how fast a routing decision can be made, how fast a packet can be made ready for transmission, etc.
The algorithm must provide service to every user in the same degree.
These criteria are sometimes conflicting, and most algorithms perform well only w.r .t.
The optimality of an algorithm depends on what is called a "best" path in the graph; there are several notions of what is "best" , each with its own class of routing algorithms:
Each channel is statically assigned a (non-negative) weight, and the cost of a path is measured as the sum of the weights of the channels in the path.
A shortest-path algorithm uses a path with lowest possible cost.
Each channel is dynamically assigned a weight , depending on the traffic on the channel.
A minimum-delay algorithm repeatedly revises the tables in such a way that paths with a (near) minimal total delay are always chosen.
The algorithm distributively computes a shortest path between each pair of nodes and stores in each source node the first neighbor on the path for each destination.
A disadvantage of this algorithm is that the entire computation must be repeated after a topological change in the network: the algorithm is not robust.
This may be too heavy a storage demand for large networks of small nodes.
In Section 4.4 some routing strategies will be discussed that code topological information in the address of a node, in order to use shorter routing tables or fewer table lookups.
These so-called "compact" routing algorithms usually do not use optimal paths.
A tree-based scheme, interval routing, and prefix routing are discussed.
In these methods, the network is partitioned into (connected) clusters, and a distinction is made between routing within a cluster and routing to another cluster.
This paradigm can be used to reduce the number of routing decisions that must be made during a path, or to reduce the amount of space needed to store the routing table in each node.
The routing decision made when forwarding a packet is usually based only on the destination of the packet (and the contents of the routing tables) , and is independent of the original sender (the source) of the packet.
The results do not depend on the choice of a particular optimality criterion for paths, but the following assumptions must hold.
Recall that a path is simple if it contains each node at most once, and the path is a cycle if the first node equals the last node.
The cost of sending a packet via a path P is  independent of the actual utilization of the path, in particular, the use of edges of P by other messages.
This assumption allows us to regard the cost of using path P as a function of the path; thus denote the cost of P by C(P) E R.
The cost of the concatenation of two paths equals the sum of the costs of the concatenated paths, i .e.
The graph does not contain a cycle of negative cost.
If a path from u to v exists in G, then there exists a simple path that is optimal.
As there are only a finite number of simple paths, there exists a simple path from u to v ,  say So , of lowest cost , i .e.
It remains to show that C (So ) is a lower bound for the cost of every (non-simple) path.
These properties imply that TN satisfies the requirements for Td.
To has the latter property, and in each stage as many nodes as edges are added.
If the tables contain a cycle for some destination d a packet for d is never delivered if its source is a node in the cycle.
Traffic for node v splits at nodes x and y.
Because bifurcated routing methods are usually very intricate, they will not be treated in this chapter.
To compute all distances, the Floyd-Warshall algorithm uses the notion of.
For (5) : each V-path is a path, and vice versa.
For (6) : each V-path is a path, and vice versa, hence an optimal V-path.
The main loop is executed N times, and contains N2 operations (which can be executed in parallel or serially) , which implies the time bound stated in the theorem.
A distributed algorithm for computing routing tables was given by Toueg [Tou80a] , based on the Floyd-Warshall algorithm described in the previous subsection.
It must be verified that the Floyd-Warshall algorithm is suitable for this purpose, i .e.
The most important assumption of the algorithm is that the graph does not contain cycles of negative weight.
This assumption is indeed realistic for distributed systems, where it is usually the case that each individual channel is assigned a positive cost.
An even stronger assumption will be made; see Al below.
Each node in the network initially knows the identities of all nodes.
Each node knows which of the nodes are its neighbors (stored in Neighu for node u) and the weights of its outgoing channels.
To arrive at a distributed algorithm the variables and operations of the Floyd-Warshall algorithm are partitioned over the nodes of the network.
The variable D[u, v] is a variable belonging to node u; by convention, this will be expressed by subscripting: write Du [v] from now on.
An operation that assigns a value to Du [v] must be executed by node u, and when the value of a variable of node w is needed in this operation, this value must be sent to u.
In the Floyd-Warshall algorithm all nodes must use information from the pivot node (w in the loop body) , which sends this information to all nodes simultaneously by a "broadcast" operation.
Finally, the algorithm will be augmented with an operation to maintain not only the lengths of shortest S-paths (as in the variable Du [v] ) ,  but also the first channel of such a path (in the variable Nbu [v] )
The assumption that a cycle in the network has positive weight can be used to show that no cycles occur in the routing tables after each pivot round.
Then the directed graph Tw = (Vw ,  Ew) ,  where.
So for each node u E Vw , u i- w there is a node x for which Nbu [w] = x,  and this node satisfies x E Vw.
For each node u i- w in Vw there is one edge in Ew , so the number of nodes of Tw exceeds the number of edges by one and it suffices to show that Tw contains no cycle.
Some care must be given to the operation "pick w from V \ S" , in order to guarantee that the nodes select the pivots in the same order.
When the algorithm terminates in node u, Du [v] = d(u , v) , and if a path from u to v exists then Nbu [v] is the first channel of a shortest path from u to v, otherwise Nbu [v] = udef.
The termination and the correctness of Du [v] on termination follow from the correctness of the Floyd-Warshall algorithm (Theorem 4.6)
The statement about the value of Nbu [v] follows because Nbu [v] is updated each time Du [v] is assigned.
Consequently, only the nodes that belong to Tw (at the beginning of the w-pivot round) need to receive w's table, and the broadcast operation can be done efficiently by sending the table Dw only via the channels that belong to the tree Tw.
That is, w sends Dw to its sons in Tw , and each node of Tw that receives the table (from its father in Tw) forwards it to its sons in Tw o.
Therefore each node v must send a message to each of its neighbors u, telling u whether v is a son of u in Tw o The full algorithm is now given as Algorithm 406 0  A node can participate in the forwarding of w 's table when it knows which of its neighbors are its sons in Tw o The algorithm uses three types of messages :
A ( ys ,  w )  message (ys stands for "your son" ) i s  sent by u to x at the beginning of the w-pivot round if x is the father of u in Tw.
A ( nys, w )  message (nys stands for "not your son" ) is sent by u to x at the beginning of the w-pivot round if x is not the father of u in Tw.
A ( dtab, w, D )  message is sent during the w-pivot round via each edge of Tw to transmit the value of Dw to each node that must use this value.
Assuming that a weight (of an edge or path) together with a node name can be represented by W bits, the complexity of the algorithm is expressed in the following theorem.
During the w-pivot round a node is allowed to receive and process only the messages of that round, i .e.
If the channels satisfy the fifo property then the ( ys ,  w )  and ( nys, w )  messages arrive as the first messages after a node has started that round, one via each channel, and then the ( dtab, w, D )  message is the next to arrive from Nbu [w] (if the node is in Vw)
It is possible by careful programming to omit the parameter w from all messages if the channels are fifo.
If the channels are not fifo it is possible that a message with parameter w' arrives while a node expects messages for round w, where w' is the pivot after w.
If U2 changes its distance to v in the w-pivot round, then Ul changes its distance to v in the w-pivot round.
According to this lemma, Algorithm 4.6 can be modified as follows.
When forwarding the table it suffices to send those entries D[v] for which Du [v] has changed as a result of the local w-pivot operation.
With this modification the routing tables are cycle-free not only between pivot rounds (as expressed in Lemma 4.7) ,  but also during pivot rounds.
Specific properties of the sequential algorithm can be exploited to minimize the required amount of communication.
Toueg's algorithm is reasonably simple to understand, has low complexity, and routes via optimal paths; its main disadvantage is its bad robustness.
When the topology of the network changes the entire computation must be performed anew.
In addition, the algorithm has two properties that make it less attractive from the viewpoint of distributed algorithms engineering.
First , as already mentioned, the uniform selection by all nodes of the next pivot node (w) requires that the set of participating nodes is precisely known in advance.
Second, Toueg's algorithm is based on repeated application of the triangle inequality d(u, v) :::; d(u, w) + d(w, v)
Evaluating the right-hand side (by u) requires information about d(w, v) , and this information is in general remote, i .e.
Dependence on remote data necessitates the transport of information to remote nodes, which can be observed in Toueg's algorithm (the broadcasting part)
Two properties of  this equation make algorithms based upon it  different from Toueg's algorithm.
Only distances to v (namely, d(w, v) for neighbors w of u) are needed to compute the distance from u to v.
Thus, the computation of all distances to a fixed destination Vo can proceed independently of the computation of distances to other nodes, and also, can be studied in isolation.
The algorithm proposed by Merlin and Segall [MS79] computes the routing tables for each destination completely separately; the computations for different destinations do not influence each other.
For a destination v ,  the algorithm starts with a tree Tv rooted towards v, and repeatedly updates this tree so as to become an optimal sink tree for destination v.
For destination v ,  each node u maintains an estimate for the distance to  v (Du [v] ) and the neighbor to which packets for u are forwarded (Nbu [v] ) , which is also the father of u in Tv.
In an update round each node u sends its estimated distance, Du [v] , to all neighbors except Nbu [v] (in a ( mydist , v ,  Du [v] ) message)
If node u receives from neighbor w a message ( mydist , v , d ) and if d + wuw < Du [v] , u will change Nbu [v] to w and Du [v] to d + Wuw.
The update round is controlled by v and requires the exchange of two messages of W bits on each channel.
It is shown in [MS79] that after i update rounds all shortest paths of at most i hops have been correctly computed, so that after at most N rounds all shortest paths to v are computed.
Shortest paths to each destination are computed by executing the algorithm independently for each destination.
The algorithm can also adapt to changes in the topology and the weight of channels.
An important property of the algorithm is that during update rounds also the routing tables are cycle-free.
Fix an optimal sink tree T for Vo and number the nodes other than Vo by VI through VN-I in such a way that if Vi is the father of Vj , then i < j.
An advantage of the algorithm of Chandy and Misra over that of Merlin and Segall is its simplicity, its smaller space complexity, and its lower time complexity.
Nodes are notified of failures and repairs of their adjacent channels.
The cost of a path equals the number of channels in the path.
The algorithm can handle the failure and repair or addition of channels, but it is assumed that a node is notified when an adjacent channel fails or recovers.
The failure and recovery of nodes is not considered; instead it is assumed that the failure of a node is observed by its neighbors as the failure of the connecting channel.
The algorithm maintains in each node u a table Nbu [v] ,  giving for each destination v a neighbor of u to which packets for v will be forwarded.
It cannot be required that the computation of these tables terminates within a finite number of steps in all cases because the repeated failure or repair of channels may ask for recomputation indefinitely.
If the topology of the network remains constant after a finite number of topological changes, then the algorithm terminates after a finite number of steps.
When the algorithm terminates the tables Nbu [v] satisfy (a) if v = u then Nbu [v] = local; (b) if a path from u to v =1= u exists then Nbu [v] = w, where w is.
The steps of the algorithm will first be motivated by an informal description of the operation of the algorithm, and subsequently the correctness of the algorithm will be proved formally.
For sake of clear exposition the modeling of topological changes is simplified as compared to [Lam82] by assuming that the notification of the change is processed simultaneously in the two nodes affected by the change.
The selection of a neighbor to which packets for destination v will be forwarded is based on estimates of the distance of each node to v.
The preferred neighbor is always the neighbor with the lowest estimate of this distance.
Node u maintains an estimate Du [v] of d(u, v) and estimates ndisu [w, v] of d(w, v) for each neighbor w of u.
The computation of the estimates Du [v] proceeds as follows.
If u :I v, a shortest path from u to v (if such a path exists) consists of a channel from u to a neighbor, concatenated with a shortest path from this neighbor to v, and consequently.
A node may suspect that no path exists if it computes an estimated distance of N or more; the value N is used in the table to represent this.
The reaction of the algorithm to the failures and repairs is as follows.
When the channel between U and w fails , w is removed from Neighu and vice versa.
The distance estimate for each destination is recomputed and, of course, sent to all remaining neighbors if it has changed.
This is the case if the best route previously was via the failed channel and there is no other neighbor w' with ndisu [w' , v] = ndisu [w , v]
When the channel is repaired (or a new channel is added) w is added to N eighu , but U has as yet no estimate of the distance d( w,  v) (and vice versa)
Until U receives similar messages from w, U uses N as an estimate for d(w, v) ,  i .e.
Let the predicate up( u, w) be true i f  and only i f  a (bidirectional) channel between U and w exists and is operating.
The assertion L( u, v) states that u's estimate of d( u, v) is always in agreement with u's local knowledge, and Nbu [v] is set accordingly.
The computation of the algorithm terminates when there are no more messages of the algorithm in transit in any channel.
These configurations are not terminal for the whole system, because the system's computation.
We shall call message-less configurations stable, and define the predicate stable by.
It must be assumed that initially the variables Neighu correctly reflect the existence of working communication channels, Le.
To prove the invariance of the assertions three types of transition must be considered.
Lemma 4.14 For all uo , wo , and vo , P(uo , wo , vo ) is an invariant.
We consider the three types of state transition mentioned above in turn.
Assume that u receives a ( mydist , v,  d )  message from w.
This causes no topological change and no change in the Neigh sets, hence ( 1 )  remains true.
If v i- Vo this receipt does not change anything in P( uo , wo , va)
The same happens if u = Wo and W = uo.
In all other cases nothing changes in P(uo , wo , va )
If u = Uo and W = Wo this makes up(uo , wo) true, but by the addition of Wo to Neighuo (and vice versa) this preserves ( 1 )
In all other cases nothing changes in P(uo ,  wo , va)
Lemma 4.15 For each Uo and va , L( uo , va) is an invariant.
Initially Duo [uo] = 0 and Nbuo [uo] = local.
Assume that u receives a ( mydist , v , d )  message from w.
Vo no variable mentioned in L( uo , vo ) changes.
If u = Uo then N eighuo changes by the addition of w, but as u sets ndisuo [w , vo] to N this preserves L ( Uo , vo )
The two correctness requirements for the algorithm will now be proved.
Theorem 4.16 When a stable configuration is reached, the tables Nbu [v] satisfy.
First it is shown by induction on d( u, v) that if u and v are in the same connected component then Du [v] ::; d(u, v)
Case d(u, v) = 0: this implies u = v and hence Du [v] = O.
Case d(u, v) = k + 1 :  this implies that there exists a node w E Neighu.
By induction Dw [v] ::; k ,  which by (4.3) implies Du [v] ::; k + l.
Now it will b e  shown by induction on Du [v] that i f  Du [v] < N then there is a path between u and v and d(u, v) ::; Du [v]
By induction there is a path between w and v and d(w, v) ::; k ,  which implies there is a path between u and v and d(u, v) ::; k + l.
To prove that a stable situation is eventually reached if topological changes cease, a norm function with respect to stable will be defined.
Theorem 4.18 If the topology of the network remains constant after a finite number of topological changes, then the algorithm reaches a stable configuration after a finite number of steps.
It follows from the well-foundedness of the domain of f that only a finite number of these transitions can take place; hence the algorithm reaches a configuration satisfying stable after a finite number of steps.
It has been assumed in -this section that the notifications of topological changes are processed atomically together with the change in a single transition.
The processing takes place at both sides of the removed or added channel simultaneously.
Lamport [Lam82] has carried out the analysis in a little more detail to allow a delay in processing these notifications.
The communication channel from w to u is modeled as the concatenation of three queues.
OQwu , the output queue of w; (2) TQwu , the queue of messages (and data packets) currently being.
The routing algorithms discussed so far all require that each node maintains a routing table with a separate entry for each possible destination.
When a packet is forwarded through the network these tables are accessed in each.
The cyclic interval [a, b) in 7l,N is the set of integers defined by.
Theorem 4.20 The nodes of a tree T can be numbered in such a way that for each outgoing channel of each node the set of destinations that must be routed via that channel is a cyclic interval.
Pick an arbitrary node Va as the root of the tree and for each w let T[w] denote the subtree of T rooted at w.
In this order, w is the first node of T[w] to be visited and after w all nodes of T[w] are visited before a node not in T[w] is visited; hence the nodes in T[w] are numbered by the linear interval [lw , lw + IT [w] l )  (lw is the label of w)
A neighbor of w is either a son or the father of w.
Node w forwards to a son u the packets with destinations in T[u] , i .e.
Node w forwards to its father the packets with destinations not in T[w] , i .e.
The tree labeling scheme routes optimally on trees , because in a tree there exists only one simple path between each two nodes.
The scheme can also be used if the network is not a tree.
A fixed spanning tree T of the network is chosen, and the scheme is applied to this tree.
Channels not belonging to the spanning tree are never used; each is labeled with a special symbol in the routing table to indicate that no packet is routed via this channel.
To compare the path lengths chosen by this scheme with the optimal paths , let dr (u, v) denote the distance from u to v in T and dc (u, v) the distance from u to v in G.
Let Dc denote the diameter of G, defined as the maximum over u and v of dc (u, v)
Lemma 4.21 There is no uniform bound on the ratio between dr (u, v) and de (u, v)
This holds already in the special case of the hop measure for paths.
Choose G to be the ring on N nodes, and observe that a spanning tree of G is obtained by removing one channel, say xv, from G.
The ratio can be made arbitrarily large by choosing a large ring.
The following lemma relies on the symmetry of channel costs, i .e.
This implies that dc (u, v) = dc (v, u) for all u and v.
This implies that the scheme is good if most communication is between nodes at distance 8(Dc) , but should not be  used if most communication is between nodes at a short distance (in G) from each other.
Besides the factor concerning the lengths of the chosen paths , the tree routing scheme has the following disadvantages.
Channels not belonging to T are not used, which is a waste of network resources.
Traffic is concentrated on a tree , which may lead to congestion.
Each single failure of a channel of T partitions the network.
Van Leeuwen and Tan [LT87] extended the tree labeling scheme to non-tree networks in such a way that (almost ) every channel is used for packet traffic.
Definition 4.23 An interval labeling scheme (JLS) for a network is.
Definition 4.24 An interval labeling scheme is valid if all packets forwarded in this way eventually reach their destination.
It will be shown that a valid interval labeling scheme exists for every connected network G (Theorem 4.25) ; for arbitrary connected networks, however, the scheme is usually not very efficient.
The optimality of the paths chosen by interval routing schemes will be studied after the existence proof.
Theorem 4.25 For each connected network G a valid interval labeling scheme exists.
A valid interval labeling scheme is constructed by an extension of the tree labeling scheme of Santoro and Khatib, applied to a spanning tree T of the network.
Given a spanning tree, a frond edge is an edge that does not belong to this spanning tree.
Furthermore, v is an ancestor of u iff u E T[v]
As the main problem of the construction is how to assign labels to frond edges (the tree edges will be labeled as in the tree labeling scheme) , a spanning tree is chosen in such a way that all frond edges take a restricted form, as we will show.
Lemma 4.26 There exists a spanning tree such that all frond edges are between a node and an ancestor of that node.
In the sequel, let T be a fixed depth-first search spanning tree of G.
Definition 4.27 A depth-first search ILS for G (with respect to T) is a labeling scheme for which the following rules are satisfied.
The label of edge uw at node u is called Quw.
In the latter situation, the frond edge is labeled 0 at u by the rule (a), so assigning the label ku would violate the requirement that all edge labels at u are different.
Observe that all frond edges are labeled according to rule (2a) , the father-edges of nodes.
The lowest common ancestor of two nodes u and v is the lowest node in the tree that is an ancestor of both u and v.
It can now be shown that each packet reaches its destination.
Let a packet for v be generated in node u.
By Lemma 4.28, the node label decreases at every hop, until, within a finite number of hops , the packet is received by a node w with lw :s: lv.
Every node to which the packet is forwarded after w also has a label :s: lv , by Lemma 4.29
Within a finite number of hops the packet is received by v, because in each hop fv decreases or the packet arrives at v , by Lemma 4.30
Theorem 4.25 states that a valid ILS exists for each network, but does not imply anything about the efficiency of the paths chosen by the scheme.
It should be clear that depth-first search ILSs are used to demonstrate the existence of a scheme for each network, but that they are not necessarily the best possible schemes.
There exists an ILS for the same ring that delivers each packet via a minimum-hop path (Theorem 4.34)
In order to analyze the quality of the routing method in this respect , the following definitions are first made.
Definition 4.31 An ILS is optimal if it forwards all packets via optimal paths.
An ILS is neighborly if it delivers a packet from one node to a neighbor of that node in one hop.
An ILS is linear if the interval corresponding with each edge is linear.
We shall call an ILS minimum-hop (or shortest-path) if it is optimal with respect to the minimum-hop (or shortest-path, respectively) cost measure.
It is easily seen that if a scheme is minimum hop then it is neighborly.
It turns out that for general networks the quality of the routing method is poor, but for several classes of special network topology the quality of the scheme is very good.
This makes the method suitable for processor networks with a regular structure, such as those used for the implementation of parallel computers with a virtual global shared memory.
It is not known exactly how, for an arbitrary network, the best interval labeling scheme compares with an optimal routing algorithm.
Some lower bounds for the path lengths, implying that an optimal ILS does not always exist , were given by Ruzicka.
The efficiency of the routing method can be improved by allowing more than one label to be assigned to each edge; we speak of multiple interval routing in this case.
Indeed, this defines the set of destinations for this edge to be the union of several intervals and by increasing the number of intervals even optimal routing can be achieved for arbitrary networks.
To see this , first consider optimal routing tables , such as computed by the Netchange algorithm for example, and observe that the set of destinations routed through any particular edge can be written as the union of cyclic intervals.
With this elementary approach one finds at most N /2 labels per edge, and at most N labels altogether in any node; the storage of such a table takes as much space as the storage of classical, full routing tables.
It is now possible to trade storage complexity against routing efficiency; the natural question arises how many labels are really necessary in any network to achieve optimal routing.
But allowing just a small compromise in the length of the paths, a large reduction in table size can be achieved; results regarding this tradeoff are summarized by Ruzicka [Ruz98]
Theorem 4.33 There exists a network for which no valid linear-interval labeling scheme exists.
Let x be the first node from the center in this leg.
Consequently, x also forwards packets for its other neighbor towards the center, and these packets never reach their destination.
Bakker, Van Leeuwen and Tan completely characterize the class of network topologies that admit a shortest-path linear ILS,  and present a number of results concerning the classes of graph topologies that admit adaptive and minimum-hop linear ILSs.
Networks that allow a linear scheme at all were characterized by Fragniaud and Gavoille [FG94] ; Eilam et ai.
As the ILS in the proof of Theorem 4.34 is optimal, it is neighborly; it is not linear.
The node labels are assigned in row major order, i .e.
It i s  now easy to verify that when node u forwards a packet to  node v, Case 1 :  if v is in a row higher than u, then u sends the packet via its.
The channel labeling for node (j - l)n + (i - 1)
Case 3: if v is in the same row as u but to the left , then u sends the packet via its left-channel; and.
Case 4: if v is in the same row as u but to the right , then u sends the packet via its right-channel.
In all cases, u sends the packet to a node closer to v, which implies that the chosen path is optimal.
Theorem 4.36 There exists a minimum-hop linear ILS for the Hypercube.
Interval routing has a number of attractive advantages , as follows, over classical routing mechanisms based on storing a preferred outgoing channel for each destination separately.
The routing method is able to choose optimal paths for several classes of network, cf.
These advantages make the method suitable for processor networks with a regular topology.
It is not possible to adapt a depth-first search ILS slightly if a channel or node is added to or removed from the network.
The depth-first search tree on which the ILS is based may no longer satisfy the requirement that there are fronds only between a node and its ancestor.
As a result , a minor modification of the topology may require a complete recomputation of the routing tables, including the assignment of new addresses (labels) to each node.
A depth-first search ILS may route packets via paths of length O(N) , even in cases of networks of small diameter; see Exercise 4.7
The prefix routing algorithm assumes that a prefix labeling scheme (PLS) is given, and forwards packets as in Algorithm 4.20
Definition 4.39 A prefix labeling scheme is valid if all packets forwarded in this way eventually reach their destination.
Theorem 4.40 For each connected network G a valid PLS exists.
We shall define a class of prefix labeling schemes and prove, as in Theorem 4.25,  that the schemes in this class are valid.
Let T denote an arbitrary rooted spanning tree of G.
Definition 4.41 A tree PLS for G (with respect to T) is a prefix labeling scheme in which the following rules are satisfied.
If w is a son of u then lw  extends lu  by  one letter; i.
If w is a son of u then ctuw = lw.
If w is the father of u then ctuw = t unless u has a frond to the root;
In a tree PLS each node except the root has a channel labeled t, and this channel connects the node to an ancestor (the father of the node or the root of the tree)
Observe that for every channel uw, ctuw = lw or ctuw = t.
For all u and v ,  v is an ancestor of u if and only if lv <J lu.
It must first be shown that a packet never gets "stuck" in a node different from its destination, that is , each node different from the destination can forward the packet using Algorithm 4.20
If u is not the root of T then u has a channel labeled t, which is a prefix of lv.
If u is the root then v is not the root , and v E T[u] holds.
If w is the son of u such that v E T[w] then by construction ctuw <J lv.
The following three lemmas concern the situation where node u forwards a packet for node v to a node w (a neighbor of u) using Algorithm 4.20
Lemma 4.43 If u E T[v] then w is an ancestor of u.
If ctuw = t then w is an ancestor of u as mentioned above.
This implies that w is an ancestor of v, and also of u.
Lemma 4.44 If u is an ancestor of v then w is an ancestor of v, closer to v than u.
Let w' be the son of  u such that v E T[w'] then ctuw' = lw' is a non-empty prefix of lv.
T[v] , then w is an ancestor ofv or dT (w, v) < dT(U, v)
If ctuw = t then w is the father of u or the root ; the father of u is closer to v than u because u rf.
T[v] , and the root is an ancestor of v.
If ctuw = lw then, as ctuw <J lv , w is an ancestor of v.
Let the value depth be the depth of T, i .e.
If the packet is generated in the subtree T[v] then an ancestor of v is reached within depth hops by Lemma 4.43, after which v is reached within another depth hops by the previous observation.
Because the path contains only ancestors of the source in this case , its length is actually bounded by depth also.
In all other cases an ancestor of v is reached within depth hops by Lemma 4.45 , after which v is reached within another depth hops.
Use a tree PLS with respect to a tree chosen as in Lemma 4.22
We conclude the discussion of the tree labeling scheme with a brief analysis.
As each of the N nodes has a different address, each address consists of at least log N bits; more bits may even be required if information is encoded in addresses , such as in prefix routing.
The cost of a single table lookup is likely to be larger for a large routing table or for larger addresses.
The total table-lookup time for the delivery of a single message also depends on the number of times the tables must be accessed.
In a hierarchical routing method, the network is divided into clusters, each cluster being a connected subset of nodes.
If the source and destination of a packet are in the same cluster, the cost of forwarding the message is low, because to route within the cluster, the cluster is treated as a smaller isolated network.
Thus, longer routing tables and the manipulation of long addresses are only necessary in the centers.
Each cluster itself can be divided into sub clusters in order to obtain a multi-level division of the nodes.
It is not necessarily desirable that each communication between clusters must take place via a cluster center; this type of design has the disadvantage that the entire cluster becomes vulnerable to failure of the center.
Yet the method uses only small tables, because entire clusters to which a node does not belong are treated as a single node.
All the routing methods discussed so far require that routing decisions are made at each intermediate node, which means that for a route of length l the routing tables must be accessed l times.
For minimum-hop strategies l is bounded by the diameter of the network, but for general, cycle-free routing strategies (such as interval routing) N - 1 is the best bound one can give.
In this subsection we shall discuss a method by which the number of table-lookups can be decreased.
We make use of the following lemma, which concerns the existence of a suitable division of the network into connected clusters.
The routing method assigns a color to each packet , and it is assumed that only a few colors are used.
Depending on its color, a packet is either forwarded immediately over a fixed channel (corresponding to the color) or a more complex routing decision is called for.
It is allowed that nodes have different protocols for handling packets.
Let T be a minimum-size subtree of G connecting all the Ci.
We refer to the nodes of T as centers (the Ci ) , branch points, and path nodes (the remaining nodes)
The routing method first sends a packet to the center Ci of the cluster of its source node (green phase) , then via T to the center Cj of the cluster of its destination node (blue phase) , and finally within Gj to the destination itself (phase red)
The green phase uses a fixed sink tree for the center of each cluster, and requires no routing decisions.
The path nodes of T have two incident tree channels, and forward each blue packet via the tree channel through which they did not receive the packet.
Branch points and centers in T must make routing decisions.
For the red phase a shortest-path routing strategy within the cluster can be used, which bounds the number of decisions in this phase to 2s.
The tree obtained after one level of clustering has at most N / s centers and N / s branch points, i .e.
If the tree obtained after i levels of clustering has mi essential nodes, then the tree obtained after i + 1 levels of clustering has at most mils centers and mils branch points, i .e.
The tree obtained after f levels of clustering has at most mf = N(2/s)f essential nodes.
The use of approximately log N colors leads to a routing method that requires o (log N) routing decisions.
The inspection of the color of a packet also becomes a kind of routing decision in this case, but it involves small tables (of length 0 (log N) at most) and is actually required only in a small fraction of the nodes.
Does this guarantee that packets are always delivered even when the network is subject to a possibly infinite number of topological changes ? Prove that no routing algorithm can guarantee delivery of packets under continuing topological changes.
Is it possible to modify the algorithm in this way ? What happens to the complexity of the algorithm ?
Exercise 4.4 Give the values of all variables in a terminal configuration of the Netchange algorithm when the algorithm is applied to a network of the following topology:
After a terminal configuration has been reached, a channel between A and F is added.
What messages does F send to A when processing the ( repair, A )
Exercise 4.6 Does there exist an ILS that does not use all channels for routing? Does there exist a valid one ? An optimal one ?
Exercise 4.8 Give the depth-first search ILS for a ring of N nodes.
Situations where a group of packets can never reach their destination because they are all waiting for the use of a buffer currently occupied by another packet in the group are referred to as store-and-forward deadlocks.
Other types of deadlock will be discussed briefly at the end of this chapter.
An important problem in the design of packet-switching networks is how to deal with store-and-forward deadlocks.
As usual, the network is modeled by a graph G = (V, E) ; the distance between nodes is measured in hops.
The set of all buffers is denoted B, and the symbols b ,  c , bu , etc.
The handling of packets by the nodes is described by the following three types of moves that can occur in the network.
A node u "creates" a new packet p (actually by accepting the packet from a higher level protocol) and places it in an empty buffer in u.
The node u is called the source of p in this case.
A packet p is forwarded from a node u to an empty buffer in the next node w on its route (the route is determined by the routing algorithm used)
As a result of the move the buffer previously occupied by p becomes empty.
Although the controllers that we shall define may forbid moves, it is assumed that the network always allows this move, i .e.
Node u repeatedly transmits p to w, but does not discard the packet from the buffer as long as no acknowledgement is received.
When node w receives the packet it decides whether it will accept the packet.
If so, the packet is placed in the buffer and an acknowledgement is sent to u, otherwise the packet is simply ignored.
Of course, more efficient protocols can be designed to implement the move, for example those where u does not transmit p until u knowns that w will accept p.
A packet p occupying a buffer in its destination node is removed from the buffer.
It is assumed that the network always allows this move.
Denote by P the collection of all paths followed by the packets.
This collection is determined by the routing algorithm (see Chapter 4) ; how it is determined need not concern us here.
Let k be the number of hops in the longest path in P.
It is not assumed that k equals the diameter of G; k may exceed the diameter if the routing algorithm does not select minimum-hop paths, and k may be smaller than the diameter if all communication in G is between nodes at limited distances.
The consumption of a packet (at its destination) is always allowed.
The generation of a packet in a node where all buffers are empty is.
As in Chapter 2, let Zu denote the set of states of node u, and M the set of possible messages (packets)
The controller con allows the generation of a packet p in node u, where the state of u is Cu ,  if and only if (cu , p) E Genu , and allows the forwarding of packet p from u to w if and only if (cw , p) E For w.
The formal definition of a controller does not include conditions for the consumption of packets , because the consumption of a packet (at its destination) is always allowed.
The moves of the network under controller con are exactly those moves of the network that are permitted by con.
A packet in the network is deadlocked if it can never reach its destination in any sequence of moves of the network.
The task of the controller is of course to avoid that the network ever enters such a configuration.
The initial configurations of the network are configurations in which there are no packets in the network.
Definition 5.3 A controller is deadlock-free if no deadlock configuration is reachable under the controller from an initial configuration.
The principle of these buffer graphs is based on the observation that (in the absence of a controller) a deadlock is due to a cyclic wait situation.
Ps- I of packets ,  such that for each i ,  Pi wants to move to a buffer occupied by Pi+1 (index counted modulo s)
Cyclic waits are avoided by moving the packets along paths in an acyclic graph (the buffer graph)
Let a network G with a set B of buffers be given.
Definition 5.4 A buffer graph (for G, B) is a directed graph BG on the buffers of the network, i.
A packet cannot be placed in an arbitrarily chosen buffer; it must be placed in a buffer from which it can still reach its destination via a path in BG, i .e.
A buffer b in u is suitable for p if there is a path in BG from b to a buffer c in v, whose image is a path that p can follow in G.
One such a path in BG will be designated as the guaranteed path and nb(p, b) denotes the next buffer on the guaranteed path.
For each newly generated packet p in u there exists a designated suitable buffer fb(p) in u.
Here fb and nb are acronyms for first buffer and next buffer.
Observe that the buffer nb(p, b) is always suitable for p.
In all buffer graphs used in this section nb(p, b) resides in a node different from the node where b resides.
Definition 5.6 The controller bgc BG is defined as follows.
The generation of a packet p in u is allowed if and only if the buffer fb(p) is free.
If the packet is generated it is placed in this buffer.
The forwarding of a packet p from a buffer in u to a buffer in w.
If the forwarding takes place p is placed in nb (p , b)
If u is a node with all buffers empty the generation of any packet in u is allowed, which implies that bgcBG is a controller.
For each b E B, define the buffer class of b as the length of the longest path in BG that ends in b.
Observe that the buffer classes of buffers on a path in BG (in particular, on a guaranteed path) are strictly increasing, i .e.
As the controller allows placement of packets only in suitable buffers and as there are no packets initially, each reachable configuration of the network under bgc BG contains only packets in suitable buffers.
It can be easily shown by a downward induction on the buffer classes that no buffer of buffer class r contains a deadlocked packet in such a configuration.
Case r = R: A buffer b in node u with the highest possible buffer class has no outgoing edges in BG.
Consequently, a packet for which b is suitable has destination u and can be consumed when it is in b.
It follows that no buffer of class R contains a deadlocked packet.
Let 'Y be a reachable configuration with a packet p in a buffer b of class r < R in node u.
If u is the destination of p then p can be consumed and consequently is not deadlocked.
Otherwise, let nb(p, b) = c be the next buffer on the guaranteed path from b, and observe that the buffer class r' of c exceeds r.
By the induction hypothesis c does not contain a deadlocked packet , hence there is a configuration 8, reachable from 'Y, in which c is empty.
It can be seen from the proof that if the guaranteed path contains "internal" edges of the buffer graph (edges between two buffers in the same node) then the controller must allow additional moves by which a packet is placed in a different buffer in the same node.
They are only used as optional moves to increase the.
Two examples of the use of buffer graphs, namely the destination scheme and the hops-so-far scheme, will now be discussed.
The destination scheme uses N buffers in each node u, with a buffer bu [v] for each possible destination v.
Node v is called the target of buffer bu [v]
It must be assumed for this scheme that the routing algorithm forwards all packets with destination v via a directed tree Tv rooted towards v.
Actually this assumption can be relaxed; it suffices that the channels used for routing towards v form an acyclic subgraph of G.
To see that BCd is acyclic , observe that no edges exist between buffers with different targets and that the buffers with the same target v form a tree isomorphic to Tv.
Each path P E P  with endpoint v is a path in Tv , and by construction there exists a path in BCd of buffers with target v whose image is P.
This means that for a packet p with destination v, generated in node u ,  fb(p) = bu [v] , and if  this packet must be forwarded to w then nb(p,  b) = bw [v]
Definition 5.8 The controller dest is defined as bgc BCd ' with fb and nb as defined in the previous paragraph.
Theorem 5.9 There exists a deadlock-free controller for arbitrary connected networks that uses N buffers in each node and allows packets to be routed via arbitrarily chosen sink trees.
As was mentioned earlier the requirement that routing is via sink trees can be relaxed to the requirement that packets towards one destination are sent via channels that form an acyclic graph.
The controller dest is  very simple to use, but has the disadvantage that a large number of buffers is required in each node, namely N.
A dual source scheme can be defined in which the buffers indexed by Vi are used to store packets generated in Vi.
It is assumed that each packet contains a hop count indicating how many hops the packet has made from its source.
To see that BGh is acyclic, observe that the index of buffers increases strictly along each edge of BGh.
This guaranteed path is described by fb(p) = bu [O] (for p generated in u) and nb(p, bu [i] ) = bw [i + 1] for a packet that must be forwarded from U to w.
In the hops-so-far scheme the buffers indexed i are used to store packets that have traveled i hops so far.
When an acyclic orientation cover of size B is available a controller using only B buffers per node can be constructed.
A packet in buffer bu [i] that must be forwarded.
Theorem 5.13 If an acyclic orientation cover for P of size B exists, then there exists a deadlock-free controller using only B buffers in each node.
Write uw E Ei if edge uw is directed towards w in Gi , and wu E Ei if edge uw is directed towards u in Gi.
To see that this graph is acyclic , note that no cycle exists containing buffers with different indices because there is no edge from a given buffer to another with a smaller index.
There is no cycle of buffers with the same index i because these buffers are arranged according to the acyclic graph Gi.
It is left to the reader (see Exercise 5.4) to demonstrate that for each P E P  there is a guaranteed path with image P, and that such a path is described by the following definitions:
Theorem 5.14 There exists a deadlock-free controller for a ring network that uses only three buffers per node and allows packets to be routed via shortest paths.
First consider the simple case where there exist nodes u and v with d( u, v) = C /2
Let x, y be a pair of nodes not equal to the pair u, v.
Then, as d(x, y) :::; C/2 ,  there exists a shortest path P between x and y that does not contain both u and v.
If P contains neither u nor v it is contained completely in either Gl or G2
It can now be shown that if there is a pair x ,  y such that no shortest path is found as the concatenation of paths in the orientations of the cover, then d(x, y) is closer to C /2 than d(u, v)
Acyclic orientation covers can be used to construct a controller using only two buffers per node for a network that is a tree.
A simple path from u to v is the concatenation of a path from u to the lowest common ancestor, which is in TI , and a path from the lowest common ancestor to v ,  which is in T2
Because the scheme described here can be applied to (minimum depth) spanning trees of arbitrary networks, we have in fact shown that in every network deadlock free routing is possible with 2 buffers per node.
The paths used for routing would not be optimal in general.
A lot of research in this area is still going on.
These controllers do not prescribe in which buffer a packet must be placed, and they use only simple local information such as the hop count or the number of occupied buffers in a node.
The controller accepts a packet if the node contains more free buffers than the packet has hops to go.
To show that generation of a packet is allowed in an empty node, observe that if all buffers of u are empty, fu = B.
A new packet has at most k hops to go, so B > k implies that the packet is accepted.
Deadlock-freedom of Fe will be shown by contradiction; assume r is a reachable deadlock configuration of the controller.
Obtain configuration 8 by applying to r a maximal sequence of forwarding and consumption moves.
Let p be a packet in 8 with minimal distance to its destination, i .e.
As u is not the destination of p (p could be consumed in 8 otherwise) there is a neighbor w of u to which p must be forwarded.
Of the packets in w ,  let q be the one most recently accepted by w ,  and let f:V denote the number of free buffers in w just before the acceptance of q by w.
Because packet q now occupies one of these f:V buffers and (by the choice of q) all packets accepted by w after q have thereafter been removed from w , f:V :'S fw + l.
The acceptance of q by w implies Sq < f:V , and combining the three derived inequalities we obtain.
A controller that is a "dual" to Fe is obtained when the decision to accept a packet is based on the number of steps a packet has already made.
Let , for a packet p, tp be the number of hops it has made from its source.
By using more detailed information about the packets residing in a node, a controller can be given that is very similar to the forward count controller, but allows more moves.
Now use the fact that p is not accepted by w, i .e.
There is also a controller, "dual" to the forward-state controller, that uses more detailed information than the backward-count controller and allows more moves.
The forward-state controller is more liberal then the forward-count controller in the sense that it allows more moves :
Lemma 5.22 Each move allowed by FC is also allowed by FS.
It was shown in [TU81 ] that FC is more liberal then BC, FS is more liberal then BS,  and BS is more liberal then BC.
In the results of this chapter the number of buffers needed by a controller has always played a role.
It is usually the case that the throughput is increased if more buffers are available.
In the unstructured solutions only a lower bound on the number of buffers is given; a larger number can be used without any modification.
In the structured solutions additional buffers must somehow be inserted into the buffer graph, which can be done either statically or dynamically.
If this is done statically each buffer has a fixed location in the buffer graph, i .e.
Instead of a single buffer fb(p) or nb(p, b) usually several buffers are specified as the possible beginning or continuation of a path through the buffer graph.
If the insertion of additional buffers is done dynamically the buffer graph is first constructed containing as few buffers as possible; we refer to the buffers in the graph as logical buffers.
During operation each actual buffer (referred to as a physical buffer) may be used as any of the logical buffers, where it must always be ensured that for each logical buffer there is at least one physical buffer assuming its role.
With such a scheme only a small number of buffers must be reserved in order to avoid deadlocks , while the rest of the buffers can be used freely in a very flexible manner.
It was assumed in this chapter that packets are of fixed size: the buffers are equally large and each buffer can contain exactly one packet.
The problem can also be considered under the assumption that each packet may have a different size.
Up to this point we have not explicitly considered the possibility of the occurrence of topological changes in the network during a packet 's travel from source to destination.
After the occurrence of such a change the routing tables of each node will be updated and the packet is then forwarded using the changed values of these tables.
As a result of the modification of the tables a packet may follow a path that would never have been followed if no changes had occured; it may even be the case that the ultimate route of a packet now contains cycles.
The impact of this on the deadlock-avoidance methods treated in this chapter is rather counter-intuitive.
The dest controller , whose correctness relies on the property that P contains only simple paths , can still be used without any modification.
The controllers that only assume an upper bound on the number of hops of a path require extra care when used in this case.
Within a finite time after the occurrence of the last topological change the routing tables have converged to cycle-free tables.
Even though a cyclic wait situation may have existed during the computation of the tables , when the computation has been completed the buffer graph is again acyclic, and all packets are stored in suitable buffers.
Consequently, when the routing tables have converged the resulting configuration contains no deadlocked packets.
Situations of this kind can only be resolved in cooperation with protocols in higher levels of the protocol hierarchy.
If forwarding a packet takes at least time J-lo , a bound of J-l on the end-to-end lifetime of a packet implies a bound of k = J-l/ J-lo on the number of hops a packet can go.
Merlin and Schweitzer consider four types, namely progeny deadlock, copy-release deadlock, pacing deadlock, and reassembly deadlock, and show how these types of deadlock can be avoided by an extension of the buffer-graph method.
Progeny deadlock may arise when a packet p in the network can create another packet q, for example a failure report to the source if a failed channel is encountered.
Progeny deadlock can be avoided by having two copies of the buffer graph, one for original messages and one for secondary messages (the progeny)
If the progeny may again create a next generation of progeny, multiple levels of the buffer graph must be used.
Copy-release deadlock may arise when the source holds a copy of the packet until an (end-to-end) acknowledgement for the packet is received from the destination.
Pacing deadlock can be avoided by making a distinction between paceable.
Reassembly deadlocks can be avoided by using separate groups of buffers for packet forwarding and reassembly.
Because in general a large number of different computations are possible, this does not imply that each packet is eventually delivered at its destination, even in an infinite computation, as is illustrated by Figure 5.6
Assume that u has an infinite stream of packets to send to v, and each time a buffer in w becomes empty the next packet from u is accepted.
Node s has a packet for t ,  which is not deadlocked because each time a buffer in w becomes empty there is a possible continuation of the computation in which the packet is accepted by w and forwarded to t.
Although this continuation is possible, it is not enforced, and the packet may remain in s forever.
A situation of this kind is referred to as livelock.
The controllers discussed in this chapter can be extended so as to avoid livelock situations as well.
Configuration 'Y is a livelock configuration if it contains livelocked packets.
A controller is live lock-free if no livelock configuration is reachable from a configuration having no packets under the moves of the controller.
If  in configuration 'Y packet p must be forwarded from u to w then each infinite computation starting in 'Y in which nb(p, b) is free in infinitely many configurations contains the forwarding of p.
If a packet p is in its destination in configuration 'Y then each infinite computation starting in 'Y contains the consumption of p.
The proof is by downward induction on the buffer classes.
Remember that in configurations reachable under bgc all packets reside in suitable buffers.
Case r = R: A buffer of class R has no outgoing edges, and consequently a packet in such a buffer has reached its destination.
Hence, by assumption F3, after each configuration in which such a buffer is occupied there will be a configuration in which it is empty.
This implies that it is empty in infinitely many configurations.
If p has reached its destination there will be a.
Otherwise, p must be forwarded to a buffer nb(p,  b) of a buffer class r' > r.
By induction, in each infinite computation starting in 'Y this buffer is empty infinitely often.
This implies by F2 that p will be forwarded and hence that there will be a configuration after 'Y in which b is empty.
One may simply assume that the mechanism for making non-deterministic choices in the distributed system ensures that the three fairness assumptions are satisfied.
Alternatively the assumptions can be enforced by adding to the controller a mechanism that ensures that when a buffer becomes empty older packets are allowed entry with higher priority.
Exercise 5.3 (The hops-to-go scheme) Give the buffer graph and the fb and nb functions for a controller that uses buffer bu [i] to  store packets that have i more hops to travel towards their destination.
What is the buffer class of bu [i] ? Is it necessary to maintain a hop count in each packet ?
Show that, as claimed, fb and nb do indeed describe a path in BGa.
Prove that there exists a deadlock-free controller, for packet switching on a hypercube, which uses only two buffers in each node and allows packets to be routed via minimum-hop paths.
Is it possible to use a linear interval labeling scheme ?
Exercise 5.7 Prove that each move allowed by Be is also allowed by Fe.
The treatment of wave algorithms as a separate issue, even though they are usually employed as subroutines in more involved algorithms, is useful for.
Unless stated otherwise, it is assumed throughout in this chapter that the network topology is fixed (no topological changes occur) , undirected (each channel carries messages in both directions) , and connected (there is a path between any two processes)
The set of all processes is denoted by JPl, and the set of channels by E.
As in earlier chapters, it is assumed that the system uses asynchronous message passing and that there is no notion of global time or real-time clocks.
The algorithms of this chapter can also be used with synchronous message passing (possibly with some small modifications to avoid deadlocks) or with global clocks if these are available.
As was observed in Chapter 2, a distributed algorithm usually allows a large collection of possible computations, due to non-determinism in the processes as well as the communication subsystem.
The number of events of computation C is denoted I C I and the subset of the events that occur in process p is denoted Cpo It is assumed that there is a special type of internal event called a decide event ; in the algorithms in this chapter such an event is simply represented by the statement decide.
A wave algorithm exchanges a finite number of messages and then makes a decision, which depends causally on some event in each process.
In each computation each decide event is causally preceded by an event in each process:
A computation of a wave algorithm is called a wave.
A variety of wave algorithms exists because algorithms may differ in many respects.
An algorithm may be designed for a specific topology, such as a ring, tree, clique, etc.
The complexity measures considered in this chapter are the number of exchanged messages , the number of exchanged bits, and the time needed for one computation (as defined in Section 6.4)
Each wave algorithm in this chapter will be given with the variables it uses and, if necessary, the information exchanged in its messages.
Most of these algorithms send "empty messages" , without any actual information: the messages carry causality, not information.
When a wave algorithm is applied there are generally more variables and other information may be included in the message.
Many applications rely on the simultaneous or sequential propagation of several waves; in this case information about the wave to which a message belongs must be included in messages.
Also a process may keep additional variables to administer the wave or waves in which it is currently active.
An important subclass of wave algorithm is formed by centralized wave algorithms having the following two additional properties: the initiator is the only process that decides ; all events are ordered totally by the causal order.
Wave algorithms with these properties are called traversal algorithms and considered in Section 6.3
In this subsection some lemmas that provide more insight in the structure of a wave computation are proved, and two trivial lower bounds on the message complexity of wave algorithms are presented.
First , each event in a computation is preceded by an event in an initiator.
Lemma 6.2 For each event e E C there exists an initiator p and an event f in Cp such that f :::S e.
Choose for f a minimal element in the history of e, i .e.
Such an f exists because the history of each event is finite.
It remains to show that the process , p, where f takes place is an initiator.
First , note that f is the first event of p, otherwise the earlier events of p would precede f.
The first event of a non-initiator is a receive event , which would be preceded by the corresponding send event , contradicting the minimality of f.
A wave with one initiator defines a spanning tree of the network when for each non-initiator the channel is selected through which the first message is received.
Proal As the number of  nodes of  T exceeds the number of  edges by one it suffices to show that T contains no cycle.
This follows because, with eq the first event in q, qr E ET implies er ::::; eq , and ::::; is a partial order.
Lemma 6.4 Let C be a wave and dp E C a decide event in process p.
As C is a wave there exists an f E Cq that causally precedes dp ; choose f to be the last event of Cq that precedes dp.
As f is the last event in q that precedes dp , el occurs in a process different from q, hence f is a send event.
Theorem 6.5 Let C be a wave with one initiator p, such that a decide event dp occurs in p.
By Lemma 6.4 a send event also occurs in each other process, which brings the number of send events to N.
Theorem 6.6 Let A be a wave algorithm for arbitrary networks without initial knowledge of the neighbors ' identities.
Then A exchanges at least lE I messages in each computation.
Consider network G' obtained by inserting one node z on the channel between x and y.
As the nodes have no knowledge of the neighbors' identity, the initial state of x and y in G' is identical to the initial state in G.
The same holds of course for all other nodes of G.
Consequently all events of C can be applied, in the same order, starting from in the initial configuration of G' , but now the decide event is not preceded by an event in z.
Notification by the PIF algorithm is considered as a decide event.
It is required that each computation of P is finite and that in each computation a notification event ( decide) occurs.
To employ A as a PIF algorithm, processes initially knowing M are starters of A.
The information M is appended to each message of A.
This is possible because by construction starters of A know M initially and followers do not send a message before having received one, thus also knowing M.
A decide event in the wave is preceded by an event in each process; hence when the former occurs each process knows M and it is considered the required notify of the PIF algorithm.
The bit complexity can b e  reduced by appending M only to the first message sent over each channel.
In this subsection it will be shown that wave algorithms are precisely the algorithms one needs when a global synchronization between the processes must be achieved.
The requirement for synchronization (SYN) is stated in the following problem [Fin79]
In each process q an event aq must be executed, and in some processes an event bp must be executed, such that the execution of all aq events must have taken place temporally before any of the bp events is executed.
In a SYN algorithm the bp events will be considered as decide events.
It is required that each computation of S is finite and that in each computation an event bp ( decide) occurs.
Theorem 6.10 Every wave algorithm can be employed as a SYN algorithm.
To employ A as a SYN algorithm, each process q is required to execute aq before q sends any message of A or decides in A.
The event bp takes place after a decide event in p.
By Lemma 6.4 each decide event is causally preceded by aq for each q.
The write event , which sets a value to out , is considered a decide event in an INF algorithm.
Give each process q an extra variable vq , which is initialized to jq.
Whenever process q sends a message of A, the current value of Vq is included in the message.
Whenever process q receives a message of A, with the value v included, Vq is set to the value Vq J..
When a decide event is executed in p, the current value of vp is written to outp.
The constructed INF algorithm shares all properties with A except the bit complexity, because an element of X is appended to each message of A.
Operators that satisfy these three criteria include the logical conjunction or disjunction, the minimum or maximum of integers, the greatest common divisor or lowest common multiple of integers, and the intersection or union of sets.
A collection of wave and traversal algorithms will be presented in the next three sections.
In all cases the algorithm text is given for the process p.
In this subsection a wave algorithm for a ring network will be given.
The same algorithm can be used for Hamiltonian networks in which one fixed Hamiltonian cycle is encoded in the processes.
Assume that for each process p a dedicated neighbor N extp is given such that all channels selected in this way form a Hamiltonian cycle.
For the initiator: begin send ( tok ) to Nextp ; receive ( tok ) ; decide end.
For non-initiators: begin receive ( tok ) ; send ( tok ) to Nextp end.
As each process sends at most one message the algorithm exchanges at most N messages altogether.
The receipt and sending of ( tok ) by each process P =1= Po precedes the receipt by Po , hence the dependence condition is satisfied.
In this subsection a wave algorithm for a tree network will be given.
The same algorithm can be used in an arbitrary network if a spanning tree of the network is available.
It is assumed that all leaves of the tree initiate the algorithm.
If a process has received a message via each of its incident channels except one (this condition is initially true for leaves) , the process sends a message via the remaining channel.
To prove that this algorithm is a wave algorithm, a few notations are introduced.
Let fpq be the event where p sends a message to q, and 9pq the event where q receives a message from p.
As each process sends at most one message the algorithm uses at most N messages altogether.
This implies that the algorithm reaches a terminal configuration 'Y after a finite number of steps ; it will be shown that in 'Y at least one process has executed a decide event.
Let F be the number of rec bits with value false in 'Y and let K be the number of processes that have already sent a message in 'Y.
The N - K processes that have not yet sent a message in 'Y have at least two false ree bits each; otherwise they could send a message, contradicting that 'Y is terminal.
The K processes that have sent a message in 'Y have at least one false ree bit each; otherwise they could decide , contradicting that 'Y is terminal.
Finally it  must be shown that a decision is  preceded by an event in each process.
Again let fpq be the event where p sends a message to q, and gpq the event where q receives a message from p.
It is proved by induction on the receive events that.
Assume this i s  true for all receive events preceding gpq.
Now gpq is preceded by fpq (in process p) and the program of p implies that for all r E Neighp with r =1= q ,  fpq is preceded by grp.
The induction hypothesis now implies that for all such r, and for all s E Trp , there is an event e E Cs with e ::; grp , hence e ::; gpq.
For the initiator: begin forall q E N eighp do send ( tok ) to q ;
For non-initiators: begin receive ( tok ) from neighbor q ; father p := q ; recp := recp + 1 ;
The echo algorithm is a centralized wave algorithm for networks of arbitrary topology.
It was first presented in isolation by Chang [Cha82] and therefore sometimes called Chang's echo algorithm.
A slightly more efficient version was given by Segall [Seg83] and this version is presented here.
Tokens are "echoed" back via the edges of this tree very much like the flow of messages in the tree algorithm.
Upon receipt of the first message a non-initiator forwards messages to all its neighbors except the one from which the message was received; when a non-initiator has received messages from all its neighbors an echo is sent to the father.
When the initiator has received a message from all its neighbors it decides.
Let 'Y be the terminal configuration reached in a computation C with initiator PO.
The tree is  rooted towards Po ; denote by Tp the set of nodes in the subtree of p.
The edges of the network not belonging to T are called frond edges.
In 'Y each process P has at least sent messages to all its neighbors except its father father p ' hence each frond edge has carried a message in both directions in C.
Let fp be the event in which p sends a message to its father (if it occurs in C) and 9p the event in which p's father receives a message from p (if it occurs)
By induction on the nodes in the tree it can be shown that.
Thus the sending of ( tok ) to p's father was applicable, and as 'Y is terminal it has taken place.
By induction, C contains fp' for each son p' of p, and as 'Y is terminal, C contains 9p' as well.
Hence the sending of ( tok ) to p's father was applicable, and as 'Y is terminal it has taken place.
Tp consists of the union of Tp' over the sons of p and p itself.
The induction hypothesis can be used to show that in each process of this set there is an event preceding 9p.
It follows also that Po has received a message from each neighbor and that Po has executed a decide event , which is preceded by an event in each process.
For non-initiators: begin receive ( tok ) from q ; send ( tok )  to q end.
In clique networks a channel exists between each pair of processes.
A process can decide if it has received a message from each neighbor.
In the polling algorithm, given as Algorithm 6.6 ,  the initiator asks each neighbor to reply with a message, and decides after receipt of all messages.
The algorithm sends two messages via each channel that is incident to the initiator.
Each neighbor of the initiator replies once to the original poll , hence the initiator receives N - 1 replies.
This is exactly the number it needs to decide, which implies that the initiator will decide , and that its decision is preceded by an event in each process.
Polling can also be used in a star network in which the initiator is the center.
This algorithm can be used as a wave algorithm for directed networks.
The definition of causality gives j(mh) -< g(h) pq - pq.
As each process sends at most D messages through each channel the algorithm terminates after a finite number of steps.
Because there are no messages in the channels in ,,/, for each channel qp it is the case that Recp [q] = Sentq.
Subsequently it will be shown that each process has decided.
In particular this holds if q is an in-neighbor of p, and using Recp [q] = Sentq this implies that minq Recp [q] 2: Sentp.
But this implies Sentp = D; otherwise, P would have sent additional messages the last time it received a message.
Consequently, Sentp = D for all p, and hence also Recp [q] = D for all qp, which implies indeed that each process has decided.
It now remains to show that each decision is preceded by an event in each process.
The algorithm sends D messages through each channel, which gives a message complexity of IE I .D.
It should be noted, however, that in this expression lE I  stands for the number of directed channels.
If the algorithm is used in an undirected network each channel counts for two directed channels, which brings the message complexity to 2 IE I .D.
Finn's algorithm [Fin79] is another wave algorithm that can be used in arbitrary, directed networks.
It does not require the diameter of the network to be known in advance, but relies on the availability of unique identities for the processes.
Sets of process identities are exchanged in messages, which causes the bit complexity of the algorithm to be rather high.
It is demonstrated in the correctness proof that this will happen for each p, and that the equality of the two sets implies that the decision is preceded by an event in each process.
Observe that the two sets maintained by each process can only grow.
As the size of the two sets sum up to at least 1 in the first message sent.
Let C b e  a computation in which there is at least one initiator , and let , be the last configuration, which is a terminal configuration.
This implies that each process has sent at least one message (through each channel)
Second, it can be shown similarly that NIncp = NIncq for each p and q.
As every process has sent at least one message via each channel, each process p satisfies \fq E Inp : recp [q] , and consequently p E NIncp holds for each p.
Also, the NInc sets contain only process identities , which implies that NIncp = lfD for each p.
It must now be shown that a decision dp in process p i s  preceded by an event in each process.
In this section a special class of wave algorithms will be discussed, namely, wave algorithms in which all events of a wave are totally ordered by the causality relation and in which the last event occurs in the same process as the first event.
Definition 6.24 A traversal algorithm is an algorithm with the following three properties.
In each computation there is one initiator, which starts the algorithm by sending out exactly one message.
A process, upon receipt of a message, either sends out one message or decides.
For non-initiators: begin receive ( tok ) from q ; send ( tok ) to q end.
The first two properties imply that in each finite computation exactly one process decides.
The algorithm is said to terminate in the single process that decides.
The algorithm terminates in the initiator and when this happens, each process has sent a message at least once.
In each reachable configuration of a traversal algorithm there is either exactly one message in transit , or exactly one process that has just received a message and not (yet) sent a message in reply.
In a more abstract view the messages of a computation taken together can be seen as a single object (a token) that is handed from process to process and so "visits" all processes.
In Section 7.4 ,  traversal algorithms are used to construct election algorithms and for this construction it is important to know not only the overall number of token passes in one wave, but also how many token passes are necessary to visit the first x processes.
Definition 6.25 An algorithm is an f -traversal algorithm (for a class of networks) if.
It is easily seen that when the algorithm terminates in the initiator each process has sent a reply.
It is assumed that the torus has a sense of direction (see Section B .3) , i .e.
The coordinate pairs (i , j )  are a convenient means of defining the network topology and its sense of direction, but we assume that the processes do not know these coordinates; the topological knowledge is restricted to the channel labels.
After the kth step of the token it is sent upwards if n lk (n divides k) , otherwise it is sent to the right.
For the initiator, execute once: send ( num, 1 )  to Up.
For each process , upon receipt of the token ( num, k ) : begin if k = n2 then decide.
Let the initiator be Po and let Pk be the process that receives the token ( num, k )
Because all processes Po through PnLl are different , x + 1 processes have been visited after x token passes.
The n-dimensional hypercube is a graph G = (V, E) where.
For each process , upon receipt of the token ( num, k ) : begin if k = 2n then decide.
As is easily seen from the algorithm a decision is taken after 2n hops of the token.
Let the initiator be Po and let Pk be the process that receives the token ( num, k )
From this it follows that all processes have been visited when termination occurs, and that x + 1 processes have been visited after x token passes.
For the initiator only, execute once: begin father p := p ; choose q E Neighp ;
For each process , upon receipt of ( tok )  from qo : begin if father p = udef then father p :=  qo ;
A process never forwards the token twice through the same channel.
Because each process sends the token through each channel at most once, each process receives the token through each channel at most once.
Each time the token is held by a non-initiator p, process p has received the token once more often than p has sent the token.
It follows that the decision takes place in the initiator.
It will be proved in three steps that when the algorithm terminates each process has forwarded the token.
All channels incident to the initiator have been used once in each direction.
Each channel has been used by the initiator to send the token, otherwise the algorithm would not terminate.
The initiator has received the token exactly as often as it has sent the token; because it has been received through a different channel each time, it follows that the token has been received once through each channel.
For each visited process p, all channels incident to p have been used once in each direction.
Assuming this is not the case, choose p to be the earliest visited process for which this property is not true, and observe that by ( 1 )  p is not the initiator.
By the choice of p, all channels incident to father p have been used once in each direction, which implies that p has sent the token to its father.
This implies that p has used all incident channels to send the token; but as the token ends in the initiator, p has received the token exactly as often as p has sent the token, so p has received the token once through each incident channel.
All processes have been visited and each channel has been used once in both directions.
If there are unvisited processes , there are neighbors p and q such that p has been visited and q has not been visited, contradicting that each channel of p has been used in both directions.
So all processes were visited, and all channels are used once in both directions by point (2)
The root of this tree is the initiator, and each non-initiator p has stored its father in the tree in the variable father p at the end of the computation.
If it is desired that each process should also know (at the end of the computation) which of its neighbors are its sons in the tree, this can be achieved by sending a special message to father p.
The processes in Tarry's algorithm are given sufficient freedom, in choosing a neighbor to which to forward the token, to allow a large class of spanning.
A process can execute any finite number of events in zero time.
The time between sending and receipt of a message is at most one time unit.
It is left as an exercise to the reader to verify the values given in this table that are not proved in this chapter.
As the algorithm implements Tarry's algorithm, it is a traversal algorithm and computes a spanning tree T.
It remains to show that rule R3 implies that the resulting tree is a depth-first search tree.
First , rule R3 implies that the first traversal of a frond is immediately followed by the second traversal, in the reverse direction.
Assume pq is a frond and p is the first process to use the frond.
When q receives the token from p, q has been visited already (otherwise q would set father q to p and the edge was not a frond) and usedq [p] is false (because by assumption p was the first of the two processes to use the edge)
Consequently, by R3, q sends the token back to p immediately.
It can now be shown that if pq is a frond, first used by p, then q E A[P]
For the initiator only, execute once: begin father p := p ; choose q E Neighp ;
For each process , upon receipt of ( tok ) from qo : begin if father p = udef then father p :=  qo ;
Consider the path followed by the token until i t  i s  sent via pq.
As pq i s  a frond, q has been visited before the token reached q via this edge:
By the previous observation all frond edges have now been now removed, which implies that the resulting path is a path in T, consisting only of edges used before the first use of pq.
If q is not an ancestor of p this implies that the edge from q to father q was used before the edge qp was used, contradicting rule R2 of the algorithm.
A distributed version of the depth-first search was first given by Cheung [Che83]
When process p is first visited by the token (for the initiator this occurs when the algorithm is started) p informs each neighbor r, except its father, of the visit by sending a ( vis ) message to r.
The forwarding of the token is suspended until p has received an ( ack ) message from each neighbor.
This ensures that each neighbor r of p knows, at the moment p forwards the token, that p has been visited.
The exchange of  ( vis ) messages causes , in  most cases , usedp[Jatherp] to be true even when p has not yet sent the token to its father.
It must therefore be programmed explicitly in the algorithm that only the initiator may decide; a non-initiator p for which usedp [q] is true for all neighbors q forwards the token to its father.
For the initiator only, execute once: begin father p := p ; choose q E Neighp ;
For each process , upon receipt of ( tok ) from qo : begin if father p = udef then.
For each process, upon receipt of ( vis ) from qo : begin usedp [qo] := true ; send ( ack ) to qo end.
At each node the token waits at most once, before it can be forwarded, for the exchange of ( vis ) / ( ack ) messages , which gives rise to a delay of at most two time units at each node.
The sending of a ( vis ) message can be omitted for the neighbor to which.
For the initiator only, execute once: begin father p := p ; choose q E Neighp ;
For each process, upon receipt of ( vis ) from qo : begin usedp [qo] := true ;
The algorithm of Cidon [Cid88] improves on the time complexity of Awerbuch's algorithm, by not sending the ( ack ) messages used in Awerbuch's algorithm.
In Cidon's modified algorithm, the token is forwarded immediately, i .e.
Process p has been visited by the token and has sent a ( vis ) message to its neighbor r.
The token later visits r, but at the moment r receives the token the ( vis ) message of p has not yet reached r.
In this case r may forward the token to p, actually sending it via a frond edge.
Observe how the ( ack ) messages in Awerbuch's algorithm prevent this scenario from taking place.
To handle this situation, process p records (in variable mrsp ) to  which neighbor it most recently sent the token.
When the token only traverses tree edges p receives it the next time from the same neighbor mrsp.
In the scenario defined above p receives the ( tok ) message from a different neighbor, namely from r; the token is ignored in this case, but p marks the edge rp as used, just as if a ( vis ) message had been received from r.
Process r receives p's ( vis ) message after sending the token to p, i .e.
In this case r acts as if it had not yet.
For each process, upon receipt of ( tok ) from qo : begin if mrsp i- udef and mrsp i- qo.
The time between two successive transmissions of the token through a tree edge is bounded by one time unit.
If the token is sent along a tree edge to p at time t ,  then at time t all ( vis ) messages of previously visited neighbors q of p have been sent , and consequently these messages arrive at the latest.
In many cases the algorithm will send fewer messages than Awerbuch's algorithm.
The analysis of the number of messages in Cidon's algorithm assumes the most pessimistic case, namely, that the token message is sent through each frond in both directions.
It may be expected that the ( vis ) messages are successful in avoiding many of these undesirable transmissions , in which case only two or three messages will be transmitted through each channel.
If processes know the identity of their neighbors the traversal of fronds by the token can be avoided by including a list of visited processes in the token.
Process p, receiving the token with an included list L, does not forward the token to a process in L.
The bit complexity of this algorithm is high; if w is the number of bits needed to represent one identity, the list L may require up to N.
For the initiator only, execute once: begin father p : =  p ; choose q E Neighp ;
For each process , upon receipt of ( tlist , L )  from qo : begin if father p = udef then father p : =  qo ;
A large number of functions are not computable in this way, these include the sum over all inputs, because the sum operator is not idempotent.
Summing inputs can be used to count processes with a certain property (by setting the input to 1 if a process has the property and to 0 otherwise) , and the results of this subsection can also be used for other operators that are commutative and associative, such as the product of integers or the union of multisets.
It turns out that there does not exist a general method for computing sums using a wave algorithm, but in special cases the computation of a sum is possible.
This is the case when the algorithm is a traversal algorithm, when the processes have identities , or when the algorithm induces a spanning tree that can be used.
There exists a wave algorithm for the class of networks including all undirected anonymous networks of diameter two, namely the phase algorithm (with parameter D = 2)
There does not exist an algorithm that can compute the sum of all inputs and that is correct for all undirected anonymous networks of diameter two.
Using the techniques introduced in Chapter 9 it can be shown that any algorithm will output the same result in each of the two networks and hence is not correct in both networks.
If A is a traversal algorithm the sum of all inputs can be computed as follows.
Process p has a variable jp , initialized to p 's input.
Consequently, when the algorithm terminates , s equals the sum over all inputs.
Some wave algorithms make available for each decision event dp in process p a spanning tree rooted at p, via which messages are sent towards p.
In fact , each computation of any wave algorithm contains such spanning trees ; however, it may be the case that a process q sends several messages , and does not know which of its outgoing edges belongs to one such a tree.
If the processes are aware of which outgoing edge is the edge to their father in such a tree , the tree can be used to compute the sums.
Each process sends to its father in the tree the sum of all inputs in its subtree.
This principle can be applied to the tree algorithm, the echo algorithm, and the phase algorithm for cliques.
The tree algorithm is easily adapted to include the sum over the inputs in Tpq in the message sent from p to q.
A deciding process computes the final result by summing the values contained in the two messages that cross on one edge.
The phase algorithm for cliques is adapted by sending, in each message from q to p, the input of q.
The process p adds all received values and its own input , and the result is the correct answer when p decides.
This solution requires the availability of unique identities for each process , and it increases the bit complexity considerably.
The time complexity of distributed algorithms can be defined in several ways.
Definition 6.37 The one-time complexity of an algorithm is the maximum time of a computation of the algorithm under the following assumptions.
A process can execute any finite number of events in zero time.
The flaw in this argument is that the variations in transmission time allowed under T2 make a larger class of computations possible, including perhaps computations with poor time behavior.
Theorem 6.38 The one-time complexity of the echo algorithm is O (D)
A process at hop distance d from the initiator receives the first ( tok ) message exactly d time units after the start of the computation and has depth d in the resulting tree T.
This can be shown by a backward induction on d.
A process at hop distance d from the initiator receives the first ( tok ) message at the latest at d time units after the start of the computation.
Assume the spanning tree is completed F time units after the start of the computation, then F ::; D.
A process with depth d in T sends the ( tok ) message to its father at the latest at (F + 1 ) + (DT - d) time units after the start of the computation.
This can be shown by a backward induction on d.
It follows that the initiator decides at the latest at (F + 1 ) + DT time units after the start of the computation, and this is O(N)
The time complexity suffers from the disadvantage that the result is determined by computations (such as in the proof of Theorem 6.38) that , although possible, are considered to be extremely unlikely to occur.
Indeed, in this computation a single message is "bypassed" by a chain of N - 1 serially transmitted messages.
Where ? Can you give an algorithm that computes infima in a partial order with bottom, and is not a wave algorithm?
What are the message, time, and bit complexities of your algorithm '?
Exercise 6.9 Suppose you want to use a wave algorithm in a network where duplication of messages may occur.
What modifications should be made to the echo algorithm'? (2) What modifications should be made to Finn's algorithm '?
In this chapter the problem of election, also called leader finding, will be discussed.
The problem is to start from a configuration where all processes are in the same state, and arrive at a configuration where exactly one process is in state leader and all other processes are in the state lost.
An election under the processes must be held if a centralized algorithm is to be executed and there is no a priori candidate to serve as the initiator of this algorithm.
For example, this could be the case for an initialization procedure that must be executed initially or after a crash of the system.
Because the set of active processes may not be known in advance it is not possible to assign one process once and for all to the role of leader.
A large number of results about the election problem (algorithms as well as more general theorems) exist.
The results in this chapter were selected for inclusion with the following criteria in mind.
The election problem has taken the role of a "benchmarking problem" to compare the efficiency of different computation models.
The election problem requires that , starting from a configuration where each process is in the same state, a configuration is reached where exactly one process is in a special the state leader, while all other processes are in the state lost.
The process in state leader at the end of the computation is called the leader and is said to be elected by the algorithm.
The last property is sometimes weakened to require only that exactly one process is in the state leader.
It is then the case that the elected process is aware that it has won the election, but the losers are not (yet) aware of their loss.
If an algorithm satisfying this weaker requirement is given, it can easily be extended by a flooding, initiated by the leader, in which all processes are informed of the result of the election.
This additional notification is omitted in some algorithms in this chapter.
In all algorithms in this chapter process p has a variable statep , having possible values that include leader and lost.
We will sometimes assume that the value of statep is sleep before p has executed any step of the algorithm, and cand if p has joined the computation but is not yet aware whether it has lost or won the election.
Some algorithms use additional states , such as active, passive, etc.
The election problem has been studied in this chapter under assumptions that we now review.
It has been assumed that the processes have no access to a common clock and that the message transmission times can be arbitrarily long or short.
A global timing assumption, such as the assumption that processes can observe real time and that message.,delay is bounded, does have an important impact on solutions to the' election problem.
Although some of the algorithms discussed in this chapter were originally formulated so as to elect the largest process, we shall in fact formulate most of these algorithms so as to elect the smallest process ; in each case an algorithm to elect the largest process is of course obtained by reversing the order of comparisons between identities.
Each message may contain only up to a constant number of process identities.
In the tree algorithm it is required that at least all leaves are initiators of the algorithm.
To obtain progress in the algorithm in case only some processes are initiators also, a wake-up phase is added.
Processes that want to start the election flood a message ( wakeup ) to all processes.
The boolean variable ws is used to make every process send ( wakeup ) messages at most once, and the wr variable is used to count the number of ( wakeup ) messages a process has received.
Two ( wakeup ) messages and two ( tok, r ) messages are sent via each.
Within D time units after the first process starts the algorithm, each process has sent ( wakeup ) messages, hence within D + 1 time units each process has started the wave.
A more careful analysis reveals that the algorithm always terminates within.
The number of messages can be reduced by two modifications.
First , it may be arranged that a non-initiator does not send a ( wakeup ) message to the process from which it received the first ( wakeup ) message.
Second, the ( wakeup ) message sent by a leaf can be combined with the ( tok, r ) message sent by that leaf.
In the algorithm of LeLann [LeL 77] each initiator computes a list of the identities of all initiators , after which the initiator with the smallest identity is elected.
Each initiator sends a token, containing its identity, via the ring, and this token is forwarded by all processes.
It is assumed that the channels are fifo and that an initiator must generate its token before the token of any other initiator is received.
When a process receives a token, it will not initiate the algorithm thereafter.
When an initiator p receives its own token back, the tokens of all initiators have passed p, and p becomes elected if and only if p is the smallest among the initiators ; see Algorithm 7.2
Because the order of the tokens on the ring is preserved (by the fifo assumption) and initiator q sends out ( tok, q ) before q receives ( tok, P ) , initiator P receives ( tok, q ) before P receives ( tok, p ) back.
It follows that each initiator p ends up with Listp equal to the set of all initiators, and the initiator with smallest identity is the only process that becomes elected.
There are at most N different tokens and each makes N steps , which brings the message complexity to O(N2 )
At N - 1 time units at the latest after the first initiator has sent out its token, each initiator has done so , and each initiator receives its token back within N time units after the generation of that token.
The non-initiators all enter the lost state, but remain waiting for more ( tok, r )  messages forever.
The waiting can be aborted if the leader sends a special token around the ring to announce that the election is over.
Each process is either a non-initiator or an initiator with an identity larger than Po , so all processes forward the token ( tok , po ) emitted by Po.
Non-initiators do not become elected, but they all enter the state lost, at the latest when Po 's token is forwarded.
Initiator p with p > Po does not become elected; Po does not forward the token ( tok, p ) , so p never receives its own token.
Such an initiator p enters the state lost, at the latest when ( tok, po ) is forwarded.
At most N different tokens are used, and each token is forwarded by at most N hops, which proves an O(N2 ) bound on the message complexity.
The Chang-Roberts algorithm is no improvement over LeLann's when the time complexity or the worst-case message complexity is considered.
There is an improvement when the average case is considered, where the average is taken over all possible arrangements of the identities around the ring.
This proof is based on a suggestion by Friedemann Mattern.
Assuming all processes are initiators, we compute the average number of.
Consider a fixed set of N identities , and let s be the smallest identity.
To compute the total number of token passings over all arrangements, we compute first the total number of times that the token ( tok, Pi ) is passed in all arrangements, and subsequently sum over i.
The token ( tok, s )  is passed N times in each arrangement , hence it is passed N(N - 1 ) !  times altogether.
The token ( tok, Pi ) is passed at most i times because it will be removed from the ring if it has reached s.
Let Ai,k be the number of cyclic arrangements in which ( tok, Pi ) is passed exactly k times.
Token ( tok, Pi ) is passed exactly i times , if Pi is the smallest of identities PI through Pi , which is the case in ( 1 /i )
Pi is a fraction l/k of all arrangements, i .e.
Now, for k < i , ( tok, Pi ) i s  passed exactly k times, if it is passed at least k times, but not more, i .e.
Consequently, the number of arrangements in which this happens is.
Next we sum the token passes over i to obtain the total number of token passings (excluding those for ( tok, s ) ) in all arrangements.
The algorithm first computes the smallest identity and makes it known to each process, then the process with that identity becomes leader and all others are defeated.
The algorithm is more easily understood if one first considers it as if it were an algorithm executed by the identities rather than by the processes.
Initially each identity is active, but in each round some identities become passive as will be shown later.
In a round an active identity compares itself with the two neighboring active identities in clockwise and anticlockwise directions ; if it is a local minimum, it survives the round, otherwise it becomes passive.
Because all identities are different , an identity next to a local minimum is not a local minimum, which implies that at least half of the identities do not survive the round.
Consequently, after at most log N rounds only one active identity remains, which is the winner.
We say a process is in the ith round when it executes the main loop for the ith time.
The rounds are not globally synchronized; it is possible that one process is several rounds ahead of another process in a different part of the ring.
But , as each process sends and receives exactly two messages in each round and channels are fifo, a message is always received in the same round as it is sent.
In the first round all initiators are active and each active process holds a different "current identity"
At the end of the round again all current identities of active processes are different and include the smallest identity.
By the exchange of ( one, q ) messages , which are relayed by the passive processes, each active process obtains the current identity of its first anticlockwise active neighbor, which is in all cases different from the own identity.
Consequently, each active process continues the round with the exchange of ( two, q ) messages , by which each active process also obtains the current identity of the second anticlockwise active neighbor.
Each active process now holds a different value of acn, which implies that the survivors of the round all have a different identity at the end of the round.
At least the identity that was the smallest at the beginning of the round survives , so there is at least one survivor.
An identity next to a local minimum is not a local minimum, which implies that the number of survivors is at most k/2
Claim 7.9 If a round starts with exactly one active process p, with current identity cip , the algorithm terminates after that round with winq = cip for each q.
The ( one, cip ) message of  p i s  relayed by all processes and finally received by p.
Process p obtains acnp = cip and sends a ( smal, acnp ) message around the ring, which causes each process q to exit the main loop with winq = acnp.
The algorithm terminates in each process and and all processes agree on.
The result is due to Pachl, Korach, and Rotem [PKR84] and is obtained under the following assumptions.
This assumption does not weaken the results because it describes a possible situation for each decentralized algorithm.
Under these assumptions each computation starting from a given initial configuration contains the same set of events.
The set of all such sequences is denoted D, i .e.
The ring labeled with S is also called the s-ring.
If t is a cyclic shift of s, the t-ring is of course the same ring as the s-ring.
With each message, sent in the algorithm, a sequence of process identities will be associated, called the trace of the message.
If m is a message sent by process p before p has received a message, the trace of m is (p)
Sk ) then the trace of m is (Sl '
The lower bound will be derived from the properties of the set of all traces of messages that can be sent by an algorithm.
It  will be shown below that the set of all traces of an algorithm is exhaustive; in order to derive from this fact a lower bound on the complexity of the algorithm two measures of a set E are defined.
A sequence t appears as a consecutive sequence of identities in the s-ring if t is a prefix of any r E C 8 (s )
In what follows, let A be an algorithm that computes the smallest identity and define E A to be the set of sequences s such that an s-message is sent when algorithm A is executed in the s-ring.
Lemma 7.10 If both t and u contain s as a substring and an s-message is sent when algorithm A is executed on the t-ring, then an s-message is also sent when A is executed on the u-ring.
Their initial state in the u-ring is the same as in the t-ring (here we remember that the ring size is unknown) ,  and consequently, the collection of events preceding the sending of the message is applicable in the u-ring also.
To show that EA cyclicly covers D, consider a computation of A on the s-ring.
This trace is a cyclic shift of s and is in E.
In a computation on the s-ring, algorithm A sends at least M(s ,  EA) messages.
Let t E EA be a prefix of a cyclic shift r of s.
By definition of EA , A sends a t-message in a computation on the t-ring, hence also on the r-ring, which equals the s-ring.
For a finite set I of process identities, let Per(I) denote the set of all permutations of I.
Let aveA (I) denote the average number of messages used.
The lower bound is now shown by an analysis of arbitrary exhaustive sets.
Averaging over all initial configurations labeled with the set I, we find.
The proof given in this subsection essentially relies on the assumptions.
Summarizing, it turns out that the complexity of election on a ring is insensitive to almost all the assumptions one can make.
Whether the ring size is known or not , whether the ring is bidirectional or unidirectional, and whether worst-case or average-case complexity is considered, the complexity is 8 (N log N) in all cases.
As a leader can be elected in a single computation of a decentralized wave algorithm, the lower bound on election implies a lower bound for wave algorithms as well.
Theorem 7.15 Any comparison election algorithm for arbitrary networks has a (worst-case and average-case) message complexity of at least O( IE I  + N log N)
The O (N log N) term is a lower bound because arbitrary networks include rings , for which an O(N log N) lower bound holds.
To see that lE I messages i s  a lower bound, even in the best o f  all computations, assume.
Construct a network G' by connecting two copies of G with one edge between nodes inserted on an edge that is not used in C.
The identities in both parts of the network have the same relative order as in G.
Computation C can be simulated simultaneously in both parts of G' , yielding a computation in which two processes become elected.
A decentralized wave algorithm for arbitrary networks without neighbor knowledge has a message complexity of at least O ( IE I  + N log N)
For a wave algorithm A, the election algorithm Ex(A) is as follows.
Each process is active in at most one wave at a time; this wave is its currently active wave, denoted caw , with initial value udef Initiators of the election act as if they initiate a wave and set caw to their own identity.
If a message of some wave, say the wave initiated by q, arrives at p, p processes the message as follows.
If q > cawp , the message is simply ignored, effectively causing q's wave to fail.
If q = cawp , the message is treated exactly according to the wave algorithm.
If q < cawp or cawp is udef, p joins the execution of q's wave.
When the wave initiated by q executes a decision event (in most wave algorithms this decision always takes place in q) , q will be elected.
If the wave algorithm is such that the decider is not necessarily equal to the initiator, the decider informs the initiator via the spanning tree as defined in Lemma 6.3
This takes at most N - 1 messages; we ignore these in the next theorem.
If A is a centralized wave algorithm using M messages per wave, the algorithm Ex(A) elects a leader using at most N M messages.
The wave initiated by Po is joined immediately by every process that receives a message of this wave, and every process completes this wave because there is no wave with smaller identity for which the process would abort the execution of the wave of Po.
Consequently, the wave of Po runs to completion, a decision will take place and Po becomes leader.
If p is a non-initiator,  no wave with identity p is ever initiated, hence p does not become leader.
As Po never executes a send or internal event of the wave with identity p, such a decision does not take place, and p is not elected.
At most N waves are started, and each wave uses at most M messages, which brings the overall complexity to N M.
It is a more delicate question to estimate the time complexity of Ex(A)
In many cases it will be of the same order of magnitude as the time complexity of A, but in some unlucky cases it may occur that the initiator with smallest identity starts its wave very late.
In the general case a time complexity of o (N.t) can be shown (where t is the time complexity of the wave algorithm) , because within t time units after initiator p starts its wave, p's wave decides or another wave is started.
The election problem in arbitrary networks is closely related to the problem of computing a spanning tree with a decentralized algorithm, as the following argument reveals.
Let CE be the message complexity of the election problem, and CT the complexity of computing a spanning tree.
It will be assumed here that w (e) is a real number, but integers are also possible as edge weights.
Let G = (V, E) be a weighted graph, where w (e) denotes the weight of edge e.
The weight of a spanning tree T of G equals the sum of the weights of the N - 1 edges contained in T, and T is called a minimal spanning tree, or MST, (sometimes minimal-weight spanning tree) if no tree has a smaller weight than T.
It is assumed in this subsection that each edge has a unique weight , i .e.
Proposition 7.18 If all edge weights are different, there is only one MST.
We will first describe how the algorithm operates in a global fashion, i .e.
We then describe the local algorithm that each node must execute in order to obtain this global operation of the fragments.
A computation of the GHS algorithm proceeds according to the following steps.
A collection of fragments is maintained, such that the union of all fragments contains all nodes.
Initially this collection contains each node as a one-node fragment.
The nodes in a fragment cooperate to find the lowest-weight outgoing.
When the lowest-weight outgoing edge of a fragment is known, the.
The efficient implementation of these steps requires the introduction of some notation and mechanisms.
Lemma 7.20 If these combining rules are obeyed, the number of times a process changes its fragment name or level is at most N log N.
The level of  a process never decreases , and only when it increases does the process change its fragment name.
A fragment of level L contains at least 2L processes , so the maximum level is log N, which implies that each individual process increases its fragment level at most log N times.
Hence, the overall total number of fragment name and level changes is bounded by N log N.
A fragment F with name F N and level L is denoted as F = (F N, L) ; let eF denote the lowest-weight outgoing edge of F.
If eF leads to a fragment F' = (FN', L') with L < L', F combines into F' , after which the new fragment has name F N' and level L'
These new values are sent to all processes in F.
If eF leads to a fragment F' = (FN' , L') with L = L' and eF' = eF , the two fragments combine into a new fragment with level L+ 1 and name w(eF)
These new values are sent to all processes in F and F'
This status is branch if the edge is known to be in the MST, reject if it is known not to be in the MST, and basic if the edge is still unused.
As the first action of each process, the algorithm must be initialized: begin let pq be the channel of p with smallest weight ;
Upon receipt of ( initiate, L, F, S )  from q: begin levelp := L ; namep := F ; statep := S ; father p := q ;
Upon receipt of ( accept ) from q: begin testchp := udef ;
Upon receipt of ( reject ) from q: begin if stachp [q] = basic then stachp [q] := reject ;
The nodes in a fragment cooperate to find the lowest-weight outgoing edge of the fragment , and when the edge is found a ( connect , L )  message is sent through it ; L is the level of the fragment.
If the fragment consists of a single node, as is the case after the initialization of this node, the required edge is simply the lowest-weight adjacent edge of this node; see ( 1 )
A ( connect , 0 )  message is sent via this edge.
Next consider the case that a new fragment is formed by combining two fragments ,  connection being by edge e = pq.
If the two combined fragments were of the same level, L, both p and q will have sent a ( connect , L ) message via e ,  and will have received a ( connect , L ) message in return while.
Upon receipt of ( report , w ) from q: begin if q =f.
Upon receipt of ( changeroot ) : begin changeroot end.
Edge pq becomes the core edge of the fragment , and p and q exchange an ( initiate, L + 1 ,  N, S )  message, giving the new level and name of the fragment.
The name is w (pq) and the status find causes each process to start searching for the lowest-weight outgoing edge; see action (3)
The message ( initiate, L + 1 ,  N, S )  is flooded to each node in the new fragment.
If the level of p was smaller than the level of q , p will have sent a ( connect , L )  message via e,  and will have received an ( initiate, L' , N, S )  message in return from q; see action (2)
In this case, L' and N are the current fragment level and name of q, and the name and level of the nodes on q's side of the edge do not change.
On p's side of the edge the initiate message is flooded to all the nodes (see action (3) ) ,  causing every.
If q is currently searching for the lowest-weight outgoing edge (8 = find) the processes in p's fragment join the search by calling test.
The lowest-weight outgoing edge is reported for each subtree using ( report , w ) messages ; see (8)
Each process sends a ( report , w ) message to its father when it has received such a message from each of its sons and has finished the local search for an outgoing edge.
The ( report , w ) messages are sent in the direction of the core edge by each process , and the messages of the two core nodes cross on the edge; both receive the message from their father; see (9)
Each core node waits until it has sent a ( report , w ) message itself before it processes the message of the other process.
When the two ( report , w ) messages of the core nodes have crossed, the core nodes know the weight of the lowest-weight outgoing edge.
If an outgoing edge was reported , the best edge is found by following the bestch pointer in each node, starting from the core node on whose side the best edge was reported.
A ( connect , L )  message must be sent through this edge, and all father pointers in the fragment must point in this direction; this is done by sending a ( changeroot ) message.
When the ( changeroot ) message arrives at the node incident to the lowest-weight outgoing edge, this node sends a ( connect , L )  message via the lowest-weight outgoing edge.
To inspect edge pq, p sends a ( test , levelp , namep ) message to q and waits for an answer, which can be a ( reject ) ,  ( accept ) ,  or ( test , L, F )  message.
A ( reject ) message is sent by process q (see (5) ) if q finds that p's fragment.
On receipt of the ( reject ) message p rejects edge pq and continues the local search; see (7)
The ( reject ) message is omitted if the edge pq was just used by q also to send a ( test , L, F )  message; in this case q's ( test , L, F )  message serves as the reply to p's message; see (5)
If the fragment name of q differs from p's, an ( accept ) message is sent.
On receipt of this message p terminates its local search for outgoing edges with edge pq as the best local choice; see (6)
The processing of a ( test , L, F )  message by p is deferred if L > levelp.
The reason is that p and q may actually belong to the same fragment , but the ( initiate, L, F, S )  message has not yet reached p.
Node p could erroneously reply to q with an ( accept ) message.
According to the combining rules discussed earlier, the ( connect , level ) is answered with an ( initiate, L, F, S )  message in two cases.
Case B:  If the two fragments have the same level and the best outgoing edge of fragment F' is also pq, a new fragment is formed, of which the level is one higher and the name is the weight of edge pq; see (2)
This case occurs if the two levels are equal and the connect message is received via a branch edge; observe that the status of an edge becomes branch if a connect message is sent through it.
If neither of these two cases occurs, fragment F must wait until either q.
Deadlock potentially arises in situations where nodes or fragments must wait until some condition occurs in another node or fragment.
The waiting introduced for ( report , w ) messages on the core edge does not lead to a deadlock because each core node eventually receives reports from all sons (unless the fragment as a whole waits for another fragment) ,  after which the message will be processed.
Each edge is rejected at most once and this requires two messages, which.
Their algorithm distributively computes a clustering of the network as indicated in Lemma 4.47 and a spanning tree and a center for each cluster.
While the construction of arbitrary and of minimal spanning trees is of equal complexity in arbitrary networks, this is not true in cliques.
Many results have been obtained for the election problem, not only for the case of ring networks and arbitrary networks, but also for the case of other specialized topologies, such as clique networks , etc.
In several cases the best known algorithms have an O(N log N) message complexity and in some cases this result is matched by an O(N log N) lower bound.
Their main result is a general construction of an efficient election algorithm for a class of networks, given a traversal algorithm for this class.
In order to bring tokens of the same level together in one process, each token can be in one of three modes : annexing, chasing, or waiting.
A token is represented by (q, l ) ,  where q is the initiator of the token and 1 its level.
The variable levp gives the level of process p and the variable catp gives the initiator of the last annexing token forwarded by p (the currently active traversal of p)
The variable waitp is udef if no token is waiting at p and its value is q if a token (q, levp) is waiting at p.
The variable lastp is used for tokens in chasing mode: it gives the neighbor to which p forwarded an annexing token of level levp , unless a chasing token was already sent after it ; in this case lastp = udef.
The algorithm interacts with the traversal algorithm by a call to the function trav : this function returns either the neighbor to which the token must be forwarded, or decide if the traversal terminates.
The token arrives at node p of level levp > l : the token is killed in this case.
The token arrives at a node with level l that has been most recently.
A chasing token (q, l) is forwarded in each node via the channel through which the most recently passed token was sent , until one of the following situations occurs.
The token arrives in a process of level levp > l :  the token is killed in this case.
The token arrives in a process with a waiting token of level l :  the two tokens are removed and a new traversal is started by this process (see Case II)
The token arrives at a process of level l where the most recently passed token was chasing: the token becomes waiting (see Case III)
A waiting token resides in a process until one of the following situations occurs.
A token of higher level arrives at the same process : the waiting token is killed (see Case I)
A token of equal level arrives: the two tokens are removed and a traversal of higher level is started (see Case II)
Observe that if p receives a token of level higher than levp , this is an annexing token of which p is not the initiator.
If a traversal terminates in p, p becomes leader and floods a message to all processes, causing them to terminate.
Let q be the process with maximal identity that generates a token at level l.
The token (q , l) does not complete the traversal, because it will be received by a process p that has already forwarded a different token of level l.
When this happens for the first time, (q, l) becomes chasing or, if p has already forwarded a chasing.
In either case, there are chasing tokens at level l.
Let (r, l) be the token with minimal identity after which a chasing token is sent.
Token (r, l) is itself not chasing, because a token only chases tokens with smaller identity.
We may therefore assume that (r, l) became waiting when it first arrived at a process p' that had already forwarded a different token of level l.
Let pI! be the last process on the path followed by (r, l ) that received, after it forwarded (r, l) , an annexing token that turned into a token chasing r.
This chasing token either meets (r, l) in p' , or abandons the chase if a waiting token was found before the token reached p'
In both cases a token at level l + 1 is generated, a contradiction.
By the previous lemma, the number of tokens generated in each level decreases , and there will be a level, l say, at which exactly one token, say (q, l) is generated.
No token of level l' < l completes the traversal, hence none of these tokens causes a process to become elected.
The unique token at level l only encounters processes at levels smaller than l ,  or with cat = q (if it arrives at a process it has already visited) , and is forwarded in both cases.
Hence the traversal of this token completes and q is elected.
So there are at most (1 + log k) different levels, and at most f (N) + N messages are sent in each level, which proves the result.
Rings are x-traversable, hence the KKM algorithm elects a leader in a ring using 2N log N messages.
These results are only true for cliques without sense of direction; with sense of direction a linear algorithm exists.
Without sense of direction Tarry's algorithm can be used for traversal, and this algorithm operates linearly on the torus.
Peterson [Pet85] has given an election algorithm for grids and tori that uses O(N) messages and does not require the edges to be labeled.
Exercise 7.5 Consider the Chang-Roberts algorithm under the assumption that every process is an initiator.
For what distribution of identities over the ring is the message complexity minimal and exactly how many messages are exchanged in this case ?
Exercise 7.6 What is the average case complexity of the Chang-Roberts algorithm if there are exactly S initiators, where each choice of S processes is equally likely to be the set of initiators ?
Is it possible for the algorithm to terminate in one round?
Assume that the GHS algorithm uses an additional wake-up procedure that guarantees that each node starts the algorithm within N time units.
Prove that the algorithm terminates within 5 N  log N time units.
Show that an O(N log N) algorithm for election in planar networks exists.
Show that there exists an O (N log N) election algorithm for tori without a sense of direction.
Hint :  analyse the performance of Tarry 's algorithm in tori.
Show that there exists an 0 (N log N) election algorithm for hypercubes without a sense of direction.
A computation of a distributed algorithm terminates when the algorithm has reached a terminal configuration ; that is , a configuration in which no further steps of the algorithm are applicable.
It is not always the case that in a terminal configuration each process is in a terminal state; that is , a process state in which there is no event of the process applicable.
Consider a configuration where each process is in a state that allows the receipt of messages, and all channels are empty.
Such a configuration is terminal, but the processes ' states could also occur as intermediate states in the computation.
In this case, the processes are not aware that the computation has terminated; and the termination of the computation is said to be implicit.
Termination is said to be explicit in a process if the state of that process in the terminal configuration is a terminal state of the process.
Implicit termination of the computation is also called message termination because no more messages are exchanged when a terminal configuration has been reached.
Explicit termination is also called process termination because the processes have terminated if an algorithm explicitly terminates.
It is usually easier to design an algorithm that terminates implicitly than one that terminates explicitly.
Indeed, during the design of an algorithm all aspects regarding the proper termination of processes can be ignored; the design concentrates on bounding the overall number of events that can take place.
On the other hand, the application of an algorithm may require that processes terminate explicitly.
Only after explicit termination can the results of a computation be regarded as final, and variables used in the computation discarded.
Also, a deadlock of a distributed algorithm results in a terminal configuration; in this case the computation must be restarted when a terminal configuration is reached.
In this chapter general methods will be investigated for transforming.
In this subsection a model of distributed computations will be defined that treats the problem of terminating distributed computations.
The model is derived from the model in Chapter 2, but all aspects irrelevant to the termination problem are ignored.
The state set Zp of process p is partitioned into two subsets, the active and the passive states.
A state Cp of p is active if an internal or send event of p is applicable in Cp ,  and passive otherwise.
In a passive state cp only receipts are applicable, or no event is applicable at all ,  in which case cp is a terminal state of p.
Process p is simply said to be active if it is in an active state, and process p is said to be passive otherwise.
Clearly, a message can be sent only by an active process, and a passive process can become active only when a message is received.
An active process can become passive when it enters a passive state.
Some assumptions are made in order to simplify the description of the algorithms in this chapter.
The internal events in which p becomes passive are the only internal events of p.
Of p 's state it is only relevant whether it is active or passive; this will be represented in the variable statep.
As usual, initial configurations are assumed to have no messages in transit.
The predicate term is defined to be true in every configuration where no event of the basic computation is applicable; according to the following theorem, this is the case if all processes are passive and no basic messages are in transit.
If all processes are passive, no internal or send event is applicable.
If, moreover, no channel contains a ( mes ) message, no receive event is applicable, hence no basic event is applicable at all.
If some process is active, a send or internal event is possible in that process, and if some channel contains a ( mes ) message the receipt of this message is applicable.
In a terminal configuration of the basic algorithm each process is waiting to receive a message and remains waiting forever.
The problem discussed in this chapter is that of adding a control algorithm to the system that brings the processes into a terminal state after the basic computation has reached.
For the combined algorithm (basic plus control algorithm) a configuration satisfying term is not necessarily terminal; in general there will be applicable events of the control algorithm.
The control algorithm exchanges (control) messages , and these can be sent by passive processes and do not make a passive process active when they are received.
The termination detection algorithm calls Announce, and this announcement algorithm brings the processes in a terminated state.
It must not influence the computation of the basic algorithm.
If term holds, Announce must be called within a finite number of steps.
The complexity of termination detection is also related to the cost of executing a wave algorithm; let W denote the message.
It will be shown that the worst-case complexity of termination detection, for centralized as well as decentralized computations, is bounded from below by M.
It will then be shown that the complexity of termination detection for decentralized basic computations is bounded from below by W.
At the end of this subsection a lower bound of lE I  messages, given by Chandrasekaran and Venkatesan [CV90] , will be discussed.
If the basic algorithm is centralized, such a configuration can be reached from the initial configuration by exchanging one basic message; otherwise, such configurations include initial configurations.
Consider the basic computation that does not exchange any message and where each active process becomes passive as its first event.
This basic computation requires the detection algorithm to be a wave algorithm if the detection (the call to Announce) is regarded as the decision.
Indeed, a call to Announce must occur within a finite number of steps, which proves that the detection algorithm itself terminates and decides.
If the decision is not preceded by an event in some process q, a different basic computation is considered where q does not become passive.
The decision does not depend causally on any event in q, so the detection algorithm may erroneously call Announce while q is still active.
As the detection algorithm is a wave, it exchanges at least W messages.
It will be assumed in this chapter that the control algorithm is started in the initial configuration of the basic computation, i .e.
To announce the termination to all processes, a ( stop ) message is flooded to all processes.
Each process sends such a message to all neighbors, but does so at most once, either in a local call of Announce or upon receipt of the first ( stop ) message.
When a ( stop ) message has been received from every neighbor a process executes the statement stop, causing the process to enter a terminal state.
The initiator of the basic computation (called the environment in [DS80] ) , also plays a special role in the detection algorithm and is denoted by po.
The detection algorithm dynamically maintains a computation tree T = (VT ' ET) with the following two properties.
Either T is empty, or T is a directed tree with root PO.
The set VT includes all active processes and all basic messages in.
The removal of nodes from T is also necessary, for two reasons.
First , a basic message is deleted when it is received.
Second, to ensure progress of the detection algorithm the tree must collapse within a finite number.
Messages are leaves of T; processes maintain a variable that counts the number of their sons in T.
The deletion of a son of process p occurs in a different process q; it is either the receipt of a son message, or the deletion of the son process q.
To prevent corruption of p's son count , a signal message ( sig, p )  can be sent to p when a son of p is deleted.
This message replaces the deleted son of p, and its deletion, i .e.
The variable scp gives the number of sons of p in T.
The correctness proof rigorously establishes that the graph T, as defined, is a tree and that it becomes empty only after termination of the basic computation.
The safety of the algorithm will follow from the assertion P, defined by.
By definition, the node set of T includes all messages (basic as well as control messages) , and by ( 1 )  it also includes all active processes.
Clause (2) is rather technical; it states that T is indeed a graph and all edges are directed towards processes.
Clause (5) is used to show that the tree indeed collapses if the basic computation terminates.
For the correctness proof, note that P implies that father p = p holds only for p = Po.
Lemma 8.4 P is an invariant of the Dijkstra-Scholten algorithm.
No process becomes a passive leaf and no process is inserted in VT, so (5) is preserved.
If a value is assigned to fatherp , its new value is q, and if a signal is sent by p its father is also q, and q is in VT, so (2) is preserved.
The number of sons of q does not change, because the son (mes, q) of q is replaced either by the son p or the son ( sig, q ) , so sCq remains correct , which preserves (3)
The structure of the graph does not change, so (4) is preserved.
Process p is in VT after the action in any case, so (5) is preserved.
This implies that the number of control messages never exceeds the number of basic messages in any computation.
To prove liveness of the algorithm, assume that the basic computation has terminated.
After termination only actions Ap can take place and because S decreases by one in every such transition, the algorithm reaches a terminal configuration.
Consequently, T has no leaves, which implies that T is empty.
The tree became empty when Po removed itself, and the program is such that Po called Announce in this step.
To prove the safety, note that only Po calls Announce, and does so when it removes itself from VT.
By (4) , T is empty when this happens, which implies term.
The verification that all trees have collapsed is done by a single wave.
The forest is maintained with the additional property that if the tree Tp has become empty, it remains empty thereafter.
Note that this does not prevent p from becoming active; but if p becomes active after the collapse of its tree, p is inserted in the tree of another initiator.
Each process participates in the wave only if its tree has collapsed; when the wave decides, Announce is called.
Calling Announce is superfluous if the chosen wave algorithm generates a decision in every process; in this case, a process simply halts after deciding and completing the wave algorithm.
The correctness proof of the algorithm is very similar to the proof of the Dijkstra-Scholten algorithm.
Lemma 8.6 Q is an invariant of the Shavit-Jilrancez algorithm.
Initially statep = passive for every non-initiator p, and father p = p.
Rp: Either p was in VF already (father p i= udef) or p is inserted in the action, so ( 1 )  is preserved.
If a value is assigned to fatherp , its new value is q, and if a signal is sent , its father is also q, and q is in VT, so (2) is preserved.
The number of sons of q does not change, because the son (mes, q) of q is replaced either by the son p or the son ( sig, q ) , so SCq remains correct (3)
The structure of the graph does not change, so (4) is preserved.
No process becomes a passive leaf and no process is inserted in VF, so (5) is preserved.
No tree becomes empty or becomes non-empty, hence (6) is preserved.
The processes concurrently execute a wave algorithm in which sending or deciding by p is allowed only if emptyp is true and in which decide calls Announce.
Ap : The receipt of the signal decreases the number of sons of p by one and the assignment to sCp ensures that (3) is preserved.
That p was the father of the signal implies that p was in VF.
Besides signals , the control algorithm only sends messages for one wave.
It follows that at most M + W control messages are sent.
To prove liveness of the algorithm, assume that the basic computation has terminated.
Consequently, all events of the wave are enabled in every process, and that the configuration is terminal now implies that all events of the wave have been executed, including at least one decision, which caused a call to Announce.
To prove the safety, note that Announce is called when a decision occurs in the wave algorithm.
This implies that each process p has sent a wave message or has decided, and the algorithm implies that emptyp was true when p did so.
No action makes emptyp false again, so (for each p) emptyp is true when Announce is called.
Second, although the message complexity of these algorithms is optimal in the worst case, there exist algorithms that have a better complexity in the average case.
The algorithms of the previous section use their worst-case number of messages in every execution.
In this section some algorithms based on the repeated execution of a wave algorithm will be studied; at the end of each wave, either termination is detected, or a new wave is started.
Termination is detected if a local condition turns out to be satisfied in each process.
We shall first consider concrete instances of the algorithms, where in all cases the wave algorithm is the ring algorithm.
A ring is supposed to be embedded as a subtopology of the network for this purpose; but the exchange of basic messages is not restricted to the channels belonging to the ring.
The token tour ends when the token is received back by process Po.
In the following, t denotes the number of the process that holds the token or, if the token is in transit , the number of the process to which the token is under way.
The invariant must hold when Po initiates the wave, i .e.
As a first attempt , set P = Po , where.
The forwarding of the token preserves Po if only passive processes forward the token, hence we adopt the following rule.
A process only handles the token when it is passive.
Under this regime, P is preserved by token forwarding and also by internal actions; unfortunately, P is not preserved by communication actions.
Because Po can be falsified, P is replaced by a weaker assertion (Po V PI ) '  where PI is chosen such that after each falsification of Po , PI is true.
We provide each process with a color, which is either white or black, and let P = (Po V PI ) where.
Each falsification of Po ensures that PI is or becomes true if a sending process colors itself black.
Weakening P is successful in preventing falsification by communication events; but the weaker assertion can be falsified by token forwarding, namely if process t is the only black process and forwards the token.
The situation is saved by a further weakening of P.
The token is now supposed to have a color (white or black) also, and P is weakened to (Po V PI V P2) ,  where.
Token forwarding preserves P2 if black processes forward the token black.
When a black process other than Po forwards the token, the token becomes black.
When the wave ends unsuccessfully, Po initiates a new one.
The next wave would certainly be as unsuccessful as its predecessor if there was no way for black processes to become white again; indeed, the black processes would blacken the token when forwarding it , causing the next wave to fail as well.
Observe that whitening process Pi does not falsify P if i > t, and that P always becomes true when Po initiates the wave by sending the token to PN-I.
This implies that whitening can safely take place upon forwarding the token.
This whitening regime suffices to guarantee the eventual success of a wave.
Start the detection, executed once by Po : begin send ( tok, white ) to PN-1 end.
The predicate P == (Po V PI V P2 ) and the algorithm have been designed in such a way that P is an invariant of the algorithm.
Termination is detected when the passive, white Po handles the white token.
From then on, all processes forward the token without delay when they receive it.
When the token completes its first full tour started after termination, all processes.
We shall now attempt to estimate the number of control messages used by the algorithm.
Define, for each configuration, the number of messages in transit as B.
The invariance of Pm is obtained when initially mcp = 0 for each p, and processes obey the following rule.
When process P sends a message, it increments its message counter; when process p receives a message, it decrements its message counter.
The invariant must allow Po to decide that term holds when it receives the token (t = 0)
Because term now also includes a restriction on the value of B,  the token will be used to carry an integer q to compute the sum of the message counters of the processes that have forwarded it.
As a first attempt , set P = Pm 1\ Po , where.
Assertion Po is established when Po initiates the wave by sending the.
The forwarding of the token preserves Po if only passive processes forward the token and add the value of their message counter; hence we adopt the following rule.
A process only handles the token when it is passive, and when a process forwards the token it adds the value of its message counter to q.
Under this regime, P is preserved by token forwarding and also by internal actions; unfortunately, P is not preserved by the receipt of a message by process Pi with i > t.
The falsification of Po takes place in a receipt , i .e.
Because Po holds before its falsification, this implies that PI holds, where.
Assertion PI remains true under a receipt by Pi with i > t; consequently, the.
Each receipt that falsifies PI makes P2 true, so P is not falsified by any basic action.
Weakening P was successful in preventing falsification by basic events; but the weaker assertion can be falsified by token forwarding, namely, if process t is the only black process and forwards the token.
The situation is saved by a further weakening of P.
Token forwarding preserves P3 if black processes forward the token black.
When a black process forwards the token, the token becomes black.
Because (the token is white) =? ,P3 ,  termination can still be detected by Po , namely, by whether it receives a white token (and is white itself, and passive)
Indeed, it can now be verified that internal actions , basic communication, and token forwarding preserve P.
A wave ends unsuccessfully if, when the token returns to Po , the token is black, Po is black, or mcpo + q ::f.
If the wave ends unsuccessfully, a new one must be started.
When the wave ends unsuccessfully, Po initiates a new one.
The next wave would be as unsuccessful as its predecessor if there were no way for black processes to become white again; indeed, the black processes would blacken the token when forwarding it , causing the next wave to fail as well.
Observe that whitening process Pi does not falsify P if i > t ,  and that P always becomes true when Po initiates the wave by sending the token to PN-I.
This implies that whitening can safely take place upon forwarding the token.
To show the safety, observe that termination is detected when t = 0, statepo = passive , colorpo = white , and mCpo +q = o.
To show liveness, observe that after termination of the basic computation the message counters are constant and their sum equals o.
A wave started in such a configuration ends with mcpo + q = 0 and with all processes colored white,  after which the next wave is guaranteed to be successful.
Start the detection, executed once by Po : begin send ( tok, white ) to PN- 1 end.
Safra's algorithm counts the basic messages that are sent and received in order to establish that no basic messages are under way.
This variation of the principle is discussed only briefly because the resulting algorithm is not in any respect an improvement on the Shavit-Francez algorithm, and has therefore become obsolete.
First , observe that no message is in transit is equivalent to for all p, no message sent by p is in transit.
Each process is responsible for the messages it has sent , i .e.
The detection method defines for each process p a local condition quiet (p) in such a way that.
When sending a message, p increments unackp ; when receiving a message from q, p sends an acknowledgement to q; when receiving an acknowledgement , p decrements unackp.
The requirement set above for quiet (namely, that quiet (p) implies that p is passive and no basic message sent by p is in transit) is now satisfied if quiet is defined by.
The introduction of PI needs some care, because the activation of process.
Pj with j > t by Pi with i :S t does not take place in the same event as the sending of a message by Pi.
It is the case, however, that when Pj is activated (thus falsifying Po) ,  unackpi > O.
When a process sends it becomes black; a process becomes white only when it is quiet.
Termination is announced when Po is quiet and processes the white token.
These conditions imply that ,P2 and ,PI ,  and hence Pa A Pb A Po holds.
This implies with quiet (po) that all processes are quiet , hence term holds.
Tan and Van Leeuwen [TL86] gave decentralized solutions for ring networks, for tree networks, and for arbitrary networks.
Inspection of these solutions reveals that they are almost similar, except for the wave algorithm on which they rely.
For each wave, the first event in which a process sends a message for the wave or decides , is called the visit to that process.
It is assumed that , if necessary, the process can suspend the visit until the process satisfies a local condition.
Later events of the same wave in that process are never suspended.
The invariant of the algorithm must allow termination to be detected when the wave decides; therefore, as a first attempt , we set P = Po , where.
Indeed, as all processes have been visited when the decision occurs , this assertion allows termination detection when the wave decides.
Moreover, Po is established when the wave is initiated (no process has been visited yet)
The activity of the wave algorithm preserves Po if rule 1 below is observed.
Unfortunately, Po is falsified when a visited process is activated by an unvisited process.
Therefore, each process is provided with a color, and P is weakened to (Po V PI ) ,  where.
The weaker assertion can be falsified by the wave if a visit is made to the only unvisited black process.
The situation is saved by a further weakening of P.
Each process submits a color, either white or black, as input to the wave.
The wave is modified so as to compute the darkest of the submitted colors ; recall that waves can compute infima, and "the darkest" is an infimum.
When the wave decides , the darkest of all submitted colors will be computed; this will be white if all processes submitted white and black if at least one process submitted black.
During the wave, the wave will be called white if no process has yet submitted black; and black if at least one process has already submitted black.
Thus a process , when it is visited, either submits white ,  which leaves the color of the wave unchanged, or submits black, which colors the wave black.
A process , when visited, submits its current color to the wave.
Indeed, all basic communication as well as wave activity preserves this assertion, which is therefore an invariant.
A wave ends unsuccessfully if the processes decide for black, but in this case a new wave is simply initiated.
The new wave can only be successful if processes can become white, and this happens immediately after the visit of the wave.
A decider in a black wave initiates a new wave.
Each process turns white immediately after each visit of a wave.
These rules guarantee the eventual success of a wave after the termination of the basic computation.
Indeed, if the basic computation has terminated, the first wave started after termination makes all processes white, and the next wave terminates successfully.
In this algorithm only one wave may be running at any time.
If two waves , say A and B ,  run concurrently, the whitening of a process after a visit of B may violate the invariant for wave A.
Therefore, if the detection algorithm must be decentralized, a decentralized wave algorithm must be used so that all initiators of the detection algorithm cooperate in the same wave.
It is also possible to use a different detection principle, in which different waves can compute concurrently without violating the correct operation of the detection algorithm; see Subsection 8.4
The initiator acts as a bank, collecting all credits sent to it in a variable ret (for returned)
When a process becomes passive, it sends its credit to the initiator.
The invariant must be maintained during the computation; this is taken care of by the following rules.
First , each basic message must be given a positive credit when it is sent ; fortunately, its sender is active, and hence has a positive credit.
When the active process p sends a message, its credit is divided among p and the message.
A process must be given a positive credit when it is activated; fortunately, the message it receives at that point contains a positive credit.
When a process is activated it is given the credit of the message activating it.
The only situation not covered by these rules is the receipt of a basic message by an already active process.
The receiving process may handle the credit in two different ways, both giving rise to a correct algorithm.
When an active process receives a basic message, the credit of that message is sent to the initiator.
When an active process receives a basic message, the credit of that message is added to the credit of the process.
When the initiator becomes passive, it sends a message to itself.
Termination is detected when ret = 1 ,  which together with the invariant implies that term holds.
To show liveness, observe that after termination no basic actions occur, hence only receipts of ( ret , c )  messages occur and each receipt decreases the number of messages in transit by one.
The credit-recovery algorithm is the only algorithm in this chapter that requires additional information (namely, the credit) to be included in basic messages.
If piggybacking is not desirable,  the credit of a message can be transmitted in.
The algorithm of the next subsection also requires piggybacking if it is implemented using Lamport 's logical clock.
The algorithm aims to check whether, for a certain point in time t, all processes were quiet at time t ;  termination at time t follows.
This is done by a wave, which asks each process to confirm that it was quiet at that time and later; a process that was not quiet does not respond to messages of the wave, effectively extinguishing the wave.
The visit of the wave may affect variables of the wave algorithm and, if Lamport 's logical clock is used, the clock of the process.
As a consequence the correct operation of the algorithm is not disturbed by the concurrent execution of several waves.
To prove the safety of the algorithm, assume process Po calls Announce; this occurs when Po is quiet and receives back its token ( tok, 0, qt , Po ) , which has been forwarded by all processes.
Assume that term does not hold when Po detects termination; this implies that there is a process p such that p is not quiet.
In this case p has become unquiet after forwarding Po 's token; indeed, p was quiet when it forwarded this token.
Let q be the first process that became unquiet after forwarding the token ( tok, 0 ,  qt , po )
This implies that q was activated by the receipt of a message from a process, say r, that had not yet forwarded Po 's token.
Otherwise r would have become unquiet after forwarding the token, but before q became unquiet , contradicting the choice of q.
Now after forwarding the token, Oq > qtpo continues to hold.
According to the algorithm r does not forward the token; this is a contradiction.
Globally unique identities have also been used to detect termination, e.g.
It is shown in this chapter that it is possible to break the symmetry in anonymous networks but that it is not possible to detect termination unless the size of the network is known.
In such algorithms processes repeatedly "flip a coin" until their outcomes differs; when this happens, the symmetry is broken.
Needless to say, the distributed algorithm for comparing and repeating the tosses turns out to be more complicated than this concise statement of the principle (see Section 9.3)
Termination detection is possible if the network size is known; the number of processes agreeing on the result is counted, and termination is detected when the count equals the network size.
It is not possible to detect termination by using coin tosses because, with a very small probability, processes will draw identical results when tossing, after which termination may be concluded erroneously (see Subsection 9.4)
In this subsection it will be indicated how the model of distributed systems presented in Chapter 2 can be extended to cover algorithms that rely on randomization.
Randomization is the use of "electronic coin flipping" and random-number generators to obtain a random behavior of a process, where each of several behaviors occurs with known probability.
Randomization differs from the non-determinism that results from the unpredictability of relative speeds and the possibility that a process may continue its execution with different events.
It is a programming tool that introduces a probability distribution on the collection of the computations of a system, and allows us to make statements about properties of the algorithm with a certain probability.
Inherent non-determinism is not controllable and therefore an algorithm designer should always cope with the worst case over all choices that arise from inherent non-determinism.
A probabilistic process is modeled as a process that tosses a coin with every step it executes.
The collection of applicable steps depends not only on the state of the process, but also on the outcome of the coin toss in the previous step of the process.
The first step of the process depends on a toss made in the initial state.
In order to understand the formal definition, we first consider processes having only internal events.
The ith step of the process must be according to f-0 if the ith coin flip is a zero, and according to f-l otherwise.
To introduce communication between processes, a process is defined as an octuple consisting of a set of states, a set of initial states, and six event relations , namely for send, receive, and internal events, both for a coin flip of zero and for a coin flip of one.
A probabilistic distributed algorithm is modeled as a collection JP> of probabilistic processes.
A configuration of a probabilistic distributed algorithm consists of a state and an integer for each process, and a collection of messages if the algorithm uses asynchronous message passing.
The integer for each process counts the number of events that the process has executed.
If p is an assignment of an infinite sequence Pp of zeros and ones to each process p, a p-computation of the system is a computation in which the ith transition concerning process p is in the event relation specified by the ith element of pp.
Given a collection p of sequences, the probabilistic algorithm behaves like a deterministic distributed algorithm, which can be seen as a particular instantiation of the probabilistic distributed algorithm.
A deterministic algorithm is terminating if a terminal configuration is reached in every computation.
A deterministic algorithm is correct if it is both terminating and partially correct.
A probabilistic algorithm is terminating if it is p-terminating for every p.
A probabilistic algorithm is partially correct if it is p-partially correct for every p.
A probabilistic algorithm is correct if it is p-correct for every p.
Termination (or partial correctness or correctness) with probability one is not equivalent to termination (or partial correctness or correctness)
A probabilistic algorithm may fail for a non-empty collection of assignments, where the probability of p being in that collection is zero.
The p-message complexity of a probabilistic algorithm is the maximal.
The average message complexity of a probabilistic algorithm is the expected value of the p-message complexity.
The time and bit complexity of a probabilistic algorithm are defined similarly.
In this chapter some negative results will be proved, i .e.
For deterministic algorithms such a proof is generally easier to obtain; it suffices to construct a single computation that does not terminate with a configuration satisfying 'ljJ.
Let P A b e  a probabilistic algorithm that is correct.
We shall not be concerned with Sherwood algorithms in this book.
The probability of termination of a Las Vegas algorithm is often one; infinite executions do exist , but occur only if an unfavorable pattern of random bits is selected infinitely often.
This means in practice that a Las Vegas algorithm always terminates, although only an expected number of steps can be given, no upper bound on the number of steps taken by the algorithm.
Carlo algorithm is the algorithm by Itai and Rodeh to compute the ring size (Algorithm 9.5)
The algorithms in this chapter can be classified according to the following four criteria; we briefly indicate what type of algorithm is considered more attractive.
It is not our aim to present a complete theory of computing on anonymous networks, including the numerous results that have been obtained for these networks.
Rather we intend to investigate the computational power of such networks when considering problems relating to election and computation of functions, including the network size.
If a leader can be elected, the anonymous network is as powerful as a named network, because names can be assigned by a centralized algorithm as is demonstrated below.
Because the network size is easily computed by a centralized algorithm (as demonstrated below) election and computing the size are equivalent problems.
It will be seen that the network size cannot be computed probabilistically and the election problem is unsolvable if the network size is not known.
For non-initiators: begin receive ( tok, s )  from neighbor q ; father p := q ; recp := reep + 1 ;
Therefore it is assumed in the study of anonymous networks , in addition to the absence of names, that no leader is available.
Theorem 9.4 There exists a deterministic, process terminating algorithm for election in a network of two processes that communicate by synchronous message passing.
Each process has three states ,  namely sleep, leader, and lost, and the initial state is sleep.
Each process can either send and enter the terminal state leader, or receive and enter the terminal state lost.
The solution provided by Angluin is quite topology specific and will not work in rings, for example.
The impossibility results of this chapter will generalize to systems with synchronous message passing of arbitrary or ring topology.
Theorem 9.5 There exists no deterministic algorithm for election in an anonymous ring of known size.
The result is proved for the case of unidirectional rings , but it is fairly easy to obtain the proof for the bidirectional case when this proof is understood.
For a configuration of the ring, let Ci denote the state of Pi and let Mi denote the collection of messages with destination Pi.
A configuration is called symmetric if all states are equal and the same set of messages is in transit to each process, i .e.
Vi , j : (Ci = Cj 1\ Mi = Mj)
If a message is received in the event , it is removed from each of the identical collections Mi , and if a message is sent it is added to each of the identical collections Mi.
Hence the computation is extended by N steps, after which a symmetric configuration I'(k+l)N results.
It follows that each algorithm for the anonymous ring has a computation that is either infinite or ends in a symmetric terminal configuration.
In a symmetric configuration either all processes are in the state leader, or no process is , so that in a symmetric configuration there is not exactly one process in the state leader.
For a deterministic election algorithm, all computations end in a terminal configuration with one process in the state leader; consequently, no such algorithm exists.
The results of this subsection indicate the importance of knowledge of the ring size, as well as the importance of the difference between message and process termination.
If the ring size is known each cyclic function can be computed by a deterministic algorithm by collecting all inputs in every process, a solution known as input collection.
Input collection costs N(N - 1) messages , and this is also the lower bound for computing some functions by a deterministic algorithm.
To keep the argument simple, assume the channels are fifo.
In the first phase, each process sends its own input to its clockwise neighbor and receives the input of its counterclockwise neighbor.
In each next phase, each process sends the value it received in the previous round to its clockwise neighbor and receives a new value from its anticlockwise neighbor.
As each input is passed one process further in every round, a process receives in the ith phase the input of its ith anticlockwise neighbor, and has collected all inputs (in the order in which they appear in the ring) after N - 1 phases.
After the communication rounds each process computes the value of f locally and terminates.
If p sends a message before it has received any message, the trace of this message is (p)
The three problems mentioned in the theorem have in common that , at least for one symmetric initial configuration, a process must receive a trace of length N - 1 before it can terminate.
For the SUM problem this is the case for every initial configuration.
For AND this is the case for the situation where each process has input true, and for OR this is the case for the situation where each process has input false.
To see that a message with a trace of length N - 1 must be received, consider the evaluation of AND in the situation where each process has input true; the answer is true in this case.
The event in which the process terminates causally depends only on events in k + 1 processes, and consequently it can also occur in a computation starting from an initial configuration where these k processes have input true and others have input false.
But as each process executes the same set of events, the number of messages sent with a trace of length i is a multiple of N, and hence is at least N.
Let the longest trace of any message received by p in Cx have length K; it  is  possible that K > k because A may circle messages around the ring several times before p terminates.
Consider a (larger) ring that contains a segment S of K + 1 processes with the following property.
The ith anticlockwise neighbor of p' is given the same input as the ith anticlockwise neighbor of p in computation Cx.
Case i = K: The Kth anticlockwise neighbor of p'  has the same initial state as the Kth anticlockwise neighbor of p in Cx , and hence there is a.
In the constructed computation, p' receives the same collection of messages as p does in Cx , and p' has the same initial state as p; consequently, there is a computation in which p' terminates with resultpl = a.
The constructed computation "fools" process p' into terminating with the outcome resultpl = f (x) , while actually the input is a longer sequence x' , different from x.
The computation would still give the correct answer if it were the case that f (x') = f(x)
To arrive at a contradiction, consider a ring that contains also a segment in which processes simulate computation Cy and a process q' terminates with resultql = b.
For this ring there is a computation in which two processes terminate with a different value of result; a contradiction because at most one answer is correct.
Theorem 9.9 OR and AND are computable by a deterministic message terminating algorithm in a ring of unknown size using at most N messages.
The value of OR is true if at least one process has input true and the value is false if all processes have input false.
Upon initialization, process p assigns resultp := xp and, if xp is true, sends a ( yes ) message to its.
When such a message is received by process p, it  is ignored if resultp has the value true, and otherwise resultp is set to true and the message is forwarded; see Algorithm 9.3
First consider the case where each process has input false.
Each process p assigns to resultp the value false and starts waiting for a message.
As no process sends a message, the configuration in which every process waits is terminal, and moreover, resultp contains the correct answer (false) in this case.
The computation of AND is similar, but with all occurrences of true and false reversed.
As a compromise between known and unknown ring size one may consider the situation where N is not known exactly, but is known to be between certain bounds.
The following results are easily proved by modifying the proofs given in this subsection.
There does not exist a deterministic algorithm for computing SUM that is correct for two different ring sizes.
There does not exist a deterministic algorithm for computing XOR that is correct for both an even and an odd ring size.
Replay arguments can also be given for other classes of network topology, but it requires that a small graph can be "unrolled" several times over a larger class of the graph.
The required type of mapping from one graph to another is called a cover.
Because several processes may select the same identity, a hop counter in the token is used by processes to recognize the receipt of their own token; p receives its own token when the hop count equals N.
The receipt of the token implies that no process has selected a smaller identity; but this is not a sufficient condition to become leader, as there may be another process with the same identity.
To detect this situation, each token also carries a boolean value un, which is true when the token is generated, but set to false.
A process can thus become leader when it receives its own token with un = true.
The level is also indicated in the token, and tokens of some level abort all activity of smaller levels.
Summarizing, p's receipt of its own token makes it win the Chang-Roberts algorithm if it is still in state cando If p is the only winner, it becomes leader, otherwise p initiates the Chang-Roberts algorithm at the next level.
The receipt of another process 's token causes p to become lost if this token has a higher level or the same level and a smaller identity; the token is forwarded.
A token with equal level and identity is forwarded, but with un set to false.
We shall continue by proving that the algorithm is partially correct , and that it terminates with probability one.
Assume that process p generates a token at level 1 but its identity is not minimal among the identities chosen at level l.
The token does not return to p in this case ; if it is not purged by a process at a higher level, it is received by a process at the same level but with smaller identity, where it is purged.
Observe that all processes that generate a token at level l do so before the receipt of p's token of level l.
Assume that p generates a token at level 1 and its identity is minimal among the identities chosen at level l.
Two cases arise: either p was the only process that chose the minimal identity at level l ,  or there are more processes that chose this identity.
In the first case, the token returns with un = true and p becomes elected.
It  remains to show that the algorithm terminates with probability one.
Various results in this chapter have indicated the importance of knowledge of the network size.
In this section the problem of computing the size of an anonymous ring will be considered.
This subsection contains two impossibility results concerning computation of the ring size.
The proofs use the idea in the proof of Theorem 9.S ,  namely, the replay of a given (presumably correct) computation in different parts of a larger ring.
For probabilistic algorithms it must be demonstrated that such a replay has a non-zero probability, but this follows from the fact that in a finite execution a process uses only a finite prefix of its sequence of random bits.
Assume that A is a probabilistic algorithm that has a p-computation CN on a ring of size N and in CN each process p terminates with resultp = N.
Select one process p and let the longest trace of a message received by p in CN have length K.
Each process executes only finitely many steps in CN ; let L be the largest number of steps executed by any process in computation CN.
There exists a p'-computation C of A on the larger ring in which process Pi sends exactly the same messages with traces of length :s: K + 1 - i as the ith anticlockwise neighbor of p in CN.
Assume A is a probabilistic algorithm that has a p-computation CN on the ring of size N, which message-terminates with resultp = N for some p.
Each process executes only finitely many steps in CN; let L be the largest number of steps executed by any process in computation CN.
Number the processes of the size-N ring as Po through p N -1
Process p maintains a local estimate, estp , of the ring size, for which it is guaranteed that at any time this estimate is conservative , i .e.
Whenever p receives information that implies that estp is not equal to the ring size, p increments its estimate.
In order to gain confidence in the correctness of its estimate, process p generates a token and sends it over a distance of estp hops over the ring.
If the estimate turns out to be correct , p receives its own token and if this happens, p will be (temporarily) satisfied and not take further actions.
Process p receives a similar token in the case where N is in fact larger than estp , but the ( estp)th anticlockwise neighbor of p has the same estimate and has initiated a token as well.
Process p increases its estimate in the following two situations.
A token is received that contains an estimate est with est > estp.
As it is ensured that all estimates are conservative , the receipt of this token implies that N > estp.
In the terminal configuration all estp are equal, and their common value is bounded by N, i.
The crucial step in proving termination is to demonstrate that the estimates indeed remain conservative.
Observe that estp never decreases, and that lblp changes only when estp increases.
This implies that as long as p's token ( test , est , lbl ,  h ) circulates and est = estp , also lblp = lbl.
Using this it can be shown inductively that all estimates that are computed are conservative.
Because the estimates found in tokens are estimates of processes, it suffices to consider the computation of estimates by processes, which takes place in three situations.
Process P may increase estp to est + 1 upon receipt of a token with estimate est and hop count est.
In this case est is conservative by induction, and N I- est because the initiator of the token is est hops away and is not equal to p.
Finally, process P may increase estp to est + 1 upon receipt of a token with est = h = estp , but lbl l- lblp.
The initiator of this token is est hops away and in this case lbl I- lblp implies that the initiator is not equal to p.
Again this implies N I- est ,  and the conservativity of est implies N > est.
If process p increases estp , say to e, p sends to Nextp a token that includes the value of e.
After receipt of this token, estNextp has at least the value e, so in a terminal configuration est Nextp 2: estp.
This holds for all p, which implies that all estimates are equal.
It has already been shown that they are bounded by N.
Process Pi chose a label lblpi and generated a token ( test , e ,  lblpi , h )  when the value of estpi was set to e.
This token was forwarded e hops and reached the eth clockwise neighbor of Pi ,  Pi+e , which accepted the token without increasing its estimate further.
Show how to construct a Las Vegas algorithm C to establish 'ljJ.
Let 'ljJ be some postcondition and assume that a Las Vegas algorithm A to establish 'ljJ is given and the expected number of messages exchanged by A is known, namely K.
Generalize the proof to show the impossibility of election for all composite ring sizes.
Show that there exists a deterministic, process terminating algorithm for election on non-periodic pseudo-anonymous rings of known size.
The algorithms we have considered so far (with the exception of those in Chapter 8) perform tasks related to the network structure or global network functionality.
In this chapter we shall discuss algorithms whose task is to analyze properties of computations, usually arising from other algorithms.
It is , however, surprisingly hard to observe the computation of a distributed system from within the same system.
An important building block in the design of algorithms operating on system computations is a procedure for computing and storing a single configuration of this computation, a so-called snapshot.
The construction of  snapshots i s  motivated by several applications, of which we list three here.
First , properties of the computation, as far as they are reflected within a single configuration, can be analyzed off-line, i .e.
These properties include stable properties; a property P of configurations is stable if.
Examples of stable properties include termination, deadlock, loss of tokens, and non-reachability of objects in dynamic memory structures.
In Section 10.4 we shall apply snapshots to the deadlock detection problem.
Second, a snapshot can be used instead of the initial configuration if the computation must be restarted due to a process failure.
To this end, the local state cp for process p, captured in the snapshot , is restored in that process, after which the operation of the algorithm is continued.
Third, snapshots are a useful tool in debugging distributed programs.
An off-line analysis of a configuration taken from an erroneous execution may reveal why a program does not act as expected.
On the events of process p a local causal order is defined by.
Thus, the channel PIP3 contains one message in the snapshot , and this message is said to be "in transit" in the snapshot.
The sending of this message is a postshot event , while its receipt is a preshot event.
The feasibility of a snapshot implies that in the construction of the implied configuration no messages "remain" in rcvd;q that cannot be deleted from sent;q.
We shall call a message a preshot message (or postshot message, respectively) if it is sent in a preshot event (or postshot event , respectively)
There is a one-to-one correspondence between snapshots and finite cuts in the event collection of the computation.
A cut is a collection of events that is left-closed with respect to local causality.
A snapshot will be used to derive information about the computation from which it is taken, but an arbitrarily taken snapshot provides little information about this computation.
Assume that process p takes its snapshot while it has the token, and some time later process q also takes its snapshot while it has the token.
In the constructed configuration two processes have the token, which is a situation that never occurs in any computation of the algorithm.
We require the snapshot algorithm to coordinate the registration of  the local snapshots in such a way that the resulting global snapshot is meaningful.
By the definition of ::::S it suffices to show that e' E L in the following two cases.
The gist of the proof is to construct an execution in which all preshot events occur before all postshot events.
The taking of a local snapshot must be triggered in each process.
In all snapshot algorithms it is ensured that a process takes its snapshot before the receipt of a posts hot message.
The two algorithms that are treated in this section differ in how these messages are recognized and how it is ensured that each process takes a snapshot.
To initiate the algorithm: begin record the local state ; takenp := true ;
If a marker has arrived: begin receive ( mkr ) ;
To distinguish the messages of the snapshot algorithm from the messages of the computation proper, the former are called control messsages and the latter are called basic messages.
In this subsection the additional assumption is used that channels are fifo, that is , messages sent via any single channel are received in the same order as they were sent.
In the algorithm of Chandy and Lamport [CL85] , processes inform each other about the snapshot construction by sending special ( mkr ) messages (markers) via each channel.
Each process sends markers exactly once, via each adjacent channel, when the process takes its local snapshot ; the markers are control messages.
The receipt of a ( mkr ) message by a process that has not yet taken its snapshot causes this process to take a snapshot and send ( mkr ) messages as well; see Algorithm 10.4 ,  which is executed concurrently with the computation C.
Lemma 10.6 If at least one process initiates the algorithm, all processes take a local snapshot within finite time.
Because each process takes a snapshot and sends ( mkr ) messages at most once, the activity of the snapshot algorithm ceases within finite time.
If p is a process that has taken a snapshot by then, and q is a neighbor of p, then q has taken a snapshot as well.
This is so because the ( mkr ) message, sent by p,  has been received by q and has caused it to take a snapshot.
Because at least one process initiated the algorithm, at least one process has taken a snapshot ; the connectivity of the network implies that all processes have taken a snapshot.
Observe that the algorithm must be initiated by at least one process, but it works correctly if initiated by any arbitrary non-empty set of processes.
By the previous lemma, the algorithm computes a snapshot in finite time.
It remains to show that the resulting snapshot is feasible, i .e.
Let m be a postshot message sent from p to q.
Before sending m, p took a local snapshot and sent a ( mkr ) message to all its neighbors, including q.
Because the channel is fifo, q received this ( mkr ) message before m and, by the algorithm, q took its snapshot upon receipt of this message or earlier.
The algorithm of Lai and Yang [LY87] does not rely on the fifo property of channels.
Therefore, it is not possible to "separate" preshot and postshot messages by markers as is done in the algorithm of Chandy and Lamport.
Instead, each individual basic message is tagged with information revealing whether it is preshot or postshot ; to this end, process p, sending a message of C, appends the value of takenp to it.
Because the contents of the messages of C are not of concern here, we denote these messages simply as ( mes, c ) , where c is the value of taken included by the sending process.
Consider process p, which is not an initiator of the snapshot algorithm, and assume that the neighbors of p do not send messages to p after taking their local snapshots.
In this situation p never records its state, and the snapshot algorithm terminates with an incomplete snapshot.
To initiate the algorithm: begin record the local state ; taken := true end.
To send a message of C: send ( mes, takenp )
If a message ( mes, c ) has arrived: begin receive ( mes, c ) ;
The algorithm of Chandy and Lamport (Algorithm 10.4) uses markers to distinguish between preshot and postshot messages, and hence requires the fifo property of channels.
Taylor [Tay89] has shown that if the channels are not fifo and piggybacking is not used, any solution to the snapshot problem must be inhibitory, i .e.
A classification of different types of inhibition and a characterization of the necessary inhibition under various assumptions about communications is found in Critchlow and Taylor [CT90]
The channel state can be constructed explicitly by relying on the following lemma.
On the one hand, it is easily seen that a message m E sent;q \ rcvd;q is sent preshot and received postshot.
As a solution to this, it was proposed by Mattern [MatS9c] that p may count the total number of preshot messages sent to q, and inform q about this number (either in a separate message, or by piggybacking the number on postshot messages)
Process q counts the number of preshot messages received (both those received in a preshot event and those received in a postshot event) and terminates the construction of the channel state when sufficiently many preshot messages have been received.
Construct a sequence f of events by first enumerating the preshot events of E in the order in which they occur in E, and then enumerating the postshot events of E in the order in which they occur in E.
This enumeration is consistent with the causal order of E, hence defines an execution F.
Let P be a stable property of configurations; once an execution reaches a configuration in which P holds, P thereafter holds forever.
Consider an algorithm that circulates tokens among processes, and processes may consume tokens.
The property "There are at most k tokens" is stable, because tokens may be consumed, but not generated.
In  order to  take appropriate action when P becomes true, the truth of  P may be detected by means of an additional algorithm that observes the computation and triggers the action when P is found to be true.
For a particular property P, the (distributed) evaluation of P('y* ) may require a fairly complicated algorithm.
This is the case for some models of the deadlock-detection problem; see Section lOA for an algorithm to test deadlock in a given configuration.
Snapshots may also be used to evaluate monotonic functions of the system configuration; a function f is monotonic if.
Consider a set lP' of processes, executing a basic computation of which the purpose and working are of no concern here.
A process may have to suspend its local computation because it is waiting for messages from other processes ; this is modeled by the active state of a process being replaced by a blocked state.
To become active again (action F p) ,  p must receive grant messages from processes in Reqsp , but not necessarily from all.
The subsets of Reqsp from which the grants suffice to make p active again are those that satisfy the predicate Freep.
Requests received by p (action Rp) are stored in a set Pendp and replies can be given to them (action Gp) when p is itself active.
The request numbers issued by process p are attached to every request and grant message and enable p to discard grant messages sent in reply to obsolete requests (see Fp)
First , if p needs replies to all request messages , Preep is true only for Reqsp.
Second, if a single grant suffices for p, Preep is true for every non-empty set.
If A and any of the other disks become available , p can continue.
Also, if B, C, and D become available, p can continue.
The predicate Freep may be different each time p becomes blocked, but always satisfies the following two assumptions.
According to Dl ,  p does not become active before receiving any grant , and does become active when grants are received from all processes in Reqsp.
According to D2, p may still become active if more grants have been received than any minimal collection of grants.
Given the basic algorithm, we can now define a deadlocked process or deadlock configuration.
The deadlock detection problem is to design a control algorithm that can be superimposed on the basic computation and satisfies the following three requirements.
If a deadlock occurs, it will be detected by the control algorithm.
The algorithm detects deadlock only if there is a deadlock.
The ariable alivep is initially false,  but set to true when p detects that it is alive in the snapshot.
This is the case when p is in the active state in the snapshot, but also if the set of ( grant, v; )  and ( Alive ) messages received so far satisfies Free;
Variable GrRecp contains the set of grants (for the current request) already received or in transit in the snapshot , and the ( Alive ) messages received by p.
The ( grant , vq ) message was sent in one of the events h through fi-l.
If GrRecq contains G after initialization, Mq is applicable and will set aliveqi to true; otherwise, aliveq will be set to true (at the latest) when the ( Alive ) message completing G is received.
The model of the basic computation used in this section is the most general model used for studying the deadlock-detection problem, and not many algorithms are known for detecting deadlocks in this model.
A distributed database consists of a collection of files, dispersed over a number of sites (computers)
The database management system allows users to access this data, either just to read it , or to modify the data.
Accessing the data occurs in a structured way by means of database transactions, which usually address data at different sites.
Due to the dispersion of data items, care must be taken to avoid certain interleavings of steps taken by different transactions, and correct operation of the database is usually ensured by means of locking the data items on which the transaction operates.
Of course, if the required data is already locked by another transaction, the transaction must wait.
To become active, a transaction must obtain all the locks it has requested, i .e.
The restricted model of deadlocks where all requests must be granted before a process becomes active is referred to as the AND model.
It can be shown that in the AND model a deadlock is equivalent to the occurrence of a cycle in the wait-for graph; this is a graph on the processes with an edge pq if P is blocked waiting for q.
Algorithms to check for cycles in this graph have been proposed by, e .g.
A process cooperating with other processes in a distributed algorith!U may enter a blocked state, in which the only possible events are communications with other processes.
The execution of any communication event brings the process into another state, from which it may continue its computation.
Hence to become active it suffices that a single request of a process is granted, i .e.
Freep is true for every non-empty subset of Reqsp - The restricted model of deadlocks where a single grant suffices for a process to become active is referred to as the OR model.
In the OR model, a deadlock is equivalent to the occurrence of a knot in the wait-for graph.
Exercise 10.1 Consider the registration of the local snapshot of p as an additional internal event ap.
Exercise 10.2 Give a full description of the Lai-Yang algorithm, including mechanisms to enforce completion of the snapshot and construction of the channel states.
Networks of regular structure, such as the torus or the hypercube, usually have the links labeled with their direction.
We now discuss some recent research evaluating the benefits of such a labeling, called Sense of Direction or SoD.
The availability of sense of direction strengthens the model and allows processors to communicate more efficiently with each other, and to exploit topological properties of the network algorithmically.
We shall define sense of direction for several specific classes of network, and show how the election problem on rings can be solved more efficiently if chords and a sense of direction are available.
We shall show that elections can be performed with linear complexity in hypercubes and cliques if SoD is available, but also that a randomized algorithm can achieve the same complexity without using SoD.
Algorithms to compute an SoD in networks where none is given will also be discussed.
As usual we model a processor network by a graph with N nodes and m edges.
The edges of each processor are locally named so a processor can distinguish from which edge it has received information, and can choose through which edge to send information.
Processors have distinct identities , but these identities are uninterpreted numbers , and have no topological significance.
Although sense of direction has received quite a bit of attention in recent years, no easy definition of what it is has been agreed upon.
We shall follow a group-theoretical approach and therefore we recall some group theory notions first.
For some students, unfamiliar with this theory, some of the definitions may appear mysterious , but we shall give clear examples for concrete structures where possible.
A commutative or abelian group is a set G with special zero element 0 and a binary operator +,  satisfying the following requirements.
Inverse: For all x E G there exists a y E G such that x+y = y+x = O.
Associativity: For all x ,  y, z E G, (x + y) + z = x + (y + z)
Commutativity: For all x ,  y ,  x + y = y + x.
Because of associativity we may omit parentheses in summations ; we write -x for x 's inverse, and if s E Z, s.
The number of elements in G is called its order; ord( G) is assumed finite for use in sense of direction.
In finite groups, for each x there are positive numbers k such that k.
For elements gl through gk , consider the set of elements that can be written as a sum of gi :
The group is cyclic if it is generated by one single element , i .e.
Another group deserving special attention is the group (Zk )d : its elements are vectors of length d, and the group operation adds them pointwise modulo k.
In  the sequel, let G denote a commutative group and for neighboring processors p and q, let Cp (q) denote the name of link pq at p.
The idea behind group sense of direction is that the topology of the network matches the structure of the group.
The edge labeling C is a sense of direction (based on G) if the edge labels are elements of G, and there exists an injection N from the nodes to G such that for all neighbors p and q, Nq = Np + Cp (q)
Given a sense of direction C, a node labeling as specified in the definition is called a witnessing labeling or witness for C.
The processors know the link labels C, but a witnessing node labeling is not required or assumed to be known to the processors and is not part of the sense of direction.
It can easily be shown [TeI95] that witnessing node labelings are not unique; any given witnessing labeling can be modified by adding a fixed element s E G.
It is usually the case that the order of the group equals the number of nodes, hence the witnessing labelings are bijective.
This property is not essential in most of the algorithms we give, but we often assume it implicitly.
Given a network of N processors and a group G of order N, a sense of direction can be constructed easily.
If first the nodes are labeled arbitrarily with distinct elements of G, and then Lp(q) is taken as Nq - Np for each edge pq, a sense of direction is obtained.
The resulting labeling already allows a significant reduction in complexity for certain tasks , as will be shown later , but most of the labelings that have turned out to be very efficient satisfy an additional property: uniformity.
Let L be the common set of link labels ; some properties of uniform sense of direction are immediate.
If g E L, then so is -g: a link labeled 9 is labeled -g at the other end.
For a g E G a shortest path P with SU M.c(P) = 9 can be locally computed by a generalized version of the coin exchange problem.
It suffices to compute a minimal sequence of labels in L with sum 9 and, because every processor has these labels, this sequence defines a path in the network.
We describe a network with uniform sense of direction by the set of labels in a processor, often omitting inverses.
We now redefine some already known topologies in the group framework.
We shall now give a few key techniques that underlie the exploitation of sense of direction.
Later in the chapter complete algorithms will be presented to illustrate and clearify these techniques.
Indeed, all we need to do is to add the labels on the paths; if and only if the sums of the labels are the same, the endpoints of the paths are the same.
The possibility of path comparison is so fundamental to sense of direction, that Flocchini et al.
In their approach, the labeling proper is accompanied with explicit translation functions to compute the relative position of the endpoint of a path.
On the one hand, this definition is more general because it describes a larger class of labelings.
The advantage of this is limited, however : the larger class includes neighbor knowledge (in our definition not an instance of sense of direction) and some labelings rarely exploited in the design of distributed algorithms.
On the other hand, the group approach compactly obtains the translation functions (as addition of group elements) , thus allowing one to concentrate on more advanced usages of sense of direction.
If the sense of direction is uniform, it also implies full knowledge of the network topology; indeed, in this case the availability of sense of direction cannot be considered independently from full topological awareness.
Uniform sense of direction allows efficient routing to any node of which the relative position in the network is known, using the generalized coin exchange problem as mentioned above.
Because one of our main directives is to compare complexity of broadcasting with and without sense of direction, we shall now demonstrate that sense of direction allows a broadcast within O (N) messages.
If no topological information is available, a broadcast requires the exchange of at least one message through every channel (Theorem 6.6) , hence has D ( IE I )  message complexity.
For the initiator only, execute once: begin father p := p ; choose q E Neighp ;
For each process, upon receipt of ( tlist , L ) through link A: begin forall x E L do x := x + A ;
The list circulated in the token contains no longer processor names (these are not assumed to be available) , but an indication of the position of a processor relative to the holder of the message.
The algorithm uses the "path comparison" capability: a processor q adding itself to the list always inserts the number 0 in the list.
This algorithm not only can be used for broadcasting, but also can be used in the election algorithm of Kutten et al.
As a result we obtain a first success for sense of direction, because for broadcasting and election without SoD.
We emphasize that the sense of direction need not be uniform, hence exploitation of the path comparison capability suffices to obtain this result.
Uniform sense of direction allows the processors to determine a spanning tree of the network "on the fly" and send the broadcast message through it , achieving the broadcast in exactly N - 1 messages.
This number is optimal, because each of the N - 1 processors different from the initiator of the broadcast must learn the information by receiving a message.
If such is required, N - 1 additional feedback messages can turn this algorithm into a PIF algorithm.
If such is required, the non-initiators can store the link through which they received the message to build a spanning tree of the network with the initiator as the root.
We shall not pursue this issue (which is related to the time complexity of the broadcast) further here.
However, this algorithm only uses path compression but leaves the network structure unexploited.
Franklin's algorithm distinguishes active and relay processors and uses a succession of rounds.
A round starting with more than one active processor will turn at least half of them, and at most all but one of them, into relay processors , and will be followed by another round.
A round starting with one active processor elects this processor and is the last round.
In a ring network, this is unavoidable; even if the active processors know the distance to the next one, the message must be forwarded through the nodes in the ring as this is the shortest path.
Attiya's algorithm preserves the basic control structure of Franklin, but ensures that the active processors know the distance to the next one and uses chords and path compression to forward the ( name, p )  messages quickly.
The chords are not used to modify the basic competition structure of the algorithm.
Leadership is detected at the end of the round with one survivor, by the processor finding it is its own neighbor (Left = 0)
Receive ( ( pos, r ) , Right) ; Send ( ( pos, r + Right ) ,  Left) ; Receive ( ( pos, l ) , Left) ; Send( ( pos, 1 + Left ) ,  Right)
The lemma expresses how the overall complexity of the election depends on the cost of sending messages over the ring.
Attiya's result determined the complexity of election on chordal rings to be linear; attention focussed on the minimal number of chords necessary to obtain this complexity.
In this section we shall determine the number of chords needed asymptotically by Attiya's algorithm using "backward graph engineering" from mathematical formulae.
The results illustrate a trend in algorithmic research to become less "algorithmic" and more focussed on analysis.
As a geometric series (with growth rate smaller than 1 )  has a bounded sum we shall first investigate how many.
Chords are of the form 22i and the number of chords is log log N.
Allowing other routing strategies probably does not improve this result any more than by a constant factor; the proof of this is left as an open question.
Determining F from L is related to the coin exchange problem, asking to pay some amount using coins from a given set of denominations.
General routing strategies mean allowing the use of coin returns in the coin exchange problem.
So we select a sequence that decreases more slowly, but still has a bounded sum; the square-harmonic series L:(1/i2 )
The proof is based on a summation that decreases even more slowly and has unbounded sum: the harmonic series 2:(l/i)
Similar to the previous lower bound; for each i there must be a chord between i and 2i.
This subsection presents a linear chordal ring election algorithm that needs only one chord.
We have already seen that with Attiya's algorithm this is not possible; that algorithm only exploits the path compression capability, and leaves the network structure unexploited.
Now assume a chordal ring with only one chord of length t ,  and assume t is approximately ViV.
A chordal ring with a single chord of length close to ViV is topologically very reminiscent of a torus, and our algorithm is adapted from Peterson's [Pet85] torus algorithm.
The first is very similar to the "local maxima" promotion in Franklin's algorithm: if a processor sees a smaller one and is seen by a smaller processor, it can be promoted.
At most half of the processors can be promoted by this rule, because if a processor is promoted, at least one smaller processor that saw it isn't.
The crucial achievement of the algorithm is to design the search procedure in such a way that (i) the search area is sufficiently large to have only few processors promoted by the second rule; and (ii) the search is sufficiently efficient to have a good overall complexity.
Now imagine we draw larger and larger squares in an infinite grid and observe that the number of points on the border grows more slowly than the number of points in the interior.
An active processor searches for other active processors in an I-square, but not by sending messages to all processors in the square because this would be too expensive.
A processor in round i starts looking for other processors by sending an explorer token along the boundary of an l-square.
Traversing the entire boundary takes 4l messages, but the traversal can be interrupted for several reasons.
If the token of p enters a processor already visited by a token of a larger round number, traversal is aborted; p will never receive its explorer back and it will never enter a subsequent round.
If the token enters for the first time a processor already visited by a (different) token (of q, say) of the same round, p "sees" q.
If q is smaller than p, processor p must become aware because seeing q is essential for p in order to be promoted by rule one.
If q is larger than p, p's chances of being promoted are gone, but q must be informed because being seen by p could be essential for q in order to be promoted by rule one.
So either the token goes back to p, or a chasing token is sent to q, via the boundary traversed by q's token, to inform q that it was seen by a smaller processor.
Now, as it suffices for q to be informed about one smaller processor that saw it , a processor that already forwarded a chasing token to q will not forward p's chasing token any more.
We argue that each round promotes at least one processor.
If at least one process sees another, let r be the largest processor that is seen; either r sees no processor and is promoted by the second rule, or r sees a smaller processor and is promoted by the first rule.
Our algorithm is very close to Peterson's [Pet85] and the same calculations are found there also.
Let Ai be the number of active processors starting round i ;  we have Ao ::; N.
We shall first bound A for all rounds in which the size of the squares is less than N, i .e.
We have seen that each processor promoted by the first promotion rule was seen by another processor that is not promoted.
So at most half the processors that saw another processor are promoted by this rule.
From the above and the earlier choice of li = o:i we can conclude that.
The explorer of an active processor p can make 4o:i steps , charged to p.
The chasing tokens routed towards p are also charged to p; as each processor visited by p's explorer forwards at most one chasing token, at most 4o:i chasing steps are charged to p.
The summation over the rounds is a geometric series with sum.
The algorithm elects a leader using O (N) messages in a chordal ring with one chord (of length VN)
As no round in the first phase kills all active processors, at least one processor makes it to the second phase.
The second phase ensures that exactly one of these processors is elected, which establishes the correctness of the algorithm.
In this section we shall study the question whether sense of direction reduces the complexity of broadcast and election in hypercubes.
The answer has long been hypothesized to be affirmative because the known algorithms using sense of direction outperformed the best known algorithms for unlabeled hypercubes.
However, because there were no matching lower bounds for the unlabeled case, firm conclusions could not be drawn.
Computing in Hypercubes 375 both topological awareness and sense of direction are assumed and we shall see that there are elegant , simple, and efficient algorithms there.
On the other hand, broadcasting can be done by flooding or the echo algorithm (Algorithm 6.5) and election using the Gallager et al.
Thus, both tasks are performed in O (N log N) messages.
The match-making election algorithm for hypercubes with sense of direction uses the recursive structure of the hypercube graph.
To elect a leader in a hypercube of dimension d+ 1 , the algorithm first elects a leader in each of the faces of dimension d generated by all but the last generator, and then elects one of the two leaders.
To avoid confusion between leadership at different stages of the algorithm, a node is called a d-leader if it has won the election in a d-dimensional face.
The base case of this algorithm, election in a face of dimension 0, is easy; the network consists of exactly one node, which becomes a O-leader immediately.
The tournament between the two d-leaders is organized in the same way, but with the difficulty that the nodes do not know how to reach the leader in the other (or even their own) face.
As a first step, node p that becomes d-leader sends a tournament message ( tour, p, d ) ,  containing its name and the phase number d, through its edge in direction d; it is received by a node in the other d-dimensional face.
The difficulty in forwarding the message to the d-leader is that the entry node does not know the relative position of the d-leader.
It is too expensive for the d-leader to announce its position to all nodes in the d-cube so that the entry node can forward the message in d steps.
As each row intersects each column in exactly one node (as will be shown below) , there is one node, called the match node, that receives both the announcement from the d-leader and the tournament message.
The match node forwards the tournament message further to the d-leader via the spanning tree induced by the announcement messages.
The tournament between the two d-leaders i s  organized as follows.
A d-leader p sends a ( tour , p , d )  message via link d.
The nodes in the row store the link through which they receive the broadcast message, thus computing a spanning tree of the row.
A d-leader q receiving a ( tour , p , d )  message compares p with q; if q > p, q becomes (d + I )-leader, and q becomes non-leader otherwise.
Let T(d) be the number of messages exchanged in a tournament between two d-leaders and E(n) the number of messages used by the election algorithm (on a hypercube of dimension n)
As an election requires two elections on smaller cubes and one tournament , we find the recursion.
Counting the number of messages exchanged in each of the steps of the tournament between two d-Ieaders, we find:
Now we use the expression for T above for odd and even d, sort the terms with exponential, linear, and constant numerator, and find:
As a stepping stone in the development of a linear broadcasting algorithm, we now present an algorithm to compute sense of direction in a hypercube of dimension n.
The initiator's labeling uniquely defines a sense of direction as expressed in the following theorem (given here without proof)
The leader initiates the algorithm by sending a ( dmn, i )  message over the link labeled i.
When a non-leader processor p has learned its distance disp from the leader and has received the messages from its predecessors, p is able to compute its node label nlap.
Processor p forwards this label in an ( iam, nlap ) message to its successors.
To show that p is indeed able to do so , first consider the case where p receives a ( dmn, i )  message via link l.
As the message i s  sent by the leader, disp = 1 ,  and all other neighbors are successors.
The node label of p has a 1 in position i ,  and the other bits are o.
The label of link 1 becomes i in this case.
Then p forwards ( iam, nlap ) via all links k i= l.
Next , consider the case where p receives an ( iam, label ) message.
The distance d of the sender of this message from the leader is derived from the message (the number of 1 's in label)
This also reveals the number of predecessors, and p waits until disp ( iam, label ) messages have been received.
Then p computes its node label as the logical disjunction of the node labels received, and forwards it to the neighbors from which no ( iam, label ) was received, as these are the successors.
In the first phase, each non-leader processor p computes its node label.
In the second phase, each non-leader processor p learns from its successors the orientation of the links to the successors, and computes the orientation of the links to its predecessors.
This information is sent over the link in ( labl ,  i ) messages.
A processor sends ( labl ,  i )  messages to its predecessors as soon as it has received these messages from all successors, and then terminates.
The leader terminates when ( labl ,  i )  messages have been received from all neighbors.
For d = 0, only the leader itself has distance d from the leader and it may send the messages.
Similarly it is shown that all processors send the messages of phase 2 and terminate.
In phase 1 the processors compute the node labeling N, as is seen by using induction on the distance from the leader.
Now assume all nodes q at distance d from w compute lblq = N(q) and consider node p at distance d + 1 from w.
Thus the disjunction of the d + 1 labels N(q) is indeed N(p)
After phase 1 ,  for predecessor q of p connected via link l ,  neip [l] = lblq.
The same label is used by q for the link, after q receives p's ( labl, 7rp [l] ) message.
A technical detail is now to be explained: as x and y are predecessors of v, v is a common successor of x and y, but x and y also have a common predecessor, let us say w.
There are exactly two processors that receive the messages of both x and y, and these are the processors v and w.
We shall require that the common predecessor of x and y has computed its label before x and y send by assuming that w is also in the mask.
Given a (commonly known) mask M, the operation of  the algorithm is as follows.
The first task we consider is FarSend, where the initiator wants to send a message to a specific node at distance d, w.l.o.g.
This address is to be interpreted in the "private" sense of direction of the initiator, that is , the local link labeling of the initiator, which need not of course coincide with the orientation at other nodes.
Recall that at the beginning, no node except the initiator has any knowledge of its node or link labels !
The broadcast task would be solved with any mask that covers the entire network, that is , each node is contained in the mask or adjacent to a node in the mask.
However, in order to have a linear complexity, the mask should contain at most O(N In) nodes and it is not known if a covering mask of this size exists.
The broadcasting algorithm therefore uses FarSend to transfer the message to a sparser set of selected nodes , from which it is then flooded over a bounded number of hops.
In the second phase, each chief floods the message over distance 3, that is, sends the message to each neighbor, the neighbors forward it to their neigbors, and the latter forward it one step further to their neighbors.
Because a chief has n neighbors and (less than) n2
The mask algorithm can be used to orient one node with respect to the other; assume node v wants to orient a specific node at distance d, that is, this node must relabel all its edges consistently with the edge labeling at v.
Node u starts the mask algorithm with a mask M that contains all neighbors of v; as v will receive an ( iam, 1 ) message through each link, it will be able to compute the direction of each link.
It was shown [DRT98] that such masks exist of size close to n.
To keep the cost of the election low, we need to bound the number of.
If a tree contains all nodes at distance 5 and less from some center node, a different expansion strategy is used.
In a tree center, the tree is represented by a list of relative positions, but expressed in the "private" sense of direction; that is , the sense of direction that would result if the local edge labeling of the center were extended to the entire graph.
Having the entire tree available in this format allows the center to decide on a position where the tree must be extended: a point at distance 5 from the tree.
If no such point exists, the center becomes leader; the condition implies that nowhere in the hypercube there is a ball of radius 5 belonging to a distinct subtree.
The chosen point will be approached using FarOrient and from there the tree is extended, either by a BFS for five levels, or by merging with another subtree.
To merge two trees, the list of positions in one center is translated to the sense of direction of the other center; this is done by FarOrient, which orients the two nodes consistently w.r.t.
The resulting algorithm is far from simple; its analysis is a subject of current research.
The main themes concerning sense of direction and topological awareness were developed in the previous sections; we now continue with some other, less interrelated issues, some of which are central in current research.
Knowing that sense of direction reduces the complexity of some tasks, the question arises if sense of direction can be computed in networks where it is not given.
Risking confusion with a graph operation in which edges of the graph are directed, we call the problem of computing sense of direction orienting the network.
In this section we shall prove a lower bound on the.
Consider an execution of an orientation algorithm, with initial labeling P, that terminates with a permutation trv for each node (where C = 7r(P) i s  an orientation)
Assume furthermore that in this execution some node u did not send or receive any message to or from its two neighbors v and w.
As u has not communicated with v, or with w, the same execution is possible if the network is initially labeled with pu,v ,w , and all processors terminate with the same permutation.
However, L' = 7r(PU,V,W) = CU,V,W is not an orientation, and the algorithm is not correct.
It follows that in every execution, every node must communicate with at least all its neighbors except one.
The orientation of the n-dimensional hypercube requires the exchange of n(n2n) messages.
The orientation of the n x n torus requires the exchange of n (n2 ) messages.
The algorithm relies on the availability of distinct identities in the nodes, but is easily adapted to use a leader instead.
We shall now evaluate the complexity on a more detailed level by considering the bit complexity of orientation on the hypercube.
It was already explained that the node must communicate at least n - 1 messages to be able to distinguish between its n links.
In order to really distinguish between the links, these messages must be different , which implies they contain at least O (log n) bits.
It will now be shown that the algorithm can be implemented using only messages of 0 (log n) bits.
The computation of disp is trivial as wgt (N(q) ) equals d(q, w)
Now let d + 1 summaries of predecessors of p be given.
This gives low (N(p) ) ,  but also identifies the index sum ixso of a node label which differs from N(p) in position low.
Thus ixs (N(p) )  = ( ixso + low (N (p) ) )  mod n.
To start an election: begin i := p ; s := 0 ; maxidp := i ;
Upon arrival of token ( walk, i, s ) : begin if statep = sleep then start an election ;
We now present Verweij 's [VT95] randomized, Monte Carlo algorithm for the election problem; it works for any topology and does not use sense of direction because it is based on competing random walks.
Upon becoming awake (this happens spontaneously or upon receipt of a message in the sleep state) the processor generates a token containing its identity and a hop counter.
Tokens are forwarded through the network in a random walk and are removed from the network by a processor already visited by a token with larger identity.
How does it work? The largest token initiated is not killed; consequently, this token completes K steps and causes a process to become leader; this is.
This property implies that the algorithm can only fail by choosing multiple leaders , but not by choosing no leader at all.
The probability of false claims is reduced by increasing K, but because tokens can make as many as K steps, increasing K also increases the message complexity.
A mathematical analysis of the complexity and the necessary value of K are not known; this is an open problem.
In the sequel we shall sketch why this is difficult and that a full, competitive analysis can only be expected in combination with timing assumptions.
On the one extreme, consider the case where the largest token is infinitely slow as compared to the second largest one.
Here the second token is killed only when entering the maximal processor, and, for example, in the clique, a linear value of K would be necessary to ensure this with probability close to l.
Considering message complexity, the worst timing arises when any token is received only when all tokens with smaller identities have been killed.
Then every token is killed only by being received by a processor with larger identity, and for the (i + 1 )th largest identity this happens only after an.
The discussion implies that in a fully asynchronous clique the message complexity is O (N log N)
On the other hand, consider the case of synchronous cliques, where at any moment , each of the surviving tokens has made equally many steps.
Consequently K = O ( VN) suffices to kill the second largest token with high probability; the probability of false leader claims by smaller tokens is very small.
Remarks: ( 1 ) Topological awareness is not relevant for this class.
It was shown that fewer chords are necessary to obtain a linear election algorithm if this knowledge is exploited in the algorithm.
It gives the message complexity for broadcast and election on various network classes , considering three assumptions.
First , neither topological awareness nor sense of direction is assumed; because the algorithm does not "know" in what topology it is, both tasks require the algorithm to send through every edge.
Second, topological awareness is assumed; this allows the algorithm to exploit properties of the network graph, but the directions of the edges is not known.
Third, both topological awareness and sense of direction are given.
If it isn't in asymptotic message complexity, sense of direction brings other advantages: simpler algorithms, smaller implicit constants, and better time complexities.
Here are some directions of the current research on this topic.
For what values of the chord length t can the one-chord algorithm.
Generalize the results in this chapter for sense of direction based on non-commutative groups.
Such a generalization would be of interest , for example , w.r.t.
In this chapter we investigate how the theory of distributed computing is affected by the assumption that there exists a global time, to which processes have access.
A global time frame is part of the physical reality that surrounds us; this reality includes the processes of a distributed system and the design of distributed algorithms may profit from exploiting time.
The results of this chapter show that time can be exploited to reduce the communication complexity of distributed algorithms if processing and communication times are bounded.
Each process performs local computations in each pulse, and it is ensured that a message that is sent in one pulse is received before the next pulse.
This globally synchronized operation is also referred to as a lockstep operation.
A network operating in lockstep is  usually referred to as a synchronous network.
In a synchronous distributed system the operation of each process takes place in a (possibly infinite) sequence of discrete steps, called pulses.
In a pulse a process first sends (zero or more) messages, then receives (zero or more) messages, and finally performs local computations (changing the state)
Messages are received in the same pulse as that in which they are sent ; i .e.
The pulses can be thought of as the ticks ( "pulses" ) of a global clock.
To define synchronous systems formally, let M ,  as in Chapter 2 ,  be a set of messages and let PM denote the collection of multisets of messages.
The definition of a process in a synchronous system differs from that of a process in an asynchronous system, because collections of messages are sent and received in a single system transition.
To avoid confusion we shall refer to processes of a synchronous system as synchronous processes.
A synchronous process is a four-tuple p = (Z ,  I, My, f-) where.
We write (c, M) f- d for (c, M, d) E f-
A process starts its computation in a state CO E I.
When the process starts a pulse in state c E Z, it sends the collection My(c) of messages.
Before the next pulse, it receives the collection of messages , say M, sent to it in this pulse and enters a state d such that (c, M) f- d.
The message complexity of this computation is the number of messages exchanged, and the time complexity equals the index of the last configuration (assuming the computation is finite)
Where useful it will be assumed implicitly in the algorithms of this chapter that the state of a process includes a pulse counter, indicating the number of pulses executed by the process.
In this subsection we shall discuss a number of paradigms that can be used in synchronous networks in order to decrease the communication complexity of distributed algorithms.
It is possible to save on the number of bits exchanged by sending an arbitrary message using only two bits.
To this end, the contents of the message will be "coded" in the time (number of pulses) between the sending of the two bits.
Assume process P must send a message to process q.
It is assumed here that the message contents can be represented as an integer m.
To transmit the value m, two messages, ( start ) and ( stop ) ,  are used.
Process P sends a ( start ) message in pulse i ,  the pulse in which the transmission is initiated, and a ( stop ) message in pulse i + m.
Process q receives a ( start ) and a ( stop ) message, say in pulses a and b, respectively, and accepts the message b - a in pulse b.
Coding in time causes message transmission to be time consuming; q accepts the message m pulses later than the pulse in which P initiated its transmission.
As an example, consider Toueg's algorithm for the all-pairs shortest-paths problem (Algorithm 4.6)
In the pivot round for pivot w, each process must learn which of its neighbors are sons in the tree Tw , to which end a ( ys ,  w ) or.
We have met distributed algorithms that start several sub computations one of which is guaranteed to produce the desired result , while the results of all other subcomputations are ignored.
The assumption underlying synchronous networks, namely, that all state changes of processes are globally synchronized, is a very strong one.
A more moderate assumption is that processes are equipped with (not necessarily synchronized) physical clocks, and that there is an upper bound on the message delivery time.
We now present some elementary results by Tel et al.
It is possible that a process receives a message of pulse i before starting that pulse itself.
The message must then be stored and processed only in the next pulse.
They have shown that a pulse time of 2/1 is optimal for arbitrary networks, for rings , and for hypercubes.
In practice the clocks of processors suffer from drift , and the time for processing a message is not equal to zero.
The algorithms assume that the identities of the processes are distinct integers; if the identities are taken from an arbitrary countable set , they can be converted into integers using a fixed enumeration of this set.
It is also assumed that all initiators of the algorithm start the election in the same pulse (pulse 0)
The algorithms considered in this subsection rely on knowledge of an upper bound D on the network diameter.
If the number of processes, N, is known, the value N - 1 can always be used as an upper bound on the diameter.
In the election algorithm exactly one message is flooded, namely, by the process Po with the smallest identity, and this flooding is initiated in pulse poD.
By not initiating the flooding of a message, all processes have implicitly flooded the message "my identity exceeds Po - 1"
Summarizing, process p waits until either a message arrives, or the pulse number reaches pD before p has received a message.
In the latter case p initiates the flooding of a message and becomes the leader.
The message complexity is O ( IE I ) and the time complexity is poD + (D - 1 ) ,  where Po is the smallest identity of any process.
Observe that the time complexity is not bounded by a function of N.
As no information need be exchanged in messages, the bit complexity of these algorithms equals their message complexity.
If process P is an initiator of the election algorithm, P initiates a traversal algorithm and labels the token by p.
The token is forwarded according to the traversal algorithm, until either it is purged by a process that has already seen a smaller identity, or it terminates its traversal in process p.
The token of initiator p is delayed by 2P pulses in each hop, i .e.
It remains to be shown how the selective delay bounds the complexity to O(W) messages, where W is the complexity of the traversal algorithm.
In pulse 2PoW each process has seen the identity Po , hence no other token is forwarded in or after that pulse.
The total number of message passings is bounded by W (the number of hops for the token of po ) plus the number of hops by tokens other than po 's token:
In this case, coding in time does not increase the time complexity further, because the delay that a token suffers in each node exceeds the time needed for its transmission.
The time complexity of this algorithm is also not bounded by a function of N, and is in fact exponential in Po ,  the smallest identity of any process.
Frederickson and Lynch [FL84] have shown that comparison algorithms for election on synchronous rings exchange at least O (N log N) messages, that is, the asynchronous lower bounds hold for synchronous comparison algorithms.
Also, Frederickson and Lynch have shown that if the time complexity is bounded by a function on N and the size of the set of possible identities is not bounded, each algorithm behaves like a comparison algorithm in the worst case, which implies that at least O(N log N) messages are necessary.
This shows that the reduction in message complexity can be achieved only at the cost of increasing the time complexity to a value not bounded by a function of N.
It is still an open question whether a synchronous election algorithm exists for networks of unknown size and having the same message complexity as the algorithm given above and a time complexity that is polynomial in the smallest identity.
With the algorithms of this section a leader can be elected in a clique using'O (N) messages.
These algorithms require, however, that all processes start the algorithm in the same pulse.
They presented an algorithm with this message complexity and an O (log N) time complexity.
Minit , the message complexity of the initialization phase; 71nit '  the time complexity of the initialization phase ; Mpulse , the message complexity of simulating each pulse ; Tpulse , the time complexity of simulating each pulse.
A process is said to simulate a certain pulse when it sends the messages of that pulse.
The time complexity of the initialization phase is measured as the maximal time between the first step of any process and the last simulation of the first pulse.
The time complexity of a pulse is the maximal time between the last simulation of one pulse and the last simulation of the next pulse.
In both cases the time complexity is measured under the idealized timing assumptions of Definition 6.31
The message complexity of the pulse only counts additional messages, not the messages of the simulated algorithm.
In all synchronizers of this chapter a process certifies that all messages of the current pulse have been received; when this is the case all messages are processed and the next pulse is started.
In the ABD synchronizer the certification is based on the use of clocks, but in synchronizers for fully asynchronous systems this is not possible.
As all asynchronous algorithms can be written in message-driven form, each process must receive a message to trigger the next pulse, which implies that at least N messages are needed to simulate each pulse.
The initialization is executed spontaneously by starters , and upon receipt of the first message by followers.
The a and /3 synchronizers presented below are two special cases of the 'Y synchronizer presented thereafter.
The first synchronizer, called the a synchronizer, is very similar to the simple synchronizer.
Communication in the synchronizer takes place via each channel: each process, when safe, sends an ( iamsafe ) message to each neighbor.
A process learns that all neighbors are safe when an ( iamsafe ) message has been received from each neighbor.
No initialization procedure is necessary, and clearly O( IE I )  messages are sent by the synchronizer for each pulse.
A process sends a ( ts )  ( "tree safe" ) message to its father in the tree when all processes in its subtree are safe;  this is the case when the process is safe itself and a ( ts )  message was received from each son in the tree.
When the root is safe and has received a ( ts )  message from each son, all processes are safe.
The root sends a ( pulse ) message via the spanning tree to let each process know that all processes are safe in the current pulse.
The receipt of a ( pulse ) message indicates that all processes are safe;  this implies that all neighbors of the receiving process are safe, hence the process proceeds to the next pulse.
This synchronizer requires an initialization phase in which a spanning tree is computed.
It was shown in Section 7.3 how this can be done using O(N) time-units solution is possible.
A centralized algorithm can compute a spanning tree using O( IE I )  messages in O (N) time units.
In addition to the acknowledgement, each pulse requires ( ts ) and ( pulse ) messages to be sent via each edge of the tree, to a total of O (N) control messages.
The time this takes is proportional to the depth of the tree, which is D.(N) in the worst case.
The 'Y synchronizer requires an initialization phase in which the network is divided into clusters.
Each cluster contains a cluster spanning tree and a cluster leader, which is the root of the cluster spanning tree.
For each two neighboring clusters (two clusters are neighbors if there exists an edge between nodes in the clusters)
A ( ts )  message is sent by a process to its father in the cluster tree if all processes in its subtree are safe, i .e.
If the leader of a cluster is safe and has received a ( ts )  message from each son, all processes of the cluster are safe.
A ( cs ) ( "cluster safe" ) message is sent via the cluster spanning tree to let each process in the cluster know that all processes in the cluster are safe.
The processes of a cluster that are incident to a preferred edge leading to another cluster send messages to the other clusters to indicate that the cluster is safe.
To this end an ( ocs ) ( "our cluster safe" ) message is sent through the preferred edge when a ( cs )  message is received.
When the cluster leader has received an ( ncs ) message from each son and an ( ocs ) message from each incident preferred edge, all neighboring clusters are safe.
When this is the case and the cluster itself is safe, a ( pulse ) message is sent via the cluster tree to trigger the start of the next pulse in each process of the cluster.
When a process receives a ( pulse ) message, its cluster and the neighboring clusters are safe;  this implies that all its neighbors are safe.
To establish the complexity of this synchronizer, for a clustering c, let Ee be the number of tree edges and preferred edges in c, and He the maximum height of a tree in c.
As the synchronizer sends four messages via each tree edge ( ( ts ) , ( cs ) , ( ncs ) ,  and ( pulse ) )  and two messages via each preferred edge (two ( ocs ) messages) to simulate each pulse, Mpulse is O(Ee)
Let t be the time of the last simulation of pulse i.
If each single node is taken to be one cluster, Ee = E and He = 0(1 ) ,  and the synchronizer is the same as synchronizer Q.
The algorithm uses O (k.N2) messages and the time complexity is O (N log N/ log k) time units.
As an application of synchronizers we shall present in this section several algorithms for computing a breadth-first search (BFS) spanning tree of a network.
A spanning tree T of network G is a breadth-first search tree if, for each node, the tree path to the root is a minimum-hop path in G.
That is, a breadth-first search tree is an optimal sink tree for its root with respect to the minimum-hop distance measure (see Theorem 4.2)
The computation of a BFS tree is done by a centralized algorithm initiated by the root of the tree (which is assumed to be defined a priori)
The complexity of breadth-first search algorithms is analyzed in this section in terms of the parameters N and lE I.
Observe that the depth of the tree is bounded by N.
A more careful analysis may establish the complexity in terms of the parameters N, lE I , and D; the depth of the tree is bounded by D.
The natural method for building a breadth-first search tree is to compute the tree level by level.
In a synchronous algorithm for computing a breadth first search tree, each process will send messages to each of its neighbors in exactly one pulse.
Each other process sends messages in pulse i + 1 if i is the first pulse in which the process receives one or more messages.
A process that receives messages for the first time in pulse i assigns itself to level i, and chooses one of the processes from which it received a message in pulse i as its father.
The time complexity is O(N) because after pulse N each process has determined its father.
In this subsection three asynchronous algorithms for breadth-first search that have a lower complexity than the algorithms derived in the previous subsection are discussed.
The reader should not conclude that it is useless to apply the concept of synchronizers in the design of asynchronous algorithms; synchronizer algorithms are implicitly used in the design of the following algorithms.
The level of a node in the final BFS tree equals its distance to the root in the full network.
Assume the construction of level 1 is complete, but the algorithm has not terminated.
The nodes at level f await a reply to all ( explore, f + 1 )  messages they send.
Each node of level at most f forwards a ( reverse, b )  message to its father in the tree to report that all ( explore, f + 1 )  messages sent by nodes in its subtree have been given a reply.
The value of b is true if and only if new nodes have been added to the subtree.
A node at level f sends the ( reverse, b )  message when it has received a reply to each ( explore, f + 1 )  message it sent , with b = true if there was at least one ( reverse, true ) message among the replies.
A node at level smaller than f sends a ( reverse, b )  message to its father if it has received a ( reverse, b' ) message from each of its sons.
Again the value of b is true if there has been at least one .( reverse, true ) message among the received messages.
The construction of level f + 1 terminates when the root has received a ( reverse, b )  message from each son.
The construction terminates after at most N levels and each tree edge carries for each level at most one forward or explore message and one reverse message sent in reply to it.
In either case, a frond edge carries exactly two messages for the entire algorithm.
In the simple algorithm the exploration of each individual level is synchronized by the root (by means of reverse and forward messages)
As can be seen from the complexity analysis, the synchronization messages dominate the complexity of the algorithm.
Assume the construction of level I of the tree is complete.
To construct the next l levels, the initiator broadcasts a message ( forward, I )  down the tree.
The first parameter of the explore message represents the new level of the receiver if the latter becomes a child of the sender.
The second parameter gives the number of levels that must still be explored, starting from the sender of the message.
If an ( explore, I, l )  message arrives at node p, I is compared with the current level of p, levelp.
If I 2:: levelp , a ( reverse, lalse ) message is sent in reply.
The latter is the case if an ( explore, I + 2, l ) message has previously been received from this neighbor.
The termination of the construction of levels I + 1 through I + l is detected by a flood of reverse messages sent in reply to explore and forward messages , as for the simple algorithm.
The complexity bounds stated in the theorem follow by choosing l as JN2IE.
Depending on the availability of physical clocks, time may or may not be observable to processes , but even if time is not observable, the so-called Archimedean assumption may be justified.
Under this assumption there is a bound on the ratio between the maximum and the minimum possible times consumed in a single step of a process or the transmission of a message.
The "elapsed time" in a process is then the number of clock ticks counted by that process.
In the sequel, tl and t2 denote instances of physical time.
The slowest component of the system determines (together with the fastest component) the value of the Archimedean ratio.
If the time complexity of an algorithm is influenced by the ratio (as it usually is) , it is the slowest component that determines the speed of the entire system.
The analysis of the timing assumptions may be extremely difficult , and give rise to incorrect algorithms.
As a consequence, the pulse number of a message can be determined using its time of receipt and the parity of the pulse number.
Give algorithms for flooding messages in cliques, tori, and hypercubes that require N - 1 messages and O(D) time units.
It must be assumed that the tori and hypercubes are labeled.
Give synchronous election algorithms for networks of known size, in the case where processes do not necessarily start the election in the same pulse, but may initiate the algorithm in different pulses.
Election is performed by  applying extinction to a traversal algorithm with message complexity W.
Each message of process p is delayed by f (p) - 1 clock ticks in each process.
A separate wake-up procedure ensures that each process launches its traversal within Du time units after the start of the algorithm.
Prove that the algorithm terminates within D.u + W.u.f(po) time units, where Po is the process with the smallest identity.
Show, by varying f, that a linear message complexity can be obtained.
The earlier parts of this book studied the coordinated behavior that can be achieved in distributed systems where processes are reliable.
For several reasons it is attractive to study what coordinated behavior of processes is possible under the assumption that processes may fail; this study is the subject of the last part of the book.
Many solutions for fault tolerance are ad hoc and, also, in the large collection of impossibility proofs structure and underlying theory are sometimes hard to find.
On the other hand, some problems have elegant and well-established theory and simple solutions whose presentation fits well in this introductory textbook.
This chapter serves as a general introduction to the later chapters.
Because of the dispersion of processing resources in a distributed system, these systems have the partial-failure property; no matter what kind of failure occurs, it usually affects only a part of the entire system.
And, while the growth of the number of components makes it extremely likely that a failure will occur in some component , it becomes extremely unlikely that a failure will occur in all components.
Therefore it may be hoped that the tasks of failing processes can be taken over by the remaining components, leading to a graceful degradation rather than an overall malfunctioning.
Indeed, as can be seen in the following chapters, the design of distributed algorithms for systems with defective processes is possible, and coordinated behavior can be achieved.
The partial-failure property makes the use of distributed ( "replicated" ) architectures an attractive option for the design of computer applications that are not distributed by nature, but require high reliability.
The primary computer system of the Space Shuttle aircraft serves as an example; Spector and Gifford [SG84] describe its development.
Controlling the Shuttle is a task well within the capacity of a single off-the-shelf microprocessor, but the possibility of such a processor's breaking down during a flight was a serious concern in its design.
The final control system used four identical processors, each performing exactly the same computation1 ; the actuators vote on the outcome, which allows perfect control even if one processor fails.
The physical realization of the actuators allows the system to survive the later failure of a second processor as well.
Although replication is an appealing option for increasing reliability, the design of the algorithms necessary to coordinate a cluster of (unreliable) processors is far from.
Two radically different approaches towards fault tolerance are followed in the literature, and both will be studied in this book.
In robust algorithms each step of each process is taken with sufficient care to ensure that , in spite of failures , correct processes only take correct steps.
In stabilizing algorithms correct processes can be affected by failures , but the algorithm is guaranteed to recover from any arbitrary configuration when the processes resume correct behavior.
A process is called initially dead if it does not execute a single step of its local algorithm.
A process is said to be Byzantine if it executes.
In particular, a Byzantine process may send messages with an arbitrary content.
The correctness requirements set forward for robust algorithms always refer to the local state (or output) of correct processes only.
An initially dead process never produces output and its state always equals the initial state.
The output of a crashing process, if it produces any, will be correct , because up to the occurrence of the crash the process behaves correctly.
Needless to say, the local state or output of a Byzantine process can be arbitrary, and no non-trivial requirement on it can be satisfied by any algorithm.
Initially dead processes and crashes are called benign failure types, while Byzantine failures are called malign failures.
Because malign failures often cannot be excluded in practice but are very rare compared with benign failures, Garay and Perry [GP92] extended some results to a mixed-failure model, where t processes may fail , b out of which in.
In synchronous distributed systems there is an additional failure mode, namely, where a process executes correct steps but at the wrong time (due to a slow or fast clock of the process)
This type of incorrect process behavior is called a timing error.
The study of robust algorithms is centered around the solution of so-called decision problems, where it is required that each of the (correct) processes irreversibly writes a "decision" value to its output.
The required decision value usually depends in a quite trivial manner on the input values of the processes, making these problems fairly easy to solve in a fault-free (reliable) environment.
The requirements set for the decisions are usually of three types, namely, termination, consistency, and non-triviality.
The termination requirement states that all correct processes will decide, i .e.
In deterministic algorithms termination is required in all computations; in probabilistic algorithms it is required to occur with probability one (see Chapter 9)
The requirement that all correct processes write ia value to their output excludes any solution in which a process must wait to receive some information from more than N - t processes.
Indeed, the stopping of t processes causes an indefinite wait in a correct process, violating the termination requirement.
The consistency requirement imposes a relation between the decisions taken by different processes.
In the simplest case, all decisions are required to be equal; we speak of a consensus problem in this case (which value is required may depend on the inputs)
In more complicated problems a class of output vectors can be defined, and the decisions of correct processes should form a vector in this class.
A termination and a consistency requirement are usually sufficient for a distributed algorithm to be useful; in cases where a task is shown to be impossible to solve, an additional requirement of a more technical nature is needed.
The non-triviality requirement excludes algorithms based on a fixed output for the problem, on which every process decides without any communication.
The consensus problem, for example, could be solved by an algorithm in which each process writes a "0" to the output immediately.
In a distributed database a transaction involving various sites must be executed in all involved sites or in none of them.
Therefore, after announcing the update to these sites, each site determines whether the update can be executed locally and votes either "yes" or "no"
Subsequently, all (correct) sites must decide whether to commit the transaction, meaning that it will be executed everywhere, or to abort it , meaning that it will not be executed.
If all processes voted "yes" the processes must decide commit, if some processes vote "no" the outcome must be abort.
Consistency here means that all decisions are equal, and the problem is non-trivial because, depending on the input , both a commit and an abort may be required.
In cases where consensus among inputs is not achievable, a weaker form of consistency may suffice for some applications.
The decisions of processes are required to differ by at most 1, and to lie.
A demonstration should be given of some of the techniques used to achieve robustness or to prove its impossibility.
Failure detectors are a promising new paradigm, rapidly making the move to being applied technology, and should be included.
Chapter 15 studies the robustness that can be achieved in synchronous systems.
Unlike in the asynchronous model, the crashing of a process can be detected by the remaining processes , because a bounded response time of a correct process is guaranteed.
This and the impossibility results of Chapter 14 imply that robust synchronizers for fully asynchronous networks do not exist.
Many problems and results remain for further study; below we indicate a few of these and give pointers to the literature.
In this book interprocess communication by means of message passing is considered.
Some authors have studied the fault tolerance of distributed systems where communication is based on shared variables; see Taubenfeld and Moran [TM89]
These algorithms are discussed because of their historical value and because they are elegant and insightful.
Stabilizing solutions exist for a number of problems solved earlier in this book, such as data transmission, election, and the computation of routing tables and a depth-first search tree.
Definition 14.2 A t -crash fair execution is an execution in which at least N - t processes execute infinitely many events, and each message sent to a correct process is received.
A process is correct if it executes infinitely many events.
In every 1 -crash fair execution, all correct processes decide.
In a v-decided configuration, some process has decided on v.
A configuration is called v-valent if all decided configurations reachable from it are v-decided.
In a univalent configuration, although no decision has necessarily been taken by any process the eventual decision is implicitly determined already.
Let 'Y be a reachable configuration and T a subset of at most t processes.
We shall first exploit the non-triviality of the problem to show that there exists a bivalent initial configuration (Lemma 14.6)
Subsequently it will be shown that , starting from a bivalent configuration, every enabled step can be executed without moving to a univalent configuration (Lemma 14.7)
This suffices to show the impossibility of consensus algorithms (Theorem 14.8)
In the sequel, let A be a l-crash-robust consensus algorithm.
Lemma 14.6 There exists a bivalent initial configuration for A.
Lemma 14.7 Let / be a reachable bivalent configuration and s an applicable step for process p in /
Two configurations on these paths are called neighbors if one is obtained from the other in a single step.
Because a O-decided configuration is reachable from s (o:o )  and a I-decided configuration from S (O:I ) , it follows that.
Theorem 14.8 There exists no asynchronous, deterministic, l -crash-robust consensus algorithm.
The construction gives an infinite fair execution in which all processes are correct but a decision is never taken.
Fortunately, some assumptions underlying the result of Fischer, Lynch, and Paterson can be made explicit , and the result turns out to be very sensitive to the weakening of any of them.
Despite the impossibility result , many non-trivial problems do have solutions, even in asynchronous systems and where processes may fail.
Section 14.3 considers problems that require a less close coordination between processes than does consensus, and demonstrates that some of these problems, including renaming, are solvable in the crash model.
Section 14.4 considers randomized protocols, where the termination requirement is sufficiently relaxed to make solutions possible even in the presence of Byzantine failures.
Section 14.5 considers a different relaxation of the termination requirement , namely where termination is required only when a given process is correct ; here also Byzantine-robust solutions are possible.
In the model of initially dead processes, no process can fail after having executed an event, hence in a fair execution each process executes either 0 or infinitely many events.
Definition 14.9 A t -initially-dead fair execution is an execution in which at least N - t processes are active, each active process executes infinitely many events, and each message sent to a correct process is received.
Because processes do not fail after sending a message, it is safe for a process to wait for the receipt of a message from p if it knows that p has already sent at least one message.
A larger number of initially dead processes cannot be tolerated (see Exercise 14.3)
First an algorithm by Fischer, Lynch, and Paterson [FLP85] is presented by which each of the correct processes computes the same collection of correct processes.
Observe that processes send messages to  themselves; this i s  done in many robust algorithms, and facilitates analysis.
Here and in the remainder, let the operation "shout ( mes )"  stand for.
The processes construct a directed graph G by shouting their identity (in a ( name, p )  message) and waiting for the receipt of L messages.
As there are at least L correct processes, each correct process receives sufficiently many messages to complete this part.
The successors of p in graph G are the nodes q from which p has received a ( name, q )  message.
An initially dead process has not sent nor received any message, hence.
A knot is a strongly connected component without outgoing edges, containing at least two nodes.
There is a knot in G, containing correct processes , and, because each correct process has out-degree L, this knot has size at least L.
Finally, as the correct process p has L successors, at least one successor belongs to K, implying that all processes in K are descendants of p.
Therefore, in the second stage of the algorithm, processes construct an induced subgraph of G, containing at least their descendants, by receiving the set of successors from every process they know to be correct.
Because processes do not fail after sending a message, no deadlock occurs in this stage.
Indeed, p waits to receive a message from q only if in the first stage some process has received a ( name, q ) message, showing that q is correct.
The algorithms for knot-agreement , consensus , and election exchange O(N2) messages, where a message may contain a list of L process names.
Any election algorithm choosing a correct process as leader also solves the consensus problem; the leader broadcasts its input and all correct processes decide on it.
Consequently, the above-mentioned upper bounds hold for the consensus problem for initially dead processes as well.
In the crash model, however, the availability of a leader does not help in solving the consensus problem; the leader itself can crash before broadcasting its input.
In addition, the election problem is not solvable in the crash model as is demonstrated in the next section.
The consensus problem studied so far requires the same value to be decided upon in each process; this section studies the solvability of tasks that require a less close coordination between processes.
A distributed task is described by sets X and D of possible input and output values , and a (possibly partial) mapping.
XN) describes the input of the processes , then T(x) is the set of legal outputs of the algorithm, described as a decision vector J = (dl '
If T is a partial function, not every combination of input values is allowed.
Definition 14.10 An algorithm is a t -crash robust solution for task T if it satisfies the following.
In every t -crash fair execution, all correct processes decide.
If all processes are correct, the decision vector J is in T(x)
The consistency condition implies that in executions where a subset of the processes decide, the partial vector of decisions can always be extended to a vector in T(x)
The set DT denotes the collection of all output vectors , i .e.
The consensus problem requires all decisions to be equal, i .e.
In this subsection an algorithm for renaming, by Attiya et al.
Owing to the possibility that t processes fail ,  a group must be able to decide "on its own" , i .e.
But then the groups can independently reach a decision in a single execution; the crux of the proof is to show that these decisions can be mutually inconsistent.
We proceed with the formal argument for the case of renaming.
Vp :=  Vp U V ; shout ( set , Vp ) end.
In the renaming algorithm (Algorithm 14.2 ) , process p maintains a set Vp of  process inputs that p has seen; initially, Vp contains just xp.
Every time p receives a set of inputs including ones that are new for p, Vp is extended by the new inputs.
Upon starting and every time Vp is extended, p shouts its set.
Consequently, process p shouts its set at most N times, showing that the algorithm terminates and that the message complexity is bounded by O(N3)
Further, p counts (in the variable cp) the number of times it has received copies of its current set Vp.
Initially cp is 0, and cp is incremented each time a message containing Vp is received.
The receipt of a message ( set , V )  may cause Vp to grow, necessitating a reset of Cp.
Process p is said to reach a stable set V if Cp becomes N - t when the value of Vp is V.
In other words, p has received for the (N - t)th time the current value V of Vp.
Assume q that reaches the stable set VI and r reaches the stable set l/2
This implies that q has received ( set , VI ) from N - t processes and r has received ( set , V2 ) from N - t processes.
Consequently, VI and V2 are both values of Vp, implying that one is included in the other.
Each correct process reaches a stable set at least once in every fair t -crash execution.
Let p be a correct process; the set Vp can only expand, and contains at most N input names.
Process p shouts this value, and a ( set , Vo ) message is received by every correct process , which shows that every correct process eventually holds a superset of Vo.
However, this superset is not strict ; otherwise, a correct process would send a strict superset of Vo to p, contradicting the choice of Vo (as being the largest set ever held by p)
Consequently, every correct process q has a value Vq = Vo at least once in the execution, and hence every correct process sends a ( set , Vo ) message to p during the execution.
All these messages are received in the execution, and as Vp is never increased beyond Vo , they are all counted and cause Vo to become stable in p.
Upon reaching a stable set V for the first time, process p decides on the pair (s ,  r) , where s is the size of V and r is the rank of xp in V.
A stable set has been received from N - t processes, and hence contains at least.
Because , in any fair t-crash execution, each correct process reaches a stable set , each correct process decides on a new name.
To show that the new names are all distinct , consider the stable sets VI and V2 reached by processes q and r respectively.
If the sets have different sizes , the decisions of q and r are different because the size is included in the decision.
The simple algorithm presented here is not the best in terms of the size of name space used for renaming.
The results of the next subsection imply a lower bound of N + 1 on the size of the new name space for crash-robust renaming.
Task T is called connected if CT is a connected graph, and disconnected otherwise.
It was assumed by Moran and Wolfstahl that the input graph of T (defined similarly to the decision graph) is connected, i .e.
Furthermore, the impossibility result was shown for non-trivial algorithms, i .e.
For each dE DT, there is a reachable configuration in which the processes have decided on d.
Algorithm A' first simulates A, but instead of deciding on value d, a process shouts ( vote, d )  and awaits the receipt of N - 1 vote messages.
No deadlock arises, because all correct processes decide in A; hence at least N - 1 processes shout a vote message.
After receiving the messages, process p holds N - 1  components of a vector in DN.
This vector can be extended by a value for the process from which no vote was received, in such a way that the entire vector is in DT.
Indeed, a consistent decision was taken by this process, or is still possible.
Now observe that different processes may compute different extensions, but that these extensions belong to the same connected component of CT.
Each process that has received N - 1 votes decides on the name of the connected component to which the extended vector belongs.
It remains to show that A' is a consensus algorithm.
It has already been argued above that every correct process receives at least N - 1 votes.
Case 2: All processes except one, say r, found a decision in A.
All correct processes receive the same N - 1 decisions, namely those of.
The non-triviality requirement , stating that every decision vector in DT is reachable, is fairly strong.
One may ask whether some algorithms that are trivial in this sense may nonetheless be of interest.
As an example, consider Algorithm 14.2 for renaming; it is not immediately clear that it is non-trivial, i .e.
Fundamental work concerning the decision tasks that are solvable and unsolvable in the presence of one faulty processor was done by Biran, Moran, and Zaks [BMZ90]
They gave a complete combinatorial characterization of the solvable decision tasks.
The probability i s  taken over all executions starting in  a given initial configuration.
In order for the probabilities to be meaningful, a probability distribution over these executions must be given.
This can be done by using randomization in the processes (as in Chapter 9) , but here a probability distribution on message arrivals is defined instead.
For all k, and different processes p, q, r, the events R(q, p, k) and.
Observe that Proposition 14.4 also holds for probabilistic algorithms when convergence (termination with probability one) is required.
Indeed, because a reachable configuration is reached with positive probability, a decided configuration must be reachable from every reachable configuration (albeit not necessarily reached in every execution)
In this subsection the consensus problem is studied in the crash failure model.
The existence of  such a protocol, say P, implies the following three claims.
This is similar to the proof of Lemma 14.6 ;  details are left to the reader.
Indeed, the high resilience of the protocol implies that both 8 and T can reach a decision independently; if different decisions are possible, an inconsistent configuration can be reached by combining the schedules.
However, "I is bivalent , so also (clearly in cooperation between the groups) a O-decided configuration 80 is reachable from "I.
Let p be the process causing the transition from "II to "10
The crash-robust consensus algorithm proposed by Bracha and Toueg [BT85] operates in rounds : in round k ,  a process sends a message to all processes (including itself) and awaits the receipt of N - t round-k messages.
If process p receives a witness in round k, p votes for its value in round k + 1 ;  otherwise p votes for the majority of the received votes.
A decision is taken if more than t witnesses are received in a round; the decided process exits the main loop and shouts witnesses for the next two rounds in order to enable other processes to decide.
Votes arriving for later rounds must be processed in the appropriate round; this is modeled in the algorithm by sending this message to the process itself for later processing.
Observe that in any round a process receives at most one vote from each process, to a total of N - t votes ; because more than N - t processes may shout a vote, processes may take different subsets of the shouted votes into account.
We subsequently show several properties of the algorithm that together imply that it is a probabilistic crash-robust consensus protocol (Theorem 14.24)
Lemma 14.20 In any round, no two processes witness for different values.
Lemma 14.21 If a process decides, then all correct processes decide for the same value, and at most two rounds later.
Let k be the first round in which a decision is taken, p a process deciding in round k, and v the value of p's decision.
The decision implies that there were v-witnesses in round k; hence by Lemma 14.20 there were no witnesses for other values, and so no different decision is taken in round k.
In round k there were more than t witnesses for v (this follows from p's decision) , hence all correct processes receive at least one v-witness in round k.
This implies that if a decision is taken at all in round k + 1 ,  it is a decision for v.
Let S be a set of N -t correct processes (such a set exists) and assume that no decision was taken until round ko.
If  this happens , the processes in S receive the same votes in round ko and hence choose the same value, say vo , in round ko.
This implies that they all decide on v in that round.
Again it is left to the reader to show the existence of a bivalent initial configuration of any such protocol (exploit , as usual, the non-triviality)
As / is reached by a sequence of correct steps , all possibilities for choosing a set of t processes that fail are still open.
Assume, on the contrary, that different decisions can be reached by S and T, i .e.
An inconsistent state can be reached by assuming that the processes in S n T are malicious and combining schedules as follows.
Starting from configuration /, the processes in S n T cooperate with the other processes in S as in the sequence leading to a v-decision in S.
When this decision has been taken by the processes in S, the malicious processes restore their state as in configuration /, and subsequently cooperate with the processes in T as in the sequence leading to an v decision in T.
This results in a configuration in which correct processes have decided differently, which conflicts with the agreement requirement.
Like the crash-robust protocol, the Byzantine-robust protocol (Algorithm 14.5) operates in rounds.
In each round, every process can submit votes, and a decision is taken when sufficiently many processes vote for the same value.
Lemma 14.28 If correct process p accepts in round k the vote v for correct process r, then r has voted for v in round k.
Lemma 14.29 If correct processes p and q accept a vote for process r zn round k, they accept the same vote.
Assume that in round k process p accepts a v-vote for r ,  and process q accepts a w-vote.
Because there are only N processes , more than t processes must have sent a ( vote, ee, r, v ,  k )  to p and a ( vote, ee , r, w, k ) to q.
This implies that at least one correct process did so, and hence that V = W.
Correct process r starting round k with valuer = v shouts an initial vote for that round, which is echoed by all correct processes.
Thus, for correct processes p and r ,  a ( vote, ee, r, v ,  k )  is sent to p by at least N - t processes , allowing p to accept the v-vote for r in round k unless N - t other votes are accepted earlier.
It follows that process p accepts N - t votes in this round.
The proof of correctness of the protocol now follows similar lines to the correctness proof of the crash-robust protocol.
Lemma 14.31 If a correct process decides on v in round k, then all correct processes choose v in round k and all later rounds.
To show that all correct processes choose v in later rounds, assume that all correct processes choose v in some round l ;  hence, all correct processes vote.
Let S be a set of at least N - t correct processes and assume that p has not decided before round k.
Lemma 14.33 If all correct processes start the algorithm with input v, a decision for v is eventually taken.
As in the proof of Lemma 14.31 it can be shown that all correct processes choose v again in every round.
Algorithm 14.5 is described as an infinite loop for ease of presentation; we finally describe how the algorithm can be modified so as to terminate in every deciding process.
After deciding on v in round k , process p exits the loop and shouts "multiple" votes ( vote, in, p ,  k+ , v )  and echoes ( vote, ee, * ,  k+ , V )
These messages are interpreted as initial and echoed votes for all rounds later than k.
Indeed, p will vote for v in all later rounds, and so will all correct processes (Lemma 14.31 )
Hence the multiple messages are those that would be sent by p when continuing the algorithm, with a possible exception for the echoes of malicious initial votes.
If the general is correct , all correct process decide on its input.
In the first one the general is assumed Byzantine; the scenario serves to define a reachable configuration 'Y.
Now assume Po is Byzantine, and, after receipt of the message, changes its state to the state in 'Y, i .e.
The impossibility results from the possibility that the general initiates a broadcast and halts (first scenario) without providing sufficient information.
It will now be shown that a (deterministic) solution is possible if termination is required only in the case where the general is correct.
Definition 14.36 A t -Byzantine-robust broadcast algorithm is an algorithm satisfying the following three requirements.
If the correct processes decide, they decide on the same value.
If the general is correct, all correct processes decide on its input.
Each process counts, for each type and value, how many messages have been received, counting at most one message received from every process.
The general initiates the broadcast by shouting an initial vote.
Upon receipt of an initial vote from the general, a process shouts an echo vote containing the same value.
When more than (N + t )/2 echo messages with value v have been received, a ready message is shouted.
The number of required echoes is sufficiently large to guarantee that no correct processes send ready messages for different values (Lemma 14.37)
The receipt of more than t ready messages for the same value (implying that at least one correct process has sent such a message) also triggers the shouting of ready messages.
The receipt of more than 2t ready messages for the same value (implying that more than t correct processes have sent such a message) causes a decision for that value.
No provision is taken in Algorithm 14.6 to prevent a correct process from shouting a ready message twice, but the message is ignored by correct processes anyway.
Lemma 14.37 No two correct processes send ready messages for different values.
A correct process accepts at most one initial message (from the general) , and hence sends echoes for at most one value.
Let p be the first correct process to send a ready message for v ,  and q the first correct process to send a ready message for w.
This is so because t + 1 ready messages must be received before sending one, implying that a ready message from at least one correct process has been received already.
Because r is correct , v = w is implied.
Lemma 14.38 If a correct process decides, then all correct processes decide, and on the same value.
Assume correct process p decides on v ;  p has received more than 2t ready messages, including more than t messages from correct processes.
A correct process sending a ready message to p sends this message to all processes , implying that all correct processes receive more than t ready messages.
Lemma 14.39 If the general is correct, all correct processes decide on its input.
If the general is correct it sends no initial messages with values different from its input.
Consequently, no correct process will send echoes with values different from the general's input , which implies that at most t processes send these bad echoes.
The number of bad echoes is insufficient for correct processes to send ready messages for bad values , which implies that at most t processes send bad ready messages.
The number of bad ready messages is insufficient for a correct process to send ready messages or to decide, which implies that no correct process sends a bad ready message or decides incorrectly.
Exercise 14.8 In this exercise we consider the problem of [k , l] -election, which generalizes the usual election problem.
Give a deterministic t -crash robust algorithm for [k , k + 2t] -election.
Prove, that if more than (N - t)/2 processes start the algorithm with input v, then a decision for v is possible.
Is a decision for v possible if exactly (N - t)/2 processes start the algorithm with input v ?
The previous chapter has studied the degree of fault tolerance achievable in completely asynchronous systems.
Although a reasonable robustness is attainable, reliable systems in practice are always synchronous in the sense of relying on the use of timers and upper bounds on the message-delivery time.
In these systems a higher degree of robustness is attainable, the algorithms are simpler, and the algorithms guarantee an upper bound on the response time in most of the cases.
The synchrony of the system makes it impossible for faulty processes to confuse correct processes by not sending information; indeed, if a process does not receive a message when expected, a default value is used instead, and the sender becomes suspected of being faulty.
Thus, crashed processes are detected immediately and pose no difficult problems in synchronous systems; we concentrate on Byzantine failures in this chapter.
Because crashing and not sending information are detected (and hence "harmless" ) in synchronous systems, Byzantine processes are only able to disturb the computation by sending erroneous information, either about their own state or by incorrectly forwarding information.
With these mechanisms it becomes impossible for a malicious process to "lie" about information received from other processes.
It remains possible, though, to send inconsistent information about the process's own state.
It is also shown that implementation of authentication is in practice possible using cryptographic techniques.
The implementation is possible even if the clocks drift and up to one-third of the processes may fail maliciously.
The initial configuration (1'0) is described by the initial states of the processes, and the configuration after the ith pulse (denoted I'i ) is also described by the states of the processes.
In pulse i ,  each process first sends a finite set of messages, depending on its state in I'i- I.
Subsequently each process receives all the messages sent to it in this pulse, and computes the new state from the old one and the collection of messages received in the pulse.
The pulse model is an idealized model of synchronous computations.
Because the pulse model guarantees delivery of messages in the same pulse, a process is able to determine that a neighbor did not send a message to it.
This feature is absent in asynchronous systems and makes a solution to the consensus problem, and even to the reliable-broadcast problem, possible in synchronous systems, as we shall see shortly.
Every correct process p will decide on a value YP E V.
If the general is correct , all correct processes decide.
It is possible to require, in addition, simultaneity, i .e.
As  in earlier proofs ,  a resilience of N /3  or higher allows a partition of the processes into three groups (S, T, and U) , each of which can be entirely faulty.
Scenario 1 is defined similarly, but here the processes in T are faulty and send the messages they should have sent in scenario O.
Finally consider scenario 2 ,  where the processes in S are faulty and behave as follows.
It is used in the proof that Byzantine processes can send the messages of a I-scenario, even if they have only the received messages of a O-scenario.
That is, the processes can "lie" not only about their own state, but also about the messages they have received.
If a failure occurs , agreement may be violated, but termination (and simultaneity) is still guaranteed.
The general sends ( value, Xg ) t o  all processes, the lieutenants do not send.
To prove the correctness of the resulting algorithm it is necessary to reason using the resilience t and the actual number of faulty processes f (see Lemma 15.3)
These N - 1 decisions are stored in the array Wp, from which the decision of p is obtained by majority voting (the value received directly from the general is ignored here! )
The general sends ( value, Xg ) to  all processes, the lieutenants do not send.
Because the protocol is recursive, its properties are proved using recursion on t.
In the same pulse, the decision in Broadcast (N, t) is taken.
To prove dependence (also by induction) it is assumed that the general is correct , hence all t faulty processes are found among the N - 1 lieutenants.
In the algorithm Broadcast (N, 0) , if the general is correct all correct processes decide on the value of the general's input.
As the general is correct , it sends its input to all lieutenants in pulse 1 ,  so every correct lieutenant q chooses Xq = xg.
Hence the application of major by p yields the desired value xg.
Lemma 15.4 (Agreement) All correct processes decide on the same value.
Because dependence implies agreement in the executions in which the general is correct , we now concentrate on the case where the general is.
But then at most t - 1 of the lieutenants are faulty, implying that the nested calls operate within the bounds of their resilience!
Thus, every correct lieutenant computes exactly the same vector W in pulse t +  1 ,  which implies that the application of major gives the same result in every correct process.
In this section we present a Byzantine broadcast algorithm by Dolev et al.
The algorithm exchanges messages of the type ( bm, v ) , where v is either the value 1 ,  or the name of a process.
Process p maintains a two-dimensional boolean table R,  where Rp[q, v] is true if and only if p has received a message ( bm, v ) from process q.
Observe that Rp is monotone in the pulses, i .e.
Unlike the Broadcast protocol of the previous subsection, the protocol of Dolev et al.
A decision on 0 is the default value, and is chosen if insufficiently many messages are exchanged.
Process p supports process q in pulse i if P has received sufficient evidence in earlier pulses that q has sent ( bm, 1 ) messages; if this is the case, p will send ( bm, q )  messages in pulse i.
Process p is a direct supporter of q if p has received a ( bm, 1 )  message from q.
Process p is an indirect supporter of q if p has received a ( bm, q ) message from at least L processes.
The set Sp of processes supported by p is defined implicitly from Rp by.
Process p confirms process q upon receiving ( bm, q ) messages from H processes, i .e.
Thus, in pulse i + 1 all correct processes send ( bm, q ) , and as the number of correct processes is at least H, each correct process receives sufficient support to confirm q.
The general, being the only process that can enforce initiations (in other processes) on its own, holds a powerful position in the algorithm.
Also, if it does not initiate there is no "critical mass" of messages that leads to the initiation by any correct process.
To show dependence, we shall assume that the general is correct.
Thus from the end of round 3 every correct process has confirmed H processes, implying that the final decision will be l.
The general is supported and confirmed by all correct processes one pulse earlier than the other processes.
At the end of pulse i no correct process supports or confirms any correct process, because as we have seen earlier, this implies that the latter process has sent a ( bm, 1 ) message.
Consequently, no correct process initiates at the end of pulse i.
This implies that no correct process ever confirms a correct process, so no correct process confirms more than t processes, and the decision at the final pulse is O.
We continue by proving agreement, and we shall assume in the following lemmas that the general is faulty.
A sufficient "critical mass" of messages, leading inevitably to a I-decision, is created by the initiation of L correct processes when there are at least four pulses to go.
Let i be the first pulse at the end of which at least L correct processes initiate, and let A denote the set of correct processes that have initiated at the end of pulse i.
All processes in A are lieutenants because the general is faulty.
At the end of pulse i + 2 all correct processes have confirmed the lieutenants in A; we show that at that time all correct processes initiate.
As the general is faulty, there at most t - 1 faulty lieutenants, so at least L correct lieutenants must have been confirmed, showing that already, in an earlier pulse , L correct processes must have initiated.
We have observed that this implies that at least L correct processes have initiated.
To facilitate the presentation of the algorithm it has been assumed that processes repeat in every round the messages they have sent in earlier rounds.
As correct processes record the messages received in earlier rounds, this is not necessary, so it suffices to send each message only once.
As there are only N + 1 different messages, each message need contain only o (log N) bits.
If p is correct , only p can feasibly compute Sp(M)
Every process can efficiently verify (given p, M, and S) whether S = Sp(M)
The first assumption does not exclude that Byzantine processes conspire by revealing their secret keys to each other, allowing one Byzantine process to forge the signature of another.
Only correct processes are assumed to keep their private keys secret.
In the next subsection, a message ( msg ) signed by process p, i .e.
In pulse 1 the general shouts the message ( value, Xg ) : g ,  containing its (signed) input.
A message ( value, v ) : g : P2 :
Pi is called valid for the receiving process p if all the following hold.
During the algorithm, process P maintains a set Wp of values contained in valid messages received by P; initially this set is empty, and the value of each valid message is inserted in it.
All processes decide in pulse t + 1, implying both termination and simultaneity of the algorithm.
Pi , then q has itself seen the value v and.
This process forwarded the message to all other processes, including q, so q sees v.
As Wp = Wq by the end of pulse t + 1 , P and q decide equally.
In all pulses up to  t ,  a correct process could receive messages created and.
The intermediate result in the previous algorithm, namely agreement on a set of values among all correct processes, is stronger than necessary to achieve agreement on a single value.
This was observed by Dolev and Strong [DS83] , who proposed a more efficient modification.
It is in fact sufficient that at the end of pulse t + 1 ,  either (a) for every correct P the set Wp is the same singleton, or (b) for no correct P the set Wp is a singleton.
In the first case all processes decide v, in the latter case they all decide 0 (or, if it is desired to modify the algorithm in this way, they decide "general faulty" )
To show agreement , we shall show that for correct processes p and q, Wp and Wq satisfy at the end of pulse t + 1 the following.
Process r has forwarded the value v to q as well, implying that v is in Wq.
Equality of Wp and Wq cannot be derived because process p will not forward its third accepted value or later ones.
Because p's signature Sp(M) should constitute sufficient evidence that p is the originator of the message, the signature must consist of some form of information, which.
Clearly, if p succeeds in keeping its key secret , this implies that only p can tractably compute Sp (M)
All processes must be able to verify signatures, that is , given a message M and a signature S, it must be possible to verify efficiently that S has indeed been computed from M using p's secret key.
This verification requires that some information is revealed regarding p's secret key; this information is referred to as p's public key.
The public key should allow verification of the signature, but it should be impossible or at least computationally hard to use it to compute p's secret key or forge signatures.
ElGamal's signature scheme [ElG85] is based on a number-theoretic function called the discrete logarithm.
For a large prime number P, the multiplicative group modulo P, denoted Zp, contains P - 1 elements and is cyclic.
Such a 9 is called a generator of Zp, or also a primitive root modulo P.
The generator is not unique; usually there are many of them.
Given fixed P and generator g , for each x E Zp there i s  a unique integer i modulo P - 1 such that gi = x (equality in Zp)
This i is called the discrete logarithm (sometimes index) of x.
Unlike the basic arithmetic operations mentioned above, computation of the discrete log is not easy.
It is a well-studied problem for which no efficient general solution has been found to date, but neither has the problem been shown to be intractable; see [Od184] for an overview of results.
The signature scheme of ElGamal [ElG85] is based on the difficulty of computing discrete logarithms.
The processes share a large prime number P and a primitive root 9 of Zp.
The signature of p can be computed efficiently knowing the log of e, and therefore forms an implicit proof that the signer knows d.
A valid signature for message M is a pair (r, s) satisfying gM = er rS.
Such a pair is easily found by p using the secret key d.
Process p selects a random number a, coprime with P - 1 ,  and computes.
The validity of a signature S = (r, s) for message M is easily verified by checking whether gM = er rS.
Because p's secret key, d, equals the discrete logarithm of its public key, e, the scheme is broken if discrete logarithms modulo P can be computed efficiently.
To date, no efficient algorithm to do this in the general case or to forge signatures in any other way is known.
Its complexity is of the same order of magnitude as the best-known algorithms for factoring integers as big as P.
The algorithm first computes several tables using only P and g, and, in a second phase, computes logarithms for given numbers.
The second phase, computing logarithms, can be performed within seconds even on very small computers.
Therefore it is necessary to change P and 9 sufficiently often, say every month, so that the tables for a particular P are obsolete before their completion.
If n is a large number, the product of two prime numbers P and Q, it is very hard to compute square and higher-order roots modulo n unless the factorization is known.
In the signature scheme by Rivest , Shamir and Adleman [RSA 78] , the public key of p is a large number n, of which p knows the factorization, and an exponent e.
The signature of p for message M is the eth root of M modulo n, which is easily verified using exponentiation.
The secret key of p consists only of the number d, i .e.
A more subtle use of the difficulty of (square) rooting is made in a scheme by Fiat and Shamir [FS86]
In the RSA scheme, a process signs by showing that it is able to compute roots modulo its public key, and the ability to compute roots presumably requires knowledge of the factorization.
In the Fiat-Shamir scheme the processes make use of a common modulus n, of which the factorization is known only to a trusted center.
Process p is given the square roots of some specific numbers (depending on p's identity) , and the signature of p for M provides evidence that the signer knows these square roots, but without revealing what they are.
An advantage of the Fiat-Shamir scheme over the RSA scheme is the lower arithmetic complexity and the absence of a separate public key for every process.
A disadvantage is the necessity of a trusted authority that issues the secret keys.
In addition there is a one-way pseudo-random function f mapping strings to Zn ; this function is known and can be computed by every process , but its inverse cannot be computed feasibly.
As its secret key, p is given the square roots Sl through Sk of k numbers modulo n, namely Sj = R, where Vj = f(p, j )
The Vj can be considered as the public keys of p, but as they can be computed from p's identity they need not be stored.
To avoid some technical nuisance we assume that these k numbers are all quadratic residues modulo n.
The square roots can be computed by the center, which knows the factors of n.
The signature of p implicitly proves that the signer knows the roots of the Vj , i .e.
Such a number is Sj , but sending Sj itself would reveal the secret key; to avoid revealing the key, the scheme uses the following idea.
Process p selects a random number r and computes x = r2
Now p is the only process that can provide a number y satisfying y2 Vj = x, namely, y = r Sj.
Thus, p may demonstrate its knowledge of Sj without revealing it by sending a pair (x ,  y) satisfying y2Vj = x.
As p does not send the number r ,  computing Sj from the pair is not possible without computing a square root.
But there are two problems with signatures consisting of such pairs.
First , anybody can produce such a pair by cheating in the following way: select y first and compute x = y2 v subsequently.
Second, the signature does not depend on the message so a process that has received a signed message from p can copy the signature onto any forged message.
The crux of the signature scheme is to have p demonstrate knowledge of the root of the product of a subset of the Vj , where the subset depends on the message and the random number.
Scrambling the message and the random number through f prevents a forger from selecting y first.
Compute f (M, z ) and verify that the first k bits are el through ek.
If the signature is computed genuinely, the value of z computed in the first step of verification equals the value of x used in signing and hence the first k bits of f (M, z ) equal el through ek.
We now consider a strategy for a forger to obtain a signature according to the above scheme without knowing the Sj.
Choose a random number y and compute x = y2 ITej=l Vj.
Compute f (M, x) and see whether the first k bits equal the values.
However, each process must store k roots, and if k must be limited due to space restrictions, an expected 2k forgery time may not be satisfactory.
We now show how to modify the scheme so as to use k roots, and obtain a 2kt expected forgery time for a chosen integer t.
The idea is to use the first kt bits of an f-result to define t subsets of the Vj , and have p demonstrate its knowledge of the t products of these.
Xt) ;  call the first kt bits eij ( 1  ::; i ::; t and.
In addition to assuming the pulse model, a second assumption underlying all solutions presented so far is that the pulse in which the broadcast starts is known to all processes (and numbered 1 for convenience)
If this is not the case a priori , the problem arises of starting the algorithm simultaneously, after one or more processes (spontaneously) initiate a request for execution of the broadcast algorithm.
A request may come from the general (after computing a result that must be announced to all processes) or from lieutenants (realizing that they all need information stored in the general)
This problem is studied in the literature as the firing-squad problem.
In this problem, one or more processes initiate (a request) ,  but not necessarily in the same pulse, and processes may fire.
Indeed, given a solution for the firing squad problem, the first pulse of the broadcast need not be agreed upon in advance; processes requesting a broadcast initiate the firing squad algorithm, and the broadcast starts in the pulse following the firing.
Using a broadcast protocol as a subroutine, virtually all decision problems can be solved for synchronous systems by reaching interactive consistency, that is , agreement on the set of inputs.
In the interactive consistency problem, processes decide on a vector of inputs, with one entry for each process in the system.
Each correct process p decides on a vector Vp,  with one entry for each process.
If q is correct , then for correct p, Vp [q] = xq.
Interactive consistency can be  achieved by multiple broadcasts :  each process broadcasts its input, and process p places its decision in q's broadcast in.
Termination, agreement , and dependence are immediately inherited from the corresponding properties of the broadcast algorithm.
Because every correct process computes the same vector (agreement) , most decision problems are easily solved using a deterministic function on the decision vector (which immediately guarantees agreement)
Consensus, for example, is solved by extracting the majority value from the decision vector.
Election is solved by selecting the smallest unique identity in the vector (beware; the elected process can be faulty)
Capitals (C, T) are used for clock times and lower case letters (c, t) for real times.
Clocks can be used for controlling the occurrence of events, as in.
The various clocks in distributed systems do not show the same clock time at any given real time, i .e.
In this subsection the degree of precision with which process p can adjust its perfect clock to the perfect clock of a reliable server s will be studied.
Probabilistic protocols may achieve an arbitrary precision, but the message complexity depends on the desired precision and the distribution of message-delivery times.
We first present the simple protocol and prove that it achieves the precision claimed in the theorem.
A better precision is achievable with the probabilistic synchronization protocol proposed by Cristian [Cri89]
The probability for any message to arrive within x is F(x) , and the delay of different messages is independent.
An arbitrary precision is only achievable if the lower bound 8min is tight , i .e.
Upon receipt of ( ask ) from q: send ( val, xp )  to q.
Upon receipt of ( val, x )  from q: if no such message was received from q before.
The output of process p is a real value YP ' and the precision of the output is defined as maxp,q I Yp - Yq I ;  the aim of the algorithm is to achieve a very small value of the precision.
For a finite set A c JR, define two functions intvl (A) = [min(A) , max(A)] and width (A) = max(A) - min(A)
After that time p has received all the inputs from the correct processes, as well as answers from a subset of faulty processes.
These answers are padded with (meaningless) values 00 for processes that did not reply.
The process then applies a filter to the received values, which is guaranteed to pass all values of correct processes and only those faulty values that are sufficiently close to correct values.
The precision will be bounded by splitting the summation in the computation of the decision into summations over correct processes (C) and incorrect processes (B)
The first bound follows because if p and r are correct processes, apr = Xr.
Indeed, as r replies to p's ( ask ) message promptly, vpr = Xr.
Similarly, vpr' = Xr, for all correct r' , and the assumption on the input implies that r 's value survives the filtering by p, hence apr = vpr.
The second bound holds because for correct p and q, width(Ap U Aq)  ::; 28 when p and q compute their decisions.
Because the added estimates lie between accepted values ,  it suffices to consider the maximal difference between values ap and aq that passed the filters of p and q, respectively.
Upon receipt of ( ask ) from q: send ( val, Cp )  to q.
Upon receipt of ( val, C )  from q: if no such message was received from q before.
To represent the received value as a clock value rather than as a constant , p stores the difference of the received clock value (plus ! (Dmax + Dmin) )  and its own as D.pr.
At time t, p's approximation of r's clock is Cp(t) + D.pr.
The wider margin compensates for the unknown message delay and the threshold is motivated by the following proposition.
Let dpr denote the value that p has inserted in Dp for process r after the first phase of p (compare with the value vpr in the previous algorithm)
When p receives this message, I Cq - [C + ! (Dmax + Dmin) ] I is bounded by ! (Dmax - Dmin) , so dpq differs by at most ! (Dmax + Dmin) from Cq - Cpo Similarly, dpr differs by.
In this proof, write Cp for the unadjusted clock, and C; for the adjusted clock, i .e.
To bound the precision of the adjusted clocks , fix a real time t that is later than the time-out by all correct processes , and let wpr = Cp + dpr.
It is implicit in the description of the algorithm that all ( val, C )  messages are sent with the unadjusted clock values; this can be achieved by deferring the clock adjustment until a reply has been sent to all ( ask ) messages.
The pulse model of synchronous computations can be simulated in systems with weaker synchrony assumptions , provided that.
The simulation algorithm is very elementary; assume that execution of.
Fault Tolerance in Synchronous Systems change of pulse i and subsequently sends the messages for pulse (i + 1)
As the clock increases to arbitrarily high values, each correct process executes infinitely many pulses with this strategy.
It remains to show that correct process p receives all messages sent to it in pulse i (by correct processes) before it executes the state change for that pulse.
Assume process q sends a message to p in pulse i.
The assumption regarding the bound on local processing time implies that the pulse-i message is sent , at the latest , at real time.
Show how t o  find P 's secret key from the two signed messages.
Examples of the former include weaker coordinations and randomization, and examples of the latter include introducing synchrony.
Failure detectors are now widely recognized as an alternative way to strengthen the computation model.
There certainly is a point here, but the same argument applies to virtually everything that was invented in computer science over the past half century.
Of course we know all the arguments for using higher order languages and drivers: they make programming easier, improve understandability and portability, and allow studies of the power of certain concepts.
Failure detection, being available in many practical systems since a long time ago, was first formulated as an abstract mechanism around 1991 by the research goup of Sam Toueg of Cornell University.
It took about five years for a foundational paper to appear [CT96] , from which most of our definitions and results are taken.
A failure detector is a module that provides to each process a collection of suspected processes; the test J E D  in the program returns if process j is suspected at the moment of evaluation.
The type of failure considered in this chapter is crashes.
The modules in different processes need not agree (that is, correct process p may suspect r while correct q doesn't) and need not be just at any time (correct p may suspect correct r or not suspect failed r)
Of course, in order to be able to reason about programs with failure detectors we must first express the properties of the detectors, most notably the relation between the detector output and actual failures.
The actual failures are expressed in a failure pattern which we shall first define.
To circumvent the use of expressions in temporal logic we use an explicit time variable t in our expressions; this t ranges over the set T of all time instances, which could be the natural numbers for example.
Please note that processes can not observe the time t, the failure pattern F, the sets Crash(F) and Corr(F) , or the failure detector history! Process q only "sees" the value H(q, t) when it accesses its failure detector at time t.
Suspicions may differ from time to time and from process to process, and are therefore modeled by a function H : JID x T ---t P(JID)
Here H(q, t) is the collection of processes suspected by q at time t.
Definition 16.1 Failure detector V is complete if every crashed process will eventually be suspected by every correct process:
This property is also called strong completeness in the literature.
Definition 16.2 Failure detector V is strongly accurate if no process is ever suspected if it has not crashed:
Failure detector V is weakly accurate if there exists a correct process that is never suspected:
Failure detector V is eventually strongly accurate if there exists a time after.
Failure detector V is eventually weakly accurate if there exist a time and a correct process that is not suspected after that time:
It is very easy to have a failure detector that is complete (a detector that suspects everybody: H(q, t) = JED) and also to have a failure etector that is accurate (a detector that suspects nobody: H(q, t) = 0)
Interesting and useful detectors always combine a completeness and an accuracy property, and four classes are considered.
Definition 16.3 A failure detector is perfect if it is complete and strongly accurate; the class of perfect detectors is denoted by P.
A failure detector is strong if it is complete and weakly accurate; the class of strong detectors is denoted by S.
A failure detector is eventually perfect if it is complete and eventually strongly accurate; the class of eventually perfect detectors is denoted by oP.
A failure detector is eventually strong if it is complete and eventually weakly accurate; the class of eventually strong detectors is denoted by oS.
It isn't particularly difficult to imagine how failure detectors can be used in designing distributed applications.
Recall the communication operations that we encountered frequently in asynchronous distributed algorithms (Chapter 14) :
Each node performs a shout, that is , sends a message to each node.
Each node collects N - t of the shouted messages.
A process should never wait for the arrival of more than N - t messages because this risks a deadlock in case of crashes.
The pitfall is that even without any crashes, the set of collected.
The set of messages collected always has the same size (N - t)
Waiting for a message to arrive from any specific process is absolutely forbidden without detectors, because it may lead to blocking if the desired sender has crashed.
The standard way of communication using failure detectors runs like this :
Each node waits until for each process q, a message from q has arrived.
The completeness of the failure detector makes this construction free of eternal blocking; indeed, each process that doesn't send the message because of having crashed will eventually be suspected.
As in the case of the previous construction, there is a pitfall: there are various scenarios leading to different sets of messages collected by different correct processes.
Process q crashes, but prior to its crash sends some or all of the required messages.
Process PI receives the message before suspecting q, process P2 doesn't ,  and ends its receive phase without q's message.
Process q is correct and sends the message, but PI suspects q and doesn't collect q's message, while P2 does not suspect q and collects the message.
Thus, a set of collected messages may include a message from a crashed process, as well as miss one from a correct processes.
The size of the set may differ and, even if a bound t on the number of crashes exists, the set can be smaller than N - t messages because of erroneous suspicions.
Only a bound on the number of processes that can be suspected may bound the size of the set from below; basically this is done in the weakly accurate detectors.
The mode of receiving messages sketched is so common that we introduce a programming shorthand for it : the statement "collect ( message, par ) from q"
The instruction waits until either a message ( message, par ) is received from q, or q is supected; a boolean value indicating the succes of reception is returned.
Xi := input ; for r := 1 to N.
This section presents a relatively simple consensus algorithm using failure detectors.
A decision equals the input of at least one process.
It is easy to see that validity implies non-triviality, and it is thus seen that we obtain a result that is impossible without failure detection (Theorem 14.8)
We call the processes PI through PN and Pj coordinates round j.
In a round the coordinator shouts its value, all nodes collect a message from the coordinator, and if a node succeeds it replaces its value by the value received from the coordinator.
The correctness proof is just outlined here because the details are fairly obvious.
The algorithm satisfies termination because no correct process blocks forever in a round; indeed, the coordinator is correct and sends the message, or is eventually suspected (because of the completeness of the detector)
Formally one proves with induction on i that every correct process reaches round i.
Validity i s  satisfied because a process can only keep its value or  replace it by a value received from the coordinator, which allows us to prove (again by induction) that each process enters each round with a value that was the input of some process.
To show agreement, let Pj be the process that is never suspected.
In round j every process (that completes the round) receives the value from Pj , hence all processes complete round j with the same value.
A round that is entered with one value is also completed with one value (the same) , which implies that the entire algorithm is completed with just one value.
Observe that the resiliency t does not occur in the program.
It is currently not known how the number of rounds can be reduced if the resiliency is limited.
The reason is that correct coordinators can be suspected; in this algorithm it doesn't help if processes are correct , what one needs is that they are unsuspected.
The number of rounds can be reduced if the accuracy of the detector is strengthened; see Exercise 16.5
Therefore, in addition to actually reaching agreement, the solution must also detect when agreement has been reached.
In JPl two disjoint sets of processes S and T can be formed, both of size N - t.
Because S contains N - t  processes, the processes in S decide in finite time, and because all alive processes have input 0, their decision is O.
Observe that in both scenarios the failure detector behavior is in accordance with the requirements for <>P.
Moreover, the processes in S erroneously suspect those in T right from the start , and those in T erroneously suspect those in S right from the start ; the requirement of eventual perfectness allows this situation to last for any finite amount of time.
The messages sent from S to T and vice versa are delivered very slowly.
After the taking of at least one decision in each group, erroneous suspicions stop and messages from S to T and vice versa arrive.
The proof that consensus is not possible at all (Theorem 14.8) constructs an infinite non-deciding execution and we shall see in the next section that this infinite behavior can be avoided with eventually correct detectors.
It first collects the current values of all processes and checks if they are uniform (see phase 2)
Now here is a pitfall because due to erroneous suspicions the coordinator could collect effectively less than half of the active values and then announce a value as the unanimous outcome while it is actually supported by a minority of the processes.
Here is where we use the bound on the number of failed processes : in the collection phase the coordinator does not use its detector, but awaits N -t votes as in the classical approaches.
Now an observed agreement by the coordinator at least implies that a strict majority of the processes has the computed value; not necessarily all correct processes do, because some conflicting votes may be missed because the coordinator only awaits a fixed number of votes.
A decision is allowed if the coordinator has proclaimed that it chose the value v from unanimous votes, as indicated by the bit d in the message ( outcome, d, v, r )  that announces the result of round r.
The processes continue their activity after a decision to help other processes decide too.
Lemma 16.6 For each round r and correct process p, p finishes round r.
A correct coordinator will complete phase 1 because sufficiently many votes are sent.
A correct process will complete phase 3 because the coordinator will send the message if it is correct , and eventually be suspected if it isn't.
Xi := input ; r := 0 ; while true do.
A process can suspect the coordinator and keep the same value; the only way to change value is to receive a message with that value from the coordinator.
This shows that the coordinator, if it sent a value in phase 2 ,  sent the value v and thus the number of processes with value v can only grow, not decrease.
But c is never suspected any more, and is coordinator again in round r + N, so at the end of this round all processes have decided.
It is fairly trivial (and left to the reader) to show by induction on r that if any process holds a value in round r, this value was among the inputs; this shows validity.
A definite advantage of the failure detector approach is that one has to deal only with the properties of the detectors, and not with their implementation.
However, we do give some possible implementations to show what happens behind the curtains and as an illustration to the definitions.
In most cases the detector is a wrapping with asynchronous interface around the careful use of timers.
The sending process, at each multiple of u : forall q do send ( alive ) to q.
The receiving process , upon receipt of ( alive ) from q: if j E Dp then.
Exercise 16.2 Let F be a failure pattern; prove there is a t such that F(t) = Crash(F)
Where does your proof use that lP' is finite ? Show that the result doesn't hold if lP' is an infinite collection of processes.
Exercise 16.3 The requirement of strong accuracy is stronger than the requirement that no correct process is ever suspected:
Give an example of a failure patern and failure detector history that satisfy this property, yet are not allowed in a strongly accurate detector.
Exercise 16.4 Process p sends a message to q and then to r; q and r collect a message from p.
Show with an example that it is possible, even if detection is perfect , that r receives the message and q doesn't.
Exercise 16.5 A failure detector is called l-accurate if in each run at least l correct processes are unsuspected.
The property implies a bound of N - l on the resiliency; weak accuracy is I -accuracy.
The concept of stabilization was proposed by Dijkstra [Dij74] , but little work on it was done until the late nineteen-eighties ; hence the subject can be considered relatively new.
Nonetheless , a large number of stabilizing algorithms and related results were proposed in the following years up to the date of the present text , and in this chapter a selection of this work will be presented.
The term "stabilizing" is used throughout , whereas "self-stabilizing" is frequently found in the literature.
The set of legitimate configurations is usually closed, i .e.
Theorem 17.3 If system S stabilizes to P, then every execution of S has a non-empty suffix satisfying P.
Stabilizing algorithms offer the following three fundamental advantages over classical algorithms.
As was observed in the introduction to this chapter, a stabilizing algorithm offers full and automatic protection against all transient process failures, because the algorithm recovers from any configuration, no matter how much the data has been corrupted by failures.
The need of proper and consistent initialization of the algorithm is eliminated, because the processes can be started in arbitrary states and yet eventual coordinated behavior is guaranteed.
A fourth advantage, namely, the possibility of "sequential" composition without the need for termination detection, is discussed in Subsection 17.3
Finally, many of the stabilizing algorithms known t o  date are simpler then their classical counterparts for the same network problem.
This, however, is partly because these algorithms are not yet optimized with respect to their complexity, so this "advantage" may vanish when the study of stabilizing algorithms develops further.
On the other hand, there are the following three disadvantages.
Before a legitimate configuration is reached the algorithm may show an inconsistent output.
It is not possible to observe from within the system that a legitimate configuration has been reached; hence the processes are never aware of when their behavior has become reliable.
The property of stabilizing algorithms that was proved in Theorem 17.3 can be taken as an alternative definition: it is then simply required that each execution has a suffix satisfying the specification.
On the other hand, in pseudo-stabilizing solutions (that are not stabilizing) there is no upper bound on the number of steps that the system can take before specification P is satisfied, while for stabilizing algorithms such a bound can be given.
In this chapter we shall restrict ourselves to the study of stabilizing algorithms.
This system has an execution consisting only of send actions by p, while all other processes remain frozen in their initial state, and this behavior does not satisfy any meaningful non-trivial specification.
Second, consider an algorithm in which, for every process , there are states in which the process does not send (but can only receive or do an internal step)
Every configuration in which every process is in such a state and all channels are empty is a terminal configuration and therefore must satisfy the specification.
Again, no non-trivial specification is satisfied by all such configurations.
A stabilizing version of the Netchange algorithm was presented in [Tel91a] , using timers in each process and an upper bound on the message-delivery.
This assumption may work out well in practice, but the analysis of the algorithm is concerned mainly with technical details about timing.
Therefore we shall assume the more usual model of communication by shared variables, where one process can write and other processes can read the same variable.
In state models a process can (atomically) read the entire state of its neighbor.
In such a model every neighbor of p reads the same state, so it is not possible for a process to transfer different information to its various neighbors.
In link-register models, communication between processes is by two registers shared between the two processes; each process can read one of these and write the other.
A process can transfer different information to various neighbors by writing different values in its link registers.
We further distinguish between read-one and read-all models , in which a process can read in one atomic step the state (or link register) of one neighbor, or of all neighbors, respectively.
The first stabilizing algorithms were proposed by Dijkstra [Dij74] and these algorithms achieved mutual exclusion in a ring of processes.
For this problem it is assumed that processes must sometimes execute critical sections of their code, but it is required that at most one process executes a critical section at any given time.
Each process is provided with a local predicate that , when true, indicates that the process has the privilege of executing the critical section.
In every configuration, at most one process has the privilege.
The problem can be solved by introducing a token that circulates over the ring and grants the privilege to the process holding it.
The strength of Dijkstra's solution is that it automatically recovers (within O(N2) process steps) from situations where no token or more than one token is present in the system.
Further, no step increases the number of privileged processes , because a process changing state loses its privilege and the only process that can obtain a privilege in this step is its successor.
To show stabilization, define .c as the set of configurations in which exactly one process has the privilege.
In a legitimate configuration exactly one step is possible, namely by the unique privileged process.
In this step, this process loses its privilege, but , because there are no configurations without a privilege, its successor necessarily obtains the privilege.
Consequently, the privilege circulates over the ring and every process holds the privilege once in every N consecutive configurations.
Again, every execution of  the system i s  infinite by the absence of terminal configurations.
In the initial configuration 'Yo at most N different states occur so there are.
Stabilization copy states, hence the first time process 0 computes such a state, its state is unique in the ring.
Corollary 17.7 Dijkstm 's token ring stabilizes to mutual exclusion.
The orientation problem requires the system to reach the postcondition 'l/J, defined by "for every edge pq, lpq is succ if and only if lqp is pred"
It was shown by Israeli and Jalfon [IJ90] that no deterministic solution is possible in the state-reading models; we present the Israeli-Jalfon algorithm for the link-register model.
In the S state, a process waits to forward a token (to its current successor) , while in the R state a process waits t o  receive a token; in the I state a process is idle.
A process holds a token if either its state is S or its state is R and its predecessor is not in state S.
In action ( 1 ) ,  if q wants to forward a token to the idle p, p agrees to accept the token (although it does not move yet) and makes q its predecessor.
In action (2) , p observes that its successor has agreed to accept the token it wants to transmit and becomes idle, thus moving the token.
In action (3) , p observes that q, from which it has accepted a token, has become idle, and starts transmitting the token to its successor.
In these cases, both processes are enabled, and the first process that takes a step breaks the symmetry1
In action (4) , p accepts q 's token, implicitly destroying its own.
In action (5) , p generates a token that will overrule q's orientation.
A configuration is called legitimate if and only if it is oriented, i .e.
Israeli and Jalfon consider a finer grain of atomicity of action, which necessitates breaking the symmetry explicitly.
Legitimate configurations are oriented by the choice of C, which immediately implies the second part.
If 'Y is not oriented there are two processes pointing at each other, i .e.
It follows that a terminal configuration without receiving processes is oriented; hence all terminal configurations are oriented.
Token creation occurs in a pair I I , and such a pair is not formed in any of the five actions, so a token is generated only in an initially idle process as the first step of that process.
If a token exists in a configuration during the execution and this token has already moved k times, then a sequence of k + 1 processes (ending in the one holding the token) is consistently oriented.
All these processes have forwarded the token and adopted its direction, and no tokens that could.
Stabilization have disturbed the orientation again have been created "behind" the token.
So if one token moves N - 1 times all processes are oriented equally, and the configuration is legitimate.
Because there are no more than N different tokens during the execution, a legitimate configuration is reached after at most N2 token steps.
A matching in a graph is a set of edges such that no node of the graph is incident to more than one of the edges.
The matching is maximal if it cannot be extended by more edges of the graph, and a maximal matching of a graph can be constructed in a time linear in the number of edges.
Each edge is considered in turn, and included in the matching if it is not incident to any edge already in the matching.
This algorithm is, however, inherently sequential, and not suited for distributed execution; we shall now present a stabilizing algorithm for constructing maximal matchings, proposed by Hsu and Huang [HH92]
If pref p = q, p has selected its neighbor q to become matched with, i .e.
We distinguish five cases depending on p 's preference and that of its neighbors.
If p has selected q, then p is waiting (for q) if q has not made a selection yet , matched if q has selected p, and chaining if q has selected a neighbor other than p.
If p has not made a selection, p is dead if all neighbors of p are matched and free if there is an unmatched neighbor.
If there is an edge pq E M, then pref p = q or pref q = p by the definition of M; but because q is not waiting or chaining, the latter implies pref p = q as well.
It follows that at most one edge incident to p belongs to M, hence M is a matching.
The algorithm is described for the read-all state model and consists of three actions for each process p; see Algorithm 17.3
Process p matches with neighbor q (action Mp) if p is free and q has selected p.
If p is free but cannot match, it selects a free neighbor if such is possible (action Sp) , and if p is chaining it unchains (action Up)
Lemma 17.12 Configuration 'Y is terminal if and only if 'Ij;( 'Y) holds.
Action Mp is enabled for p only if neighbor q is waiting, action Sp requires that p is free, and action Up that p is chaining, hence 'Ij;('Y) implies that no action is enabled in 'Y.
If 'Ij; does not hold, there is a p such that p is chaining, waiting, or free; in a chaining p, Up is enabled, and if p is waiting for q, Mq is enabled.
Finally, assume p is free and q is an unmatched neighbor.
If q is waiting, the matching action is enabled for one of its neighbors and if q is chaining Uq is enabled.
Finally, if q is free, p and q can select each other.
Hence, if 'lj; does not hold, the configuration is not terminal.
Define the norm function F by the pair (c + I + W ,  2c + J) where c ,  I, and w are the numbers of chaining, free, and waiting processes respectively.
We show that F decreases (in the lexicographic order) with every step; first observe that a matched or dead process remains matched or dead forever, so c + I + w never increases.
The sum may decrease even further if some neighbors of p and q become dead.
No waiting process becomes free or chaining, because the action is applicable only if no process waits for p, and no free process becomes chaining.
Chaining neighbors of p may become waiting, thus further decreasing c.
Process p maintains the variables Rootp , Parp , and Disp to describe the.
The intended postcondition of the algorithm is 1/J, defined by Vp : sat (p)
As lmax (p) is satisfied for all p and the network is connected, all processes have the same value of Root.
The process Po with minimal value of Dis does not have a neighbor q with Disp = Disq + 1 ,  so root (po ) holds and the common value of the Root variables is PO.
But then, for all p =1= Po , child (p) holds , and, because child (p, q) :=;
Call the value of Rootq a false root if there is no process with that identity in the network; false roots may exist initially but are not created during execution.
A main problem in the design of the algorithm is to prevent processes from becoming the child of a process with a false root infinitely often.
To this end, joining q 's tree by p is done in three steps.
First , p becomes a root (action Bp) ,  then asks for join permission (action Ap) ,  and finally joins when q grants the permission (action Jp)
Process p clears the variables for request processing (action Cp) if it is not currently processing a request and the variables are not undefined already.
If p is a root and forwards a request , it will grant it (action Gp) ,  and if p forwards the request to its parent and the parent grants it , then p relays the grant (action Rp)
The request/reply mechanism ensures that a process can join a tree with a false root only finitely often, because the non-existent root does not grant requests, and only finitely many false grants exist initially.
Then, no Root variable contains a false root forever; the process with the smallest value of Dis containing a false root does not satisfy tree, and will reset Root to its own identity.
It follows that eventually there are no false roots, and also that eventually the process with highest identity will be a root , for this is the only way to satisfy sat in the absence of higher Root values.
A node that does not belong to the tree rooted at the highest node, but does have a neighbor in that tree, does not satisfy the sat predicate and will attempt to join, while nodes in the tree never leave it.
As all incorrect requests eventually disappear from the tree, addition of nodes to the tree continues until the tree spans the whole network.
Most routing methods start by computing routing tables in every node, after which packets are forwarded using these tables.
The reason for electing a leader in a network is usually the desire to subsequently execute a centralized algorithm in the network.
A similar method cannot be used when the designed algorithm must be stabilizing, because it is not possible to observe (in a stabilizing manner) the termination of the first stage.
First , the execution of the composition must be fair w.r.t.
The development of the algorithm closely follows the proof that such a coloring exists, which is based on the following fact.
Fact 17.18 A planar graph has at least one node with degree five or less.
Lemma 17.19 A planar graph has an acyclic orientation in which every node has an out-degree of at most five.
Let G be a planar graph; consider an acyclic orientation of G in which no node has more than five outgoing edges.
Because the orientation is acyclic, the nodes can be numbered VI through Vn such that if there is.
Color the nodes in the order VI through Vn ; the property of the numbering implies that before node V only out-neighbors of V are colored.
This, and the property of the orientation, now imply that at most five neighbors of V are colored before v, hence V can be colored differently from all (already colored) neighbors.
To represent an acyclic orientation, process p is given an integer variable xP ' and we define edge pq to be directed from p to q, denoted pq, if.
Let out (p) denote the number of outgoing edges of p, i .e.
The desired postcondition of  the first phase i s  e ,  defined by.
Program 81 , for establishing e, consists of one operation for each process, directing all edges of p towards p if p has more than five outgoing edges:
A configuration is terminal if and only if e is true; hence, if the program halts e is established, and if e is established the variables xp remain constant.
Program 82 , for establishing 'ljJ,  also consists of a single operation for each process, in which p adopts a color b different from its successors if cp equals one of those colors and an unused color exists :
Finally, for every edge qr , either r is a successor of q or q is a successor of r, which now implies that Cr =f cq.
Here fpq is a function f pq : U ---t U ,  called the edge function, given for each edge pq.
Pk-l , Pk) can now be computed incrementally as fPkPk- l (
The cost function (and edge functions) are presumed to satisfy the following axioms.
There exists a number B such that for each path.
For every p there exists a simple path Jr(p) ending in p, such that all paths ending in p have a cost of at least D(Jr(p) )
There exists a spanning forest (called a minimal-path forest) such that for every p the (unique) path from a root to p has the cost D(Jr(p) )
For ( 1 ) :  as there are only finitely many simple paths ending in p, we can choose Jr(p) as a minimal simple path ending in p.
By the choice of Jr(p) , all simple paths ending in p then have a cost of at least D(Jr(p) )
The cost of a path containing a cycle is (by cycle-increase and monotonicity) higher than the cost of the simple path obtained by removing cycles, hence also bounded by D(Jr(p) )
This shows that Jr(p) witnesses the first part of the theorem.
For (2) : as minimal paths need not be unique, the second part of the theorem requires some care; the graph obtained by taking all edges occurring in minimal paths need not be a forest.
Like the optimal sink trees considered in Theorem 4.2 ,  the minimal-path forest is not unique, but it is possible to construct such a forest , as follows.
Induction from the roots shows the desired property of this forest.
The required postcondition of an algorithm for the minimal-path problem can now be stated:
In order t o  establish 'IjJ in a stabilizing way the information about existing paths may flow through the graph by pushing it through edges.
However, it is also necessary to remove from the system erroneous information present in the initial configuration; that is, values of Kp for which no corresponding path to p exists.
This is done by moving this information via longer and longer paths such that, by the length-increase property, the erroneous information will eventually be rejected in favor of information concerning an existing path.
The Update algorithm consists of a single operation for each node, in which it computes the smallest cost of the empty path and paths whose cost is stored at its neighbors; see Algorithm 17.6
In the analysis of the algorithm we shall assume, as we did for the pushing algorithm, that execution is.
As Cp is always enabled, each execution is infinite; the fairness assumption allows us to partition an execution in rounds as follows.
Round 1 ends when every process has computed at least once.
Round i + 1 ends when every process has computed at least once after the end of round i.
It will first be  shown that eventually Kp i s  a lower bound for r;,(p)
Define r;,i (p) as the minimal cost of a path of length i or less to p; we claim that at any time from the end of round i ,  Kp :'S r;,i-l (p)
Case i = 1 :  The empty path, with cost Cp ,  i s  the only path to  p of  length.
By induction, Kq :'S D(p) after the end of round i.
At the end of round i + 1 , p has taken at least one such step, showing that Kp :'S D( n-) from the end of that round.
As a minimal path contains no cycles, its length is bounded by N - 1 , which implies r;,(p) = r;,N-l (p) , hence Kp :'S r;,(p) from the end of round N.
To show that eventually Kp is an upper bound for r;,(p) , we shall show that after i rounds the value Kp corresponds to an existing path or to initial information that has traveled via at least i hops.
Moreover, if K is computed in round i ,  the length of p is at least i.
The claim is established by induction on the steps in an execution.
Base case: Initially, Kr = J(r) (K; ) , where (r) is a path of length 0; of.
Induction step: Consider the value K computed in a step by p.
Monotonicity follows from the additional nature of the edge functions, and cycle-increase from the assumption that cycles in the network have positive cost.
To show the length-increase property, note that there are only finitely many simple cycles and let 8 be the smallest weight of a simple cycle and W be the largest weight of any edge.
To build a complete routing table the algorithm is executed for each destination in parallel.
Define as the cost of 7r the list of nodes that occur in the path, i .e.
Comparison is lexicographic, but simple paths are always smaller than paths containing cycles:
First , the forest consists of a single tree, rooted at the smallest node; call this smallest node r.
Indeed, for every node p there exists a simple path from r to p; a simple path from r is smaller than a path from any other node to p, hence the minimal path to p starts in r.
This application of the Update algorithm was proposed by Herman [Her91] ' who also extended the algorithm in such a.
Project 17.5 Design a stabilizing algorithm to orient a torus, preferably, so that the algorithm terminates.
Design a stabilizing algorithm that computes a three-coloring of an outerplanar graph.
Project 17.11  Design a stabilizing algorithm that computes a five-coloring of a planar graph.
Exercise 17.12 Show that the Update algorithm can be used for election and computation of a breadth-first search spanning tree by giving an appropriate path-cost function.
Exercise 17.13 Give a stabilizing algorithm for computing the network size.
Exercise 17.14 Show how to compute the depth of a tree with the Update algorithm.
Rather than presenting the algorithms in this book as programs written in an existing programming language, a pseudocode is used for the formal description of algorithms.
Pseudocode is concise and "user-friendly" , where a real program is often obscured by details concerning a rigid syntax.
The pseudocode used in this book very much resembles the language Pascal.
An algorithm is usually given as the local algorithm of a process of the system, whose name is used as a subscript to the variables of the program.
Process names can be used as array indices, and set variables are used.
The variable Neighp is the set of processes to which p is connected (the neighbors of p)
The order in which the elements get their turn is not specified, but the statement is only used where the order does not matter.
For example, if the number of messages received from each neighbor of p must be initialized to zero, we write.
The send operation must be given a message (its type and the value of its data fields) and a destination (which is a neighbor of the sender)
The value of a data field is given as an expression that can be evaluated by the sender.
To send its identity and degree to process q, process p may execute.
The shorthand "shout" is used to send a message to all neighbors , i .e.
This operation removes one message of type info from the communication system; after this operation the first and second data field and the sender of the message are available as a, b, and q.
In this form, the receive operation does not specify from which neighbor a message must be received, but the sender of the received message becomes known after the operation.
A process may need to receive a message from a specific process, say qo ; in this case it uses the operation.
The parameter q used by p to describe a neighbor of p for the purpose of sending or receiving a message is called the address of the neighbor.
The overall control structure of an algorithm may be expressed in two ways, namely, explicitly in a control-oriented description or implicitly in an event-driven description of the algorithm.
Both alternatives are possible for each algorithm, but in many cases one of the two descriptions is more convenient than the other.
The execution of the local algorithm consists in this case of a single execution of this statement list.
As an example, consider Algorithm A.2 ,  which is an event-driven notation of the same tree algorithm.
Because each statement can be selected for execution arbitrarily often (provided its guard is true) , it must be explicitly.
In Algorithm A.2 this is done using the sentp and deep flags.
A graph can be thought of as a collection of points (called the nodes of the graph) some of which are connected by lines (the edges) ; see Figure B.
In case of directed graphs, the edges have a direction and are drawn as arrows.
In case of weighted graphs, a numerical value is assigned to each edge.
Figure B.l GRAPHS, ( a) UNDIRECTED , ( b )  DIRECTED , ( c) WEIGHTED.
In this path, Vo is the begin node and Vk the end node.
A cycle is a path of which the begin node equals the end node.
A path is called simple if the nodes Vo through Vk are all different.
A cycle is called simple if the nodes VI through Vk are all different.
The distance between u and v ,  denoted d(u, v ) , is the length of a shortest path between u and v.
The diameter of G is the largest distance between any two nodes.
An undirected graph is connected if there exists a path between each pair of nodes.
An undirected graph is called acyclic if it contains no simple cycle of length three or more.
A graph is called a planar graph if it is possible to draw the graph in the plane without crossing edges.
A graph is called outerplanar if it is possible to draw the graph in the plane without crossing the edges and with all nodes on the border of the picture.
The following properties of planar and outerplanar graphs are of interest.
A planar graph has at least one node of degree five or less.
An outerplanar graph has at least one node of degree two or less.
Directed Graphs A directed graph G is a pair (V, E) , where V is the node set , and E is a collection of ordered pairs from V.
That is, an element of E is a pair (u, v) with u, v E V.
To shorten the notation, we write uv E E instead of (u, v) E E.
Because the pair is ordered, uv E E is not equivalent to vu E E.
Most of the definitions for undirected graphs have a slightly modified counterpart for directed graphs.
The edge uv is called an outgoing edge of u and an incoming edge of v.
If  u v  E E, v i s  called an out-neighbor of v ,  and v i s  called an in-neighbor of u.
The in-degree of a node is the number of its incoming edges , and the out-degree is the number of outgoing edges.
The degree is the sum of the in-degree and the out-degree.
In this path, Vo is the begin node and Vk the end node.
A cycle is a path of which the begin node equals the end node.
As for undirected graphs, a path is called simple if the nodes Vo through Vk are all different , and a cycle is called simple if the nodes VI through Vk are all different.
The distance from u to v ,  denoted d( u, v) , is the length of a shortest path between u and v.
The diameter of G is the largest distance from any node to any other node.
A directed graph is strongly connected if there exists a path from each node to each other node.
A directed graph is called acyclic if it contains no simple cycle of length two or more.
Subgraphs, spanning subgraphs, and induced subgraphs are defined for directed graphs exactly as for undirected graphs.
Weighted Graphs A (directed or undirected) graph is weighted if for each pair u, v with uv E E, a numerical value Wuv is defined.
The weight assignment is called symmetric if, for each pair, Wuv = Wvu ' In a weighted graph, the weight of a path is defined as the sum of the edge weights over the edges in the path.
If no weight assignment is assumed for a graph, it is called unweighted.
B.2 Frequently Used Graphs This section defines and discusses some classes of graph that occur frequently in the study of distributed algorithms, namely, rings, trees, forests, grids, tori, cliques, and hypercubes.
As can be seen from many results in this book, computations in distributed systems can often be performed more.
Let , in the sequel, G = (V, E) be a graph, N the number of its nodes , and D the diameter.
The ring topology is a circular arrangement of nodes, and is often used as a control topology in distributed computations.
To give each process its turn (to perform some function) in a round-robin fashion, the processes circulate a message among them; Vo sends it to VI , VI to V2 , etc ,  until in arrives at VN- I ,  which sends it to Vo again.
Definition B.l  A ring is an undirected, connected, regular graph of degree two.
Definition B.3 A tree is an undirected, connected, acyclic graph.
Theorem B.4 The following are equivalent for an undirected graph G.
Between any two nodes there exists a unique simple path.
A tree T = (V, E) is rooted if there is a unique designated node r called the root.
If u is a node on the (unique) simple path between v and r, u is called an ancestor of v, and v is called a descendant of u.
If, moreover, u and v are neighbors, v is called a son (or child) of u, and u is called the father (or parent) of v.
The subgraph induced by the descendants of u is a rooted tree (with root u) , called the subtree of u.
The depth of the tree is the maximal length of any simple path from the root to a node.
Several classes of spanning tree are of interest in distributed computing.
If the spanning tree must be chosen to minimize the total time necessary for a computation on it , it is desirable that the diameter is as small as possible.
The optimal sink tree constructed in the proof of Theorem 4.2 has a diameter that is at most twice the diameter of the entire network.
If the total communication cost for a computation in a spanning tree must be low, the subtree must be chosen so as to minimize the.
A distributed algorithm for computing this unique tree was given by Gallager et al.
If the computation overhead per node must be low, a spanning tree must be selected that has a low degree for each node.
The distributed construction of such trees was described by Korach et al.
Forests are generalizations of trees, and stars are special cases of trees.
A graph consisting of a number of isolated trees is called a forest.
Theorem B.6 The following are equivalent for an undirected graph G.
Between any two nodes there exists at most one simple path.
If any edge is removed, the number of connected components of G.
The number of connected components of G equals IV I  - lE I.
A forest is rooted if a root node is designated in every tree of the forest.
Every graph has a spanning forest , consisting of a spanning tree of each connected component of the graph.
A star is a graph with one special node, the center, and all other nodes are connected only to this center.
Definition B.7 A star is a rooted tree of depth one.
Theorem B.8 The following are equivalent for an undirected graph G.
Cliques In a clique, sometimes also called a complete graph, each pair of nodes is directly connected by an edge.
Definition B.9 A clique is a graph with diameter one.
As is the case for stars, cliques are rarely used as a physical connection topology.
This is so because the high number of edges of the graph would require a large amount of hardware.
In most networks not every pair of nodes is directly connected, and communication between non-adjacent nodes requires that all nodes on a path between these nodes cooperate.
If, however, this cooperation is invisibly implemented in the "low" layers of the system, the network may be regarded as a clique in its "higher" layers.
In an n x n grid there are N = n2 nodes, arranged in n rows of n nodes each.
Each node is connected to the nodes above it , right and left of it , and under it ; see Figure B.2
The n x n torus is similar, but in addition the leftmost and rightmost node of each row are adjacent , and the uppermost and lowermost node of each column are adjacent ; see Figure B.2
Grids and tori are popular topologies for use in the design of multiprocessor computers.
Because the degree of each node is at most four, it is possible to realize these topologies physically from Transputer chips.
The topologies are very suitable for performing computations on matrices.
For the formal definition of these topologies, recall that Zn is the set of integers modulo n.
Hypercubes Like grids and tori , hypercubes are often used in the design of multiprocessor computers.
They combine a reasonably small diameter with a reasonably small degree; both diameter and degree equal log N, where N is the number of nodes.
If we fix one labeling of the nodes of a hypercube, it is not difficult to prove the following statement.
If u and v are adjacent , the neighbors of u and the neighbors of v are connected in a one-to-one way.
By this we mean that each neighbor of v is adjacent to exactly one neighbor of u, and vice versa.
It is more difficult to see that (together with some additional, rather trivial conditions) a one-to-one connection between neighbors of adjacent nodes is sufficient for a graph to be a hypercube.
Theorem B.l3 The following are equivalent for an undirected graph G.
Table B.4 summarizes the main characteristics of the graph classes discussed in this section.
It was observed by Santoro [San84] that the communication complexity of distributed computations is influenced by, apart from the actual topology, the following factors.
In order to exploit the advantages of a specific topology, it is sometimes necessary that the processes "know" that they are connected in a topology of this class.
The routing of information through a network can be done more efficiently if the edges incident to each node are labelled with the "direction" to which they lead in the network.
Such information allows us to find a short (usually, optimal) path to a node, to which only a long path is known.
Of course, what the "directions" in a graph are depends on its topology.
In each case we assume that , for each node, the edges of that node are locally labeled with the "meaningful directions" for that class of graphs.
If S denotes the labeling, write Sv ( u) for the label that is assigned to edge uv in node V; the same edge is labeled Su (v) in node u.
The set of existing labels is the same for each node; the size of that set , C, equals the degree of the network, because within each node, edge labels are unique.
To give a sense of direction, an additional global consistency condition must be verified.
It will now be explained for four classes of graph what sense of direction means for this particular class.
As the ring is regular of degree two, there are only two directions in the ring; we call them "Prev' (previous) and "Nexf'
Of course, if u is the "Nexf' node after v ,  then V is the "Prev' node before U; if this relation is.
Sense of direction in a ring is not unique; there are two edge labelings satisfying these constraints.
It will always be assumed that rings have a sense of direction.
Clearly, if one node sends a message via its Next edge, and each node that receives the message again forwards it via its Next edge, all nodes will receive the message before it returns to the first node.
Usually, this is the only direction in which the communication channels are used, and it is often assumed that the ring is directed as indicated in Figure B.6
A clique with a sense of direction is depicted in  Figure B.7
Sense of direction in a clique is not unique; there exist exactly (N - 1 ) !  different labelings that constitute a sense of direction.
The four directions of the torus are called up, down, right , and left, abbreviated U, D, R, L.
S(i, j) ( (i ' ,  j' ) )  = U :::;
A torus with sense of direction is depicted in Figure B.8
Table B .lO  summarizes the characteristics of sense of direction for the four classes of graph.
A general definition of sense of direction and formal description of its properties were given by Flocchini et al.
Distributed algorithms for computing a sense of direction in hypercubes and tori are given there and in [Te194]
An acyclic graph on n nodes has at most n - 1 edges.
An undirected graph is called bipartite if its nodes can be colored using two colors (red and black) in such a way that no two nodes of the same color are adjacent.
Exercise B.2 Show that an undirected graph is bipartite if and only if it contains no cycles of odd length.
Prove that an n-dimensional hypercube can be given a sense of direction in exactly n! different ways.
Exercise B.7 Let S be a labeling of a clique.
For every path P, P is a cycle if and only if Sum(P) == 0 (mod N)
Time and message bounds for election in synchronous and asynchronous complete networks.
Optimal distributed algorithms for minimum weight spanning tree, counting, leader election and related problems.
Uniform d--emulations of rings, with an application to distributed virtual ring construction.
A combinatorial characterization of the distributed tasks which are solvable in the presence of one faulty processor.
New lower bound techniques for distributed leader finding and other problems on rings of processors.
Some lower bound results for decentralized extrema-finding in rings of processors.
An O(N log N) unidirectional distributed algorithm for extrema-finding in a circle.
A public key cryptosystem and a signature scheme based on discrete logrithms.
A lower bound for the time to assure interactive consistency.
The impact of synchronous communication on the problem of electing a leader in a ring.
On an improved algorithm for decentralized extrema finding in circular configurations of processors.
Comments on "On the proof of a distributed algorithm" : Always-true is not invariant.
Self-stabilizing mutual exclusion on a ring, even if K = N.
A modular technique for the design of efficient leader finding algorithms.
Tight upper and lower bounds for some distributed algorithms for a complete network of processors.
The optimality of distributive constructions of minimum weight and degree restricted spanning trees in a complete network of processors.
Time, clocks, and the ordering of  events in  a distributed system.
Election in a complete network with a sense of direction.
An O (n log n) unidirectional algorithm for the circular extrema problem.
A new approach to the detection of locally indicative stability.
The derivation of termination detection algorithms from garbage collection schemes.
