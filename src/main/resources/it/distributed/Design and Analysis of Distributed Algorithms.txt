No warranty may be created or extended by sales representatives or written sales materials.
The advice and strategies contained herein may not be suitable for your situation.
Neither the publisher nor author shall be liable for any loss of profit or any other commercial damages, including but not limited to special, incidental, consequential, or other damages.
Wiley also publishes its books in a variety of electronic formats.
Some content that appears in print may not be available in electronic formats.
For more information about Wiley products, visit our web site at www.wiley.com.
The computational universe surrounding us is clearly quite different from that envisioned by the designers of the large mainframes of half a century ago.
Even the subsequent most futuristic visions of supercomputing and of parallel machines, which have guided the research drive and absorbed the research funding for so many years, are far from today’s computational realities.
These realities are characterized by the presence of communities of networked entities communicating with each other, cooperating toward common tasks or the solution of a shared problem, and acting autonomously and spontaneously.
It has been from the fields of network and of communication engineering that the seeds of what we now experience have germinated.
The growth in understanding has occurred when computer scientists (initially very few) started to become aware of and study the computational issues connected with these new network-centric realities.
The internet, the web, and the grids are just examples of these environments.
Whether over wired or wireless media, whether by static or nomadic code, computing in such environments is inherently decentralized and distributed.
To compute in distributed environments one must understand the basic principles, the fundamental properties, the available tools, and the inherent limitations.
This book focuses on the algorithmics of distributed computing; that is, on how to solve problems and perform tasks efficiently in a distributed computing environment.
Because of the multiplicity and variety of distributed systems and networked environments and their widespread differences, this book does not focus on any single one of them.
This universe consists of a finite collection of computational entities communicating by means of messages in order to achieve a common goal; for example, to perform a given task, to compute the solution to a problem, to satisfy a request either from the user (i.e., outside the environment) or from other entities.
Although each entity is capable of performing computations, it is the collection.
Incredibly, the terms “distributed systems” and “distributed computing” have been for years highjacked and (ab)used to describe very limited systems and low-level solutions (e.g., client server) that have little to do with distributed computing.
In this universe, to solve a problem, we must discover and design a distributed algorithm or protocol for those entities: A set of rules that specify what each entity has to do.
The collective but autonomous execution of those rules, possibly without any supervision or synchronization, must enable the entities to perform the desired task to solve the problem.
In the design process, we must ensure both correctness (i.e., the protocol we design indeed solves the problem) and efficiency (i.e., the protocol we design has a “small” cost)
As the title says, this book is on the Design and Analysis of Distributed Algorithms.
Its goal is to enable the reader to learn how to design protocols to solve problems in a distributed computing environment, not by listing the results but rather by teaching how they can be obtained.
In addition to the “how” and “why” (necessary for problem solution, from basic building blocks to complex protocol design), it focuses on providing the analytical tools and skills necessary for complexity evaluation of designs.
It covers the “distributed part” of a graduate course on Parallel and Distributed Computing (the chapters on Distributed Data, Routing, and Synchronous Computing, in particular), and it is the theoretical companion book for a course in Distributed Systems, Advanced Operating Systems, or Distributed Data Processing.
The book is written for the students from the students’ point of view, and it follows closely a well defined teaching path and method (the “course”) developed over the years; both the path and the method become apparent while reading and using the book.
It also provides a self-contained, self-directed guide for system-protocol designers and for communication software and engineers and developers, as well as for researchers wanting to enter or just interested in the area; it enables hands-on, headon, and in-depth acquisition of the material.
In addition, it is a serious sourcebook and referencebook for investigators in distributed computing and related areas.
Unlike the other available textbooks on these subjects, the book is based on a very simple fully reactive computational model.
From a learning point of view, this makes the explanations clearer and readers’ comprehension easier.
From a teaching point of view, this approach provides the instructor with a natural way to present otherwise difficult material and to guide the students through, step by step.
The instructors themselves, if not already familiar-with the material or with the approach, can achieve proficiency quickly and easily.
All protocols in the textbook as well as those designed by the students as part of the exercises are immediately programmable.
Hence, the subtleties of actual implementation can be employed to enhance the understanding of the theoretical.
An open source Java-based engine, DisJ, provides the execution and visualization environment for our reactive protocols.
The book is written so to require no prerequisites other than standard undergraduate knowledge of operating systems and of algorithms.
Clearly, concurrent or prior knowledge of communication networks, distributed operating systems or distributed transaction systems would help the reader to ground the material of this course into some practical application context; however, none is necessary.
The book is structured into nine chapters of different lengths.
Some are focused on a single problem, others on a class of problems.
The structuring of the written material into chapters could have easily followed different lines.
For example, the material of election and of mutual exclusion could have been grouped together in a chapter on Distributed Control.
Indeed, these two topics can be taught one after the other: Although missing an introduction, this “hidden” chapter is present in a distributed way.
As suggested by the figure, the first three chapters should be covered sequentially and before the other material.
Other than that, the sections can be mixed and matched depending on the instructor’s preferences and interests.
An interesting and popular sequence for a one-semester course is given by Chapters 6–9
Several important topics are not included in this edition of the book.
By design, this book does not include distributed computing in the shared memory model, focusing entirely on the message-passing paradigm.
This book has evolved from the teaching method and the material I have designed for the fourth-year undergraduate course Introduction to Distributed Computing and for the graduate course Principles of Distributed Computing at Carleton University over the last 20 years, and for the advanced graduate courses on Distributed Algorithms I have taught as part of the Advanced Summer School on Distributed Computing at the University of Siena over the last 10 years.
I am most grateful to all the students of these courses: through their feedback they have helped me verify what works and what does not, shaping my teaching and thus the current structure of this book.
Their keen interest and enthusiasm over the years have been the main reason for the existence of this book.
I would welcome any feedback that will make it grow and mature and change.
Comments, criticisms, and reports on personal experience as a lecturer using the book, as a student studying it, or as a researcher glancing through it, suggestions for changes, and so forth: I am looking foreward to receiving any.
Clearly, reports on typos, errors, and mistakes are very much appreciated.
I tried to be accurate in giving credits; if you know of any omission or mistake in this regards, please let me know.
My own experience as well as that of my students leads to the inescapable conclusion that.
I welcome you to share this experience, and I hope you will reach the same conclusion.
The universe in which we will be operating will be called a distributed computing environment.
It consists of a finite collection E of computational entities communicating by means of messages.
Entities communicate with other entities to achieve a common goal; for example, to perform a given task, to compute the solution to a problem, to satisfy a request either from the user (i.e., outside the environment) or from other entities.
In this chapter, we will examine this universe in some detail.
The computational unit of a distributed computing environment is called an entity.
Depending on the system being modeled by the environment, an entity could correspond to a process, a processor, a switch, an agent, and so forth in the system.
Note that, although setting the alarm clock and updating the status register can be considered as a part of local processing, because of the special role these operations play, we will consider them as distinct types of operations.
The arrival of a message and the ringing of the alarm clock are the events that are external to the entity but originate within the system: The message is sent by another entity, and the alarm clock is set by the entity itself.
Unlike the other two types of events, a spontaneous impulse is triggered by forces external to the system and thus outside the universe perceived by the entity.
As an example of event generated by forces external to the system, consider an automated banking system: its entities are the bank servers where the data is stored, and the automated teller machine (ATM) machines; the request by a customer for a cash withdrawal (i.e., update of data stored in the system) is a spontaneous impulse for the ATM machine (the entity) where the request is made.
For another example, consider a communication subsystem in the open systems interconnection (OSI) Reference Model: the request from the network layer for a service by the data link layer (the system) is a spontaneous impulse for the data-link-layer entity where the request is made.
Appearing to entities as “acts of God,” the spontaneous impulses are the events that start the computation and the communication.
An action is indivisible (or atomic) in the sense that its operations are executed without interruption; in other words, once an action starts, it will not stop until it is finished.
An action is terminating in the sense that, once it is started, its execution ends within finite time.
Programs that do not terminate cannot be termed as actions.
A special action that an entity may take is the null action nil, where the entity does not react to the event.
Behavior The nature of the action performed by the entity depends on the nature of the event e, as well as on which status the entity is in (i.e., the value of status(x)) when the events occur.
The behavioral specification, or simply behavior, of an entity x is the set B(x) of all the rules that x obeys.
This set must be complete and nonambiguous: for every possible event e and status value s, there is one and only one rule in B(x) enabled by (s,e)
In other words, x must always know exactly what it must do when an event occurs.
The set of rules B(x) is also called protocol or distributed algorithm of x.
The behavioral specification of the entire distributed computing environment is just.
More precisely, the collective behavior B(E) of a collection E of entities is the set.
Thus, in an environment with collective behaviorB(E), each entity x will be acting (behaving) according to its distributed algorithm and protocol (set of rules) B(x)
This means that to specify a homogeneous collective behavior, it is sufficient to specify the behavior of a single entity; in this case, we will indicate the behavior simply by B.
This means that if we are in a system where different entities have different behaviors, we can write a new set of rules, the same for all of them, which will still make them behave as before.
Example Consider a system composed of a network of several identical workstations and a single server; clearly, the set of rules that the server and a workstation obey is not the same as their functionality differs.
Still, a single program can be written that will run on both entities without modifying their functionality.
We need to add to each entity an input register, my role, which is initialized to either “workstation” or “server,” depending on the entity; for each status–event pair (s, e) we create a new rule with the following action:
If (s, e) did not enable any rule for a workstation (e.g., s was a status defined only for the server), then Aworkstation = nil in the new rule; analogously for the server.
It is important to stress that in a homogeneous system, although all entities have the same behavioral description (software), they do not have to act in the same way;
An important consequence of the homogeneous behavior property is that we can concentrate solely on environments where all the entities have the same behavior.
From now on, when we mention behavior we will always mean homogeneous collective behavior.
In a distributed computing environment, entities communicate by transmitting and receiving messages.
The message is the unit of communication of a distributed environment.
In its more general definition, a message is just a finite sequence of bits.
The directed graph G = (V, E) describes the communication topology of the environment.
We shall denote by n( G),m( G), and d( G) the number of vertices, edges, and the diameter of G, respectively.
When no ambiguity arises, we will omit the reference to G and use simply n, m, and d.
In the following and unless ambiguity should arise, the terms vertex, node, site, and entity will be used as having the same meaning; analogously, the terms edge, arc, and link will be used interchangeably.
In summary, an entity can only receive messages from its in-neighbors and send messages to its out-neighbors.
Messages received at an entity are processed there in the order they arrive; if more than one message arrive at the same time, they will be processed in arbitrary order (see Section 1.9)
The definition of distributed computing environment with point-to-point communication has two basic axioms, one on communication delay, and the other on the local orientation of the entities in the system.
Any additional assumption (e.g., property of the network, a priori knowledge by the entities) will be called a restriction.
Communication Delays Communication of a message involves many activities: preparation, transmission, reception, and processing.
In real systems described by our model, the time required by these activities is unpredictable.
For example, in a communication network a message will be subject to queueing and processing delays, which change depending on the network traffic at that time; for example, consider the delay in accessing (i.e., sending a message to and getting a reply from) a popular web site.
The totality of delays encountered by a message will be called the communication delay of that message.
Axiom 1.3.1 Finite Communication Delays In the absence of failures, communication delays are finite.
In other words, in the absence of failures, a message sent to an out-neighbor will eventually arrive in its integrity and be processed there.
Note that the Finite Communication Delays axiom does not imply the existence of any bound on transmission, queueing, or processing delays; it only states that in the absence of failure, a message will arrive after a finite amount of time without corruption.
Local Orientation An entity can communicate directly with a subset of the other entities: its neighbors.
The only other axiom in the model is that an entity can distinguish between its neighbors.
Axiom 1.3.2 Local Orientation An entity can distinguish among its in-neighbors.
In particular, an entity is capable of sending a message only to a specific out-neighbor (without having to send it also to all other out-neighbors)
Also, when processing a message (i.e., executing the rule enabled by the reception of that message), an entity can distinguish which of its in-neighbors sent that message.
In general, a distributed computing system might have additional properties or capabilities that can be exploited to solve a problem, to achieve a task, and to provide a service.
This can be achieved by using these properties and capabilities in the set of rules.
However, any property used in the protocol limits the applicability of the protocol.
In other words, any additional property or capability of the system is actually a restriction (or submodel) of the general model.
When dealing with (e.g., designing, developing, testing, employing) a distributed computing system or just a protocol, it is crucial and imperative that all restrictions are made explicit.
Failure to do so will invalidate the resulting communication software.
The restrictions can be varied in nature and type: they might be related to communication properties, reliability, synchrony, and so forth.
In the following section, we will discuss some of the most common restrictions.
Communication Restrictions The first category of restrictions includes those relating to communication among entities.
Queueing Policy A link (x, y) can be viewed as a channel or a queue (see Section 1.9): x sending a message to y is equivalent to x inserting the message in the channel.
In general, all kinds of situations are possible; for example, messages in the channel might overtake each other, and a later message might be received first.
Different restrictions on the model will describe different disciplines employed to manage the channel; for example, first-in-first-out (FIFO) queues are characterized by the following restriction.
Message Ordering: In the absence of failure, the messages transmitted by an entity to the same out-neighbor will arrive in the same order they are sent.
Note that Message Ordering does not imply the existence of any ordering for messages transmitted to the same entity from different edges, nor for messages sent by the same entity on different edges.
Link Property Entities in a communication system are connected by physical links, which may be very different in capabilities.
With a fully duplex line it is possible to transmit in both directions.
A duplex line can obviously be described as two simplex lines, one in each direction; thus, a system where all lines are fully duplex can be described by the following restriction:
For example, in Figure 1.2 a graph G is depicted where the Bidirectional Links restriction and the corresponding undirected graph G hold.
Reliability Restrictions Other types of restrictions are those related to reliability, faults, and their detection.
Detection of Faults Some systems might provide a reliable fault-detection mechanism.
Following are two restrictions that describe systems that offer such capabilities in regard to component failures:
Restricted Types of Faults In some systems only some types of failures can occur: for example, messages can be lost but not corrupted.
More general restrictions will describe systems or situations where there will be no failures:
Guaranteed delivery: Any message that is sent will be received with its content uncorrupted.
Under this restriction, protocols do not need to take into account omissions or corruptions of messages during transmission.
Under this restriction, protocols do not need to take failures into account.
Note that under Partial Reliability, failures might have occurred before the execution of a computation.
A totally fault-free system is defined by the following restriction.
Total reliability: Neither have any failures occurred nor will they occur.
Clearly, protocols developed under this restriction are not guaranteed to work correctly if faults occur.
Topological Restrictions In general, an entity is not directly connected to all other entities; it might still be able to communicate information to a remote entity, using others as relayer.
A system that provides this capability for all entities is characterized by the following restriction:
That is, from every vertex in G it is possible to reach every other vertex.
In case the restriction “Bidirectional Links” holds as well, connectedness will simply state that G is connected.
Time Restrictions An interesting type of restrictions is the one relating to time.
In fact, the general model makes no assumption about delays (except that they are finite)
Unitary communication delays: In the absence of failures, the communication delay of any message on any link is one unit of time.
The general model also makes no assumptions about the local clocks.
Synchronized clocks: All local clocks are incremented by one unit simultaneously and the interval of time between successive increments is constant.
The computing environment we are considering is defined at an abstract level.
The efficiency of a protocol in the model must somehow reflect the realistic costs encountered when executed in those very different systems.
In other words, we need abstract cost measures that are general enough but still meaningful.
We will use two types of measures: the amount of communication activities and the time required by the execution of a computation.
They can be seen as measuring costs from the system point of view (how much traffic will this computation generate and how busy will the system be?) and from the user point of view (how long will it take before I get the results of the computation?)
The transmission of a message through an out-port (i.e., to an out-neighbor) is the basic communication activity in the system; note that the transmission of a message that will not be received because of failure still constitutes a communication activity.
Thus, to measure the amount of communication activities, the most common function used is the number of message transmissions M, also called message cost.
So in general, given a protocol, we will measure its communication costs in terms of the number of transmitted messages.
Messages are sequences of bits; some protocols might employ messages that are very short (e.g., O(1) bit signals), others very long (e.g., .gif files)
Thus, for a more accurate assessment of a protocol, or to compare different solutions to the same problem that use different sizes of messages, it might be necessary to use as a cost measure the number of transmitted bits B also called bit complexity.
An important measure of efficiency and complexity is the total execution delay, that is, the delay between the time the first entity starts the execution of a computation and the time the last entity terminates its execution.
Note that “time” is here intended as the one measured by an observer external to the system and will also be called real or physical time.
In the general model there is no assumption about time except that communication delays for a single message are finite in absence of failure (Axiom 1.3.1)
Thus, even in the absence of failures, the total execution delay for a computation is totally unpredictable; furthermore, two distinct executions of the same protocol might experience drastically different delays.
The measure usually employed is the ideal execution delay or ideal time complexity, T: the execution delay experienced under the restrictions “Unitary Transmission Delays” and “Synchronized Clocks;” that is, when the system is synchronous and (in the absence of failure) takes one unit of time for a message to arrive and to be processed.
A very different cost measure is the causal time complexity, Tcausal.
It is defined as the length of the longest chain of causally related message transmissions, over all possible executions.
Causal time is seldom used and is very difficult to measure exactly; we will employ it only once, when dealing with synchronous computations.
Let us clarify the concepts expressed so far by means of an example.
Consider a distributed computing system where one entity has some important information unknown to the others and would like to share it with everybody else.
This problem is called broadcasting and it is part of a general class of problems called information diffusion.
To solve this problem means to design a set of rules that, when executed by the entities, will lead (within finite time) to all entities knowing the information; the solution must work regardless of which entity had the information at the beginning.
Let E be the collection of entities and G be the communication topology.
To simplify the discussion, we will make some additional assumptions (i.e., restrictions) on the system:
Total reliability, that is, we do not have to worry about failures.
Observe that, if G is disconnected, some entities can never receive the information, and the broadcasting problem will be unsolvable.
Thus, a restriction that (unlike the previous two) we need to make is as follows:
Further observe that built in the definition of the problem, there is the assumption that only the entity with the initial information will start the broadcast.
Thus, a restriction built in the definition is as follows:
A simple strategy for solving the broadcast problem is the following:
Hence, the protocol achieves its goal and solves the broadcasting problem.
Consider, for example, the simple system with three entities x, y, z connected to each other (see Figure 1.3)
Let x be the initiator, y and z be idle, and all messages travel at the same speed; then y and z will be forever sending messages to each other (as well as to x)
This time the communication activities of the protocol terminate: Within finite time.
Note that depending on transmission delays, different executions are possible; one such execution in an environment composed of three entities x, y, z connected to each other, where x is the initiator as depicted in Figure 1.3
Note that entities terminate their execution of the protocol (i.e., become done) at different times; it is actually possible that an entity has terminated while others have not yet started.
This is something very typical of distributed computations: There is a difference between local termination and global termination.
Notice also that in this protocol nobody ever knows when the entire process is over.
We will examine these issues in details in other chapters, in particular when discussing the problem of termination detection.
The above set of rules correctly solves the problem of broadcasting.
Let us now calculate the communication costs of the algorithm.
First of all, let us determine the number of message transmissions.
Each entity, whether initiator or not, sends the information to all its neighbors.
Currently, when an idle entity receives the message, it will broadcast the information to all its neighbors, including the entity from which it had received the information; this is clearly unnecessary.
Recall that, by the Local Orientation axiom, an entity can distinguish among its neighbors; in particular, when processing a message, it can identify from which port it was received and avoid sending a message there.
The final protocol is as before with only this small modification.
This algorithm is called Flooding as the entire system is “flooded” with the message.
Let us examine now the ideal time complexity of flooding.
Let d(x, y) denote the distance (i.e., the length of the shortest path) between x and y.
We can, however, determine exactly the ideal time complexity in the worst case.
We are, however, also interested in describing the system during the computational activities, as well as after such activities.
To do so, we need to be able to describe the changes that the system undergoes over time.
As mentioned before, the entities (and, thus the environments) are reactive.
That is, any activity of the system is determined entirely by the external events.
In distributed computing environments, there are only three types of external events: spontaneous impulse (spontaneously), reception of a message (receiving), and alarm clock ring (when)
When an external event occurs at an entity, it triggers the execution of an action (the nature of the action depends on the status of the entity when the event occurs)
The executed action may generate new events: The operation send will generate a receiving event, and the operation set alarm will generate a when event.
Note first of all that the events so generated might not occur at all.
For example, a link failure may destroy the traveling message, destroying the corresponding receiving event; in a subsequent action, an entity may turn off the previously set alarm destroying the when event.
Summarizing, each event e is “generated” at some time t(e) and, if it occurs, it will happen at some time later.
By definition, all spontaneous impulses are already generated before the execution starts; their set will be called the set of initial events.
A Receiving event r is represented as an arrow from the point tx(r) in the temporal line of the entity x generating e (i.e., sending the message) to the point ty(r) in the temporal line of the entity y where the events occur (i.e., receiving the message)
Internal states change with time and the occurrence of events.
Assume now that also by accident, exactly the same event occurs at x (e.g., the alarm clock rings or the same message is received from the same neighbor)
Then x will perform exactly the same action in both cases, and its internal state will continue to be the same in both situations.
Similarly, if two entities have the same internal state, they cannot distinguish between each other.
Furthermore, if by accident, exactly the same event occurs at both of them (e.g., the alarm clock rings or the same message is received from the same neighbor), then they will perform exactly the same action in both cases, and their internal state will continue to be the same in both situations.
Remember: Internal states are local and an entity might not be able to infer from them information about the status of the rest of the system.
We have talked about the internal state of an entity, initially (i.e., at time t = 0) and during an execution.
Let us now focus on the state of the entire system during an execution.
To describe the global state of the environment at time t, we obviously need to specify the internal state of all entities at that time; that is, the set.
In fact, the execution so far might have already generated some events that will occur after time t; these events, represented by the set Future(t), are integral part of this execution and must be specified as well.
Specifically, the global state, called configuration, of the system during an execution is specified by the couple.
The configuration C(t) is like a snapshot of the system at time t.
The topic of this book is how to design distributed algorithms and analyze their complexity.
A distributed algorithm is the set of rules that will regulate the behaviors of the entities.
The reason why we may need to design the behaviors is to enable the entities to solve a given problem, perform a defined task, or provide a requested service.
In general, we will be given a problem, and our task is to design a set of rules that will always solve the problem in finite time.
Problems To give a problem (or task, or service) P means to give a description of what the entities must accomplish.
This is done by stating what the initial conditions of the entities are (and thus of the system), and what the final conditions should be; it should also specify all given restrictions.
For example, in the problem Broadcasting (I ) described in Section 1.5, the initial and final conditions are given by the predicates.
We call initial status values those values of S that can be held at the start of the execution of B(x) and we shall denote their set by SINIT.
By contrast, terminal status values are those values that once reached, cannot ever be changed by the protocol; their set shall be denoted by STERM.
All other values in S will be called intermediate status values.
Termination Protocol B terminates if, for all initial configurations C(0) satisfying PINIT, and for all executions starting from those configurations, the predicate.
We have already remarked on the fact that entities might not be aware that the termination has occurred.
In general, we would like each entity to know at least of its termination.
This situation, called explicit termination, is said to occur if the predicate.
Solution Protocol The set of rules B solves problem P if it always correctly terminates under the problem restrictions R.
As there are two types of termination (simple and explicit), we will have two types of solutions:
The notions of information and knowledge are fundamental in distributed computing.
Informally, any distributed computation can be viewed as the process of acquiring information through communication activities; conversely, the reception of a message can be viewed as the process of transforming the state of knowledge of the processor receiving the message.
The content of the local memory of an entity and the information that can be derived from it constitute the local knowledge of an entity.
Example (muddy forehead): Imagine n perceptive and intelligent school children playing together during recess.
They are forbidden to play in the mud puddles, and the teacher has told them that if they do, there will be severe consequences.
Each child wants to keep clean, but the temptation to play with mud is too great to resist.
As a result, k of the children get mud on their foreheads.
Each child in the room clearly understands that those with mud on their foreheads are “dead meat,” who will be punished no matter what.
As each child shares the same concern, the collective goal is for the children with clean foreheads not to confess and for those with muddy foreheads to go forward simultaneously, and all of this without communication.
The second, more complex question is as follows: can all the children with mud on their foreheads find out at the same time so that they can go forward together ? In other words, can the exact value of k become common knowledge ?
The children, being perceptive and intelligent, determine that the answer to both the questions is positive and find the way to achieve the common goal and thus common knowledge without communication (Exercise 1.12.6)
When working in a submodel, all the restrictions defining the submodel are common knowledge to all entities (unless otherwise specified)
We can have various types of knowledge, such as knowledge about the communication topology, about the labeling of the communication graph, about the input data of the communicating entities.
In general, if we have some knowledge of the system, we can exploit it to reduce the cost of a protocol, although this may result in making the applicability of the protocol more limited.
A type of knowledge of particular interest is the one regarding the communication topology (i.e., the graph G)
In fact, as will be seen later, the complexity of a computation may vary greatly depending on what the entities know about G.
Following are some elements that, if they are common knowledge to the entities, may affect the complexity.
Topological Properties: knowledge of some properties of the topology; for example, “ G is a ring network,” “ G does not have cycles,” “ G is a Cayley graph,” etcetera.
As a topological map provides all possible metric and structural information, this type of knowledge is very powerful and important.
The strongest form of this type is full topological knowledge: availability at each entity of a labeled graph isomorphic to ( G,l), the isomorphism, and its own image, that is, every entity has a complete map of (v, l) with the indication, “You are here.”
What is very important is whether the labeling has some global consistency property.
We can distinguish two other types, depending on whether the knowledge is about the (input) data or the status of the entities and of the system, and we shall call them type-D and type-S, respectively.
Examples of type-D knowledge are the following: Unique identifiers: all input values are distinct; Multiset: input values are not necessarily identical; Size: number of distinct values.
Examples of type-S knowledge are the following: System with leader: there is a unique entity in status “leader”; Reset: all nodes are in the same status; Unique initiator: there is a unique entity in status “initiator.” For example, in the broadcasting problem we discussed in Section 1.5, this knowledge was assumed as a part of the problem definition.
The content of a message obviously depends on the application; in any case, it consists of a finite (usually bounded) sequence of bits.
The message is typically divided into subsequences, called fields, with a predefined meaning (“type”) within the protocol.
The examples of field types are the following: message identifier or header used to distinguish between different types of messages; originator and destination fields used to specify the (identity of the) entity originating this message and of the entity to whom the message is intended for; data fields used to carry information needed in the computation (the nature of the information obviously depends on the particular application under consideration)
If (the limit on) the size of a message is a system parameter (i.e., it does not depend on the particular application), we say that the system has bounded messages.
Such is, for example, the limit imposed on the message length in packet-switching networks, as well as on the length of control messages in circuit-switching networks (e.g., telephone networks) and in message-switching networks.
We have already introduced in Section 1.5 most of the notation for describing those rules.
Let us now complete the description of the notation we will use for protocols.
As a consequence, if no rule is described for a (status,event) pair, the default will be that the pair enables the Null action.
Although convenient (it simplifies the writing), the use of this convention must generate extra care in the description: If we forget to write a rule for an event occurring in a given status, it will be assumed that a rule exists and the action is nil.
If an action contains a change of status, this operation will be the last one before exiting the action.
The set of status values of the protocol, and the set of restrictions under which the protocol operates will be explicit.
Precedence The external events are as follows: spontaneous impulse (Spontaneously), reception of a message (Receiving), and alarm clock ring (When)
Different types of external events can occur simultaneously; for example, the alarm clock might ring at the same time a message arrives.
To determine the order in which they will be processed, we will use the following precedence between external events:
At most one spontaneous impulse can always occur at an entity at any one time.
As there is locally only one alarm clock, at any time there will be at most one When event.
By contrast, it is possible that more than one message arrive at the same time to an entity from different neighbors; should this be the case, these simultaneous.
Receiving events have all the same precedence and will be processed sequentially in an arbitrary order.
The communication mechanisms of a distributed computing environment must handle transmissions and arrivals of messages.
The mechanisms at an entity can be seen as a system of queues.
The sets Nin and Nout will in practice consist of the port numbers associated to those neighbors; this is because an entity has no other information about its neighbors (unless we add restrictions)
The command “send M to W” will have a copy of the message M sent through each of the out-ports specified by W.
When a message M is sent through an out-port l, it is inserted in the corresponding queue.
In absence of failures (recall the Finite Communication Delays axiom), the communication mechanism will eventually remove it from the queue and deliver it to the other entity through the corresponding in-port, generating the Receiving (M) event; at that time the variable sender will be set to l.
Entity’s Capability: Local processing, local storage, access to a local clock, and.
Entity’s Status Register: At any time an entity status register has a value from a.
External Events: Arrival of a message, alarm clock ring, and spontaneous impulse.
Actions: An action is an indivisible (i.e., uninterruptible) finite sequence of operations (local processing, message transmission, change of status, and setting of alarm clock)
Homogeneous System: A system is homogeneous if all the entities have the same behavior.
Neighbors: The in-neighbors of an entity are those entities from which x can receive a message directly; the out-neighbors are those to which x can send a message directly.
Communication Topology: The directed graph G = (V,E) defined by the neighborhood relation.
If the Bidirectional Links restriction holds, then G is undirected.
Axioms: There are two axioms: local orientation and finite communication delays.
Local Orientation: An entity can distinguish between its out-neighbors and its.
Finite Communication Delays: In absence of failures, a message eventually arrives.
Several attempts have been made to derive formalisms capable of describing both distributed systems and computations performed in such systems.
The models proposed for systems of concurrent processes do provide both a formalism for describing a distributed computation and a proof system that.
Other models, whose intended goal is still to provide a proof system, have been specifically tailored for distributed computations.
In particular, the Input–Output Automata model of Nancy Lynch and Mark Tuttle [4] provides a powerful tool that has helped discover and fix “bugs” in well-known existing protocols.
For the investigators involved in the design and analysis of distributed algorithms, the main concern rests with efficiency and complexity; proving correctness of an algorithm is a compulsory task, but it is usually accomplished using traditional mathematical tools (which are generally considered informal techniques) rather than with formal proof systems.
The formal models of computation employed in these studies, as well as in the one used in this book, mainly focus on those factors that are directly related to efficiency of a distributed computation and complexity of a distributed problem: the underlining communication network, the communication primitives, the amount and type of knowledge available to the processors, etcetera.
Modal logic, and in particular the notion of common knowledge, is a useful tool to reason about distributed computing environments in presence of failures.
The notion of knowledge used here was developed independently by Joseph Halpern and Yoram Moses [2], Daniel J.
The model we have described and will employ in this book uses reactive entities (they react to external stimuli)
Several formal models (including input–output Automata) use instead active entities.
To understand this fundamental difference, consider a message in transit toward an entity that is expecting it, with no other activity in the system.
In an active model, the entity will attempt to receive the message, even while it is not there; each attempt is an event; hence, this simple situation can actually cause an unpredictable number of events.
By contrast, in a reactive model, the entity does nothing; the only event is the arrival of the message that will “wake up” the entity and trigger its response.
Using the analogy of waiting for the delivery of a pizza, in the active model, you (the entity) must repeatedly open the door (i.e., act) to see if the person supposed to deliver the pizza has arrived; in the reactive model, you sit in the living room until the bell rings and then go and open the door (i.e., react)
The two models are equally powerful; they just represent different ways of looking at and expressing the world.
It is our contention that at least for the description and the complexity analysis of protocols and distributed algorithms, the reactive model is more expressive and simpler to understand, to handle, and to use.
Exercise 1.12.5 Show that any protocol B can be rewritten so that SSTART consists of only one status.
Show that, within finite time, all the children with a muddy forehead can simultaneously determine that they are not clean.
Exercise 1.12.7 Half-duplex links allow communication to go in both directions, but not simultaneously.
Design a protocol that implements half-duplex communication between two connected entities, a and b.
Exercise 1.12.8 Half-duplex links allow communication to go in both directions, but not simultaneously.
Design a protocol that implements half-duplex communication between three entities, a, b and c, connected to each other.
Answer to Exercise 1.12.2 The total number of messages sent without the improvement was.
In this case, z will see that everyone else has a clean forehead; as the teacher has said that at least one child has a dirty forehead, z knows that he/she must be the one.
Notice that a clean child sees that z is dirty but finds out that his/her own forehead is clean only when z goes forward.
Consider now the case k = 2: There are two dirty children, a and b; a sees the dirty forehead of b and the clean one of everybody else.
Clearly he/she does not know about his status; he/she knows that if he/she is clean, b is the only one who is dirty and will go forward when the teacher arrives.
So, when the teacher comes and b does not go forward, a understands that his/her forehead is also dirty.
Thus, when the teacher returns the second time, both a and b go forward.
Some of these problems (e.g., broadcast and traversal), by their nature, are started by a single entity; in other words, these computational problems have, in their definition, the restriction unique initiator (UI)
Other problems (e.g., wake-up and spanningtree construction) have no such restriction.
The computational differences created by the additional assumption of a single initiator can be dramatic.
Their fundamental importance derives from the fact that most global problems (i.e., problems that, to be solved, require the involvement of all entities), oftentimes can be correctly, easily, and efficiently solved by designing a protocol for trees and executing it on a spanning-tree of the network.
The techniques we introduce in this chapter to solve these problems are basic ones; once properly understood, they form a powerful and an essential toolset that can be effectively employed by every designer of distributed algorithms.
Consider a distributed computing system where only one entity, x, knows some important information; this entity would like to share this information with all the other entities in the system; see Figure 2.1
To solve this problem means to design a set of rules that, when executed by the entities, will lead (within finite time) to a configuration where all entities will know the information; the solution must work regardless of which entity has the information at the beginning.
Built-in the definition of the problem, there is the assumption, Unique Initiator (UI), that only one entity will start the task.
Actually, this assumption is further restricted, because the unique initiator must be the one with the initial information; we shall denote this restriction by UI+
As we have seen, the solution protocol Flooding uses O(m) messages and, in the worst case, O(d) ideal time units, where d is the diameter of the network.
The first and natural question is whether these costs could be reduced significantly (i.e., in order of magnitude) using a different approach or technique, and if so, by how much.
This question is equivalent to ask what is the complexity of the broadcasting problem.
To answer this type of questions we need to establish a lower bound: to find a bound f (typically, a function of the size of the network) and to prove that the cost of every solution algorithm is at least f.
In other words, a lower bound is needed irrespective of the protocol, and it depends solely on the problem; hence, it is an indication of how complex the problem really is.
A lower bound on the amount of ideal time units required to perform a broadcast is simple to derive: Every entity must receive the information regardless of how distant they are from the initiator, and any entity could be the initiator.
The fact that Flooding performs the broadcast in d ideal time units means that the lower bound is tight (i.e., it can be achieved) and that Flooding is time optimal.
In other words, we know exactly the ideal time complexity of broadcasting:
With a little extra effort, we can derive a more accurate lower bound:
The results we have obtained so far apply to generic solutions; that is, solutions that do not depend on G and can thus be applied regardless of the communication topology (provided it is undirected and connected)
Next, we will consider performing the broadcast in special networks.
This cost is achieved even if the entities do not know that the network is a tree.
An interesting side effect of broadcasting on a tree is that the tree becomes rooted in the initiator of the broadcast.
Broadcasting in Oriented Hypercubes A communication topology that is commonly used as an interconnection network is the (k-dimensional) labeled hypercube, denoted by Hk.
This labeling l of the links is symmetric (i.e., lx(x, y)= ly(x, y)) and is called the.
These names are used only for descriptive purposes; they are not known to the entities.
By contrast, the labels of the links (i.e., the port numbers) are known to the entities by the Local Orientation axiom.
The only difference between HyperFlood and the normal Flooding is in step 2: Instead of sending the message to all neighbors except the sender, the entity will forward it only to some of them, which will depend on the label of the port from where the message is received.
LetHk(x) denote the subgraph ofHk induced by the links where messages are sent by HyperFlood when x is the initiator.
To prove that every entity will receive the information sent by x, we need to show that, for every node y, there is a path from x to y such that the sequence of the labels on the path from x to y is decreasing.
Note that the labels on the path do not need to be consecutive integers.
To do so we will use the following property of hypercubes.
The use of a generic protocol will require O(n2) messages.
Very often, in a distributed environment, we are faced with the following situation: A task must be performed in which all the entities must be involved; however, only some of them are independently active (because of a spontaneous event, or having finished a previous computation) and ready to compute, the others are inactive, not even aware of the computation that must take place.
In these situations, to perform the task, we must ensure that all the entities become active.
Clearly, this preliminary step can only be started by the entities that are active already; however, they do not know which other entities (if any) are already active.
This problem is called Wake-up (Wake-Up): An active entity is usually called awake, an inactive (still) one is called asleep; the task is to wake all entities up; see Figure 2.4
It is not difficult to see the relationship between broadcasting and wake-up: Broadcast is a wake-up with only one initially awake entity; conversely, wake-up is a broadcast with possibly many initiators (i.e., initially more than one entity has the information)
In other words, broadcast is just a special case of the wake-up problem.
Interestingly, but not surprisingly, the flooding strategy used for broadcasting actually solves the more general Wake-Up problem.
The modified protocol, called WFlood, is described in Figure 2.5
Initially all entities are asleep; any asleep entity can become spontaneously awake and start the protocol.
It is not difficult to verify that the protocol correctly terminates under the standard restrictions (Exercise 2.9.7)
The number of messages is at least equal to that of broadcast; actually, it is not much more (see Exercise 2.9.6):
As broadcast is a special case of wake-up, not much improvement is possible (except perhaps in the size of the constant):
The ideal time will, in general, be smaller than the one for broadcast:
However, in the case of a single initiator, the two cases coincide.
As upper and lower bounds coincide in order of magnitude, we can conclude that protocol WFlood is both message and, worst case in the time optimal.
Let us see if we can achieve a similar result also for the wake-up.
In other words, can we exploit the properties of a labeled hypercube to do better than generic protocols?
As a consequence, we might as well employ the generic protocol WFlood, which uses O(n log n) messages.
Let us see if, by exploiting the properties of complete graphs, we have been able to construct a wake-up protocol that uses only O(n) messages, instead of the O(n2) we have achieved so far.
After all, we have been able to do it in the case of the broadcast problem.
This implies that the use of WFlood for wake-up is a message-optimal solution.
Complete Graphs with ID To reduce the number of messages, a more restricted environment is required; that is, we need to make additional assumptions.
To see why this is true, we will construct a “bad” but possible case, which any protocol can encounter, and show that, in such a case, O(n log n) messages will be exchanged.
The lower bound will hold even if there is message ordering.
For simplicity of discussion and calculation, we will assume that n is a power of 2; the results hold also if this is not the case.
To construct the “bad” case for an (arbitrary) solution protocol A, we will consider a game between the entities on one side and an adversary on the other: the entities obey the rules of the protocol; the adversary will try to make the worst possible scenario occur, so, to force the use of as many messages as possible.
Sending a message to more than one port will be treated as sending the message to each of those ports one at a time (in an arbitrary order)
Whatever the adversary decides, it can happen in a real execution.
Let us see how bad a case can the adversary create for A.
Two sets of entities will be said to be connected at a time t if at least a message has been transmitted from an entities of one set to an entity of the other.
From now on, the adversary will act in a similar way; always ensure that messages are sent to already-awake nodes, and that the set of awake nodes is connected.
Consider an entity x executing a send operation to an unassigned label a.
In other words, the adversary will always try to make the awake entities send messages to other awake entities.
The adversary will then assign label a to the link connecting x to its corresponding entity z in the new set; the message will be held in transit until z (like x did) will need to transmit a message on an unused link (say, with label b) but all the edges connecting it to its set of awake entities have already been used.
When this happens, the adversary will assign the label b to the link from z to x and make the two messages between x and z arrive and be processed.
Let us summarize the strategy of the adversary: The adversary tries to force the protocol to send messages only to already-awake entities and awakens new entities only when it cannot do otherwise; the newly awake entities are equal in number to the already awake entities; and they are forced by the adversary to have the same execution between them as did the other entities before any communication takes place between the two sets.
When this happens, we will say that the adversary has started a new stage.
Let us now examine the situations created by the adversary with this strategy and analyze the cost of the protocol in the corresponding executions.
Once the communication takes place, how many messages (including those two) are transmitted before the next stage?
More efficient wake-up protocols can be derived if we have in our system a “good” labeling of the links instead.
Traversal of the network allows every entity in the network to be “visited” sequentially (one after the other)
Its main uses are in the control and management of a shared resource and in sequential search processes.
In abstract terms, the traversal problem starts with an initial configuration where all entities are in the same state (say unvisited) except the one that is visited and is the sole initiator; the goal is to render all the entities visited but sequentially (i.e., one at the time)
A traversal protocol is a distributed algorithm that, starting from the single initiator, allows a special message called “traversal token” (or simply, token), to reach every.
Once a node is reached by the token, it is marked as “visited.” Depending on the traversal strategy employed, we will have different traversal protocols.
A well known strategy is the depth-first traversal of a graph.
According to this strategy, the graph is visited (i.e., the token is forwarded) trying to go forward as long as possible; if it is forwarded to an already visited node, it is sent back to the sender, and that link is marked as a back-edge; if the token can no longer be forwarded (it is at a node where all its neighbors have been visited), the algorithm will “backtrack” until it finds an unvisited node where the token can be forwarded to.
When first visited, an entity remembers who sent the token, creates a list of all its still unvisited neighbors, forwards the token to one of them (removing it from the list), and waits for its reply returning the token.
When the neighbor receives the token, it will return the token immediately if it had been visited already by somebody else, notifying that the link is a backedge; otherwise, it will first forward the token to each of its unvisited neighbors sequentially, and then reply returning the token.
Upon the reception of the reply, the entity forwards the token to another unvisited neighbor.
Should there be no more unvisited neighbors, the entity can no longer forward the token; it will then send the reply, returning the token to the node from which it first received it.
When the neighbor in step (2) determines that a link is a back-edge , it knows that the sender of the token is already visited; thus, it will remove it from the list of unvisited neighbors.
We will use three types of messages: “T” to forward the token in the traversal, “Backedge” to notify the detection of a back-edge, and “Return” to return the token upon local termination.
To determine how efficient is the protocol, we are going to determine what is the complexity of the problem.
Therefore, the 2mmessage cost of protocol DF Traversal is indeed excellent, and the protocol is message optimal.
The time requirements of a depth-first traversal are quite different from those of a broadcast.
In fact, since each node must be visited sequentially, starting from the sole initiator, the time complexity is at least the number of nodes:
When measuring ideal time, we consider only synchronous executions; however, when measuring messages and establishing correctness we must consider every possible schedule of events, especially the nonsynchronous executions.
Basic Hacking The protocol we have constructed is totally sequential: in a synchronous execution, at each time unit only one message will be sent, and every message requires one unit of time.
By definition of traversal, each entity must receive the token (message T) at least once.
In the execution of our protocol, however, some entities receive it more than once; those links from which these other T messages arrive are precisely the backedges.
To answer this question we must understand why T messages are sent on back-edges.
When an entity x sends a T message to y, it does not know whether the link is a back-edge or not; that is, whether y has already been visited by somebody else or not.
If x knew which of its neighbors are already visited, it would not send a T message to them, there would be no need for Backedge messages from them, and we would be saving messages and time.
Suppose that, whenever a node is visited (i.e., it receives T) for the first time, it notifies all its (other) neighbors of this event (e.g., sending a “Visited” message) and waits for an acknowledgment (e.g., receiving an “Ack” message) from them before forwarding the token.
The consequence of such a simple act is that now an entity ready to forward the token (i.e., to send a T message) really knows which of its neighbors have already been visited.
The price we have to pay is the transmission of the Visited and Ack messages.
Notice that now an idle entity (that is an entity that has not yet been involved in the traversal) might receive a Visited message as its first message.
In the revised protocol, we will make such an entity enter a new status, available.
Let us examine the effects of this change on the overall time cost of the protocol; call DF+ the resulting protocol.
The time is really determined by the number of sequential messages.
There are four types of messages that are sent: T, Return, Visited, and Ack.
To determine how many ideal time units are added by the transmission of Visited and Ack messages, consider an entity: its transmission of all the Visited messages takes only a single time unit, since they are sent concurrently; the corresponding Ack messages will also be sent concurrently, adding an additional time unit.
Since every node will do it, the sending of the Visited messages and receiving the Ack messages will increase the ideal time of the original algorithm by exactly 2n.
Summarizing, we have been able to reduce the time costs from O(m) to O(n) that, because of Theorem 2.3.2, is optimal.
The price has been the doubling of the number of messages.
Advanced Hacking Let us see if the number of messages can be decreased without significantly increasing the time costs.
But the algorithm now is rather different (we are using Visited messages, no longer Backedge messages) and this situation might not happen all the time.
Although the correctness will not be affected (Exercise 2.9.15), mistakes cost additional messages.
Let us examine what is really the cost of this modified protocol, which we shall call DF++
Then there are the “mistakes”; each mistake costs one message.
We have an improvement in that the Ack messages are no longer sent, saving n time units.
As there are no more Ack to wait for, an entity can forward the token at the same time as the transmission of the Visited messages; if it does not have any unvisited neighbor to send the T to, the entity will send the Return at the same time as the Visited.
Hence, the sending of the Visited is done in overlap with the sending of either a T or a Return message, saving another n time units.
Strange as it might sound, when we attempt to measure the ideal execution time of this protocol, in the execution no mistakes will ever occur.
This is because mistakes can only occur owing to arbitrarily long communication delays; on the contrary, ideal time is only measured under unitary delays.
It is crucial to understand this inherent limit of the cost measure we call ideal time.
Unlike the number of messages, ideal time is not a “neutral” measure; it influences (thus limiting) the nature of what we want to measure.
In other words, it should be treated and handled with caution.
Even greater caution should be employed in interpreting the results it gives.
Extreme Hacking As we are on a roll, let us observe that we could actually use the T message as an implicit Visited, saving some additional messages.
The value of f', unlike n and m, is not a system parameter.
Rings In a ring network, every node has exactly two neighbors.
Depth-first traversal in a ring can be achieved in a simple way: the initiator chooses one direction and the token is just forwarded along that direction; once the token reaches the initiator, the traversal is completed.
In other words, each entity will send and receive a single T message.
Hence both the time and the message costs are exactly n.
Traversal as Access Permission The main use of a traversal protocol is in the control and management of shared resources.
For example, access to a shared transmission medium (e.g., bus) must be controlled to avoid collisions (simultaneous frame transmission by two or more entities)
A typical mechanism to achieve this is by the use of a control (or permission) token.
This token is passed from one entity to another according to the same set of rules.
An entity can only transmit a frame when it is in possession of the token; once the frame has been transmitted, the token is passed to another entity.
A traversal protocol by definition “passes” the token sequentially through all the entities and thus solves the access control problem.
The only proviso is that, for the access permission problem, it must be made continuous: once a traversal is terminated, another must be started by the initiator.
The access permission problem is part of a family of problems commonly called Mutual Exclusion, which will be discussed in details later in the book.
Traversal as Broadcast It is not difficult to see that any traversal protocol solves the broadcast problem: the initiator puts the information in the token message; every entity will be visited by the token and thus will receive the information.
The converse is not necessarily true; for example, Flooding violates the sequentiality requirement since the message is sent to all (other) neighbors simultaneously.
The use of traversal to broadcast does not lead to a more efficient broadcasting protocol.
This is not surprising since a traversal is constrained to be sequential; flooding, by contrast, exploits concurrency at its outmost.
We have considered three basic problems (broadcast, wake-up, and depth-first traversal) and studied their complexity, devised solution protocols and analyzed their efficiency.
Let us see what the theoretical results we have obtained tell us about the situation from a practical point of view.
Summarizing, the cost of broadcasting, wake-up, and traversal depends on the number of links: The more links the greater the cost; and it can be as bad as O(n2) messages per execution of any of the solution protocols.
This result is punitive for networks where a large investment has been made in the construction of communication links.
As broadcast is a basic communication tool (in some systems, it is a primitive one) dense networks are penalized continuously.
Similarly, larger operating costs will be incurred by dense networks every time a wake-up (a very common operation, used as preliminary step in most computations) or a traversal (fortunately, not such a common operation) is performed.
The theoretical results, in other words, indicate that investments in communication hardware will result in higher operating communication costs.
Obviously, this is not an acceptable situation, and it is necessary to employ some “lateral thinking.”
Which connected spanning subnet of G should we construct? If we want to minimize the message costs, we should choose the one with the.
So, the strategy for a general graph G will be.
Spanning-tree construction (SPT) is a classical problem in computer science.
In a distributed computing environment, the solution of this problem has, as we have seen, strong practical motivations.
In a distributed computing environment, to construct a spanning tree of G means to move the system from an initial system configuration, where each entity is just aware of its own neigbors, to a system configuration where.
What is wanted is a distributed algorithm (specifying what each node has to do when receiving a message in a given status) such that, once executed, it guarantees that a spanning tree T(G) of G has been constructed; in the following we will indicate T(G) simply by T, if no ambiguity arises.
Note that T is not known a priori to the entities and might not be known after it has been constructed: an entity needs to know only which of its neighbors are also its neighbors in the spanning tree T.
As before, we will restrict ourselves to connected networks with bidirectional links and further assume that no failure will occur.
We will then consider the general problem when any number of entities can independently start the construction.
As we will see, the situation changes dramatically from the single-initiator scenario.
Consider the entities; they do not know G, not even its size.
The only things an entity is aware of are the labels on the ports leading to its neighbors (because of the Local Orientation axiom) and the fact that, if it sends a message to a neighbor, the message will eventually be received (because of the Finite Communication Delays axiom and the Total Reliability restriction)
How, using just this information, can a spanning tree be constructed? The answer is surprisingly simple.
An entity x = s will reply “Yes” only the first time it is asked and, in this occasion, it will ask all its other neighbors; otherwise, it will reply “No.” The initiator s will always reply “No.”
Each entity terminates when it has received a reply from all neighbors to which it asked the question.
For an entity x, its neighbors in the spanning tree T are the neighbors that have replied “Yes” and, if x = s, also the neighbor from which the question was first asked.
The corresponding set of rules is depicted in Figure 2.11 where in bold are shown the tree links and in dotted lines the nontree links.
The protocol Shout implementing this strategy is shown in Figure 2.12
Initially, all nodes are in status idle except the sole initiator.
Before we discuss the correctness and the efficiency of the protocol, consider how it is structured and operates.
First of all observe that, in Shout the question Q is broadcasted through the network (using flooding)
Further observe that, when an entity receives Q, it always sends a reply (either Yes or No)
Summarizing, the structure of this protocol is a flood where every information message is acknowledged.
This type of structure will be called Flood + Reply.
Correctness Let us now show that Flood + Reply, as used above, always constructs a spanning tree; that is, the graph defined by all the Tree-neighbors computed by the entities forms a spanning tree of G; furthermore, this tree is rooted in the initiator s.
This protocol consists of the flooding of Q, where every Q message is acknowledged.
Because of the correctness of flooding, we are guaranteed that every entity will receive Q and by construction will reply (either Yes or No) to each Q it receives.
The execution of protocol Shout ends with local termination: each entity knows when its own execution is over; this occurs when it enters status done.
Notice however that no entity, including the initiator, is aware of global termination (i.e., every entity has locally terminated)
Should we need the initiator to know that the execution has terminated (e.g., to start another task), Flood+ Reply can be easily modified to achieve this goal (Exercise 2.9.24)
Costs The message costs of Flood+Reply, and thus of Shout, are simple to analyze.
As mentioned before, Flood+Reply consists of an execution of Flooding(Q) with the addition of a reply (either Yes or No) for every Q.
The time costs of Flood+Reply, and thus of Shout, are also simple to determine; in fact (Exercise 2.9.21):
The efficiency of protocol Shout can be evaluated better taking into account the complexity of the problem it is solving.
Since every node must be involved, using an argument similar to the proof of Theorem 2.1.1, we have:
This implies that protocol Shout is both time optimal and message optimal with respect to order of magnitude.
In the case of the number of messages some improvement might be possible in terms of the constant.
Hacking Let us examine protocol Shout to see if it can be improved, thereby, helping us to save some messages.
When constructing the spanning tree, an entity needs to know who its tree-neighbors are; by construction, they are the ones that reply Yes and, except for the initiator, also.
Thus, for this determination, the No messages are not needed.
On the contrary hand, the No messages are used by the protocol to terminate in finite time.
Consider an entity x that just sent Q to neighbor y; it is now waiting for a reply.
If the reply is Yes, it knows y is in the tree; if the reply is No, it knows y is not.
Should we remove the sending of No–how can x determine that y would have sent No?
More clearly: Suppose x has been waiting for a reply from y for a (very) long time; it does not know if y has sent Yes and the delays are very long, or y would have sent No and thus will send nothing.
Because the algorithm must terminate, x cannot wait forever and has to make a decision.
The question is relevant because communication delays are finite but unpredictable.
Fortunately, there is a simple answer to the question that can be derived by examining how protocol Shout operates.
Focus on a node x that just sent Q to its neighbor y.
Summarizing, if y replies No to x, it must have already sent Q to x.
We can clearly use this fact to our advantage: after x sent Q to y, if it receives Yes it knows that y is its neighbor in the tree; if it receives Q, it can deduce that y will definitely reply No to x’s question.
All of this can be deduced by x without having received the No.
In other words: a message Q that arrives at a node waiting for a reply can act as an implicit negative acknowledgment; therefore, we can avoid sending No messages.
Let us now analyze the message complexity of the resulting protocol Shout+
The df-tree is obtained by removing the back-edges from G (i.e., the edges where a Back-edge message was sent in DF Traversal)
In other words, the tree-neighbors of an entity x will be those from which it receives a Return message and, if x is not the initiator, the one from which x received the first T.
Notice that these modifications involve just local bookkeeping and no.
Notice that, like in protocol Shout, all entities will become aware of their local termination, but only the initiator will be aware of global termination, that is, that the construction of the spanning tree has been completed (Exercise 2.9.27)
As a consequence, it would appear that, to solve SPT, we just need to execute a broadcast algorithm without any real modification, just adding some local variables (Tree-neighbors) and doing some local bookkeeping.
This is generally not the case; in fact, knowing its parent in the tree is not enough for an entity.
To solve SPT, when an entity x terminates its execution, it must explicitly know which neighbors are its children as well as which neighbor are not its treeneighbors.
If not provided already by the protocol, this information can obviously be acquired.
For example, if every entity sends a notification message to its parent, the parents will.
To find out which neighbors are not children is more difficult and will depend on the original broadcast protocol.
In protocol Shout this is achieved by adding the “Yes” (I am your child) and “No” (I am not your child) messages to Flooding.
In DF Traversal protocol this is already achieved by the “Return” (I am your child) and the “Backedge” (I am not your child) messages; so, no additional communication is required.
This fact establishes a computational relationship between the broadcasting problem and the spanning-tree construction problem.
If I know how to broadcast, then (with minor modifications) I know how to construct a spanning tree with a unique initiator.
The converse is also trivially true: Every protocol that constructs a spanning tree solves the broadcasting problem.
We shall say that these two problems are computationally equivalent and denote this fact by.
Since, as we have discussed in section 2.3.4, every traversal protocol performs a broadcast, it follows that, under RI, the execution of any traversal protocol constructs a spanning tree.
Call a problem global if every entity must participate in its solution; participation implies the execution of a communication activity: transmission of a message and/or arrival of a message (even if it triggers only the Null action, i.e., no action is taken)
We have seen how, with few more messages than those required by flooding and the same messages as a df-traversal, we can actually construct a spanning tree.
As discussed previously, once such a tree is constructed, we can from now on perform broadcast and traversal using only O(n) messages (which is optimal) instead of O(m) (which could be as bad as O(n2))
It is even possible that the same protocol constructs different spanning trees when executed at different times.
This is for example the case of Shout: Because communication delays are unpredictable, subsequent executions of this algorithm on the same graph may result in different spanning trees.
Prior to its execution, it is impossible to predict which spanning tree will be constructed; the only guarantee is that Shout will construct one.
In general, the trees constructed by depth-first traversal have usually terrible diameters.
The ones generated by Shout usually perform better, but there is no guarantee on the diameter of the resulting tree.
The other ingredient we need is a breadth-first spanning tree (bf-tree)
A breadthfirst spanning tree of G rooted in a node u, denoted by BFT(u,G), has the following property: The distance between a node v and the root in the tree is the same as their distance in the original graph G.
This strategy will construct the desired broadcast tree (Exercise 2.9.29):
To be implemented, this strategy requires that we solve two problems: Center Finding and Breadth-First Spanning-Tree Construction.
These problems, as we will see, are not simple to solve efficiently; we will examine them in later chapters.
In Section 2.4, we have discussed the general strategy Use-a-Tree for problem solving.
Now that we know how to construct a spanning tree (using a single initiator), let us apply the strategy to a known problem.
Using the Use-a-Tree strategy, we can produce an efficient traversal protocol that is much simpler than all the algorithms we have considered before:
Construct, using Shout+, a spanning-tree T rooted in the initiator.
In other words, SmartTraversal not only is simple but also has optimal time and message complexity.
We have started examining the spanning-tree construction problem in Section 2.5 assuming that there is a unique initiator.
This is unfortunately a very strong (and “unnatural”) assumption to make, as well as difficult and expensive to guarantee.
What happens to the single-initiator protocols Shout and df-SPT if there is more than one initiator?
Let both x and y be initiators and start the protocol, and let the Q message from x to z arrive there before the one sent by y.
In this case, neither the link (x,y) nor the link (y,z) will be included in the tree; hence, the algorithm creates not a spanning tree but a spanning forest, which is not connected.
Let us examine its execution in the simple network depicted in Figure 2.14 composed of a chain of four nodes x, y, z, and w.
Let y and z be both initiators, and start the traversal by sending the T message to x and w, respectively.
Also in this case, the algorithm will create a disconnected spanning forest of the graph.
The failure of these algorithms is not surprising, as they were developed specifically for the restricted environment of a Unique Initiator.
Removing the restriction brings out the true nature of the problem, which, as we will now see, has a formidable obstacle.
Our goal is to design a spanning-tree protocol, which works solely under the standard assumptions and thus is independent of the number of initiators.
Unfortunately, any design effort to this end is destined to fail.
Theorem 2.5.6 The SPT problem is deterministically unsolvable under R.
Deterministically unsolvable means that there is no deterministic protocol that always correctly terminates within finite time.
To see why this is the case, consider the simple system composed of three entities x, y, and z connected by links labeled as shown in Figure 2.15
Let the three entities have identical initial values (the symbols x, y, z are used only for description purposes)
If a solution protocol A exists, it must work under any conditions of message delays (as long as they are finite) and regardless of the number of initiators.
Consider a synchronous schedule (i.e., an execution where communication delays are unitary) and let all three entities start the execution of A simultaneously.
Since they are in identical states (same initial status and values, same port labels), they will execute the.
In other words, by Property 1.6.2, they will remain in identical states.
In the next time unit, all sent messages (if any) will arrive and be processed.
If one entity receives a message, the others will receive the same message at the same time, perform the same local computation, compose and send (if any) the same messages, and enter the same (possibly new) status.
In other words, the entities will continue to be in identical states.
If A is a solution protocol, it must terminate within finite time.
A spanning tree of our simple system is obtained by removing one of the three links, let us say (x,y)
In other words, when they all terminate, they have distinct values for their local variable Tree-neighbors.
But this is impossible, since we just said that the states of the entities are always identical.
A consequence of this very negative result is that, to construct a spanning tree without constraints on the number of initiators, we need to impose additional restrictions.
To determine the “minimal” restrictions that, added to R, will enable us to solve SPT is an interesting research problem still open.
The restriction that is commonly used is a very powerful one, Initial Distinct Values, and we will discuss it next.
The impossibility result we just witnessed implies that, to solve the SPT problem, we need an additional restriction.
The one commonly used is Initial Distinct Values (ID): Each entity has a distinct initial value.
Distinct initial values are sometimes called identifiers or ids or global names.
Multiple Spanning Trees As in most software design situations, once we have a solution for a problem and are faced with a more general one, one approach is to try to find ways to re-use and re-apply the already existing solution.
The solutions we already have are unique-initiator ones and, as we know, they fail in presence of multiple initiators.
Let us see how can we mend their shortcomings using distinct values.
Consider the execution of Shout in the example of Figure 2.13
In this case, the reason why the protocol fails is because the entities do not realize that there are two different requests (e.g., when x receives Q from y) for spanning-tree construction.
But we can now use the entities’ ids to distinguish between requests originating from different initiators.
The simplest and most immediate application of this approach is to have each initiator construct “its own” spanning tree with a single-initiator protocol and to use.
So, instead of cooperating to construct a single spanning tree, we will have several spanning trees concurrently and independently built.
This implies that all the protocol messages (e.g., Q andYes in Shout+) must contain also the id of the initiator.
It also requires additional variables and bookkeeping; for example, at each entity, there will be several instances of the variable tree-neighbors, one for each spanning tree being constructed (i.e., one for each initiator)
Furthermore, each entity will be in possibly different status values for each of these independent SPT-constructions.
Recall that the number k' of initiators is not known a priori and can change at every execution.
The message cost of this approach depends solely on the number of initiators and on the type of unique-initiator protocol used.
Selective Construction The large message cost derives from the fact that we construct not one but k' spanning trees.
Since our goal is just to construct one, there is clearly a needless amount of communication and computation being performed.
A better approach consists of letting every initiator start the construction of its own uniquely identified spanning tree (as before), but then suppressing some of these constructions, allowing only one to complete.
In this approach, an entity faced with two different SPT-constructions will select and act on only one, “killing” the other; the entity continues this selection process as long as it receives conflicting requests.
The criterion an entity uses to decide which SPT-construction to follow and which one to terminate must be chosen very carefully.
The criterion commonly used is based on min-id: Since each SPT-construction has a unique id (that of its initiator), when faced with different SPT-constructions, an entity will choose the one with the smallest id and terminate all the others.
An alternative criterion would be the one based on max-id.
The solution obtained with this approach has some very clear advantages over the previous solution.
First of all, each entity is at any time involved only in one SPTconstruction; this fact greatly simplifies the internal organization of the protocol (i.e., the set of rules), as well as the local storage and bookkeeping of each entity.
Second, upon termination, all entities have a single shared spanning tree for subsequent uses.
However, there is still competitive concurrency: An entity involved in one SPTconstruction might receive messages from another construction; in our approach, it will make a choice between the two constructions.
If the entity chooses the new one, it will give up all the knowledge (variables, etc) acquired so far and start from scratch.
The message cost of this approach depends again on the number of initiators and on the unique-initiator protocol used.
Consider a protocol developed using this approach, using Shout+ as the basic tool.
Informally, an entity u, at any time, participates in the construction of just one.
It will ignore all messages referring to the construction of other spanning trees where the initiators have larger ids than x.
As we will see, these techniques will construct a spanning tree rooted in the initiator with the smallest initial value.
It is possible that an entity has already terminated its part of the construction of a spanning tree when it receives a message from another initiator (possibly, with a smaller id)
In other words, when an entity has terminated a construction, it does not know whether it might have to restart again.
Thus, it is necessary to include in the protocol a mechanism that ensures an effective local termination for each entity.
This can be achieved by ensuring that we use, as a building block, a uniqueinitiator SPT-protocol in which the initiator will know when the spanning tree has been completely constructed (see Exercise 2.9.24)
In this way, when the spanning tree rooted in the initiator s with the smallest initial value has been constructed, s will become aware of this fact (as well as that all other constructions, if any, have been “killed”)
It can then notify all other entities so that they can enter a terminal status.
The notification is just a broadcast; it is appropriate to perform it on the newly constructed spanning-tree (so we start taking advantage of its existence)
We denote by v(x) the id of x; initially all entities are idle and any of them can spontaneously start the algorithm.
Theorem 2.5.7 Protocol MultiShout constructs a spanning tree rooted in the initiator with the smallest initial value.
Let us now consider the message costs of protocol MultiShout.
It is clearly more efficient than protocols obtained with the previous approach.
However, in the worst case, it is not much better in order of magnitude.
The fact that this solution is not very efficient does not imply that the approach of selective construction it uses is not effective.
On the contrary, it can be made efficient at the expenses of simplicity.
We will examine it in great details later in the book when studying the leader election problem.
In this section, we consider computations in tree networks under the standard restrictions R plus clearly the common knowledge that the network is tree.
Note that the knowledge of being in a tree implies that each entity can determine whether it is a leaf (i.e., it has only one neighbor) or an internal node (i.e., it has more than one neighbor)
We have already seen how to solve the Broadcast, the Wake-Up, and the Traversal problems in a tree network.
The first two are optimally solved by protocol Flooding, the latter by protocol DF Traversal.
These techniques constitute the first set of algorithmic tools for computing in trees with multiple initiators.
We will now introduce another very basic and useful technique, saturation, and show how it can be employed to efficiently solve many different problems in trees regardless of the number of initiators and of their location.
The technique, which we shall call Full Saturation, is very simple and can be autonomously and independently started by any number of initiators.
The activation stage is just a wake-up: each initiator sends an activation (i.e., wakeup) message to all its neighbors and becomes active; any noninitiator, upon receiving the activation message from a neighbor, sends it to all its other neighbors and becomes active; active nodes ignore all received activation messages.
Within finite time, all nodes become active, including the leaves.
Each active leaf starts the saturation stage by sending a message (call it M) to its only neighbor, referred now as its “parent,” and becomes processing.
Note: M messages will start arriving within finite time to the internal nodes.
An internal node waits until it has received an M message from all its neighbors but one, sends a M message to that neighbor that will now be considered its “parent,” and becomes processing.
If a processing node receives a message from its parent, it becomes saturated.
The resolution stage is started by the saturated nodes; the nature of this stage depends on the application.
Commonly, this stage is used as a notification for all entities (e.g., to achieve local termination)
Since the nature of the final stage will depend on the application, we will only describe the set of rules implementing the first two stages of Full Saturation.
A “truncated” protocol like this will be called a “plug-in”
In its execution, not all entities will enter a terminal status.
To transform it into a full protocol, some other action (e.g., the resolution stage) must be performed so that eventually all entities enter a terminal status.
It is assumed that initially all entities are in the same status available.
Let us now discuss some properties of this basic technique.
Lemma 2.6.1 Exactly two processing nodes will become saturated; furthermore, these two nodes are neighbors and are each other’s parent.
From the algorithm, it follows that an entity sends a message M only to its parent and becomes saturated only upon receiving an M message from its parent.
Choose an arbitrary node x, and traverse the “up” edge of x (i.e., the edge along which the M message was sent from x to its parent)
By moving along “up” edges, we must meet a saturated node s1 since there are no cycles in the graph.
This node has become saturated when receiving an M message from its parent s2
It depends on the communication delays which entities will become saturated and it is therefore totally unpredictable.
Subsequent executions with the same initiators might generate different results.
The only guarantee is that a pair of neighbors will be selected; since a pair of neighbors uniquely identifies an edge, the one connecting them; this result is also called edge election.
We will now discuss how to apply the saturation technique to solve different problems.
Let us see how the saturation technique can be used to compute the smallest among a set of values distributed among the nodes of the network.
Every entity x has an input value v(x) and is initially in the same status; the task is to determine the minimum among those input values.
That is, in the end, each entity must know whether or not its value is the smallest and enter the appropriate status, minimum or large, respectively.
So, more than one entity can have the minimum value; all of them must become minimum.
This problem is called Minimum Finding (MinFind) and is the simplest among the class of Distributed Query Processing problems that we will examine in later chapters: a set of data (e.g., a file) is distributed among the sites of a communication network; queries (i.e., external requests for information about the set) can arrive at any time at any site (which becomes an initiator of the processing), triggering computation and communication activities.
A stronger version of this problem requires all entities to know the minimum value when they enter the final status.
Let us see how to solve this problem in a tree network.
If the tree was rooted, then this task can be trivially performed.
In fact, in a rooted tree not only is there a special node, the root, but also a logical orientation of the links: “up” toward the root and “down” away from the root; this corresponds to the “parent” and “children” relationship, respectively.
In a rooted tree, to find the minimum, the root would broadcast down the request to compute the minimum value; exploiting the orientation of the links, the entities will then perform a convergecast (described in more details in Section 2.6.7): starting from the leaves, the nodes determine the smallest value among the values “down” and send it “up.” As a result of this process, the minimum value is then determined at the root, which will then broadcast it to all nodes.
Notice that convergecast can be used only in rooted trees.
The existence of a root (and the additional information existing in a rooted tree) is, however, a very strong assumption; in fact, it is equivalent to assuming the existence of a leader (which, as we will see, might not be computable)
Full Saturation allows to achieve the same goals without a root or any additional information.
This is achieved simply by including in the M message the smallest value known to the sender.
Namely, in the saturation stage the leaves will send their value with the M message, and each internal node sends the smallest among its own value and all the received ones.
In other words, MinF-Tree is just protocol Full Saturation where the procedures Initialize, Prepare Message, and Process Message are as shown in Figure 2.21 and where the resolution stage is just a notification started by the two saturated nodes, of the minimum value they have computed.
This is obtained by simply modifying procedure Resolve accordingly and adding the rule for handling the reception of the notification.
The correctness follows from the fact that both saturated nodes know the minimum value (Exercise 2.9.31)
The time costs will be the one experienced by Full Saturation plus the ones required by the notification.
Let Sat denote the set of the two saturated nodes; then.
An important class of problems are those of Distributed Function Evaluation; that is, where the task is to compute a function whose arguments are distributed among the processors of a distributed memory system (e.g., the sites of a network)
An instance of this problem is the the one we just solved: Minimum Finding.
We will now discuss how the saturation technique can be used to evaluate a large class of functions.
Semigroup Operations Let f be an associative and commutative function defined over all subsets of the input values.
Examples of this type of functions are: minimum, maximum, sum, product, and so forth, as well as logical predicates.
Because of their algebraic properties, these functions are called semigroup operations.
It is possible that some entities do not have an argument (i.e., initial value) or that the function must only be evaluated on a subset of the arguments.
We shall denote the fact that x does not have an argument by v(x) = nil.
The same approach that has led us to solve Minimum Finding can be used to evaluate f.
The protocol Function Tree is just protocol Full Saturation where the procedures Initialize, Prepare Message, and Process Message are as shown in Figure 2.22 and where the resolution stage is just a notification started by the two saturated nodes, of the final result of the function they have computed.
This is obtained by simply modifying procedure Resolve accordingly and adding the rule for handling the reception of the notification.
The correctness follows from the fact that both saturated nodes know the result of the function (Exercise 2.9.32)
The time and message costs of the protocol are exactly the same as the one for Minimum Finding.
Thus, semigroup operations can be performed optimally on a tree with any number of initiators and without a root or additional information.
Cardinal Statistics A useful class of functions are statistical ones, such as average, standard deviation, and so for.
These functions are not semigroup operation but can nevertheless be optimally solved using the saturation technique.
We can collect at the two saturated nodes Sum and Size with a single execution of Saturation: the M message will contain two data fields M=(“Saturation,” sum,size), which are initialized by each leaf node and updated by the internal ones.
The resolution stage is just a notification started by the two saturated nodes, of the average they can have computed.
Similarly, a single execution of Full Saturation with a final notification of the result will allow the entities to compute cardinal statistics on the input values.
Notice that ordinal statistics (e.g., median) are in general more difficult to resolve.
We will discuss them in the chapter on selection and sorting of distributed data.
The basic technique has been so far used to solve single-valued problems; that is, problems whose solution requires the identification of a single value.
It can also be used to solve multi-valued problems such as the problem of determining the eccentricities of all the nodes.
We will now show that saturation will yield instead a O(n), and thus optimal, solution.
Our goal is to have all nodes determine their eccentricity, not just the saturated ones.
The interesting thing is that the information available at each entity at the end of the saturation stage is almost sufficient to make them compute their own eccentricity.
Summarizing, every node, except the saturated ones, is missing one piece of information: the maximum distance from the nodes on the other side of the link connecting it to its parent.
If the parents could provide this information, the task can be completed.
Unfortunately, the parents are also missing the information, unless they are the saturated nodes.
Saturation can be used to provide the missing information: starting from the saturated nodes, once an entity receives the missing information from a neighbor, it will compute its eccentricity and provide the missing information to all its other neighbors.
Notice that, in the resolution stage, an entity sends different information to each of its neighbors.
Thus, unlike the resolution we used so far, it is not a notification.
The rules for handling the reception of the message, the procedure Resolve, and the procedure to calculate the eccentricity are also shown in Figure 2.24
Notice that, even though each node receives a different message in the resolution stage, only one message will be received by each node in that stage, except.
Thus, the message cost of protocol Eccentricities will be exactly as the one of MinF-Tree and so will the time cost:
A center is a node from which the maximum distance to all other nodes is minimized.
The Center Finding problem (Center) is to make each entity aware of whether or not it is a center by entering the appropriate terminal status center or not-center, respectively.
A Simple Protocol To solve Center we can use the fact that a center is exactly a node with the smallest eccentricity.
Thus a solution protocol consists of finding the minimum among all eccentricities, combining the protocols we have developed so far:
Execute the last two stages (saturation and resolution) of MinF-Tree.
At that time, an entity can determine if it is a center or not.
Lemma 2.6.2 In a tree either there is a unique center or there are two centers and they are neighbors.
Lemma 2.6.3 In a tree all centers lie on all diametral paths.
Lemma 2.6.4 gives us the tool we need to devise a solution protocol: an entity x can determine whether or not it is a center, provided it knows the value d[x, y] for each of its neighbors y.
But this is exactly the information that was provided to x by protocol Eccentricities so it could compute r(x)
This means that to solve Center it suffices to execute Eccentricities.
Once an entity has all the information to compute its radius, it will check whether the largest and the second largest received values differ at most by one; if so, it becomes center, otherwise not-center.
Thus, the solution protocol Center Tree is obtained from Eccentricities adding this test and some bookkeeping (Exercise 2.9.40)
The time and message costs of Center Tree will be exactly the same as that of Eccentricities.
An Efficient Plug-In The solutions we have discussed are full protocols.
In some circumstances, however, a plug-in is sufficient; that is, when the centers must start another global task.
In these circumstances, the goal is just for the centers to know that they are centers.
In such a case, we can construct a more efficient mechanism, always based on saturation, using the resolution stage in a different way.
Thus, the solution is to collect such values at a node x; determine whether x is a center; and, if not, move toward a center until it is reached.
In order to collect the information needed, we can use the first two stages (Wakeup and Saturation) of protocol Eccentricities.
Once a node becomes saturated, it can determine whether it is a center by checking whether the largest and the second largest received values differ at most by one.
If it is not a center, it will know that the center(s) must reside in the direction from which the largest value has been received.
By keeping track at each node (during the saturation stage) of which neighbor has sent the largest value, the direction of the center can also be determined.
Furthermore, a saturated node can decide whether it is closest to a center or its parent.
The saturated node, say x, closest to a center will then send a “Center” message, containing the second largest received value increased by one, in the direction of the center.
A processing node receiving such a message will, in turn, be able to determine whether it is a center and, if not, the direction toward the center(s)
The Center Finding plug-in will then be the Full Saturation plug-in with the addition of the “Center” message traveling from the saturated nodes to the centers.
In particular, the routines Initialize, Process Message, Prepare Message, Resolve, and the new rules governing the reception of the “Center” messages are shown in Figure 2.25
The simple modifications to the basic technique that we have discussed in the previous sections can be applied to solve a variety of other problems efficiently.
Following is a sample of them and the key properties employed toward their solution.
Finding a Median A median is a node from which the average distance to all nodes in the network is minimized.
Since a median obviously minimizes the sum of the distances to all other nodes, it is also called a communication center of the network.
Lemma 2.6.6 In a tree either there is a unique median or there are two medians and they are neighbors.
Using these properties, it is simple to construct a full protocol as well as an efficient plug-in, following the same approaches used for center finding (Exercise 2.9.41)
Finding Diametral Paths A diametral path is a path of the longest length.
In a network there might be more than one diametral path.
The problem we are interested in is to identify all these paths.
In distributed terms, this means that each entity needs to know if it is part of a diametral path or not, entering an appropriate status (e.g., on-path or off-path)
A full protocol efficiently implementing this strategy can be designed using the tools developed so far (Exercise 2.9.45)
Consider now designing a plug-in instead of a full protocol; that is, we are only interested in that the entities on diametral paths (and only those) become aware of it.
In this case, the other key property is Lemma 2.6.4: every center lies on every diametral path.
This gives us a starting point to find the diametral paths: the centers.
In other words, we first find the centers (note: they know the diameter) and then propagate the information along the diametral paths.
A center (or for that matter, a node on a diametral path) does not know a priori which one of its neighbors is also on a diametral path.
It will thus send the needed information to all its neighbors which, upon receiving it, will determine whether or not they are on such a path; if so, they continue the execution (Exercise 2.9.46)
Rooted Trees In some cases, the tree T is actually rooted; that is, there is a distinct node, r, called the root, and all links are oriented toward r.
In this case, the tree T will be denoted by T[r]
If link (x,y) is oriented from y to x, x is called the parent of y and y is said to be a child of x.
Similarly, a descendant of x is any entity z for which there is a directed path from z to x, and an ancestor of x is any entity z for which there is a directed path from x to z.
Before examining how to compute in rooted trees, let us first observe the important fact that transforming a tree into a rooted one might be an impossible task.
Theorem 2.6.1 The problem of transforming trees into rooted ones is deterministically unsolvable under R.
This means that being in a rooted tree is considerably different from being in a tree.
Convergecast The orientation of the links in a rooted tree is such that each entity has a notion of “up” (i.e., towards the root) and “down” (i.e., away from the root)
If we are in a rooted tree, we can obviously exploit the availability of this globally consistent orientation.
In particular, in the saturation technique, the process performed in the saturation stage can be simplified as follows:
In this way, the root (that does not have a parent) will be the sole saturated node and will start the resolution stage.
Clearly, such an amount alone does not justify the difference between general trees and rooted ones.
There are however other advantages in rooted trees, as we will see later.
Totally Ordered Trees In addition to the globally consistent orientation “up and down,” a rooted tree has another powerful property.
In fact, the port numbers at a node are distinct; thus, they can be sorted, for example, in increasing order, and the corresponding links can be ordered accordingly.
Note that a node might not be aware of its order number in the tree, although this information can be easily acquired in the entire tree (Exercise 2.9.49)
This means that, in a rooted tree the root assigns unique ids to the entities.
The fact that a rooted tree is totally ordered can be exploited also in other computations.
In many systems and applications, it is necessary to occasionally select an entity at random.
This occurs for instance in routing systems where, to reduce congestion, a message is first sent to an intermediate destination chosen at random and then delivered from there to the final destination.
The same random selection is made, for example, for coordination of a computation, for control of a resource, etc.
The problem is how to determine an entity at random.
Let us concentrate on uniform choice; that is, every entity must have the same probability, 1/n, of being selected.
In a rooted tree, it becomes easy for the root to select uniformly an entity at random.
Once unique names have been assigned in preorder to the nodes and the root knows the number n of entities, the root needs only to choose locally a number uniformly at random between 1 and n; the entity with such a name will be the selected one.
At this point, the only thing that the root r still has to do is to communicate efficiently to the selected entity x the result of the selection.
Let us concentrate again on uniform choice; that is, every data item must have the same probability, 1/|D| of being selected.
Interestingly, this problem can be solved with a technique similar to that used for selecting an entity at random and with the same cost (Exercise 2.9.52)
Application: Broadcast with Termination Detection Convergecast can be used whenever there is a rooted spanning tree.
It is a “fact of life” in distributed computing that entities can terminate the execution of a protocol at different times; furthermore, when an entity terminates, it is usually unaware of the status of the other entities.
This is why we differentiate between local termination (i.e., of the entity) and global termination (i.e., of the entire system)
For example, with the broadcast protocol Flooding the initiator of the broadcast does not know when the broadcast is over.
To ensure that the initiator of the broadcast becomes aware of when global termination occurs, we need to use a different strategy.
To develop this strategy, recall that, if an entity s performs a Flood+Reply (e.g., protocol Shout) in a tree, the tree will become rooted in s: the initiator is the root; for every other node y, the neighbor x from which it receives the first broadcasted message is its parent, and all the neighbors that send the positive reply (e.g., “YES” in Shout and Shout+) are its children.
This means that convergecast can be “appended” to any Flood+Reply protocol.
The initiator s uses any Flood+Reply protocol to broadcast and construct a spanning tree T[s] of the network;
Starting from the leaves of T[s], the entities perform a convergecast on T.
At the end of the convergecast, s becomes aware of the global termination of the broadcast (Exercise 2.9.48)
Flooding: with single initiator = broadcast; with multiple initiators = wake-up.
Flooding with Reply (Shout ): with single initiator, it creates a spanning tree rooted.
Of the basic techniques, flooding is the oldest one, still currently and frequently used.
Broadcasting in a linear number of messages in unoriented hypercubes is due to Stefan Dobrev and Peter Ruzicka [6]
The use of broadcast trees was first discussed by David Wall [12]
The difficulty of performing a wake-up in labeled hypercubes and in complete graphs has been proved by Stefan Dobrev, Rastislav Kralovic, and Nicola Santoro [5]
The first formal argument on the impossibility of some global computations under R (e.g., the impossibility result for spanning-tree construction with multiple initiators) is due to Dana Angluin [1]
Exercise 2.9.2 Design a protocol to broadcast without the restriction that the unique initiator must be the entity with the initial information.
Exercise 2.9.3 Modify Flooding so to broadcast under the restriction that the unique initiator must be an entity without the initial information.
Exercise 2.9.4 We want to move the system from an initial configuration where every entity is in the same status ignorant except the one that is knowledgeable to a final configuration where every entity is in the same status.
Consider this problem under the standard assumptions plus Unique Initiator.
Exercise 2.9.5 Design a protocol to broadcast without the Bidirectional Link restriction.
Exercise 2.9.7 Prove that protocol WFlood correctly terminates under the standard set of restrictions BL,C, and TR.
Exercise 2.9.9 Show that the subgraph Hk(x), induced by the messages sent when using HyperFlood on the k-dimensional hypercubeHk with x as the initiator, contains no cycles.
Exercise 2.9.10 Show that for every x the eccentricity of x in Hk(x) is k.
Exercise 2.9.11 Prove that the message complexity of traversal under R is at least m.
Hint: use the same technique employed in the proof of Theorem 2.1.1
Show that, in this case, no Backedge messages will be sent in any execution of DF Traversal.
Exercise 2.9.15 Prove that protocol DF++ correctly performs a depth-first traversal.
Exercise 2.9.16 Show that, in the execution of DF++, on some back-edges there might be two “mistakes.”
Exercise 2.9.18 Prove that in protocol Shout, if an entity x is in Tree-neighbors of y, then y is in Tree-neighbors of x.
Exercise 2.9.19 Prove that in protocol Shout, if an entity sends Yes, then it is connected to the initiator by a path where on every link a Yes has been transmitted.
Exercise 2.9.20 Prove that the subnet constructed by protocol Shout contains no cycles.
Exercise 2.9.22 Write the set of rules for protocol Shout+
Exercise 2.9.23 Determine under what conditions on the communication delays, protocol Shout will construct a breadth-first spanning tree.
Exercise 2.9.24 Modify protocol Shout so that the initiator can determine when the broadcast is globally terminated.
Hint: integrate in the protocol the convergecast operation for rooted trees.
Exercise 2.9.27 Prove that, in the execution of df-SPT, when the initiator becomes done, a df-tree of the network has already been constructed.
Exercise 2.9.28 Prove that, for any broadcast protocol, the graph induced by relationship “parent” is a spanning tree of the network.
Exercise 2.9.29 Prove that the bf-tree of G rooted in a center is a broadcast tree of G.
Exercise 2.9.31 Prove that when a node becomes saturated in the execution of protocol MinF-Tree, it knows the minimum value in the network.
Exercise 2.9.32 Prove that when a node becomes saturated in the execution of protocol Funct-Tree, it knows the value of f.
Exercise 2.9.33 Design a protocol to determine if all the entities of a tree network have positive initial values.
Exercise 2.9.34 Consider a tree system where each entity has a salary and a gender.
Design a solution protocol that can be started by any number of entities independently.
Design a solution protocol that can be started by any number of entities independently.
Exercise 2.9.36 Design an efficient protocol to compute the number of entities in a tree network.
The investigators now want to know how many female entities are in the system.
Design a solution protocol that can be started by any number of entities independently.
Prove that the saturated nodes will compute their maximum distance from all other nodes.
Construct an efficient plug-in so that the median nodes know that they are such.
Design an efficient protocol to determine the diameter of the tree.
Design an efficient solution protocol that is generic; that is, it works in an arbitrary connected graph.
Design an efficient protocol so that each entity can determine whether or not it lies on a diametral path of the tree.
Exercise 2.9.46 A path whose length is d is called diametral.
Design an efficient plug-in so that all and only the entities on a diametral path of the tree become aware of this fact.
Exercise 2.9.48 Prove that, when an initiator of a TDCast protocol receives the convergecast message from all its children, the initial broadcast is globally terminated.
Exercise 2.9.49 Show how to assign efficiently a unique id to the entities in a rooted tree.
Exercise 2.9.51 Show why choosing uniformly at random a site and then choosing uniformly at random an element from that site is not the same as choosing uniformly at random an element from the entire set.
Exercise 2.9.52 Random Item Selection ('') Consider the task of selecting uniformly at random an item from a set of data partitioned among the nodes of a tree rooted at s.
Show how to perform this task, started by the root, with at most.
Problem 2.9.1 Develop an efficient solution to the Traversal problem without the Bidirectional Links assumption.
Problem 2.9.2 Develop an efficient solution to the Minimum Finding problem in a hypercube with a unique initiator (i.e., under the additional restriction UI)
Answer to Exercise 2.9.16 Consider a ring network with the three nodes x, y, and z.
Consider the following sequence of events that take place successively in time as a result of the execution of the DF++ protocol: x sends Visited messages to y and z, sends the Token to y, and waits for a (Visited or Return) reply from y.
When y receives the Token from x, it sends to z a Visited message and then the Token.
Assume that when z receives the Token, the Visited message from x has not arrived yet; hence z sends Visited to x followed by the Token.
This is the first mistake: Token is sent on a back-edge to x, which has already been visited.
When z finally receives the Visited message from x, it realizes the Token it sent to x was a mistake.
Since it has no other unvisited neighbors, z sends a Return message back to y.
Since y has no other unvisited neighbors, it will then send a Return message back to x.
Assume that when x receives the Return message from y, x has not received yet neither the Visited nor the Return messages sent by z.
Hence, x considers z as an unvisited neighbor and sends the Token to z.
This is the second mistake on the back-edge between x and z.
Answer to Exercise 2.9.19 Suppose some node x is not reachable from s in the graph T induced by the “parent” relationship.
This means that x never sent the Yes messages; this implies that x never received the question Q.
This is impossible because, since flooding is correct, every entity will receive Q; thus, no such x exists.
Partial Answer to Exercise 2.9.48 By induction on the height of the rooted tree, prove that, in a TDCast protocol, when an entity x receives the convergecast message from all its children, all its descendants have locally terminated the broadcast.
Partial Answer to Exercise 2.9.49 Perform first a broadcast from the root to notify all entities of the start of the protocol, and then a convergecast to collect at each entity the number of its descendents.
Afterwards use this information to assign distinct values to the entities according to a preorder traversal of the tree.
Partial Answer to Exercise 2.9.51 Show that the data items from smaller sets will be chosen with higher probability than that of the items from larger sets.
Linear broadcasting and O(n log log n) election in unoriented.
In a distributed environment, most applications often require a single entity to act temporarily as a central controller to coordinate the execution of a particular task by the entities.
In some cases, the need for a single coordinator arises from the desire to simplify the design of the solution protocol for a rather complex problem; in other cases, the presence of a single coordinator is required by the nature of the problem itself.
The problem of choosing such a coordinator from a population of autonomous symmetric entities is known as Leader Election (Elect)
Formally, the task consists in moving the system from an initial configuration where all entities are in the same state (usually called available) into a final configuration where all entities are in the same state (traditionally called follower), except one, which is in a different state (traditionally called leader)
There is no restriction on the number of entities that can start the computation, nor on which entity should become leader.
We can think of the Election problem as the problem of enforcing restriction Unique Initiator in a system where actually no such restriction exists: The multiple initiators would first start the execution of an Election protocol; the sole leader will then be the unique initiator for the subsequent computation.
There is unfortunately a very strong impossibility result about election.
In other words, there is no deterministic protocol that will always correctly terminate within finite time if the only restrictions are those in R.
To see why this is the case, consider a simple system composed of two entities, x and y, both initially available and with no different initial values; in other words, they are initially in identical states.
If a solution protocol P exists, it must work under any conditions of message delays.
Consider a synchronous schedule (i.e., an execution where communication delays are unitary) and let the two entities start the execution of P simultaneously.
As they are in identical states, they will execute the same rule, obtain the same result, and compose and send (if any) the same message; thus, they will still be in identical states.
If one of them receives a message, the other will receive the same message at the same time and, by Property 1.6.2, they will perform the same computation, and so on.
Their state will always be the same; hence if one becomes leader, so will the other.
But this is against the requirement that there should be only one leader; in other words, P is not a solution protocol.
The consequence of Theorem 3.1.1 is that to break symmetry, we need additional restrictions and assumptions.
This is the case, for example, with the assumption that there is already available a spanning tree (i.e., restriction Tree)
In fact, the two-node network in which we know election is impossible is a tree.
To determine which restrictions, added to R, will enable us to solve Elect, we must consider the nature of the problem.
The entities have an inherent behavioral symmetry: They all obey the same set of rules plus they have an initial state symmetry (by definition of election problem)
To elect a leader means to break these symmetries; in fact, election is also called symmetry breaking.
To be able to do so, from the start there must be something in the system that the entities can use, something that makes (at least one of) them different.
Remember that any restriction limits the applicability of the protocol.
The most obvious restriction is Unique Initiator (UI): The unique initiator, known to be unique, becomes the leader.
This is, however, “sweeping the problem under the carpet,” saying that we can elect a leader if there is already a leader and it knows about it.
The problem is to elect a leader when many (possibly, all) entities are initiators; thus, without UI.
How can the difference in initial values be used to break the symmetry and to elect a leader?
According to the election problem specifications, it does not matter which entity becomes the leader.
Using the fact that the values are distinct, a possible strategy is to choose as a leader the entity with the smallest value; in other words, an election strategy is as follows:
Finding the minimum value is an important problem of its own, which we have already discussed for tree networks (Section 2.6.2)
Notice that in that occasion, we found the minimum value without unique identifiers; it is the election problem that needs them.
A useful variant of this strategy is the one restricting the choice of the leader to the set of entities that initiate the protocol.
Notice that any solution implementing the strategy Elect Minimum solves Min as well as Elect, not so the ones implementing Elect Minimum Initiator.
Another strategy is to use the distinct values to construct a rooted spanning tree of the network and to elect the root as the leader.
Constructing a (rooted) spanning tree is an important problem of its own, which we have already discussed among the basic problems (Section 2.5 )
Hence the strategy Elect Minimum leads to an election protocol Tree:Elect Min where the number of messages in the worst case is as follows:
Interestingly, also the strategy Elect Minimum Initiator will have the same complexity (Exercise 3.10.1)
As the network is a tree, the only work required is to transform it into a rooted tree.
It is not difficult to see how saturation can be used to solve the problem.
In fact, if Full Saturation is applied, then a saturated node knows that it itself and its parent are the only saturated nodes; furthermore, as a result of the saturation stage, every nonsaturated entity has identified as its parent the neighbor closest to the saturated pair.
In other words, saturation will root the tree not in a single node but in a pair of neighbors: the saturated ones.
Thus, to make the tree rooted in a single node we just need to choose only one of the two saturated nodes.
In other words, the “Election” among all the nodes is reduced to an “election” between the two saturated ones.
This can be easily accomplished by having the saturated nodes communicate their identities and by having the node with the smallest identity become elected, while the other stays processing.
Thus, the Tree:Elect Root protocol will be Full Saturation with the new rules and the routine Resolve shown in Figure 3.2
The number of message transmissions for the election algorithm Tree Election will be exactly the same as the one experienced by Full Saturation with notification.
In other words, it uses two messages more than the solution obtained using the strategy Elect Minimum.
Granularity of Analysis: Bit Complexity From the discussion above, it would appear that the strategy Elect Minimum is “better” because it uses two messages less than the strategy Elect Root.
This assessment is indeed the only correct conclusion obtainable using the number of messages as the cost measure.
Sometimes, this measure is too “coarse” and does not really allow us to see possibly important details; to get a more accurate picture, we need to analyze the costs at a “finer” level of granularity.
Let us re-examine the two strategies in terms of the number of bits.
To do so, we have to distinguish between different types of messages because some contain counters and values, while others contain only a message identifier.
Messages that do not carry values but only a constant number of bits are called signals and in most practical systems, they have significantly less communication costs than value messages.
In Elect Minimum, only the n messages in the saturation stage carry a value, while all the others are signals; hence, the total number of bits transmitted will be.
In Elect Root, only the “Election” message carries a node identity; thus, the total number of bits transmitted is.
That is, in terms of number of bits, Elect Root is an order of magnitude better than Elect Minimum.
In terms of signals and value messages, with Elect Root strategy we have only two value messages and with Elect Minimum strategy we have n value messages.
Remember: Measuring the number of bits gives us always a “picture” of the efficiency at a more refined level of granularity.
Fortunately, it is not always necessary to go to such a level.
We will now consider a network topology that plays a very important role in distributed computing: the ring, sometimes called loop network.
A ring consists of a single cycle of length n.
In a ring, each entity has exactly two neighbors, (whose associated ports are) traditionally called left and right (see Figure 3.3)
Note that the labeling might, however, be globally inconsistent, that is, ‘right’ might not have the same meaning for all entities.
After trees, rings are the networks with the sparsest topology: m = n; however, unlike trees, rings have a complete structural symmetry (i.e., all nodes look the same)
Because of its structure, in a ring we will use almost exclusively the approach of minimum finding as a tool for leader election.
In fact we will consider both the Elect Minimum and the Elect Minimum Initiator approaches.
Clearly the first solves both Min and Elect, while the latter solves only Elect.
Every protocol that elects a leader in a ring can be made to find the minimum value (if it has not already been determined) with an additional n message and time (Exercise 3.10.2)
Furthermore, in the worst case, the two approaches coincide: All entities might be initiators.
Let us now examine how minimum finding and election can be efficiently performed in a ring.
The first solution we will use is rather straightforward: When an entity starts, it will choose one of its two neighbors and send to it an “Election” message containing its id; an entity receiving the id of somebody else will send its id (if it has not already done so) and forward the received message along the ring (i.e., send it to its other neighbor) keeping track of the smallest id seen so far (including its own)
This process can be visualized as follows: Each entity originates a message (containing its id), and this message travels “all the way” along the ring (forwarded by the other entities) (see Figure 3.4)
Hence, the name All the Way will be used for the resulting protocol.
Each entity will eventually see the id of everybody else id (finite communication delays and total reliability ensure that) including the minimum value; it will, thus, be able to determine whether or not it is the (unique) minimum and, thus, the leader.
When will an entity terminate its execution? Entities only forward messages carrying values other than their own: Once the.
Thus, each value will travel “All the Way” along the ring only once.
But how does an entity know that the communication activities.
An entity knows that it has seen all values once it receives its value back.
The “reason” is that the message with its own id has to travel longer along the ring to reach x than those originated by other entities; thus, these other messages will be received first.
In other words, reception of its own message can be used to detect termination.
This reasoning is incorrect because it uses the (hidden) additional assumption that the system has first in first out (FIFO) communication channels, that is, the messages are delivered in the order in which they arrive.
This restriction, called Message Ordering, is not a part of election’s standard set; few systems actually have it built in, and the costs of offering it can be formidable.
So, whatever the answer, it must not assume FIFO channels.
With this proviso, a “reasonable” but unfortunately still incorrect answer is the following:
An entity counts how many different values it receives; when the counter is equal to n, it knows it can terminate.
The problem is that this answer assumes that the entity knows n, but a priori knowledge of the ring size is not a part of the standard restrictions for election.
It is indeed strange that the termination should be difficult for such a simple protocol in such a clear setting.
Fortunately, the last answer, although incorrect, provides us with the way out.
In fact, although n is not known a priori, it can be computed.
This is easily accomplished by having a counter in the Election message, initialized to 1 and incremented by each entity forwarding it; when an entity receives its id back, the value of the counter will be n.
Summarizing, we will use a counter at each entity, to keep track of how many different ids are received and a counter in each message, so that each entity can determine n.
The message originated by each entity will travel along the ring exactly once.
The protocol can be obviously modified so as to follow strategy Elect Minimum Initiator, finding the smallest value only among the initiators.
In this case, those entities that do not initiate will not originate a message but just forward the others’
In this way, we would have fewer messages whenever there are fewer initiators.
In fact, in protocol All the Way, we were using an entity’s own message to determine n so as to be able to determine local termination.
Fortunately, this problem has a simple solution requiring only n additional messages and time (Exercise 3.10.4)
Summarizing, the costs of the modified protocol, All the Way:Minit, are as follows:
The modified protocol All the Way:Minit will in general use fewer messages than the original one.
In fact, if only a constant number of entities initiate, it will use only.
By contrast, if every entity is an initiator, this protocol uses n messages more than the original one.
Notice that All the Way (in its original or modified version) can be used also in unidirectional rings with the same costs.
In other words, it does not require the Bidirectional Links restriction.
To design an improved protocol, let us determine the drawback of the one we already have: All the Way.
In this protocol, each message travels all along the ring.
In the existing protocol, x will forward this message, even though x knows that 22 is not the smallest value.
But our overall strategy is to determine the smallest id among all entities; if an entity determines that an id is not the minimum, there is no need whatsoever for the message containing such an id to continue traveling along the ring.
We will thus modify the original protocol All the Way so that an entity will only forward Election messages carrying an id smaller than the smallest seen so far by.
In other words, an entity will become an insurmountable obstacle for all messages with a larger id “terminating” them.
Each entity will originate a message (containing its id) that travels along the ring “as far as it can”: until it returns to its originator or arrives at a node with a smaller id.
Hence the name AsFar (As It Can) will be used for the resulting protocol.
The message with the smallest id will always be forwarded by the other entities; thus, it will travel all along the ring returning to its originator.
The message containing another id will instead be unable to return to its originator because it will find an entity with a smaller id (and thus be terminated) along the way.
In other words, only the message with the smallest id will return to its originator.
If an entity receives a message with its own id, it knows that its id is the minimum, that is, it is the leader; the other entities have all seen that message pass by (they forwarded it) but they still do not know that there will be no smaller ids to come by.
Thus, to ensure their termination, the newly elected leader must notify them by sending an additional message along the ring.
Message Cost This protocol will definitely have fewer messages than the previous one.
Consider the cost caused by the Election message originated by x.
This message will travel along the ring until it finds a smaller id (or complete the tour)
Thus, the cost of its travel depends on how the ids are allocated on the ring.
Also notice that what matters is whether an id is smaller or not than another and not their actual value.
In other words, what is important is the rank of the ids and how those are situated on the ring.
In this case, including also the n messages required for the final notification, the total cost will be.
That is, we will cut the number of messages at least to half.
From a theoretical point of view, the improvement is not significant; from a practical point of view, this is already a reasonable achievement.
However we have so far analyzed only the worst case.
To see precisely how, we need to perform a more detailed analysis of the protocol’s performance.
In other words, it does not require the Bidirectional Links restriction.
The worst case gives us an indication of how “bad” things could get when the conditions are really bad.
But how likely are such conditions to occur? What costs can we generally expect? To find out, we need to study the average case and determine the mean and the variance of the cost of the protocol.
Average Case: Oriented Ring We will first consider the case when the ring is oriented, that is, “right” means the same to all entities.
In this case, all messages will travel in only one direction, say clockwise.
Because of the unique nature of the ring network, this case coincides with the execution of the protocol in a unidirectional ring.
Thus, the results we will obtain will hold for those rings.
Given a set of size a, we denote by C(a, b) the number of subsets of size b that can be formed from it.
Thus, the probability P (i, k) that id #i will travel clockwise exactly k steps is.
The smallest id, #1, will travel the full length n of the ring.
This is indeed great news: On an average, the message cost is an order of magnitude less than that in the worst case.
Average Case: Unoriented Ring Let us now consider what will happen on an average in the general case, when the ring is unoriented.
The fact that the ring is not oriented means that when two entities send a message to their “right” neighbors, they might send it in different directions.
Alternatively, assume that an entity, as its first step in the protocol, flips a fair coin (i.e., probability 12 ) to decide the direction it will use to send its value.
A similar bound holds if we use the strategy of electing the Minimum Initiator:
The difference between the two bounds is restricted to the constant and is rather limited.
In practical terms, from the algorithm design point of view, it indicates that we should try to have the entities send their initial message in different directions (as in the probabilistic protocol) and not all in the same one (like in the oriented case)
To simulate the initial “random” direction, different means can be used.
For example, each entity x can choose (its own) “right” if id(x) is even, (its own) “left” otherwise.
This result has also a theoretical relevance that will become apparent later, when we will discuss lower bounds and will have a closer look at the nature of the difference between oriented and unoriented rings.
Summary The main drawback of protocol AsFar is that there still exists the possibility that a very large number of messages (O(n2)) will be exchanged.
As we have seen, on an average, the use of the protocol will cost onlyO(n log n) messages.
To give such a guarantee, a protocol must have a O(n log n) worst case complexity.
We will now design a protocol that has a guaranteedO(n log n) message performance.
To achieve this goal, we must first of all determine what causes the previous protocol to use O(n2) messages and then find ways around it.
The first thing to observe is that in AsFar (as well as in All the Way), an entity makes only one attempt to become leader and does so by originating a message containing its id.
Next observe that, once this message has been created and sent, the entity has no longer any control over it: In All the Way the message will travel all along the ring; in AsFar it will be stopped if it finds a smaller id.
Let us take these observations into account to design a more efficient protocol.
The key design goal will be to make an entity retain some control over the message it originates.
Summarizing, an entity x will originate a message with its own id, and this message will travel until it is terminated or it reaches a certain distance dis; if it is not terminated, the message returns to the entity.
When it arrives, x knows that on this side of the ring, there are no smaller ids within the traveled distance dis.
The entity must now decide if to allow its message to travel a further distance; it will do so only if it knows for sure that there are no smaller ids within distance dis on the other side of the ring as well.
As a consequence, instead of a single global attempt at leadership, an entity will go through several attempts, which we shall call Electoral Stages: An entity enters the.
If an entity is defeated in an electoral stage (i.e., at least one of its messages does not return), it still will have to continue its participation in the algorithm forwarding the messages of those entities that are still undefeated.
Although the protocol is almost all outlined, some fundamental issues are still unresolved.
In particular, the fact that we now have several stages can have strange consequences in the execution.
Because of variations in communication delays, it is possible that at the same time instant, entities in different parts of the ring are in different electoral stages.
Furthermore, as we are only using the standard restrictions for elections, messages can be delivered out of order; thus, it might be possible that messages from a higher stage will arrive at an entity before the ones from the current one.
We said that an entity is defeated if it does not receive one of its messages back.
Consider now an entity x; it has sent its two messages and it is now waiting to know the outcome.
Let us say that one of its messages has returned but the other has not yet.
It is possible that the message is coming very slowly (e.g., experiencing long transmission delays) or that it is not coming at all (i.e., it found a smaller id on the way)
How can x know ? How long will x have to wait before taking a decision (a decision must be taken within finite time)? More specifically, what will x do if, in the meanwhile, it receives a message from a higher stage ? The answer to all these.
Notice that this creates a new situation: A message returns to its originator and finds it defeated; in this case, the message will be terminated.
A synthetic description of the protocol will thus be as follows:
Correctness The correctness of the algorithm follows from the dynamics of the rules: The messages containing the smallest id will always travel all the allocated.
When this happens, the messages with the smallest value will travel all along the ring; as a result, their originator becomes leader and all the others are already defeated.
Costs The costs of the algorithm depend totally on the choice of the function dis used to determine the maximum distance a “Forth” message can travel in a stage.
Messages If we examine the execution of the protocol at some global time t , because communication delays are unpredictable, we can find not only that entities in different parts of the ring are in different states (which is expected) but also that entities in the candidate state are in different stages.
Moreover, because there is no Message Ordering, messages from high stages (the “future”) might overtake messages from lower stages and arrive at an entity still in a lower stage (the “past”)
Still, we can visualize the execution as proceeding in logical stages; it is just that different entities might be executing the same stage at different times.
An entity starting stage i will send “Forth” messages in both directions; each message will travel at most dis(i), for a total of 2ni dis(i) message transmissions.
So, the total number of messages used by protocol Control will be at most.
To really finalize the design, we must choose the function dis.
The constant can be, however, further improved by carefully selecting dis.
Thus, with all of them, protocol Control has a guaranteed O(n log n) performance.
The “best” among those functions will be the one where 3c+1log c is minimized; as.
Thus such a best choice is c = 3 for which we obtain.
Time The ideal time complexity of procedure Control is easy to determine; the time required by stage i is the time needed by the message containing the smallest id to reach its assigned distance and come back to its originator; hence exactly 2dis(i) time units.
An additional n time units are needed for the final notification, as well as for the initial wake-up of the entity with the smallest id.
This means that the total time costs will be at most.
Thus, the total number M[Control-Minit] of messages in the worst case will be at most.
In the previous protocol, we have introduced and used the idea of limiting the distances to control the complexity of the original “as far as it can” approach.
This idea requires that an entity makes several successive attempts (at increasing distances) to become a leader.
The idea of not making a single attempt to become a leader (as it was done in All the Way and in AsFar), instead of proceeding in stages, is a very powerful algorithmic tool of its own.
Recall that “stage” is a logical notion, and it does not require the system to be synchronized; in fact, parts of the system may run very fast while other parts may be slow in their operation, so different entities might execute a stage at totally different times.
We will now see how the proper use of this tool allows us to achieve even better results, without controlling the distances and without return (or feedback) messages.
As before, we will have each candidate send a message carrying its own id in both directions.
Without setting an a priori fixed limit on the distance these messages can travel, we still would like to avoid them to travel unnecessarily far (costing too many transmissions)
A message will travel until it reaches another candidate in the same (or higher) stage.
Observe that the initiator with the smallest identity will never become defeated; by contrast, at each stage, its message will transform into defeated the neighboring candidate on each side (regardless of their distance)
This means that eventually, the only candidate left is the one with the minimum id.
When this happens, its messages will travel all along the ring (forwarded by the defeated entities) and reach it.
Thus, a candidate receiving its own messages back knows that all other entities are defeated; it will then become leader and notify all other entities of termination.
A candidate x sends a message in both directions carrying its identity; these messages will travel until they encounter another candidate node.
A defeated node will forward any received election message, and each noninitiator will automatically become defeated upon receiving an election message.
Messages It is not so obvious that this strategy is more efficient than the previous one.
Let us first determine the number of messages exchanged during a stage.
Consider the segment of the ring between two neighboring candidates in stage i, x, and.
Consider a node x that is candidate at the beginning of stage i and is not defeated during this stage; let y = r(i, x) and z = l(i, x) be the first entity to the right and to the left of x, respectively, that are also candidates in stage i (Figure 3.16)
It is not difficult to see that if x survives stage i, both r(i, x) and l(i, x) will be defeated.
Therefore, at least half of the candidates are defeated at each stage.
Removing Message Ordering The correctness and termination of Stages are easy to follow also because we have assumed in our protocol that there is Message.
This assumption ensured that the two messages received by a candidate in stage i are originated by candidates also in stage i.
If we remove the Message Ordering restriction, it is possible that messages arrive out of order and that a message sent in stage j > i arrives before a message sent in stage i.
Simple Approach The simplest way to approach this problem is by enforcing the “effects” of Message Ordering, without really having it.
First of all, each message will also carry the stage number of the entity originating it.
First of all, we will give a stage number to all the nodes: For a candidate entity, it is the current stage; for a defeated entity, it is the stage in which it was defeated.
We will then have a defeated node forward only messages from higher stages.
As for the resulting communication complexity, the number of messages is never more (sometimes less) than that with Message Ordering (Exercise 3.10.15)
Interestingly, if we attempt to measure the ideal time complexity, we will only see executions with Message Ordering.
This is yet another case showing how biased and limited (and thus dangerous) ideal time is as a cost measure.
We have seen how, with the proper use of electoral stages in protocol Stages, we can obtain aO(n log n) performance without the need of controlling the distance travelled by a message.
In addition to controlled distances, protocol Control uses also a “feedback” technique: If a message successfully reaches its target, it returns back to its originator, providing it with a “positive feedback” on the situation it has encountered.
Such a technique is missing in Stages: A message always successfully reaches its target (the next candidate in the direction it travels), which could be at an unpredictable distance; however, the use of the message ends there.
Let us integrate the positive feedback idea in the overall strategy of Stages: When an “Election” message reaches its target, a positive feedback will be sent back to its originator if the id contained in the message is the smallest seen by the target in this stage.
The fate of entity with id 7 depends on its other neighboring candidate, which is not shown; so, we do not know whether it will survive or not.
If a node sends a “feedback” message, it knows that it will not survive this stage.
How will such an entity discover that no “feedback” is forthcoming and it must become defeated? The answer is fortunately simple.
More specifically, if while waiting for a “feedback” message in stage i, an entity receives an “Election” message (clearly with a smaller id) in stage i + 1, it becomes defeated and forwards the message.
We shall call the protocol Stages with Feedback; our description was assuming message ordering.
As for protocol Stages, this restriction can and will be logically enforced with just local processing.
Correctness The correctness and termination of the protocol follows from the fact that the entity xmin with the smallest identity will always receive a positive feedback from both sides; hence, it will never be defeated.
At the same time, xmin never sends a positive feedback; hence, its left and right neighboring candidates in that stage do not survive it.
In other words, the number ni of candidates in stage i is monotonically decreasing, and eventually only xmin will be in such a state.
When this happens, its own “Election” messages will travel along the ring, and termination will be detected.
Messages We are adding bookkeeping and additional messages to the already highly efficient protocol Stages.
As in Stages, if a candidate x in stage i survives, it is guaranteed that its neighboring.
With the introduction of positive feedback, we can actually guarantee that if x survives, neither will the first candidate to the right of r(i, x) survive nor will the first candidate to the left of l(i, x) survive.
In other words, the number of stages has decreased with the use of “feedback” messages.
Let us examine now how many messages will be sent in each stage.
Consider stage i; this will be started by ni candidates.
Each candidate will send an “Election” message that will travel to the next candidate on either side.
Thus, exactly like in Stages, two “Election” messages will be sent over each link, one in each direction, for a total of 2n “Election” messages per stage.
Consider now the “feedback” messages; a candidate sends at most one “feedback” and only in one direction.
Thus, in the segment of the ring between two candidates, there will be at most one “feedback” message on each link; hence, there will be no more than n “feedback” transmissions in total in each stage.
This means that in each stage there will be at most 3nmessages.
In other words, the use of feedback with the electoral stages allows us to reduce the number of messages in the worst case.
The use of Minimum Initiator strategy yields the similar result:
In the analysis of the number of “feedback” messages sent in each stage, we can be more accurate; in fact, there are some areas of the ring (composed of consecutive defeated entities between two successive candidates) where no feedback messages will be transmitted at all.
However, the savings are not enough to reduce the constant in the leading term of the message costs (Exercise 3.10.21)
Granularity of Analysis: Bit Complexity The advantage of protocol Stages with Feedback becomes more evident when we look at communication costs at a finer level of granularity, focusing on the actual size of the messages being used.
In fact, while the “Election” messages contain values, the “feedback” messages are just signals, each containing O(1) bits.
In each stage, only the 2n “Election” messages carry a value, while the other n are signals; hence, the total number of bits transmitted will be at most.
Further Improvements? The use of electoral stages allows us to transform the election process into one of successive “eliminations,” reducing the number of candidates at each stage.
In the original protocol Stages, each surviving candidate will eliminate its neighboring candidate on each side, guaranteeing that at least half of the candidates are eliminated in each stage.
By using feedback, protocol Stages with Feedback extends the “reach” of a candidate also to the second neighboring candidate on each side, ensuring that at least two third of the candidates are eliminated in each stage.
Increasing the “reach” of a candidate during a stage will result in a larger proportion of the candidates in each stage, thus, reducing the number of stages.
So, intuitively, we would like a candidate to reach as far as possible during a stage.
Obviously the price to be paid is the additional messages required to implement the longer reach.
Whether this is possible or not is an open problem (Problem 3.10.3)
It should be clear by now that the road to improvement, on which creative ingenuity will travel, is oftentimes paved by a deeper understanding of what is already available.
A way to achieve such an understanding is by examining the functioning of the object of our improvement in “slow motion,” so as to observe its details.
We have already shown how to achieve improvements by extending the “reach” of a candidate during a stage; in a sense, this was really “speeding up” the functioning of the protocol.
Let us examine now Stages instead by “slowing down” its functioning.
In each stage, a candidate sends its id in both directions, receives an id from each direction, and decides whether to survive, be elected, or become defeated on the basis of its own value and the received ones.
Consider the example shown in Figure 3.19; the result of stages will result in candidates w, y, and v being eliminated and x and z surviving; the fate of u will depend on its right candidate neighbor, which is not shown.
We can obviously think of “sending in both directions” as two separate steps: send to one direction (say “right”) and send to the other.
Assume for the moment that the ring is oriented: “right” has the same meaning for all entities.
Consider the first step in the same example as shown in Figure 3.19; both candidates y and v already know at this time that they would not survive.
We will use each of these two steps to make an electoral decision, and we will eliminate a candidate after step (1) if it receives a smaller id in this step.
The advantage of doing so becomes clear observing that by eliminating candidates in each step of a phase, we eliminate more than that in the original phase; in the example of Figure 3.19, also x will be eliminated.
Summarizing, the idea is that at each step, a candidate sends only one message with its value, waits for one message, and decides on the basis of its value and the received one; the key is to alternate at each step the direction in which messages are sent.
Correctness The correctness of the protocol follows immediately from observing that, as usual, the candidate xmin with the smallest value will never be eliminated and that, on the contrary, it will in each step eliminate a neighboring candidate.
Hence, the number of candidates is monotonically decreasing in the steps; when only xmin is left, its message will complete the tour of the ring transforming it into the leader.
The final notification will ensure proper termination of all entities.
We can, however, be more accurate regarding the amount of elimination performed in two successive steps.
Assume that in step i, the direction is “right” (thus, it will be “left” in step i + 1)
Let di denote the number of candidates that are eliminated in step i.
Of those ni candidates that start step i, di will be defeated and only ni+1 will survive that step.
If not, it would mean that its id is smaller than id(x) and thus would eliminate x in step i + 1; but we know that x survives.
In other words, protocol Alternate is not only simple but also more efficient than all other protocols seen so far.
Recall, however, that it has been described and analyzed assuming that the ring is oriented.
If the entities have different meaning for “right,” when implementing the first step, some candidates will send messages clockwise while others in a counterclockwise direction.
Notice that in the implementation for oriented rings described above, this would lead to deadlock because we close the port we are not waiting to receive from; the implementation can be modified so that the ports are never closed (Exercise 3.10.24)
It will then happen that a candidate waiting to receive from “left” will instead receive from “right.” Call this situation a conflict.
What we need to do is to add to the protocol a conflict resolution mechanism to cope with such situations.
The first two protocols we have examined, All the Way and AsFar, did not really require the restriction Bidirectional Links; in fact, they can be used without any modification in a directed or a unidirectional ring.
The subsequent protocols Distances, Stages, Stages with Feedback, and Alternate all used the communication links in both directions, for example, for obtaining feedback.
It was through them that we have been able to reduce the costs fromO(n2) to a guaranteedO(n log n) messages.
Is “Bidirectional Links” necessary for a O(n log n) cost ?
The question is practically relevant because if the answer is positive, it would indicate that an additional investment in communication hardware (i.e., full duplex lines) is necessary to reduce the operating costs of the election task.
The answer is important also from a theoretical point of view because if positive, it would clearly indicate the “power” of the restriction Bidirectional Links.
Not surprisingly, this question has attracted the attention of many researchers.
We are going to see now that the answer is actually No.
We are also going to see that, strangely enough, we know how to do better with unidirectional links than with bidirectional ones.
First of all, we are going to show how the execution of protocols Stages and Alternate can be simulated in unidirectional links yielding the same (if not better) complexity.
Then, using the lessons learned in this process, we are going to develop a more efficient unidirectional solution.
Step (1) is clearly the difficult one because, in a unidirectional ring, messages can.
Decompose the operation “send in both directions” into two substeps: (I) “send in one direction” and then (II) “send in the other direction.”
In each stage, a candidate makes a decision on a value.
In protocol Stages, this value was always the candidate’s id.
In the unidirectional algorithm, this value is not the id; it is the first value sent by its neighboring candidate in Step (1)
Be aware that unless we add the assumption Message Ordering, it is possible that the second value arrives before the envelope.
This problem can be solved (e.g., by locally enqueueing out-of-order messages)
The cost of each stage is also the same: 2n messages.
In fact, each node will send (or forward) exactly two messages.
This shows that O(n log n) guaranteed message costs can be achieved in ring networks also without Bidirectional Links.
The corresponding protocol UniStages is shown in Figure 3.23, described not as a unidirectional simulation of Stages (which indeed it is) but directly as a unidirectional protocol.
The actions in a “right” step are the same except that “left” and “right” are interchanged.
In fact, it would lead to totally different decisions in the two cases, destroying the simulation.
Summarizing, in each step, a candidate makes a decision on a value.
In protocol Alternate, this value was always the candidate’s id.
In the unidirectional algorithm, this value changes depending on the step.
Initially, it is its own value; in the “left” step, it is the value it receives; in the “right” step, it is the value it already has.
Working out a complete example will help clarify the simulation process and dispel any confusion (Exercise 3.10.33)
Be aware that unless we add the assumption Message Ordering, it is possible that the value from step i + 1 arrives before the value for step i.
The unidirectional simulation of Alternate is shown in Figure 3.26; it has been simplified so that we elect a leader only among the initiators, and assuming Message.
The protocol can be modified to remove this assumption without changes in its cost (Exercise 3.10.34)
An Alternative Approach In all the solutions we have seen so far, both for unidirectional and bidirectional rings, we have used the same basic strategy of minimum finding; in fact in all of the protocols so far, we have elected as a leader the entity with the smallest value (either among all the entities or among just the initiators)
Obviously, we could have used maximum finding in those solution protocols, just substituting the function Min with Max and obtaining the exact same performance.
A very different approach consists in mixing these two strategies.
In all of them, what we could do is to alternate strategy in each stage: In “odd” stages we use the function Min, and in “even” stages we use the function Max.
The interesting and surprising thing is that this approach can lead to the design of a more efficient protocol for unidirectional rings.
Let us assume that every entity starts and that there is Message Ordering (we will remove both assumptions later)
When a message with value b arrives at a candidate y, y compares the received value b with the value a it sent in its last message.
If the message is discarded, y becomes defeated; otherwise, y will enter the next stage: Originate a message with content (b, j + 1) and send it.
In other words, in each stage at least one candidate will survive that stage, and the number of candidates in a stage is monotonically decreasing with the number of stages.
Thus, within finite time, there will be only one candidate left; when that happens, its message returns to it transforming it into a leader.
Note that the entity that will be elected leader will be neither the one with the smallest value nor the one with the largest value.
Let i be an odd (i.e., min) stage, and let value v survive this stage; this means that the successor of v in stage i, say u, is larger than v that is, u >v.
Let v survive also stage i + 1 (an even, i.e., max, stage)
Notice that this is exactly the same equation as the one (Equation 3.21) we derived for protocol Alternate.
In other words, we have been able to obtain the same costs of UniAlternate with a very different protocol, MinMax, described in Figure 3.29
When removing this assumption we have two options: The entities that are not initiators can be (i) made to start (as if they were initiators) upon receiving their first message or (ii) transformed into passive and just act as relayers.
The second option is the one used in Figure 3.29
As with all the other protocols we have considered, this restriction can be enforced with just local bookkeeping at each entity, without any increase in complexity (Exercise 3.10.39)
The advantage of MinMax is that it is possible to obtain additional improvements that lead to a significantly better performance.
Observe that like in most previous protocols, the defeated entities play a purely passive role, that is, they just forward messages.
The key observation we will use to obtain an improvement in performance is that these entities can be exploited in the computation.
Let us concentrate on the even stages and see if we can obtain some savings for those steps.
The message sent by a candidate travels (forwarded by the defeated entities) until it encounters the next candidate.
What we will do is to control the maximum distance to which the message will travel, following the idea we developed in Section 3.3.3
This is implemented by having in the message a counter (initially set to dis(j )) that will be decreased by one by each defeated node it passes.
What is the appropriate choice of dis(i) will be discussed next.
Every change we make in the protocol has strong consequences.
As a consequence of (I ), the message from x might not reach the next candidate y if it is too far away (more than dis(j )) (see Figure 3.31)
In this case, the candidate y does not receive the message in this stage and, thus, does not know what to do for the next stage.
It is possible that every candidate is too far away from the next one in this stage, and hence none of them will receive a message.
Controlling the distance: In even stage j , the message does not travel more than dis(j ) nodes.
In other words, we are bringing some defeated nodes back into the game making them candidates again.
This operation could be dangerous for the complexity of the protocol as the number of candidates appears to be increasing (and not decreasing)
This is easily taken care of: The originators, like y, waiting for a transfer message that will not arrive will become defeated.
The candidate that starts the next stage (e.g., z in our example) sends a message; when this message reaches a candidate (e.g., y) still waiting for a message from the previous stage, that entity will understand, become defeated, and forward the message.
We are giving decisional power to the defeated nodes, even bringing some of them back to “life.” Let us push this concept forward and see if we can obtain some other savings.
If, in the next stage, it receives a message with a smaller value, it will become candidate again and start the next stage with that value.
Notice that in this stage every message with a smaller value will be stopped earlier.
We have, however, transformed a defeated entity into a candidate.
This operation could be dangerous for the complexity of the.
This is easily taken care of: This candidates, like y, waiting for a message of an odd stage that will not arrive will become defeated.
How will y know that is defeated ? The answer again is simple.
The candidate that starts the next stage (e.g., z in our example) sends the message; when this message reaches an entity still waiting for a message from the previous stage (e.g., y), that entity will understand, become defeated, and forward the message.
First of all observe that in protocol MinMax, in each stage a message (v, i) would always reach the next candidate in that stage.
In fact, in an even stage i no message will travel more than dis(i), and in an odd stage a message can be “promoted” by a defeated node on the way.
We must concentrate on the savings in each type of stages.
With this choice of distance, we have a very interesting property.
The property also allows us to measure the number of messages we save in the odd stages.
The total number of transmissions in an odd stage i is, thus, at most.
The total number of messages in an even stage is at most n.
Thus, the total number of messages in an even stage i + 1 is at most.
If we now consider an odd stage i followed by an even stage i + 1, the total number of message transmissions in the two stages will be at most.
Thus, protocol MinMax+ is the most efficient protocol we have seen so far, with respect to the worst case.
Considering that the improvements have only been in the multiplicative constant of the n log n factor, the next question becomes: Is there a tool or a technique that would allow us to reduce the message costs for election significantly, for example, from O(n log n) to O(n)?
These type of questions are all part of a larger and deeper one: What is the message complexity of election in a ring ? To answer this question, we need to establish a lower bound, a limit that no election protocol can improve upon, regardless of the amount and cleverness of the design effort.
In this section we will see different bounds, some for unidirectional rings and others for bidirectional ones, depending on the amount of a priori knowledge the.
Unidirectional Rings We want to know what is the number of messages that any election algorithm for unidirectional rings must transmit in the worst case.
A subtler question is to determine the number of messages that any solution algorithm must transmit on the average; clearly, a lower bound on the average case is also a lower bound on the worst case.
We will establish a lower bound under the standard assumptions of Connectivity and Total Reliability, plus Initial Distinct Values (required for election), and obviously Ring.
We will actually establish the bound assuming that there is Message Ordering; this implies that in systems without Message Ordering, the bound is at least as bad.
The lower bound will be established for minimum-finding protocols; because of the Initial Distinct Values restriction, every minimum-finding protocol is also an election protocol.
Also, we know that with the additional n messages, every election protocol becomes a minimum-finding protocol.
When a minimum-finding algorithm is executed in a ring of entities with distinct values, the total number of transmitted messages depends on two factors: communication delays and the assignment of initial values.
LetA be a minimum-finding protocol under the restrictions stated above.
Consider the executions of A started simultaneously by all entities and their cost.
The average and the worst-case costs of these executions are possibly better but surely not worse than the average and the worst-case costs, respectively, over all possible executions; thus, if we find them, they will give us a lower bound.
Call global state of an entity x at time t , the content of all its local registers and variables at time t.
This means that for a fixed set of rules A, their next global state will depend solely on the current one and on what event has occurred.
In our case, once the execution of A is started, the only external events are the arrival of messages.
During an action, an entity might send one or more messages to its only outneighbor; if it is more than one, we can “bundle” them together as they are all sent within the same action (i.e., before any new message is received)
Thus, we assume that in A, only one message is sent in the execution of an action by an entity.
Associate to each message all the “history” of that message.
That is, with each message M , we associate a sequence of values, called trace, as follows: (1) If the sender has id si and has not previously received any message, the trace will be just.
Note that because of our two assumptions (simultaneous start by all entities and only one message per action), messages are uniquely described by their associated trace.
We will denote by ab the concatenation of two sequences a and b.
If d = abc, then a, b, and c are called subsequences of d; in particular, each of a, ab, and abc will be called a prefix of d; each of c, bc, and abc will be called a suffix of d.
Given a sequence a, we will denote by len(a) the length of a and by C(a) the set of cyclic permutations of a; clearly, |C(a)| = len(a)
Now we are going to measure how expensive it is for the algorithmA to distinguish between the elements of E(A)
This lower bound on the average case is also a lower bound on the number worstA(I ) of messages sent by A in the worst case in the rings labeled by I :
Thus, any improvement we can hope to obtain by clever design will at most reduce the constant; in any case, the constant cannot be smaller than 0.69
Notice that the lower bound we have established can be achieved.
In fact, protocol AsFar requires on an averagenHn messages (Theorem 3.3.1)
In other words, protocol AsFar is optimal on an average.
If the entities know n, it might be possible to develop better protocols exploiting this knowledge.
In fact, the lower bound in this case leaves a little more room but again the improvement can only be in the constant (Exercise 3.10.45):
Bidirectional Rings In bidirectional rings, the lower bound is slightly different in both derivation and value (Exercise 3.10.46):
Actually, we can improve this bound even if the entities know n (Exercise 3.10.47):
That is, even with the additional knowledge of n, any improvement can only be in the constant.
This fact has strange implications: As far as electing a leader in a ring is concerned, unidirectional rings seem to be better systems than bidirectional ones, which in turn implies that practically.
This is clearly counterintuitive: In terms of communication hardware, Bidirectional Links are clearly more powerful than half-duplex links.
On the contrary, the bounds are quite clear: Election protocols for unidirectional rings are more efficient than those for bidirectional ones.
A natural reaction to this strange status of affairs is to suggest the use in bidirectional rings of unidirectional protocols; after all, with Bidirectional Links we can send in both directions, “left” and “right,” so we can just decide to use only one, say “right.” Unfortunately, this argument is based on the hidden assumption that the bidirectional ring is also oriented, that is, “right” means the same to all processors.
In other words, it assumes that the labeling of the port numbers, which is purely local, is actually globally consistent.
This explains why we cannot use the (more efficient) unidirectional protocol in a generic bidirectional ring.
The answer is interesting—In a unidirectional ring, there is orientation: Each entity has only one out-neighbor; so there is no ambiguity as to where to send a message.
In other words, we have discovered an important principle of the nature of distributed computing:
In the case of rings, the difference is not much, just in the multiplicative constant.
As we will see in other topologies, this difference can actually be dramatic.
If the ring is both bidirectional and oriented, then we can clearly use any unidirectional protocol as well as any bidirectional one.
The important question is whether in this case we can do better than that.
That is, the quest is for a protocol for bidirectional oriented rings that.
We have seen a protocol for oriented rings, Alternate; however, it can be simulated in unidirectional rings (protocol UniAlternate)
It is not even known whether it can exist (Problem 3.10.7)
We have examined the design of several protocols for leader election in ring networks and analyzed the effects that design decisions have had on the costs.
When developing the election protocols, we have introduced some key strategies that are quite general in nature and, thus, can be used for different problems and for different networks.
Among them are the idea of electoral stages and the concept of controlled distances.
We have also employed ideas and tools, for example, feedback and notification, already developed for other problems.
Thus, the structural symmetry of the ring topology only makes the solution to the problem more difficult and more expensive.
This observation reflects a more general principle: As far as election is concerned, structural asymmetry is to the protocol designer’s advantage; on the contrary, the presence of structural symmetry is an obstacle for the protocol designer.
Mesh networks constitute a large class of architectures that includes meshes and tori; this class is popular especially for parallel systems, redundant memory systems, and interconnection networks.
These networks, like trees and rings, are sparse:m = O(n)
Using our experience with trees and rings, we will now approach the election problem in such networks.
Unoriented Mesh The asymmetry of the mesh can be exploited to our advantage when electing a leader: As it does not matter which entity becomes leader, we can elect one of the four corner nodes.
In this way, the problem of choosing a leader among (possibly) n nodes is reduced to the problem of choosing a leader among the.
The first step, to make the corners aware, can be performed doing a wake-up of all entities.
When an entity wakes up (spontaneously if it is an initiator, upon receiving a wake-up message otherwise), its subsequent actions will depend on whether it is a corner, a border, or an interior node.
In particular, the four corners will become awake and can start the actual election process.
Observe the following interesting property of a mesh: If we consider only the border and corner nodes and the links between them, they form a ring network.
We can, thus, elect a leader among the corners by using a election protocol for rings: The corners will be the only candidates; the borders will act as relayers (defeated nodes)
When one of the corner nodes is elected, it will notify all other entities of termination.
The election on the outer ring requires a little more attention.
First of all, we must choose which ring protocol we will use; clearly, the selection is among the efficient ones we have discussed at great length in the preceding sections.
Then we must ensure that the messages of the ring election protocol are correctly forwarded along the links of the outer ring.
Let us use protocol Stages and consider the first stage.
According to the protocol, each candidate (in our case, a corner node) sends a message containing its value in both directions in the ring; each defeated entity (in our case, a border node) will forward the message along the (outer) ring.
Thus, in the mesh, each corner node will send a message to the only two neighbors.
A border node y, however, has three neighbors, of which only two are in the outer ring; when y receives the message, it does not know to which of the other two ports it must forward the message.
What we will do is simple; as we do not know to which port the message must be sent, we will forward it to both: One will be along the ring and proceed safely, and the other will instead reach an interior node z; when the.
Actually, it is possible to avoid those replies without affecting the correctness (Exercise 3.10.50)
In Stages, the number of candidates is at least halved every time.
This means that after the second stage, one of the corners will determine that it has the smallest id among the four candidates and will become leader.
Broadcasting the notification can be performed using Flood, which will require less than 3n messages as it is started by a corner.
Thus in total, the protocol ElectMesh we have designed will have cost.
With a simple modification to the protocol, it is possible to save an additional.
The most expensive operation is to wake up the nodes.
Oriented Mesh A mesh is called oriented if the port numbers are the traditional compass labels (north, south, east, west) assigned in a globally consistent way.
For example, there is only one corner with link labels “south” and “west.” Thus, to elect a leader in an oriented mesh, we must just ensure that that unique node knows that it must become leader.
In other words, the only part needed is a wake-up: Upon becoming awake, and participating in the wake-up process, an entity can immediately become leader or follower depending on whether or not it is southwest corner.
Complexity These results mean that regardless of whether the mesh is oriented or not, a leader can be elected with O(n) messages, the difference being solely in the multiplicative constant.
As no election protocol for any topology can use fewer than n messages, we have.
Informally, the torus is a mesh with “wrap-around” links that transform it into a regular graph: Every node has exactly four neighbors.
In designing the election protocol, we will use the idea of electoral stages developed originally for ring networks and also use the defeated nodes in an active way.
We will also employ a new idea, marking of territory.
If the territories of two candidates intersect, one of them will see the marking of the other.
The marking is done by originating a “Marking” message (with x’s value) that will travel to distance di first north, then east, then south, and finally west to return to x.
A very important fact is that if the territory of two candidates have some elements in common, the “Marking” message of at least one of them will encounter the marking of the other (Figure 3.37)
If the id of y is instead smaller, then w will terminate the “Marking”
Summarizing, for a candidate x to survive, it is necessary that it receives its “Marking” message back.
If SawLarger= false, then that suffices; if SawLarger= true, x must also receive a “SeenbyLarger” message.
Note that if x receives a “SeenbyLarger(z, i)” message, then z did not finish marking its boundary; thus z does not survives this stage.
In other words, if x survives, either its message found no other markings, or at least another candidate does not survive.
It will only be part of the boundary of the territory of the candidate with the smallest id.
This means that if w was part of the boundary of some candidate x and now becomes part of the boundary of y, a subsequent “SeenbyLarger” message intended for x will be sent along the boundary of y.
To keep the number of messages small, we will also limit the number of “SeenbyLarger” messages sent by a relayer.
If it survives, in all subsequent stages the marking becomes simpler.
The situation where there is only one candidate left will be for sure reached after a constant number p of stages after the wrap-around occurs, as we will see later.
Let us now discuss the correctness and cost of the algorithm, protocol MarkBoundary, we have just described.
Correctness and Cost For the correctness, we need to show progress, that is, at least one candidate survives each stage of the algorithm, and termination, that is, p stages after wrap-around there will be only one candidate left.
A candidate whose “Marking” message does not encounter any other boundary will survive this stage; so the only problem would be if, in a stage, every “Marking” message encounters another candidate’s boundary, and somehow none of them advances.
In fact, if every “Marking” message encounters another candidate’s boundary, the one with the largest id will encounter a smaller id; the candidate with this smaller id will go onto the next stage unless its message encounters the boundary with an even smaller id, and so on; however, the message of the candidate with the smallest id cannot encounter a larger id (because it is the smallest) and, thus, that entity would survive this stage.
For termination, the number of candidates does decrease overall, but not in a simple way.
However, it is possible to bound the maximum number of candidates.
However, the effect is not large and will just affect the low-order terms of the cost (Exercise 3.10.55)
Unoriented Torus The algorithm we just described solved the problem of electing a leader in an oriented torus, for example, among the buildings in Manhattan (well known for its mesh-like design), by sending a messenger along east-west streets and north-south avenues, turning at the appropriate corner.
Consider now the same problem when the streets have no signs and the entities have no compass.
Interestingly, the same strategy can be still used: A candidate needs to mark off a square; the orientation of the square is irrelevant.
To be able to travel along a square, we just need to know how to.
We first consider how to forward a message in the direction opposite to the one from which the message was received, without knowing the directions.
In the example of Figure 3.38, x will receive the message originating from z via both port a and port b; it, thus, knows that a is not opposite to b.
It also receives the message from y via ports a and c; thus x knows also that a is not opposite to c.
Then, x can conclude that a is opposite to d.
It will then locally relabel one pair of opposite ports as east, west, and the other north, south; it does not matter which pair is chosen first.
As a result of the the previous operation, each entity x knows two perpendicular directions, but the naming (north, south) and (east, west) might not be consistent with the one done by other entities.
This can create problems when wanting to make a consistent turn.
To achieve the turn correctly, we add a simple information, called handrail, to a message.
The handrail is the id of the neighbor in the direction the message must turn and the name of the direction.
In the example of Figure 3.38, if x is sending a message south that must then turn east , the handrail in the message will be the id of its eastern neighbor q plus the direction “east.” Because every entity knows the ids and the relative position of all the entities within distance 2, when y receives this message with the handrail from x, it can determine what x means by “east,” and thus in which direction the message must turn (when the algorithm prescribes it)
Summarizing, even without a compass, we can execute the protocol MarkBorder, by adding the preprocessing phase and including the handrail information in the messages.
We will solve the election problem in oriented hypercubes using the approach electoral stages that we have developed for ring networks.
The metaphor we will use is that of a fencing tournament: in a stage of the tournament, each candidate, called duelist, will be assigned another duelist, and each pair will have a match; as a result.
In each stage, only half of the duelists enter the next stage; at the end, there will be only one duelist that will become the leader and notify the others.
Deciding the outcome of a match is easy: The duelist with the smaller id will win; for reasons that will become evident later, we will have the defeated duelist remember the shortest path to the winning duelist.
The crucial and difficult parts are how pairs of opposite duelists are formed and how a duelist finds its competitor.
To understand how this can be done efficiently, we need to understand some structural properties of oriented hypercubes.
We can use this property to form the pairs of duelists in each stage of the tournament:
In this way, the message from x will eventually reach y; the path information in the message is updated during its travel so that y will know the dimensions traversed by the message from x to y in chronological order.
The Match message from y will reach x with similar information.
The match between x and y will take place both at x and y; only one of them, say x, will enter stage i + 1, while the other, y, is defeated.
From now on, if y receives a Match message, it will forward it to x; as mentioned before, we need this to be done on the shortest path.
How can y (the defeated duelist) know the shortest path to x (the winner)?
We can perform the compression while the message is traveling from x to y; in this way, the message will contain at most k labels.
Finally, we must consider the fact that owing to different transmission delays, it is likely that the computation in some parts of the hypercube is faster than in others.
Thus, it may happen that a duelist x in stage i sends a Match message for its opponent, but the entities on the other side of dimension i are still in earlier stages.
A duelist in stage i will send a Match message on the edge with label i.
When a duelist y in stage j receives a Match message from a duelist x in stage i > j , y will enqueue the message and process it (as a newly arrived one) when it enters stage i or becomes defeated.
The protocol terminates when a duelist wins the kth stage.
As we will see, when this happens, that duelist will be the only one left in the network.
NextDuelist denotes the (list of labels on the) path from a defeated node to the duelist that defeated it.
Given a list of labels list, the protocol uses the following functions:
To store the delayed messages, we use a set Delayed that will be kept sorted by stage number; for convenience, we also use a set delay of the corresponding stage numbers.
Correctness and termination of the protocol derive from the following fact (Exercise 3.10.61):
Lemma 3.5.1 Let id(x) be the smallest id in one of the hypercubes of dimension i in Hk:i.
This means that when i = k, there will be only one duelist left at the end of that stage; it will then become leader and notify the others so to ensure proper termination.
Now we know how much does it cost for a Match message to reach its destination.
What we need to determine is how many such messages are generated in each stage;
Practical Considerations The O(n) message cost of protocol HyperElect is achieved by having the Match messages convey path information in addition to the usual id and stage number.
In particular, the fields Source and Dest have been described as lists of labels; as we only send compressed paths, Source and Dest contain at most log n labels each.
So it would appear that the protocol requires “long” messages.
We will now see that in practice, each list only requires log n bits (i.e., the cost of a counter)
This also implies that the cost in terms of bits of the protocol will be no more than.
Hypercubes with arbitrary labellings obviously do not have the properties of oriented hypercubes.
It is still possible to take advantage of the highly regular structure of hypercubes to do better than in ring networks.
To date, it is not known whether it is possible to elect a leader in an hypercube in just O(n) messages even when it is not oriented (Problem 3.10.9)
We have seen how structural properties of the network can be effectively used to overcome the additional difficulty of operating in a fully symmetric graph.
For example, in oriented hypercubes, we have been able to achieve O(n) costs, that is, comparable to those obtainable in trees.
The ring is the sparsest network and it is an extreme in the spectrum of regular networks.
At the other end of the spectrum lies the complete graph Kn; in Kn, each node is connected directly to every other node.
Another interesting property is thatKn contains every other networkG as a subgraph! Clearly, physical implementation of such a topology is very expensive.
Let us examine how to exploit such very powerful features to design an efficient election protocol.
To develop an efficient protocol for election in complete networks, we will use electoral stages as well as a new technique, territory acquisition.
In territory acquisition, each candidate tries to “capture” its neighbors (i.e., all other nodes) one at a time; it does so by sending a Capture message containing its id as well as the number of nodes captured so far (the stage)
If the attempt is successful, the attacked neighbor becomes captured, and the candidate enters the next stage and.
The candidate that is successful in capturing all entities becomes the leader.
Summarizing, at any time an entity is candidate, captured, or passive.
A captured entity remembers the id, the stage, and the link to its “owner” (i.e., the entity that captured it)
A candidate entity x sends a Capture message to a neighbor y.
If y is already captured, then x has to defeat y’s owner z before capturing y.
Specifically, a Warning message with x’s id and stage is send by y to its owner z.
If the attack is successful, y is captured by x, x increments stage(x) and proceeds with its conquest.
Notice that each attempt from a candidate costs exactly two messages (one for the Capture, one for the notification) if the neighbor is also a candidate or passive; instead, if the neighbor was already captured, two additional messages will be sent (from the neighbor to its owner, and back)
The strategy just outlined will indeed solve the election problem (Exercise 3.10.65)
Even though each attempt costs only four (or fewer) messages, the overall cost can be prohibitive; this is because of the fact that the number ni of candidates at level i can in general be very large (Exercise 3.10.66)
To control the number ni , we need to ensure that a node is captured by at most one candidate in the same level.
In other words, the territories of the candidates in stage i must be mutually disjoint.
First of all, we provide some intelligence and decisional power to the captured nodes:
As a consequence, a captured node y will only issue a Warning for an attack at the highest level known to y.
As a consequence, if the attack from x was successful (and the stage increased), y will send to the new owner x any subsequent Warning generated by processing the enqueued Capture messages.
After this change, the territory of any two candidates in the same level are guaranteed to have no nodes in common (Exercise 3.10.64)
How many candidates there can be in stage i? As each of them has a territory.
Let us now determine the number of stages needed for termination.
Hence, a candidate can become leader as soon as it reaches that stage (it will then broadcast a termination message to all nodes)
Let us now consider the time cost of the protocol.
It is not difficult to see that in the worst case, the ideal time of protocol CompleteElect is linear (Exercise 3.10.67):
This must be contrasted with theO(1) time cost of the simple strategy of each entity sending its id immediately to all its neighbors, thus receiving the id of everybody else, and determining the smallest id.
Appropriately combining the two strategies, we can actually construct protocols that offer optimalO(n log n) message costs withO(n/ log n) time (Exercise 3.10.68)
We have just developed an efficient protocol for election in complete networks.
Observe that this is the same as we were able to do in ring networks (actually, the multiplicative constant here is worse)
Unlike rings, in complete networks, each entity has a direct link to all other entities and there is a total of O(n2) links.
By exploiting all this communication hardware, we should be able to do better than in rings, where there are only n links, and where entities can be O(n) far apart.
The most surprising result about complete networks is that in spite of having available the largest possible amount of connection links and a direct connection between any two entities, for election they do not fare better than ring networks.
The lower bound we have just seen carries a very strong and rather surprising message for network development: in so far election is concerned, complete networks are not worth the large communication hardware costs.
The facts that Election is a basic problem and its solutions are routinely used by more complex protocols makes this message even stronger.
The message is surprising because the complete graph, as we mentioned, has the most communication links of any network and the shortest possible distance between any two entities.
The question becomes: which tool is powerful enough? As each property we assume restricts the applicability of the solution, our quest for a powerful tool should be focused on the least restrictive ones.
In this section, we will see how to answer this question.
In the process, we will discover some intriguing relationships between port numbering and consistency and shed light on some properties of whose existence we already had an inkling in earlier section.
We will first examine a particular labeling of the ports that will allow us to make full use of the communication power of the complete graph.
This approach will yield a solution requiring 2n log n messages in the worst case, thus already improving on CompleteElect.
Consider a candidate entity x executing stage i: It will send an election message each in both directions, which will travel along the ring until they reach another candidate, say y and z (see Figure 3.47)
This operation will require the transmission of d(x, y)+ d(x, z) messages.
Similarly, x will receive the Election messages from both y and z, and decide whether it survives this stage or not, on the basis of the received ids.
Assume that the Election message contains also a counter, initialized to one, which is increased by one unit by each node forwarding it.
Then, a candidate receiving the Election message knows exactly which port label connects it to the originator of that message.
In our example, the election message from y will have a counter equal to.
Similarly, when an entity forwards the Election message through a link, it will add to the counter the weight of that link.
This means that if we consider the graph G composed of all these rings, then the number of links m(G) of G is exactly m(G) = n(2)+
Thus, to determine the cost of the protocol, we need to find out the value of m(G)
Notice that if we were to use Alternate instead of Stages as ring protocol (as we can), we would use fewer messages (Exercise 3.10.72)
In any case, the conclusion is that the chordal labeling allows us to finally harvest the communication power of complete graphs and do better than in ring networks.
Clearly, we can exploit the techniques we designed for complete graph with chordal labeling to develop an efficient election protocol for the entire class of chordal ring networks.
Execute an efficient ring election protocol (e.g., Stages or Alternate) on the outer ring.
As we did in Kelect, the message sent in a stage will carry a counter, updated using the link labels, that will be used to compute the distance between two successive candidates.
Use the chords to bypass defeated nodes in the next stage.
In view of the matching upper bound (Exercise 3.10.76), we have.
We have so far studied in detail the election problem in specific topologies; that is, we have developed solution protocols for restricted classes of networks, exploiting in their design all the graph properties of those networks so as to minimize the costs and increase the efficiency of the protocols.
In this process, we have learned some strategies and principles, which are, however, very general (e.g., the notion of electoral stages), as well as the use of known techniques (e.g., broadcasting) as modules of our solution.
We will now focus on the main issue, the design of universal election protocols, that is, protocols that run in every network, requiring neither a priori knowledge of the topology of the network nor that of its properties (not even its size)
In terms of communication software, such protocols are obviously totally portable, and thus highly desirable.
In this section, we will discuss the design of an efficient algorithm for leader election, called Mega-Merger.
This protocol is topology independent (i.e., universal) and constructs a (minimum cost) rooted spanning tree of the network.
Nodes are small villages each with a distinct name, and edges are roads each with a different distance.
The goal is to have all villages merge into one large megacity.
A city (even a small village will be considered such) always tries to merge with the closest neighboring city.
When merging, there are several important issues that must be resolved.
First and foremost is the naming of the new city.
The resolution of this issue depends on how far the involved cities have progressed in the merging process, that is, on the level they have reached and on whether the merger decision is shared by both cities.
The second issue to be resolved during a merging is the decision of which roads of the new city will be serviced by public transports.
When a merger occurs, the roads of the new city serviced by public transports will be the roads of the two cities already serviced plus only the shortest road connecting them.
Let us clarify some of these concepts and notions, as well as the basic rules of the game.
A city is a rooted tree; the nodes are called districts, and the root is also known as downtown.
Each city has a level and a unique name; all districts eventually know the name and the level of their city.
Edges are roads, each with a distinct distance (from a totally ordered set)
The city roads are only those serviced by public transport.
Initially, each node is a city with just one district, itself, and no roads.
To request the merging, a Let-us-Merge message is sent on the shortest road connecting it to that city.
The decision to request for a merger must originate from downtown and until the request is resolved, no other request can be issued from that city.
When a merger occurs, the roads of the new city serviced by public transports will be the roads of the two cities already serviced plus the shortest road connecting them.
Thus, to merge, the downtown of city A will first determine the shortest link, which we shall call the merge link, connecting it to a neighboring city; once this is done, a Let-us-Merge is sent through that link; the message will contain information identifying the city, its level, and the chosen merge link.
Once the message reaches the other city, the actual merger can start to take place.
Let us examine the components of this entire process in some details.
We will consider city A, denote by D(A) its downtown, by level(A) its current level, and by e(A) = (a, b) the merge link connecting A to its closest neighboring city; let B be such a city.
Node b will be called the entry point of the request from A to B, and node a the exit point.
Once the Let-us-Merge message from a inA reaches the district b ofB, three cases are possible.
If the two cities have the same level and each asks to merge with the other, we have what is called a friendly merger: The two cities merge into a new one; to avoid any conflict, the new city will have a new name and a new downtown, and its level is increased:
If level(A) = level(B) and the merge link chosen by A is the same as that chosen by B (i.e., e(A) = e(B)), then A and B perform a friendly merger.
If a city asks a merger with a city of higher level, it will just be absorbed, that is, it will acquire the name and the level of the other city:
In all other cases, the request for merging and, thus, the decision on the name are postponed :
If level(A) > level(B), the merge process of A with B is suspended: x will locally enqueue the message until the level of b’s city is at least as large as the one of A.
As we will see later, this case will never occur.
Absorption The absorption process is the conclusion of a merger request sent by A to a city with a higher level (rule 9)
First of all, the entry point b will notify a (the exit point ofA) that the outcome of the request is absorption, and it will include in the message all the relevant information aboutB (name and level)
Once a receives this information, it will broadcast it in A; as a result, all districts of A will join the new city and know its name and its level.
To transform A so that it is rooted in the new downtown is fortunately simple.
This can be done as follows: Each of the districts of B on the path from a to D(A), when it receives the broadcast from a, will locally direct toward B two links: the one from which the broadcast message is received and the one toward its old downtown.
To make the districts ofA be rooted inD(B), the logical direction of the links (in bold) from the downtown to the exit point of A has been “flipped.”
Friendly Merger If A and B are at the same level in the merging process (i.e., level(A) = level(B)) and want to merge with each other (i.e., e(A) = e(B)), we have.
Notice that if this is the case, a must also receive a Let-us-Merge message from b.
The two cities now become one with a new downtown, a new name, and an increased level:
Both a and b will independently compute the new name, level, and downtown.
Then each will broadcast this information to its old city; as a result, all districts of A and B will join the new city and know its name and its level.
Both A and B must be transformed so that they are rooted in the new downtown.
As discussed in the case of absorption, it is sufficient to “flip” the logical direction only of the edges in the path from the a to the old downtown of A, and of those in the path from b to the old downtown of B (Figure 3.51)
Notice that in case of suspension, nobody from city A knows that their request has been suspended; because of rule (6), no other request can be launched from A.
Concentrate on part (5.1) and consider a district ai ; it must find among its incident edges the shortest one that leads to another city.
Obviously, ai does not need to consider the internal roads (i.e., those that connect it to other districts of A)
Unfortunately, if a link is unused, that is, no message has been sent or received through it, it is impossible for ai to know if this road is internal or leads to a neighboring city (Figure 3.52)
In other words, ai must also try the internal unused roads.
Thus, ai will determine the shortest unused edge e, prepare a Outside? message, send it on e, and wait for a reply.
Consider now the district c on the other side of e, which receives this message; c knows the name(C) and the level(C) of its city (which could, however, be changing)
If name(A) = name(C) (recall that the message contains the name of A), c will reply Internal to ai , the road e will be marked as internal (and no longer used in the protocol) by both districts, and ai will restart its process to find the shortest local unused edge.
In conclusion to determine if a link is internal should be simple, but, due to concurrency, the process is neither trivial nor obvious.
This is easy to accomplish; it is just a minimum finding in a rooted tree, for which we can use the techniques discussed in Section 2.6.7
Specifically, the entire process is composed of a broadcast of a message informing all districts in the city of the current name and level (i) of the city, followed by a covergecast.
Issues and Details We have just seen in details the process of determining the merge link as well as the rules governing a merger.
In fact, our rules take explicitly into account the interaction between neighboring cities at different levels.
There are a few situations where the application of the rules will not be evident and thus require a more detailed treatment.
Thus, to decide if it is a friendly merger, b needs to know both e(A) and e(B)
When the Let-us-Merge message sent from a arrives to b, it knows e(A) = (a, b)
As we have seen, the choice of e(B) is made by the downtown D(B), which will forward the merger request message of B towards the exit point.
If e(A) = e(B), b is the exit point and, thus, it will eventually receive the message to be sent to a; then (and only then) b will know the answer to the question, and that it is dealing with a friendly merger.
Thus, what really happens when the Let-us-Merge message from A arrives at b, is the following.
If b has received already a Let-us-Merge message from its downtown to be sent to a, then b knows that is a friendly merger; also a will know when it receives the request from b.
Note for hackers: thus, in this case, no reply to the request is really necessary.
Otherwise b does not know; thus it waits: if it is a friendly merger, sooner or later the message from its downtown will arrive and bwill know; ifB is requesting another city, eventually the level of b’s city will increase becoming greater than level(A) (which, asA is still waiting for the reply, cannot increase), and thus result inA being absorbed.
As b knows (it just found out receiving the message) but a possibly does not, b will send to a the reply Internal.
However, if b also had sent to a an Outside? message, when a receives that message, it will find out that (a, b) is internal, and the Internal reply would be redundant.
In other words, if a and b from the same city independently send to each other an Outside? message, there is no need for either of them to reply Internal to the other.
Suppose that, when b receives the message, it is computing the merge link for its city B; as its level is i, we will call it the i-level merge link.
What b will do in this case, is to first proceed with the absorption of A (so to involve it in the i-level merge-link computation), and then to continue its own computation of the merge link.
More precisely, b will start the broadcast in A of the name and level of B asking the districts there to participate in the computation of the i-level merge link for B, and then resume its computation.
Suppose instead that b has already finished computing the i-level merge link for its city B; in this case, b will broadcast in A the name and level of B (so to absorb A), but without requesting them to participate in the computation of the i-level merge link for B (it is too late)
Notice that broadcasts are already used following the discovery of a friendly merger or an absorption.
When the two exit points know that it is a friendly merger, the notification they broadcast will inform all districts in the merged city of the new level, new name, and to start computing the next merge link.
In other words, the notification is exactly the “start next” broadcast.
In the case of an absorption, as we just discussed, a “start-next” broadcast is needed only if it is not too late for the new districts to participate in the current calculation of the merge link.
If it is not too late, the notification message contains the request to participate in the next merge-link calculation; thus, it is just the propagation of the current “start-next” broadcast in this new part of the city.
In other words, the “notification” broadcasts act as “start-next” broadcasts, if needed.
A city only carries out one merger request at a time, but it can be asked concurrently by several cities, which in turn can be asked by several others.
Due to communication delays, some districts will be taking decisions on the basis of the information (level and name of its city) that is obsolete.
It is not difficult to imagine very intricate and complex scenarios that can easily occur.
How do we know that, in spite of concurrency and postponements and communication delays, everything will eventually work out? How can we be assured that some decisions will not be postponed forever, that is, there will not be deadlock? What guarantees that, in the end, the protocol terminates and a single leader will be elected? In other words, how do we know that the protocol is correct?
Because of its complexity and the variety of scenarios that can be created, there is no satisfactory complete proof of the correctness of the Mega-Merger protocol.
We will discuss here a partial proof that will be sufficient for our learning purposes.
Finally, we will discuss the assumption of having distinct lengths associated to the links, examine some interesting connected properties, and then remove the assumption.
Observe that if there is no suspension, there is no problem.
Property 3.8.1 If a city at level l will not be suspended, its level will eventually increase (unless it is the megacity)
To see why this is true, consider the operations performed by a city C at a level l: Compute the merge edge and send a merge request on the merge edge.
If it is not suspended, its merge request arrives at a city D with either a larger level (in which case, C is absorbed and its level becomes level(D)) or the same level and same merge edge (the case in which the two cities have a friendly merger and their level increases)
So, only suspensions can create problems, but not necessarily so.
Property 3.8.2 Let city C at level l be suspended by a district d in city D.
If the level of the city of D becomes greater than l, C will no longer be suspended and its level will increase.
This is because once the level of D becomes greater than the level of C, d can answer the Outside? message in case (i), as well as the Let-us-Merge message in case (ii)
Thus, the only real problem is the presence of a city suspended by another whose level will not grow.
We are now going to see that this cannot occur.
Consider the smallest level l of any city at time t , and concentrate on the cities C operating at that level at that time.
Property 3.8.3 No city in C will be suspended by a city at higher level.
This is because for a suspension to exist, the level of D can not be greater than the level of C (see the cases above)
Property 3.8.4 There will be no cycles of suspensions within C.
Remember this fact and let us proceed with the proof.
As a consequence of the property, all cities in C will eventually increase their level: first, the ones involved in a friendly merger, next those that had chosen them for a merger (and thus absorbed by them), then those suspended by the latter, and so on.
This implies that at no time there will be deadlock and there is always progress: Use the properties to show that the ones with smallest level will increase their value; when this happens, again the ones with smallest level will increase it, and so on.
Property 3.8.5 Protocol Mega-Merger is deadlock free and ensures progress.
Termination We have just seen that there will be no deadlock and that progress is guaranteed.
This means that the cities will keep on merging and eventually the.
Fortunately, termination detection is simple to achieve; as one might have suspected, it is the downtown of the megacity that will determine that the process is terminated.
Cost In spite of the complexity of protocol Mega-Merger, the analysis of its cost is not overly difficult.
We will first determine how many levels there can be and then calculate the total number of messages transmitted by entities at a given level.
The Number of Levels A district acquires a larger level because its city has been either absorbed or involved in a friendly merger.
Notice that when there is absorption, only the districts in one of the two cities increase their level, and thus the max level in the system will not be increased.
The max level can only increase after a friendly merger.
How high can the max level be ? We can find out by linking the minimum number of districts in a city to the level of the city.
Property 3.8.8 No city will reach a level greater than log n.
We do know that every district (except the downtown) of a city of level i receives a broadcast message informing it that its current level is i, and to start computing the i-level merge-link (this last part may not be included)
Hence at most every district will receive such a message, accounting for a total of n messages.
If the received broadcast also requests to compute the i-level edge-merge link, a district must find its shortest outgoing link, by using Outside? messages.
This means that excluding the number of level i messages Outside? whose reply.
In fact, dealing with networks, usually there is a value associated with a link denoting, for example, the cost of using that link, the transmission delays incurred when sending a message through it, and so forth.
In these situations, when constructing a spanning tree (e.g., to use for broadcasting), the prime concern is how to construct the one of minimum cost, that is, where the sum of the values of its link is as small as possible.
For example, if the value of the link is the cost of using it, a minimum-cost spanning tree is one where broadcasting would be the cheapest (regardless of who is the originator of the broadcast)
Not surprisingly, the problem of constructing a minimum-cost spanning tree is important and heavily investigated.
We have seen that protocol Mega-Merger constructs a rooted spanning tree of the network.
What we are going to see now is that this tree is actually the unique minimumcost spanning tree of the network.
We are also going to see how the nonstandard assumptions that we have made about the existence of unique lengths can be easily removed.
Minimum-Cost Spanning Trees In general, a network can have several minimumcost spanning trees.
For example, if all links have the same value (or have no value), then every spanning tree is minimal.
Property 3.8.11 If the link values are distinct, a network has a unique minimum-cost spanning tree.
Assuming that there are distinct values associated to the links, protocol MegaMerger constructs a rooted spanning tree of the network.
What we are going to see now is that this tree is actually the unique minimum-cost spanning tree of the network.
To see why this is the case, we must observe a basic property of the minimum-cost spanning tree T.
Property 3.8.12 Let A be a fragment of T, and let e be the link of minimum value among those connecting A to other fragments; let B be the fragment connected by A.
Then the tree composed by merging A and B through e is also a fragment of T.
Notice that the correctness of the process depends crucially on Property 3.8.11, and thus on the distinctness of the link values.
Creating Unique Lengths We will now remove the assumptions that there are values associated to the links and these values are unique.
Summary Protocol Mega-Merger is a universal protocol that constructs a (minimum-cost) spanning tree and returns it rooted in a node, thus electing a leader.
If there are no initial distinct values on the links, a preprocessing phase needs to be added, in which each entity exchanges its unique id with its neighbors; then the actual execution of the protocol can start.
The total cost of the protocol (with or without preprocessing phase) is O(m+ n log n), which, we will see, is worst case optimal.
The main drawback of Mega-Merger is its design complexity, which makes any actual implementation difficult to verify.
We will now examine another universal protocol for leader election.
Unlike the previous one, it has simple specifications, and its correctness is simple to establish.
This protocol, called YO-YO, is a minimum-finding algorithm and consists of two parts: a preprocessing phase and a sequence of iterations.
Setup In the preprocessing phase, called Setup, every entity x exchanges its id with its neighbors.
As a result, it will receive the id of all its neighbors.
Then, x will logically orient each incident link (x, y) in the direction of the entity (x or y), with the largest id.
In fact, the orientation of each link will be consistent at both end nodes.
As a result of the setup, each node will know whether it is a source, a sink, or an internal node.
We will also use the terminology of “down” referring to the direction toward the sinks, and “up” referring to the direction toward the sources (see Figure 3.53)
Once this preprocessing is completed, the second part of the algorithm start.
As YO-YOs is a minimum-finding protocol, only the local minima (i.e., the sources) will be the candidates (Figure 3.54)
Iteration The core of the protocol is a sequence of iterations.
Each iteration acts as an electoral stage in which some of the candidates are removed from consideration.
Each iteration is composed of two parts, or phases, called YO- and -YO.
Its purpose is to propagate to each sink the smallest among the values of the sources connected to that sink (see Figure 3.54(a))
A source sends its value down to all its out-neighbors.
A sink waits until it receives a value from all its in-neighbors.
It then computes the minimum of all received values and starts the second part of the iteration.
In the sense that there is a directed path from the source to that sink.
Its purpose is to eliminate some candidates, transforming some sources into sinks or internal nodes.
This is done by having the sinks inform their connected sources of whether or not the id they sent is the smallest seen so far (see Figure 3.54(b))
A sink sends YES to all in-neighbors from which the smallest value has been received.
An internal node waits until it receives a vote from all its out-neighbors.
If all votes are YES, it sends YES to all in-neighbors from which the smallest value.
If at least a vote was NO, it sends NO to all its in-neighbors.
YES, it survives this iteration and starts the next one.
If at least a vote was NO, it is no longer a candidate.
Before the next iteration can be started, the directions on the links in the DAG must be modified so that only the sources that are still candidate (i.e., those that received only YES) will still be sources; clearly, the modification must be done.
In other words, we must transform the DAG into a new one, whose only sources are the undefeated ones in this iteration.
We need only to “flip” the direction of each link where a NO vote is sent (see Figure 3.55(a))
When a node x sends NO to an in-neighbor y, it will reverse the (logical) direction of that link (thus, y becomes now an out-neighbor of x)
When a node y receives NO from an out-neighbor x, it will reverse the (logical) direction of that link (thus, x becomes now an in-neighbor of y)
As a result, any source that receives a NO will cease to be a source; it can actually become a sink.
Some sinks may cease to be such and become internal nodes, and some internal nodes might become sinks.
However, no sink or internal node will ever become a source (Exercise 3.10.83)
A new DAG is, thus, created, where the sources are only those that received all YES in this iteration (see Figure 3.55(b))
Once a node has completed its part in the -YO phase, it will know whether it is a source, a sink, or an internal node in the new DAG.
The next iteration could start now, initiated by the sources of the new DAG.
Property 3.8.14 Applying an iteration to a DAG with more than one source will result into a DAG with fewer sources.
The source with smallest value will still be a source.
In each iteration, some sources (at least one) will be no longer sources; in contrast to this, the source with the smallest value will be eventually the only one left under consideration.
In other words, eventually the DAG will have a single source (the overall minimum, say c), and all other nodes are either sinks or internal nodes.
How can c determine that it is the only source left, and thus it should become the leader?
If we were to perform an iteration now, only c’s value will be sent in the YO- phase, and only YES votes will be sent in the -YO phase.
The source c will receive only YES votes; but c has received only YES votes in every iteration it has performed (that is why it survived as a source)
How can c distinguish that this time is different, that the process should end? Clearly, we need some additional mechanisms during the iterations.
We are going to add some meta-rules, called Pruning, which will allow to reduce the number of messages sent during the iterations, as well as to ensure that termination is detected when only one source is left.
Pruning The purpose of pruning is to remove from the computation, nodes and links that are “useless,” do not have any impact on the result of the iteration; in other words, if they were not there, still the same result would be obtained: The same sources would stay sources, and the others defeated.
Once a link or a node is declared “useless,” during the next iterations it will be considered nonexistent and, thus, not used.
If a sink is a leaf (i.e., it has only one in-neighbor), then it is useless; it then asks its parent to be pruned.
If a node is asked to prune an out-neighbor, it will do so by declaring useless (i.e., removing from consideration in the next iterations) the connecting link.
Notice that after pruning a link, a node might become a sink; if it is also a leaf, then it becomes useless.
The other meta-rule is geared toward reducing the communication of redundant information.
During YO- phase, a (internal or sink) node might receive the value of the same source from more than one in-neighbor; this information is clearly redundant as, to do its job (choose the minimum received value), it is enough for the node to receive just one copy of that value.
This means that in the DAG, there are directed paths from s to (at least) k distinct in-neighbors of x.
In fact, if we had removed the links between x and all those in-neighbors except one, x would still have received the value of s from that neighbor.
Notice that the choice regarding the link that should be kept is irrelevant.
If in the YO- phase, a node receives the same value from more than one inneighbor, it will ask all of them except one to prune the link connecting them and it will declare those links useless.
If a node receives such a request, it will declare useless (i.e., remove from consideration in the next iterations) the connecting link.
We will have this communication take place during the -YO phase: The message containing the vote will also include the request, if any, to declare that link useless.
Let us return now on our concern on how to detect termination.
To understand how and why, consider the effect of performing a full iteration (with pruning) on a DAG with only one source.
Property 3.8.15 If the DAG has a single source, then, after an iteration, the new DAG is composed of only one node, the source.
In other words, when there is a single source c, all other nodes will be removed, and c will be the only useful node left.
This situation will be discovered by c when, because of pruning, it will have no neighbors (Figure 3.59)
Costs The general formula expressing the costs of protocol YO-YO is easy to establish; however, the exact determination of the costs expressed by the formula is still an open research problem.
In the Setup phase, each node sends its value to all its neighbors; hence, on each link there will be two messages sent, for a total of 2m messages.
In the YO- stage, every useful node (except the sinks) sends a message to its out-neighbors; hence, on each link still under consideration, there will be exactly one message sent.
Similarly, in the -YO stage, every useful node (except the sources) sends a message to its in-neighbors; hence, on each link there will be again only one message sent.
Thus, in total in iteration i there will be exactly 2mi messages, where mi is the number of links in the DAG used at stage i.
To see why this is the case, consider any two neighbors a and b in G(1)
As, by definition, the corresponding sources in D(1) have a common sink, at least one of these two sources will be defeated (because the sink will vote YES to only one of them)
This means that if we take any path in G(1), at least half of the nodes on that path will correspond to sources that will cease to be such at the end of this iteration.
In a DAG, two sources a and b are said to have a common sink c if c is reachable from both a and b.
We can thus establish that without pruning, that is, with mi = m, we have a O(m log n) total cost.
The unsolved problem is the determination of the real cost of the algorithm, when the effects of pruning are taken into account.
We have seen a complex but rather efficient protocol, MegaMerger, for electing a leader in an arbitrary network.
In fact, it uses O(m+ n log n) messages in the worst case.
This means that in a ring network it uses O(n log n) messages and it is thus optimal, without even knowing that the network is a ring.
The next question we should ask is how efficient a universal election protocol can be.
First of all observe that any election protocol requires to send a message on every.
This means that protocol MegaMerger is the worst case optimal and we know the complexity of the election problem.
We are now going to see that constructing a spanning tree SPT and electing a leader Elect are strictly equivalent: Any solution to one of them can be easily modified so as to solve the other with the same message cost (in order of magnitude)
First of all, observe that , similarly to the Election problem, SPT also requires a message to be sent on every link (Exercise 3.10.85):
We are now going to see how we can construct a spanning-tree construction algorithm from any existing election protocol.
Let A be an election protocol; consider now the following protocol B:
Recall that protocol Shout (seen in Section 2.5) will correctly construct a spanning tree if there is a unique initiator.
What is the cost ? As Shout uses exactly 2m messages, we have.
Using C as the first step, it is easy to construct an election protocol D where (Exercise 3.10.86)
In other words, the message complexity of Elect is no more than that of Elect plus at most another O(n) messages; as election requires more than O(n) messages anyway (Property 3.8.17), this means that.
Using similar arguments, it is possible to establish the computational and complexity equivalence of election with several other problems (e.g., see Exercise 3.10.87)
Election in a ring network is one of the first problems studied in distributed computing from an algorithmic point of view.
The first solution protocol, All the Way, is due to Gerard Le Lann [29] proposal for unidirectional rings.
James Burns developed the first lower bound for bidirectional rings [9]
The fact that a chordal labeling allows to fully exploit the communication power of the complete graph was observed by Michael Loui, Teresa Matsushita, and Douglas West, who developed the first O(n) protocol for such a case [32]
The quest for the smallest cord structure has seen k being reduced from O(log n) first toO(log log n) by T.Z.
Exercise 3.10.2 Design an efficient single-initiator protocol to find the minimum value in a ring.
Exercise 3.10.7 Show that in protocol Stages, there will be at most one enqueued message per closed port.
Describe how to construct the “worst configuration” for any n.
Exercise 3.10.10 Determine the ideal time complexity of protocol Stages.
Produce an example in which the costs of Stages* are actually smaller.
Exercise 3.10.16 Write the rules of protocol Stages with Feedback assuming message ordering.
Exercise 3.10.17 Derive the ideal time complexity of protocol Stages with Feedback.
Exercise 3.10.18 Write the rules of protocol Stages with Feedback enforcing message ordering.
Exercise 3.10.21 Give a more accurate estimate of the message costs of protocol Stages with Feedback.
Describe how to construct the “worst configuration” for any n.
Exercise 3.10.24 Implement the alternating step strategy under the same restrictions and with the same cost of protocol Alternate but without closing any port.
Exercise 3.10.25 Determine initial configurations that will force protocol Alternate to use k steps when n = Fk.
Exercise 3.10.27 Determine the ideal time complexity of protocol Alternate.
Indicate for each step, the values know at the candidates.
Exercise 3.10.30 Determine the ideal time complexity of protocol UniStages.
Exercise 3.10.32 Design an exact simulation of Stages with Feedback for unidirectional rings.
Indicate for each step, the values know at the candidates.
Exercise 3.10.34 Without changing its message cost, modify protocol UniAlternate so that it does not require Message Ordering.
Exercise 3.10.35 Prove that the ideal time complexity of protocol UniAlternate is O(n)
Exercise 3.10.37 Prove that in protocol MinMax, if a candidate x survives an even stage i, its predecessor l(i, x) becomes defeated.
Exercise 3.10.38 Show that the worst case number of steps of protocol MinMax is achievable.
Exercise 3.10.39 Modify protocol MinMax so that it does not require Message Ordering.
Exercise 3.10.41 Write the rules of Protocol MinMax+ assuming message ordering.
Exercise 3.10.42 Write the rules of Protocol MinMax+ without assuming message ordering.
Exercise 3.10.50 In Protocol ElectMesh, in the first stage of the election process, if an interior node receives an election message, it will reply to the sender “I am in the interior,” so that no subsequent election messages are sent to it.
Explain why it is possible to achieve the same goal without sending those replies.
Design a protocol that allows to wake-up all the entities in an oriented mesh using less than 2n messages regardless of the location and the number of the initiators.
Exercise 3.10.56 Show that the ideal time of protocol MarkBorder can be as bad as O(n)
Exercise 3.10.59 Determine the cost of electing a leader in an oriented hypercube if in protocol HyperElect the propagation of the Match messages is done by broadcasting in the appropriate subcube instead of “compressing the address.”
Exercise 3.10.63 ("") Prove that it is possible to elect a leader in a hypercube using O(n) messages with any sense of direction (Hint: Use long messages)
Exercise 3.10.68 Design an election protocol for complete graphs that, like CompleteElect, uses O(n log n) messages but uses only O(n/ log n) time in the worst case.
R(k) where messages are sent by protocol Kelect do not have links in common.
Exercise 3.10.71 Write the code for, implement, and test protocol Kelect-Stages.
Exercise 3.10.73 ("") Determine the average message costs of protocol KelectStages.
Exercise 3.10.75 ("") Prove that it is possible to elect a leader in a complete graph using O(n) messages with any sense of direction.
Exercise 3.10.79 Consider a merger message from city A arriving at neighbouring city B along merge link (a, b) in protocol Mega-Merger.
Prove that if we reverse the logical direction of the links on the path from D(A) to the exit point a and direct toward B the merge link, the union of A and B will be rooted in the downtown of A.
Exercise 3.10.81 Find a way to avoid notification of termination by the downtown of the megacity in protocol Mega-Merger (Hint: Show that by the time the downtown understands that the mega-merger is completed, all other districts already know that their execution of the protocol is terminated)
Show that protocol Mega-Merger uses at most O(n log n) ideal time units.
Exercise 3.10.83 Prove that in the YO-YO protocol, during an iteration, no sink or internal node will become a source.
Exercise 3.10.84 Modify the YO-YO protocol so that upon termination, a spanning tree rooted in the leader has been constructed.
Exercise 3.10.85 Prove that to solve SPT under IR, a message must be sent on every link.
Exercise 3.10.86 Show how to transform a spanning-tree construction algorithm C so as to elect a leader with at most O(n) additional messages.
Exercise 3.10.87 Prove that under IR, the problem of finding the smallest of the entities’ values is computationally equivalent to electing a leader and has the same message complexity.
In stage i, a candidate x sends its id and receives the id from its two neighboring candidates, r(i, x) and l(i, x): x does not survive this stage if and only if its id is larger than both received ids.
Analyze the corresponding protocol Josephus, determining in particular the number of stages and the total number of messages both in the worst and in the average case.
Determine what will be the cost if we use dis(i) = 2i instead.
Problem 3.10.6 MinMax+ Variations ("") In protocol MinMax+ we use “promotion by distance” only in the even stages and “promotion by witness” only in the odd stages.
Prove or disprove that there is an efficient protocol for bidirectional oriented rings that cannot be used nor simulated neither in unidirectional rings nor in general bidirectional ones with the same or better costs.
Design a protocol that can elect a leader in a hypercube with arbitrary labelling using O(n log log n) messages.
Prove or disprove that it is possible to elect a leader in an hypercube in O(n) messages even when it is not oriented.
Problem 3.10.15 Optimal Time ("") Show how to elect a leader in O(d) time using at most O(m log d) messages.
Let y = hi(u) be the first candidate after x in the ring in stage i, and (u, i) the message it originated.
As v survives this stage, which is odd (i.e., min), it must be that v < u.
According to the strategy, y will issue a Warning to its owner z for each of them.
Time and message bounds for election in synchronous and asynchronous complete networks.
Optimal distributed algorithms for minimum weight spanning tree, counting, leader election, and related problems.
A better lower bound for distributed leader finding in bidirectional, asynchronous rings of processors.
New lower bound techniques for distributed leader finding and other problems on rings of processors.
Some lower bound results for decentralized extrema-finding in rings of processors.
New upperbounds for distributed extrema-finding in a ring of processors.
Improving the time complexity of message-optimal distributed algorithms for minimum-weight spanning trees.
An improved algorithm for decentralized extrema-finding in circular configurations of processes.
Linear broadcasting and O(n log log n) election in unoriented hypercubes.
An O(n log n) unidirectional algorithm for extremafinding in a circle.
On an improved algorithm for decentralized extrema-finding in a circular configuration of processes.
Finding a leader in a network withO(e)+O(n log n) messages.
Selecting a leader in a clique in O(n log n) messages.
A modular technique for the design of efficient distributed leader finding algorithms.
Optimal lower bounds for some distributed algorithms for a complete network of processors.
Distributed election in a circle without a global sense of orientation.
Analysis of a distributed algorithm for extrema finding in a ring.
Average number of messages for distributed leader-finding in rings of processors.
Exact average message complexity values for distributed election on bidirectional rings of processors.
An 1.44...n log n algorithm for distributed leader finding in bidirectional rings of processors.
An O(n log n) unidirectional algorithm for the circular extrema problem.
An improved upperbound for distributed election in bidirectional rings of processors.
Communication is at the base of computing in a distributed environment, but the task to achieve it efficiently is neither simple nor trivial.
The (shortest-path) routing problem is commonly solved by storing at each entity x the information that will allow to address a message to its destination through a (shortest) path.
In this chapter we will discuss several aspects of the routing problem.
First of all, we will consider the construction of the routing tables.
Finally, we will discuss how to represent routing information in a compact way, suitable for systems where space is a problem.
In the following, and unless otherwise specified, we will assume the set of restrictions IR: Bidirectional Links (BL), Connectivity (CN), Total Reliability (TR), and Initial Distinct Values (ID)
The routing table of an entity contains information on how to reach any possible destination.
In this section we examine how this information can be acquired, and the table constructed.
As we will see, this problem is related to the construction of particular spanning-trees of the network.
In the following, and unless otherwise specified, we will focus on shortest-path routing.
Different types of routing tables can be defined, depending on the amount of information contained in them.
We will consider for now the full routing table: For each destination, there is stored a shortest path to reach it; if there are more than one shortest path, only the lexicographically smallest1 will be stored.
We will see different approaches to construct routing tables, some depending on the amount of local storage an entity has available.
A first obvious solution would be to construct at every entity the entire map of the network with all the costs; then, each entity can locally and directly compute its shortest-path routing table.
This solution obviously requires that the local memory available to an entity is large enough to store the entire map of the network.
The lexicographic order will be over the strings of the names of the nodes in the paths.
This is a particular instance of a general problem called input collection or gossip: every entity has a (possibly different) piece of information; the goal is to reach a final configuration where every entity has all the pieces of information.
The solution of the gossiping problem using normal messages is simple:
Since it relies solely on broadcast, this operation is more efficiently performed in a tree.
An arbitrary spanning tree of the network is created, if not already available; this tree will be used for all communication.
At the end of the execution, each entity has a complete map of the network with all the link costs; it can then locally construct its shortest-path routing table.
The construction of the initial spanning-tree can be done using O(m+ n log n) messages, for example using protocol MegaMerger.
The acquisition of neighborhood information requires a single exchange of messages between neighbors, requiring in total just 2m messages.
Each entity x then broadcasts on the tree deg(x) items of information.
Hence the total number of messages will be at most.
This means that, in sparse networks, all the routing tables can be constructed with at most O(n2) normal messages.
Such is the case of meshes, tori, butterflies, and so forth.
The time costs of gossiping on a tree depend on many factors, including the diameter of the tree and the number of initial items an entity initially has (Exercise 4.6.2)
The solution we have just seen requires that each entity has locally available enough storage to store the entire map of the network.
If this is not the case, the problem of constructing the routing tables is more difficult to resolve.
This approximation of the routing table will be refined, and eventually corrected, through a sequence of iterations.
In each iteration, every entity communicates its current distance vector with all its neighbors.
On the basis of the received information, each entity updates its current information, replacing paths in its own routing table if the neighbors have found better routes.
If w[z] < V ix [z], then the new cost and the corresponding path to z is chosen, replacing the current selection.
The main advantage of this process is that the amount of storage required at an entity is proportional to the size of the routing table and not to the map of the entire system.
In each iteration, an entity sends its distance vector containing costs and path.
That is, this approach is more expensive than the one based on constructing all the maps; it does, however, require less local storage.
The first solution we have seen, protocol Map Gossip, requires that each entity has locally available enough storage to store the entire map of the network.
The second solution, protocol Iterative Construction, avoids this problem, but it does so at the expense of a substantially increased amount of messages.
Our goal is to design a protocol that, without increasing the local storage requirements, constructs the routing tables with a smaller amount of communication.
Fortunately, there is an important property that will help us in achieving this goal.
It is called the shortest path spanning tree rooted in s(PT(s)), sometimes also known as the sink tree of s.
This fact is important because it tells us that, to construct the routing table RT(s) of s, we just need to construct the shortest path spanning tree PT(s)
Protocol Design To construct the shortest path spanning tree PT(s), we can adapt a classical serial strategy for constructing PT(s) starting from the source s:
Add to T the link (a, b) for which v(a, b) is minimum; in case of a tie, choose the one leading to the node with the lexicographically smallest name.
The reason this strategy works is because of the following property:
That is, the new tree, obtained by adding the chosen (a, b) to T, is also a connected fragment of PT(s), containing s, and it is clearly larger than T.
In other words, using this strategy, the shortest path spanning-tree PT(s) will be constructed, starting from s, by adding the appropriate links, one at the time.
The algorithm based on this strategy will be a sequence of iterations started from the root.
In each iteration, the outgoing link (a, b) with minimum cost v(a, b) is chosen; the link (a, b) and the node b are added to the fragment, and a new iteration is started.
The process terminates when the fragment includes all the nodes.
Our goal is now to implement this algorithm efficiently in a distributed way.
First of all, let us consider what a node y in the fragment T knows.
Let us assume for the moment that y also knows which of its links are outgoing (i.e., lead to nodes outside of the current fragment) and which are internal.
In this case, to find the outgoing link (a, b) with minimum cost v (a, b) is rather simple, and the entire iteration is composed of four easy steps:
The root s broadcasts in T the start of the new iteration.
The overall minimum v(a, b) among all the locally selected v(e)’s is computed at s, using a minimum-finding for (rooted) trees (e.g., see Section 2.6.7), and the corresponding link (a, b) is chosen as the one to be added to the fragment.
Each iteration can be performed efficiently, in O(n) messages, as each operation (broadcast, min-finding, notifications) is performed on a tree of at most n nodes.
However, not all unexplored links are outgoing: An unexplored link might be internal (i.e., leading to a node already in the fragment), and selecting such a link would be an error.
We could allow for errors: We choose among the unexplored links and, if the link (in our example: (e, k)) selected by the root s in step 3 turns out to be internal.
In fact, since initially all links are unexplored, we might have to perform the entire selection process for every link.
This means that the cost will be O(nm), which in the worst case is O(n3): a high price to construct a single routing table.
A more efficient approach is to add a mechanism so that no error will occur.
Fortunately, this can be achieved simply and efficiently as follows.
When a node b becomes part of the tree, it sends a message to all its neighbors notifying them that it is now part of the tree.
Upon receiving such a message, a neighbor c knows that this link must no longer be used when performing shortest path calculations for the tree.
We have used a similar strategy with the protocol for depth-first traversal, to decrease its time complexity.
It is necessary for b to ensure that all its neighbors have received its message before a new iteration is started.
In other words, it is still possible that an internal link is selected during an iteration.
Summarizing, to avoid mistakes, it is sufficient to modify rule 4 as follows:
The protocol, called PT Construction, is shown in Figures 4.3–4.6
Analysis Let us now analyze the cost of protocol PT Construction in details.
There are two basic activities being performed: the expansion of the current fragment of the tree and the announcement (with acknowledgments) of the addition of the new node to the fragment.
The cost due to announcements and acknowledgments is simple to calculate: Each node will send a Notify message to all its neighbors when it becomes part of the tree.
Thus, the total number of messages due to the notifications is.
By adding a little bookkeeping, the protocol can be used to construct the routing table RT(s) of the source (Exercise 4.6.13)
Hence, we have a protocol that constructs the routing table of a node using O(n2) messages.
We will see later how more efficient solutions can be derived for the special case when all the links have the same cost (or, alternatively, there is no cost on the links)
Note that we have made no assumptions other than that the costs are non-negative; in particular, we did not assume first in first out (FIFO) channels (i.e., message ordering)
Protocol PT Construction allows us to construct the shortest-path tree of a node, and thus to construct the routing table of that entity.
The complexity of resulting protocol PT All follows immediately from equation 4.4:
Definitively better than protocol Iterative Construction, protocol PT All matches the worst case cost of Map Gossip without requiring large amounts of local storage.
However, to date, it is not clear how this fact can be used to derive a more efficient protocol for constructing all the routing tables.
A (a, b)-sparser is just a partition of the set V of nodes into subsets such that its radius is r(S) = a and its density is den(S) = b.
Then the execution of the protocol in G is simulated in the sparser.
An interesting consequence of (5) above is that the cost of a node u sending a message to all its neighbors, when simulated in the sparser, will depend on the number of subsets in which u has neighbors as well as on the distance between the corresponding leaders.
Property 4.2.4 Any connected graph G of n nodes has a (log n, n)-sparser.
The existence of this good sparser is not enough; we must be able to construct it with a reasonable amount of messages.
When constructing it, there are several important details that must be taken care; in particular, the paths between the centers must be uniquely determined.
Once all of this is done, we must then define the set of rules (Exercise 4.6.17) to simulate protocol MapGossip.
At this point, the resulting protocol, called SparserGossip, yields the desired performance.
Using Long Messages In systems that allow very long messages, not surprisingly the problem can be solved with fewer messages.
If messages can contain O(n2) items, then any graph problem including the construction of all shortest path trees can be solved using O(n) messages once a leader has been elected (requiring at least O(m+ n log n) normal messages)
A summary of all these results is shown in Figure 4.7
An interesting consequence is that the shortest path spanning tree of a node coincides with its breadth-first spanning tree.
In other words, a breadth-first spanning tree rooted in a node is the shortest path spanning tree of that node when all links have the same cost.
Protocol PT Construction works for any choice of the costs, provided they are non-negative; so it constructs a breadth-first spanning tree if all the costs are the same.
However, we can take advantage of the fact that all links have the same costs to obtain a more efficient protocol.
Clearly, the problem is to determine, in step i, which nodes are at distance i from s.
So the protocol, which we shall call BF, is rather simple.
Initially, the root s sends a “start iteration 1” message to each neighbor indicating the first iteration of the algorithm and considers them its children.
Each recipient marks its distance as 1, marks the sender as its parent, and sends an acknowledgment back to the parent.
The tree is now composed of the root s and its neighbors, which are all at distance 1 from s.
When a neighbor y receives the “Explore” message, the content of its reply will depend on whether or not y is already part of the tree.
If y is not part of the tree, it now knows that it is at distance i + 1 from s; it then marks the sender as its parent, sends a positive acknowledgment to it, and becomes part of the tree.
If y is part of the tree (even if it just happened in this iteration), it will reply with a negative acknowledgment.
When x receives the reply from y, if the reply is positive, it will mark y as a child, otherwise, it will mark y as already in the tree.
Once all the replies have been received, it participates in a convergecast notifying the root that the iteration has been completed.
Cost Let us now examine the cost of protocol BF.
Denote by ni the number of nodes at distance at most i from s.
We know that the execution of protocol BF is a sequence of iterations, started by the root.
Each iteration i + 1 of protocol BF can be thought of as composed of three different phases:
Initialization: the root node broadcasts the “start iteration i + 1” along the already constructed tree, which will reach the leaves (i.e., the nodes at distance i from the root)
Expansion: in this phase, which is started by the leaves, new nodes (i.e., all those of level i + 1) are added to the tree forming a larger fragment.
Termination: the root is notified of the end of this iteration using a convergecast on the new tree.
Initialization and termination are bookkeeping operations that allow the root to somehow synchronize the execution of the algorithm, iteration by iteration.
For this reason, the two of them, together, are also called synchronization.
Each synchronization costs O(n) messages (as it is done on a tree)
In the original protocol BF, we expand the tree one level at the time; hence.
This means that to reduce the cost of synchronization, we need to decrease the number of iterations.
To do so, we need each iteration to grow the current tree by more than a single level, that is, we need each expansion phase to add several levels to the current fragment.
Let t be the current level of the leaves; each leaf will start the exploration by sending Explore(t + 1, l) to its still unexplored neighbors.
In general, the expansion messages will be of the form Explore(level, counter), where level is the next level to be assigned and counter denotes how many more levels should be expanded by the node receiving the message.
A more detailed description of the expansion phase of the protocol, which we will call BF Levels, is shown in Figure 4.10, describing the behavior of a node x not part of the current fragment.
As mentioned, the expansion phase is started by the leaves of the current fragment, which we will call sources of this phase, upon receiving the start iteration message from the root.
Each source will then send Explore(t + 1, l) to their unexplored neighbors, where t is the level of the leaves and l (a design parameter) is the number of levels that will be added to the current fragment in this iteration.
The terminating phase also is started by the sources (i.e., the leaves of the already existing fragment), upon receiving a reply to all their expansion messages.
If j < levelx , a shorter path from the root s to x has been found.
If the level of the reply is not (levelx + 1) then the message is discarded.
Correctness During the extension phase all the nodes at distance at most t + l from the root are indeed reached, as can be easily verified (Exercise 4.6.23)
Thus, to prove the correctness of the protocol, we need just to prove that those nodes will be attached to the existing fragment at the proper level.
Cost To determine the cost of protocol BF Levels, we need to analyze the cost of the synchronization and of the expansion phases.
As for each Explore there is at most one reply, the total number of messages sent in this phase will be no more than 4lmi.
This fact, observing that the set of links involved in each iteration are disjoint, yields less than.
In fact, it depends not only on n but also on the square root of the number m of links.
If the network is sparse (i.e., it has O(n) links), then the protocol uses only.
The worst case will be with very dense networks (i.e., m = O(n2))
In other words, protocol BF Levels will have the same cost as protocol BF only.
Reducing Time with More Messages ( ) If time is of paramount importance, better results can be obtained at the cost of more messages.
For example, if in protocol BF Levels we were to choose l = d(G), we would obtain an optimal time costs.
The cost in messages choosing l = d(G) is given by expression (4.11) that becomes.
This quantity is reasonable only for networks of small degree.
By the way, a priori knowledge of d(G) is not necessary to obtain these bounds (either time or messages; Exercise 4.6.24)
If we are willing to settle for a low but suboptimal time, it is possible to achieve it with a better message complexity.
In protocol BF Levels the network (and thus the tree) is viewed as divided into “strips,” each containing l levels of the tree.
The way the protocol works right now, in the expansion phase, each source (i.e., each leaf of the existing tree) constructs its own bf-tree over the nodes in the next l levels.
These bf-trees have differential growth rates, some growing quickly, some slowly.
Thus, it is possible for a quickly growing bf-tree to have processed many more levels than a slower bf-tree.
Whenever there are conflicts due to transmission delays (e.g., the arrival of a message with a better level) or concurrency (e.g., the arrival of another message with the same level), these conflicts are resolved, either.
It is the amount of work performed to take care of these conflicts that drives the costs of the protocol up.
For example, when a node joins a bf-tree and has a (new) parent, it must send out messages to all its other neighbors; thus, if a node has a high degree and frequently changes trees, these adjacent edge messages dominate the communication complexity.
Clearly, the problem is how to perform these operations efficiently.
Conflicts and overlap occurring during the constructions of those different bf-trees in the l levels can be reduced by organizing the sources into clusters and coordinating the actions of the sources that are in the same cluster, as well as coordinating the different clusters.
This in turn requires that the sources in the same cluster must be connected so as to minimize the communication costs among them.
The connection through a tree is the obvious option and is called a cover tree.
To avoid conflicts, we want that for different clusters the corresponding cover trees have no edges in common.
So we will have a forest of cover trees, which we will call the cover of all the sources.
To coordinate the different clusters in the cover, we must be able to reach all sources; this, however, can already be done using the current fragment (recall, the sources are the leaves of the fragment)
We are thus faced with the problem of constructing clusters with small amount of competition and shallow cover trees.
Up to now, we have considered only shortest-path routing, that is, we have been looking at systems that always route a message to its destination through the shortest path.
To construct optimal routing mechanisms, we had to construct n shortest path trees, one for each node in the network, a task that we have seen is quite communication expensive.
In some cases, the shortest path requirement is important but not crucial; actually, in many systems, guarantee of delivery with few communication activities is the only requirement.
If the shortest path requirement is relaxed or even dropped, the problem of constructing a routing mechanism (tables and forwarding scheme) becomes simpler and can be achieved quite efficiently.
Because they do not guarantee shortest paths, such solutions are called suboptimal.
Clearly there are many possibilities depending on what (suboptimal) requirements the routing mechanism must satisfy.
A particular class of solutions is the one using a single spanning tree of the network for all the routing, which we shall call routing tree.
The advantages of such an approach are obvious: We need to construct just one tree.
Delivery is guaranteed and no more that diam(T ) messages will be used on the tree T.
Depending on which tree is used, we have different solutions.
As the maximum number of messages used to deliver a message is at most diam(T), a natural choice for a routing tree is the spanning tree with a small diameter.
One such a tree is shortest path tree rooted in a center of the network.
In fact, let c a center of G (i.e., a node where the maximum distance is minimized) and let PT(c) be the shortest path tree of c.
To construct such a tree, we need first of all to determine a center c and then construct PT(c), for example, using protocol PT Construction.
It is not difficult to see that such a measure is exactly the sum of all distances between nodes (Exercise 4.6.28)
Hence, the best tree T to use is one that.
Thus, to construct such a tree, we need first of all to determine a median z and then construct PT(z), for example, using protocol PT Construction.
A natural choice for routing tree is a minimum-cost spanning tree (MST) of the network.
All the solutions above have different advantages; for example, the center-based one offers the best worst-case cost, while the median-based one has the best average cost.
Depending on the nature of the systems and of the applications, each might be preferable to the others.
Alternate, easier to compute, measures are obtained by taking into account only pairs of neighbors (instead of pairs of arbitrary nodes)
We also can define the edge-stretch factor 2G(T ) (or dilation factor) of a spanning tree T of G as.
As an example, consider the spanning tree PT(c) used in the center-based solution; if all the link costs are the same, we have that for every two nodes x and y.
For a given spanning tree T, the stretch factor and the dilation factor measure the worst ratio between the distance in T and in G for the same pair of nodes and the same edge, respectively.
Another important cost measure is the average stretch factor describing the average ratio:
Summarizing, the main disadvantage of using a routing tree for all routing tasks is the fact that the routing path offered by such mechanisms is not optimal.
If this is not a problem, these solutions are clearly a useful and viable alternative to shortest path routing.
The choice of which spanning tree, among the many, should be used depends on the nature of the system and of the application.
In some systems, it might be possible that the cost associated to the links change over time; think, for example, of having a tariff (i.e., cost) for using a link during weekdays different from the one charged in the weekend.
If such a change occurs, the shortest path between several pairs of node might change, rendering the information stored in the tables obsolete and possibly incorrect.
In this section, we will consider the problem of dealing with such events.
We will assume that when the cost of a link (x, y) changes, both x and y are aware of the change and of the new cost of the link.
In other words, we will replace the Total Reliability restriction with Total Component Reliability (thus, the only changes are in the costs) in addition to the Cost Change Detection restriction.
Note that costs that change in time can also describe the occurrence of some link failures in the system: The crash failure of an edge can be described by having its cost becoming exceedingly large.
Hence, in the following, we will talk of link crash failures and of cost changes as the same types of events.
In these dynamical networks where cost changes in time, the construction of the routing tables is only the first step for ensuring (shortest path) routing: There must be a mechanism to deal with the changes in the network status, adjusting the routing tables accordingly.
It requires first of all that each table contains the complete map of the entire.
The construction of the maps can be done, for example, using protocol Map Gossip discussed in Section 4.2.1
Thus, integral part of this protocol is the update mechanism:
In several existing systems, an even more expensive periodic maintenance mechanism is used: Step 1 of the maintenance mechanism is replaced by having each node, periodically and even if there are no detected changes, send its entire map to all its neighbors.
This is, for example, the case with the second Internet routing protocol:
The great advantage of this approach is that it is fully adaptive and can cope with any amount and type of changes.
The clear disadvantage is the amount of information required locally and the volume of transmitted information.
Vector Update To alleviate some of the disadvantages of the Map Update protocol, an alternative solution consists in using protocol Iterative Construction, that we designed to construct the routing tables, to keep them up-to-date should faults or changes occur.
Note that a single change might make all the routing tables incorrect.
To complicate things, changes are detected only locally, where they occur, and without a full map it might be impossible to detect if it has any impact on a remote site; furthermore, if more several changes occur concurrently, their cumulative effect is unpredictable: A change might “undo” the damage inflicted to the routing tables by another change.
Whenever an entity x detects a local change (either in the cost or in the status of an incident link), the update mechanism is invoked, which will trigger an execution of possibly several iterations of protocol Iterative Construction.
In regard to the update mechanism, we have two possible choices:
The first choice is very costly because, as we know, the construction of the routing tables is an expensive process.
For these reasons, one might want to recompute only what and when is; hence the second choice is preferred.
The second choice was used as the original Internet routing protocol; unfortunately, it has some problems.
Both nodes z and w will then start an iteration that will be performed by all entities.
In the next iteration, y sets its distance from w to 4 because the best path to w (according to the vectors it receives from x and z) is through x.
This process will continue until z sets its cost for w.
As K can be arbitrarily large, the number of iterations can be arbitrarily large.
Oscillation We have seen some approaches to maintain routing information in spite of failures and changes in the system.
As we have seen, maintaining the routing tables correct when the topology of the network or the edge values may change is a very costly operation.
Consider, for example, a system where at any time there is at most one link down (not necessarily the same one at all times), and no other changes will ever occur in the system; this situation is called single link crash failure (SLF)
Even in this restricted case, the amount of information that must be kept in addition to the shortest paths is formidable (practically the entire map)
This is because the crash failure of a single edge can dramatically change all the shortest path information.
As the tables must be able to cope with every possible choice of the failed link, even in such a limited case, the memory requirements soon become unfeasible.
Furthermore when a link fails, every node must be notified so that it can route messages along the new shortest paths; the subsequent recovery of that node also will require such a notification.
Such a notification process needs to be repeated at each crash failure and recovery, for the entire lifetime of the system.
Hence, the amount of communication is rather high and never ending as long as there are changes.
Summarizing, the service of delivering a message through a shortest path in presence of cost changes or link crash failures, called shortest path rerouting (SR), is expensive (sometimes to the point of being unfeasible) both in terms of storage and communication.
The natural question is whether there exists a less expensive alternative.
In fact, if we relax the shortest path rerouting requirement and settle for lower quality services, then the situation changes drastically; for example, as we will see, if the requirement is just message delivery (i.e., not necessarily through a shortest path), this service be achieved in our SLF system with very simple routing tables and without any maintenance mechanism.
In the rest of this section, we will concentrate on the single-link crash failure case.
Point-of-failure Rerouting To reduce the amount of communication and of storage, a simple and convenient alternative is to offer, after the crash failure of an arbitrary single link, a lower quality service called point-of-failure rerouting (PR):
This type of service has clearly the advantage that there is no need to notify the entities of a link crash failure and its subsequent reactivation (if any): The message is forwarded as there are no crash failures and if, by chance, the next link it must take has failed, it will be just then provided with an alternative route.
For this reason, the routing tables supporting such a service are called fault-tolerant tables.
The amount of information that a fault-tolerant table must contain (in addition to the shortest paths) to provide such a service will depend on what type of information is being kept at the nodes to do the rerouting and on whether or not the rerouting is guaranteed to be through a shortest path.
A solution consists in every node x knowing two (or more) edge-disjoint paths for each destination: the shortest path, and a secondary one to be used only if the link to the next “hop” in the shortest path has failed.
So the routing mechanism is simple: When a message for destination r arrives at x, x determines the neighbor y in the shortest path to r.
If (x,y) is up, x will send the message to y, otherwise, it will determine the neighbor z in the secondary path to r and forward the message to z.
The storage requirements of this solution are minimal: For each destination, a node needs to store in its routing table only one link in addition to the one in the fault-free shortest path.
As we already know how to determine the shortest path trees, the problem is reduced to the one of computing the secondary paths (see Exercise 4.6.37)
The secondary paths of a node do not necessarily form a tree.
A major drawback of this solution is that rerouting is not through a shortest path: If the crash failure occurs, the system does not provide any service other than message delivery.
Although acceptable in some contexts, this level of service might not be.
Surprisingly, it is actually possible to offer shortest path rerouting storing at each node only one link for each destination in addition to the one in the fault-free shortest path.
We are now going to see how to design such a service.
When ex fails, a new path from x to s must be found.
It cannot be any: It must be the shortest path possible between x and s in the network without es[x]
Using e we can create a new path from x to s.
The path will consist of three parts: the path from x to v in T [x/ex], the edge (u, v), and the path from u to s; see Figure 4.15
The cost of going from x to s using this path will then be.
This is the cost of using e as a swap for es[x]
For each es[x] there are several edges that can be used as swaps, each with a different cost.
If we want to offer shortest path rerouting from x to s when es[x] fails, we must use the optimal swap, that is the swap edge for es[x] of minimum cost.
Once the information about the optimal swap edges has been determined, it needs to be integrated in the routing tables so as to provide point-of-failure shortest path rerouting.
First and foremost, the routing table of x contains for each destination s the link to the neighbor in the shortest path to s if there are no failures.
The choice of symbol is not accidental: This neighbor is the parent of x in PT(s) and the link is really es[x] = (ps(x), x)
In the entry for the destination s, the routing table of x must also contain the information needed to reroute the message if es[x] = (ps(x), x) is down.
Concluding, the additional information x must keep in the entry for destination s are the rerouting link ev[x] = (pv(x), x) and the closest node v on the optimal swap edge for es[x]; this information will be used only if es[x] is down.
Any message must thus contain, in addition to the final destination (node s in our example), also a field indicating the swap destination (node v in our example), the swap link (link (u, v) in our example), and a bit to explain which of the two must be considered (see Table 4.7)
Initially, r sets the final destination to s, the swap destination and the swap link to empty, and the bit to 0; it then sends the message toward the final destination using the normal link indicated in its routing table.
If a node x receives the message with final destination s and bit set to 0, then (a) if x = s, the message has reached its destination: s processes the message; (b) if es[x] = (ps(x), x) is up, x forwards the unchanged message on that link; (c) if es[x] = (ps(x), x) is down, then x.
If a node x receives the message with final destination s and bit set to 1, and swap destination set to v, then (a) if x = v, then.
Adaptive Routing In all adaptive routing approaches, maintenance of the tables is carried out by broadcasting information about the status of the network; this can.
In all cases, news of changes detected by a node will eventually reach any node (still connected to it)
However, because of time delays, while an update is being disseminated, nodes still unaware will be routing messages on the basis of incorrect information.
In other words, as long as there are changes occurring in the system (and for some time afterwards), the information in the tables is unreliable and might be incorrect.
In particular, it is likely that routing will not be done through a shortest path; it is actually possible that messages might not be delivered as long as there are changes.
This sad status of affairs is not due to the individual solutions but solely due to the fact that time delays are unpredictable.
This situation occurs even if the changes at any time are few and their nature limited, as the SLF.
It would appear that we should be able to operate correctly in such a system; unfortunately this is not true:
It is impossible to provide shortest path routing even in the single-link crash failure case.
This is because the crash failure of a single edge can dramatically change all the shortest path information; thus, when the link fails, every node must be notified so that it can route messages along the new shortest paths; the subsequent recovery of that node will also require such a notification.
Such a notification process needs to be repeated at each crash failure and recovery, and again the unpredictable time delays will make it impossible to guarantee correctness of the information available at the entities, and thus of the routing decision they make on the basis of that information.
What, if anything, can be guaranteed? The only think that we can say is that, if the changes stop (or there are no changes for a long period of time), then the updates to the routing information converge to the correct state, and routing will proceed according to the existing shortest paths.
In other words, if the “noise” caused by changes stops, eventually the entities get the correct result.
Fault-Tolerant Tables In the fault-tolerant tables approach, no maintenance of the routing tables is needed once they have been constructed.
Therefore, there are no broadcasts or notifications of changes that, because of delays, might affect the correctness of the routing.
However, also, fault-tolerant tables suffer because of the unpredictability of time delays.
For example, even with the single-link crash failure, point-of-failure shortestpath rerouting can not be guaranteed to be correct: While the message for s is being rerouted from x toward the swap edge es[x], the link es[x] might recover (i.e., come up again) and another link on the may go down.
Thus, the message will again be rerouted and might continue to do so if a “bad” sequence of recovery failure occurs.
In other words, not only the message will not reach s through a shortest path from the first point-of-failure, but it will not reach s at all as long as there is a change.
It might be argued that such a sequence of events is highly unlikely, but it is possible.
As in the case of adaptive routing, the only guarantee is that if the changes stop (or there are no changes for a long period of time), then messages will be (during that time) correctly delivered through point-of-failure shortest paths.
There are systems that are static in nature; for example, if Total Reliability holds, no changes will occur in the network topology.
Such is, for example, any system etched on a chip; should faults occur, the entire chip will be replaced.
In these systems, an additional concern in the design of shortest path routing tables is their size, that is, an additional design goal is to construct table that are as small as possible.
For large n, this is a formidable amount of space just to store the routing tables.
Observe that for any destination, the first entry in the shortest path will always be.
Thus, it is possible to simplify the routing table by specifying for each destination y only the neighbor of x on the shortest path to it.
It appears that there is not much more that can be done to reduce the size of the table.
This is, however, not the case if we, as designers of the system, had the power to choose the names of the nodes and of the links.
The question we are going to ask is whether it is possible to drastically reduce this amount of storage if we know the network topology and we have the power of choosing the names of the nodes and the port labels.
An Example: Ring Networks Consider for example a ring network, and assume for the moment that all links have the same cost.
Suppose that we assign as names to the nodes consecutive integers, starting from 0 and continuing clockwise, and we label the ports right or left depending on whether or not they are in the clockwise direction.
In other words, in all these routing tables, the set of destinations associated to a port is an interval of consecutive integers, and, in each table, the intervals are disjoint.
This is very important for our purpose of reducing the space.
In fact, an interval has a very short representation: It is sufficient to store the two end values, that is, just 2 log n bits.
We can actually do it with just log n bits; see Exercise 4.6.43
As a table consists just of two intervals, we have routing tables of.
In other words, we are able to go from quadratic.
Note that it is true even if the costs of the links are not all the same; see Exercise 4.6.44
The phenomenon we have just described is not isolated, as we will discuss next.
Suppose that we are able to assign names to the nodes so that the shortest path routing tables for G have the following two properties.
If this is the case, then we can have for G a very compact representation of the routing tables, like in the example of the ring network.
In fact, for each link the set of destinations is an interval of consecutive integers, and, like in the ring, the intervals associated to the links of a given nodes are all disjoint.
In other words, each table consists of a set of intervals (some of them may be empty), one for each incident link.
From the storage point of view, this is very good news because we can represent such intervals by just their start values (or, alternatively, by their end values)
In other words, the routing table of x will consist of just one entry for each of its links.
This means that the amount of storage for its table is only deg(x) log n bits.
In turn, this means that the number of bits used in total to represent all the routing tables will be just.
How will the routing mechanism then work with such tables? Suppose x has a message whose destination is y.
Then x checks in its table which interval y is part of (as the intervals are disjoint, y will belong to exactly one) and sends the message to the corresponding link.
Because of its nature, this approach is called interval routing.
If it can be done, as we have just seen, it allows for efficient shortest-path routing with a minimal amount of storage requirements.
It, however, requires that we, as designers, find an appropriate way to assign names to nodes so that the interval and disjointness properties hold.
Given a network G, it is not so obvious how to do it or whether it can be done at all.
Tree Networks First of all we will consider tree networks.
As we will see, in a tree it is always possible to achieve our goal and can actually be done in several different ways.
Given a tree T, we first of all choose a node s as the source, transforming T into the tree T (s) rooted in s; in this tree, each node x has a parent and some children (possibly none)
We then assign as names to the nodes consecutive integers, starting from 0, according to the post-order traversal of T (s), for example, using procedure.
For example, any node has a larger name than all its descendents.
More importantly, it has the interval and disjointness properties (Exercise 4.6.48)
Informally, the interval property follows is because when executing Post Order Naming with input (x, k), x and its descendents will be given as names consecutive integers starting from k.
Can interval routing be done in every network? The answer is unfortunately No.
Multi-Intervals As we have seen, interval routing is a powerful technique but the classes of networks in which it is possible are rather limited.
To overcome somehow this limitation without increasing excessively the size of the routing table an approach is to associate to each link a small number of intervals.
An interval-routing scheme that uses up to k intervals per edge is called a k-intervals routing scheme.
Clearly, with enough intervals we can find a scheme for every connected graph.
The question is whether this can be achieved with a small k.
In fact, there are graphs where O(n) intervals are needed in each edge (Exercise 4.6.56)
Suboptimal Interval Routing A reason why it is impossible to do interval routing in all graphs is that we require the tables to provide shortest path.
If we ask the tables to provide us just with a path to destination, not necessarily the shortest one, then we can use the approach already discussed in Section 4.2.6: We construct a single spanning tree T of the network G and use only the edges of T for routing.
Once we have the tree T, we then assign the names to the nodes using the naming algorithm for trees that provides interval routing.
In this way, we obtain for G the very compact routing tables provided by interval routing.
Clearly, the interval routing mechanism so constructed is optimal (i.e., shortest path) for the tree T but not necessarily so for the original network G.
This means that suboptimal interval routing is always possible in any network.
How much worse can a path provided by this approach be than the shortest one to the destination?
If we choose as tree T a breadth-first spanning tree rooted in a center of the graph G, then its diameter is at most twice the diameter of the original graph (the worst case is when G is a ring)
This means that the longest route is never more than 2 diam(G)
The construction of routing table is a prerequisite for the functioning of many networks.
One of the earliest protocols is due to William Tajibnapis [31]
The basic MapGossip for the construction of all routing tables is due to Eric Rosen [29]
Fulkerson [13]; from the start it has been the main routing algorithm in the Internet.
The improvement to O(n3) is due to Baruch Awerbuch, who designed a protocol to construct a single shortest path tree.
The same bound is achieved by protocol PT Construction, the efficient distributed implementation of Dijkstra’s sequential algorithm designed by K.
The distributed construction of min-hop spanning trees has been extensively investigated.
The suboptimal solutions of center-based and median-based routing were first discussed in details by David Wall and Susanna Owicki [34]
Consider a tree network where each entity has a single item of information.
What would the time costs be if each entity x initially has deg(x) items?
Exercise 4.6.10 In protocol PT Construction, no action is provided for an idle entity receiving an Expand message.
Prove that such a message will never be received in such a state.
Exercise 4.6.11 In procedure Compute Local Minimum of protocol PT Construction, an entity might set path length to infinity.
Show that if this happens, this entity will set path length to infinity in all subsequent iterations.
Exercise 4.6.12 In protocol PT Construction, each entity will eventually set path length to infinity.
Show that when this happens to a leaf of the constructed tree, that entity can be removed from further computations.
Exercise 4.6.13 Modify protocol PT Construction so that it constructs the routing table RT(s) of the source s.
Exercise 4.6.15 Prove that any G has a (log n, n) sparser.
Exercise 4.6.16 Show how to construct a (log n, n) sparser with O(m+ n log n) messages.
Show how to construct all the shortest path trees with just O(n2) messages.
Exercise 4.6.21 Write the set of rules corresponding to protocol BF Levels.
Exercise 4.6.24 Consider protocol BF Levels when l = d(G)
Show how to obtain the same message and time complexity without any a priori knowledge of d(G)
Prove or disprove that PT(c) = MSP(Rn), where c is a center of Rn and MSP(Rn) is the minimum-cost spanning tree of Rn.
Exercise 4.6.31 Consider a ring network Rn with weighted edges.
Let c and z be a center and a median of Rn, respectively.
For each of the following spanning trees of Rn, compare the stretch factor and the edge-stretch factor: PT(c), PT(z), and the minimum-cost spanning tree MSP(Rn)
Design an efficient protocol for computing a spanning tree with low average edge-stretch of a network G with unweighted edges.
Design an efficient protocol for computing a spanning tree with low average edge-stretch of a network G with weighted edges.
Exercise 4.6.37 ( ) Design a protocol for computing the secondary paths of a node x.
You may assume that the shortest-path tree PT(x) has already been constructed and that each node knows its and its neighbors’ distance from x.
Your protocol should use no more messages than that required to construct PT(x)
Consider the following technique, called split horizon, for solving the count-to-infinity problem discussed in Section 4.3.1: During an iteration, a node a does not send its cost for destination c to its neighbor b if b is the next node in the “best” path (so far) from a to c.
In the example of Figure 4.13, in the first iteration y does not send its cost for w to z, and thus z will correctly set its cost for w to K.
Prove or disprove that split horizon solves the count-to-infinity problem.
Exercise 4.6.40 ( ) Design an efficient protocol that, given a shortest-path spanning tree PT(s), determines an optimal swap for every edge in PT(s): At the end of the execution, every node x knows the optimal swap edge for its incident link es[x]
Your protocol should use no more than O(nh(s)) messages, where h(s) is the height of PT(x)
Exercise 4.6.42 Let e = (u,v) be the optimal swap edge that x has computed for es[x]
Prove that, if es[x] fails, to achieve point-of-failure shortest path rerouting, x must send the message for s to the incident link (pv(x), x)
Exercise 4.6.43 Show how to represent the intervals of a ring with just log n bits per interval.
Exercise 4.6.44 Show how that the intervals of a ring can be represented with just log n bits per interval, even if the costs of the links are not all the same.
Exercise 4.6.45 Let G be a network and assume that we can assign names to the nodes so that in each routing table, the destinations for each link form an interval.
Determine what conditions the intervals must satisfy so that they can be represented with just log n bits each.
Exercise 4.6.47 Show an assignment of names in a tree that does not have the interval property.
Does there exists an assignment of distinct names in a tree that has the interval property but not the disjointness one? Explain your answer.
Exercise 4.6.48 Prove that in a tree, the assignment of names by Post-Order traversal has both interval and disjointness properties.
Exercise 4.6.49 Prove that in a tree, also the assignment of names by Pre-Order traversal has both interval and disjointness properties.
Exercise 4.6.52 Design an optimal interval routing scheme for d-dimensional (a) hypercube, (b) butterfly, and (c) cube-connected cycles.
Show how to assign names to the nodes of an outerplanar graph so that interval routing is possible.
Prove that there are networks for which there exists interval routing but linear interval routing is impossible.
Prove that there are graphs that require k = O(n) intervals.
If for every x all the intervals in its routing table are strictly increasing (i.e., there is no “wraparound” node 0), the interval routing is called linear.
Characterize the class of graphs for which there exists a linear interval routing.
A graph-theoretic game and its application to the k-server problem.
A new distributed algorithm to find breadth first search trees.
Graph traversal techniques and the maximum flow problem in distributed computation.
The complexity of the characterization of networks supporting shortest-path interval routing.
Swapping a failing edge of a single source shortest paths tree is good and fast.
A correctness proof of a topology information maintenance protocol for a distributed computer network.
In a distributed computing environment, each entity has its own data stored in its local memory.
Some data items held by one entity are sometimes related to items held by other entities, and we focus and operate on them.
An example is the set of the ids of the entities.
What we did in the past was to operate on this set, for example, by finding the smallest id or the largest one.
Another example is the set of the single values held by each entity, and the operation was to find the overall rank of each of those values.
In all these examples, the relevant data held by an entity consist of just a single data item.
In general, an entity x has a set of relevant data Dx.
The union of all these local sets forms a distributed set of data.
Clearly there are many different distributions of the same distributed set.
A query is a request for some information about the global data set D, as well.
If the entity where the query originates has locally the desired information, the query can be answered immediately; otherwise, the entity will have to communicate with other entities to obtain the desired information.
As usual, we are concerned with the communication costs, rather than the local processing costs, when dealing with answering a query.
An update is a request to change the composition of the distributed set.
There are two basic updates: the request to add a new element to the set, an operation called insertion; and the request to remove an element from the set, an operation called deletion.
The third basic update is the request to change the value of an existing item of the set, an operation called change.
Note that a change can be seen as a deletion of the item with the old value followed by an insertion of an item with the new value.
In a distribution, the local sets are not necessarily distinct or disjoint.
Two extreme cases serve to illustrate the spectrum of distributions and the impact that the structure of the distribution has when handling queries and performing updates.
One extreme distribution is the partition where the local sets have no elements in common:
At the other end of the spectrum is the multiple-copy distribution where every entity has a copy of the entire data set.
A multiple-copy distribution is excellent for queries but poor for updates.
Queries are easy because all entities possess all the data; hence every answer can be derived locally, without any communication.
However, an update will require modification of the data held at each and every entity; in the presence of concurrent updates, this process becomes exceedingly difficult.
As each data item is located in only one site, answering a query requires searching through all potential entities to find the one that has locally stored the required data.
By contrast, to perform an update is easy because the change is performed in only the entity having the item, and there is no danger of concurrent updates on the same item.
In most cases, the data are partially replicated; that is, some data items are stored at more than one entities while others are to be found at only one entity.
This means that, in general, we have to face and deal with the problems of both extremes, partition and multiple-copy distributions, without the advantages of either one.
In the following we will first focus on an important class of queries, called order statistics; the problem of answering such queries is traditionally called selection.
As selection as well as most queries is more easily and efficiently solved if the distribution is sorted, we will also investigate the problem of sorting the distributed data.
We will then concentrate on distributed set operations; that is, computing union, intersection, and differences of the local sets.
The ability to perform such operations has a direct impact on the processing of complex queries usually performed in databases.
To focus on the problems, we will assume the standard set of restrictions IR (Connectivity, Total Reliability, Bidirectional Links, Distinct Identifiers)
For simplicity, as local processing time does not interest us when we consider the cost of our protocols, we will assume that all of the data stored at an entity are sorted.
As we consider arbitrary distributions of the data set, it is possible that a data item a is in more than one local set.
As we assume ID, we can use the ids of the entities to break ties and create a total order even among copies of the same value; so, for example, if a is in both Dx and Dy where id(x) > id(y), then we can say that the copy of a in Dx is “greater” than the one in Dy.
In this way, if so desired, the copies can also be considered distinct and included in the global data setD by the union operation (5.1)
Given a totally ordered data set D of size N distributed among the entities, the distributed selection problem is the general problem of locating D[K], the Kth smallest element of D.
Unlike cardinal statistics, ordinal ones are more difficult to compute in a distributed environment.
Unlike the case ofD[1] andD[N ], the problem of finding the median(s) and of K selection for an arbitrary value of K is not simple, and considerably more expensive to resolve.
Before proceeding to examine strategies for its solution, let us introduce a fundamental property and a basic observation that will be helpful in our designs.
The other useful tool is based on the trivial observation.
We will first consider the selection problem when the data set is rather small; more precisely, we consider data sets where N = O(n)
A special instance of a small distributed set is when every Dx is a singleton: it contains just a single element dx ; this is, for example, the case when the only data available at a node is its id.
Input Collection As the data set is small, the simple solution of collecting all the data at the coordinator and letting s solve locally the problem is actually not unfeasible from a complexity point of view.
This approach is somehow an overkill as the entire set is collected at s.
Truncated Ranking It might be possible to reduce the amount of messages by making it dependent on the value ofK.
In fact we can use the existing ranking protocol for trees (Exercise 2.9.4) and execute it on T until the Kth smallest item is found.
The use of the ranking algorithm will then cost no more than.
The protocols we have seen are generic, in that they apply to any topology.
For particular networks, it is possible to take advantage of the properties of the topology so to obtain a more efficent selection protocol.
In the previous section we have seen how to perform selection when the number of data items is small: N = O(n)
In general, this is not the case; in fact, not only N is much larger than n but it is order of magnitude so.
So, in general, the techniques that we have seen so far are clearly not efficient.
What we need is a different strategy to deal with the general case, in particular when N >> n.
In this section we will examine this problem in a simple setting when n = 2; that is, there are only two entities in the system, x and y.
We will develop efficient solution strategies; some of the insights will be useful when faced with a more general case in later sections.
We can design such a technique on the basis of a simple observation: if we compare the medians of the two local sets, then we can immediately eliminate almost half of the elements from consideration.
Thus, by locally calculating and then exchanging the median of each set, at least half of the elements of each set, and therefore half of the total number of elements, can be discounted; shown as white circle in Figure 5.2(c)
There is a very interesting and important property (Exercise 5.6.4): the overall lower median is the lower median of the elements still under.
This means that we can reapply the same process to the elements still under consideration: the entities communicate to each other the lower median of the local elements under consideration, these are compared, and half of all this data are removed from consideration.
In other words, we have just designed a protocol, that we shall call Halving, that is composed of a sequence of iterations; in each, half of the elements still under consideration are discarded and the sought global median is still the median of the considered data; this process is repeated until only a single element is left at each site and the median can be unambiguously determined.
As we halve the problem size at every iteration, the total number of iterations is logN.
Each iteration requires the communication of the local lower medians (of the elements still under consideration), a task that can be accomplished using just one message per iteration.
The working of the protocol has been described assuming that N is a power of two and that both sets have the same number N/2 of elements.
In fact the protocol Halving can be adjusted to two arbitrarily sized sets without changing its complexity: Exercise 5.6.5
As we are looking for the Kth smallest data item overall, any data item greater than Dx[K] cannot beD[K] (as it will be larger than at least K data items)
This means that we can immediately discount all these items, keeping only K items still under consideration.
Similarly, we can keep under consideration in Dy just Dy[K] and the items that are smaller.
Notice that D[K] is also the Kth smallest item among those kept in consideration; this is because we have discounted only the elements larger than D[K]
What is the net result of this ? We are now left with two sets of items, each of size K; see Figure 5.3(c)
Among those items, we are looking for the Kth smallest.
In other words, once this operation has been performed, the problem we need to solve is to determine the lower median of the elements under consideration.
Summary Regardless of the value of K we can always transform the K-selection problem into a median-finding problem.
Notice that this is accomplished without any additional communication, once it is known that we are looking for D[K]
In the description we have assumed that both sites have the same number of element, N/2
If this is not the case, it is easy to verify (Exercise 5.6.6) that the same type of reduction can still take place.
CAUTION: The number of discarded items that are greater than the median might be larger than the number of discarded items that are smaller than the median (or vice versa)
This means that the overall lower median we are looking for is no longer the median of the elements left under consideration.
In other words, after removing items from consideration, we might be left with a general selection problem.
By now, we know how to reduce a selection problem to the median-finding one.
The resulting protocol, that we shall call GeneralHalving, will use a few more messages, in each iteration but might yield a larger reduction (Exercise 5.6.7)
Generalization This technique can be generalized to three sites; however, we are no longer able to reduce the number of items still under consideration to at most half at each iteration (Exercise 5.6.9)
Fortunately, some lessons we have learned when dealing with the two sites are immediately and usefully applicable to any n, as we will discuss in the next section.
In the previous section we have seen how to perform selection when the number of data items is small or there are only two sites.
This fact can be employed to design a simple and, as we will see, rather efficient selection strategy:
Among the data items under consideration, (initially, they all are) choose one, say d.
Most of the operations performed by this strategy are rather simple to implement.
We can assume that a spanning tree of the network is available and will be used for all communication, and an entity is elected to coordinate the overall execution (becoming the root of the tree for this protocol)
Any entity can act as a coordinator and any spanning-tree T of the network will do.
However, for efficiency reasons, it is better to choose as a coordinator the communication center s of the network, and choose as a tree T the shortest path spanning-tree PT(s) of s.
Let d(i) be the item selected at the beginning of iteration i.
Once d(i) is chosen, the determination of its rank is a trivial broadcast (to let every entity know d(i)) started by the root s and a convergecast (to collect the partial rank information) ending at the root s.
The only operation still to be discussed is how we choose d(i)
The choice of d(i) is quite important because it affects the number of iterations and thus the overall complexity of the resulting protocol.
Let us examine some of the possible choices and their impact.
Random Choice We can choose d(i) uniformly at random; that is, in such a way that each item of the search space has the same probability of being chosen.
However, on the average, the power of making a random choice is evident; in fact (Exercise 5.6.11):
Lemma 5.2.1 The expected number of iterations performed by Protocol RandomSelect until termination is at most.
As mentioned earlier, we could stop the strategy RankSelect, and thus terminate protocol RandomSelect, as soon as O(n) data items are left for consideration, and then apply protocol Rank.
Specifically, our strategy will be to include, in the broadcast started by the root s at the beginning of iteration i, the valuesN (i) andK(i)
Each entity, upon receiving this information, will locally perform the reduction (if any) of the local elements and then include in the convergecast the information about the size of the new search space.
At the end of the convergecast, s knows both n(i) and k(i) as well as all the information necessary to perform the random selection in the reduced search space.
In other words, the total number of messages per iteration will be exactly the same as that of Protocol RandomSelect.
In the worst case this change does not make any difference.
The change does however make a difference on the average cost.
Lemma 5.2.2 The expected number of iterations performed by Protocol RandomFlipSelect until termination is less than.
Also in this case, we could stop the strategy RankSelect, and thus terminate protocol RandomSelect, as soon as only O(n) data items are left for consideration, and then apply protocol Rank.
Selection in a Random Distribution So far, we have not made any assumption on the distribution of the data items among the entities.
If we know something about how the data are distributed, we can clearly exploit this knowledge to design a more efficient protocol.
In this section we consider a very simple and quite reasonable assumption about how the data are distributed.
Then the expected number of iterations performed by Protocol RandomRandomSelect until there are less than n items under consideration is at most.
Filtering The drawback of all previous protocols rests on their worst case costs: O(nN ) messages and O(r(s)N ) time; notice that this cost is more than that of input collection, that is, of mailing all the items to s.
It can be shown that the probability of the occurrence of the worst case is so small that it can be neglected.
For these systems, it is necessary to have a selection protocol that, even if less efficient on the average, can guarantee a reasonable cost even in the worst case.
The design of such a system is fortunately not so difficult; in fact it can be achieved with the strategy RankSelect with the appropriate choice of d(i)
Filter: Choose d(i) to be the weighted (lower) median of M(i)
With this choice, the number of iterations is rather small (Exercise 5.6.18):
Lemma 5.2.4 The number of iterations performed by Protocol Filter until there are no more than n elements left under consideration is at most.
Once there are at most n elements left after consideration, the problem can be solved using one of the known techniques, for example, Rank, for small sets.
However, each iteration requires a complex operation; in fact we need to find the median of the setM(i) in iteration i.
As the set is small (it contains at mostn elements), this can be done using, for example, Protocol Rank.
In the worst case, it will require O(n2) messages in each iteration.
The worst case we have obtained by using the Filter choice in strategy RankSelect is reasonable but it can be reduced using a different strategy.
This strategy, and the resulting protocol that we shall call ReduceSelect, is obtained mainly by combining and integrating all the techniques we have developed so far for reducing the search space with new, original ones.
Reduction Tools Let us summarize first of all the main basic tool we have used so far.
A different type of reduction is offered by the following tool.
The reduction protocol REDUCE based on this repeated use of the two Reduction Tools is shown in Figure 5.5
Lemma 5.2.5 After the execution of Protocol REDUCE, the number of items left under consideration is at most.
Consider the set C(2), that is, all the second-smallest items in each site.
Focus on the kth smallest element m(2) of this set, where.
This means that, as far as we know, m(2) has at least.
Thus, we have a generic Reduction Tool using columns whose index is a power of two.
The Cutting Tool can be implemented using any protocol for selection in small sets (recall that each C(l) has at most n elements), such as Rank; a single broadcast will notify all entities of the outcome and allow each to reduce its own set if needed.
Clearly, if at any time the search space becomes small (i.e., O(n)), we terminate.
This reduction algorithm, that we will call CUT, is shown in Figure 5.6
Sorting is perhaps the most well known and investigated algorithmic problem.
In distributed computing systems, the setting where this problem takes place as well as its nature is very different from the serial as well as parallel ones.
In particular, in our setting, sorting must take place in networks of computing entities where no central controller is present and no common clock is available.
In this section we will examine the problem, its nature, and its solutions.
From the definition, it follows that when sorting a distributed set the relevant factors are the permutation according to which we sort, the topology of the network in which we sort, the location of the entities in the network, as well as the storage requirements.
In the following two sections, we will examine some special cases that will help us understand these factors, their interplay, and their impact.
If no data items change of place at all during an iteration (other than the first), then the process stop.
A schematic representation of the operations performed by the technique OddEvenLineSort is by means of the “sorting diagram”: a synchronous TED (time-event diagram) where the exchange of data between two neighboring entities is shown as a bold line connecting the time lines of the two entities.
In the diagram are clearly visible the alternation of “odd” and “even” steps.
To obtain a fully specified protocol, we still need to explain two important operations: termination and data exchange.
We have said that we terminate when no data items change of place at all during an iteration.
In fact, at the end of an iteration, each entity x can set a Boolean variable change to true or false to indicate whether or not its data set has changed during that iteration.
Then, we can check (by computing the AND of those variables) if no data items have changed place at all during that iteration; if this is the case for every entity, we terminate, else we start the next iteration.
If we are to perform a invariant-sized sorting, x should retain p items and y should retain q items.
Similarly, bad time costs can be derived for equidistributed sorting and compacted sorting.
To this cost we still need to add the number of messages used for the transfer of data items.
Hence, without storage constraints on the initial distribution, the protocol has a very high cost due to the high number of iterations possible.
Property 5.3.3 OddEven-LineSort can use O(Nn) messages to perform an invariant-sized sorting.
This cost is achievable even if the data is initially equidistributed.
That is, using Protocol OddEven-LineSort can costs as much as broadcasting all the data to every entity.
This results holds even if the data is initially equidistributed.
Similar bad message costs can be derived for equidistributed sorting and compacted sorting.
Summarizing, Protocol OddEven-LineSort does not appear to be very efficient.
As the complete graph contains every graph as a subgraph, we can choose to operate on whichever graph suites best our computational needs.
Thus, for example, we can choose an ordered line and use protocol OddEven-LineSort we discussed before.
However, as we have seen, this protocol is not very efficient.
If we are in a complete graph, we can adapt and use some of the well known techniques for serial sorting.
The problem with this strategy is that the last step, the merging step, is not an obvious one in a distributed setting; in fact, after the first iteration, the two sorted distributions.
Hence the question: How do we efficiently “merge” two sorted distributions of several sets to form a sorted distribution?
There are many possible answers, each yielding a different merge-sort protocol.
In the following we discuss a protocol for performing distributed merging by means of the odd-even strategy we discussed for the ordered line.
To determine the communication costs of this protocol need to “unravel” the recursion.
In each iteration (except the last) every entity is paired with another entity, and each pair will perform a simple merge of their local sets; half of the entities will perform this operation twice during an iteration.
In the last iteration all entities, except x1 and xn, will be paired and perform a merge.
Summarizing, in each of the first log n iterations, each entity sends is data to one or two other entities.
In other words the entire distributed set is transmitted in each iteration.
Hence, the total number of messages used by Protocol OddEven-MergeSort is.
Note that this bound holds regardless of the storage requirement.
Does the protocol work ? Does it in fact sorts the data ? The answer to these questions is: not always.
In fact, its correctness depends on several factors, including the storage requirements.
It is not difficult to prove that the protocol correctly sorts, regardless of the storage requirement, if the initial set is equidistributed (Exercise 5.6.33)
Property 5.3.4 OddEven-MergeSort sorts any equidistributed set if the required sorting is (a) invariant-sized, (b) equidistributed, or (c) compacted.
However, if the initial set is not equidistributed, the distribution obtained when the protocol terminates might not be sorted.
Recall that the existence of bottlenecks was the reason for the high number of iterations of Protocol OddEven-LineSort.
It is indeed possible to modify the protocol, adding enough appropriate iterations, so that the distribution will be correctly solved.
The type and the number of the additional iterations needed to correct the protocol depends on many factors.
In general, the additional requirements depend on the specifics of the size of the initial sets; see, for example, Exercise 5.6.34
In the previous sections we have seen different protocols, examined their behavior, and analyzed their costs.
In this process we have seen that the amount of data items transmitted can be very large.
For example, in OddEven-LineSort the number of messages is O(Nn), the same as sending every item everywhere.
Even not worrying about the limitations imposed by the topology of the network, protocol OddEvenMergeSort still usesO(N log n) messages when it works correctly.
Before proceeding any further, we are going to ask the following question: How many messages need to be sent anyway? we would like the answer to be independent of the protocol but to take into account both the topology of the network and the storage requirements.
The purpose of this section is to provide such an answer, to use it to assess the solutions seen so far, and to understand its implications.
On the basis of this, we will be able to design an efficient sorting protocol.
Lower Bound There is a minimum necessary amount of data movements that must take place when sorting a distributed set.
Let us determine exactly what costs must be incurred regardless of the algorithm we employ.
How this amount translates into number of messages depends on the size of the messages.
A message can only contain a (small) constant number of data items; to obtain a uniform measure, we consider just one data item per message.
This expresses a lower bound on the amount of messages for distributed sorting; the actual value depends on the topology G and the storage requirements.
First of all, note that, by definition, for all xi, xj , we have.
OddEven-LineSort Let us focus first on the ordered line network.
The same example holds also in the case of compact sorting.
That is the total amount of communication must be at least.
It implies that the complexity of the solution for the ordered line, protocol OddEven-LineSort, was not bad after all.
In this graph dG(xi, xj ) = 1 for any two distinct entities xi and xj.
Hence, the lower bound of Theorem 5.3.1 in the complete graph K becomes simply.
By Contrast, we have seen that protocol OddEven-MergeSort always uses O(N logN ) messages; thus, there is a large gap between upper bound and lower bound.
This indicates that protocol OddEven-MergeSort, even when correct, is far from optimal.
Summarizing, the expensive OddEven-LineSort is actually optimal for the ordered line, while OddEven-MergeSort is far from being optimal in the complete graph.
Implications for Solution Design The bound of Theorem 5.3.1 expresses a cost that every sorting protocol must incur.
Examining this bound, there are two considerations that we can make.
The first consideration is that, to design an efficient sorting protocol, we should not worry about this necessary cost (as there is nothing we can do about it), but rather focus on reducing the additional amount of communication.
We must, however, understand that the necessary cost is that of the messages that move data items to their final destination (through the shortest path)
These messages are needed anyway; any other message is an extra cost, and we should try to minimize these.
The second consideration is that, as the data items must be sent to their final destinations, we could use the additional cost just to find out what the destinations are.
This simple observation leads to the following strategy for a sorting protocol, as described from the individual entity point of view:
The second step is the necessary part and causes the cost stated by Theorem 5.3.1
In the following section we will see how to efficiently determine the final destination of the data items.
In this section our goal is to design an efficient sorting protocol using the strategy of first determining the final destination of each data item, and only then moving the items there.
To achieve this goal, each entity xi has to efficiently determine the sets.
To determine each of these values we just need to solve a distributed selection problem, whose solution protocols we have discussed earlier in this chapter.
The rest of the iteration then consists of the distributed determination of the kj th smallest item among the data items still under consideration (initially, all data items are under consideration)
Unlike the other two sorting protocols we have examined, Protocol SelectSort is generic, that is, it works in any network, regardless of its topology.
Let M[K,N] denote the number of messages used to determine the kth smallest out of a distributed set of N elements.
As we have chosen protocol ReduceSelect, then ( recall expression 5.18) we have.
Notice that M[Rank] is a function of n only, whose value depends on the topology of the network G, but does not depend on N.
So as long as this quantity is of the same order (or smaller) than the necessary cost for G, protocol SelectSort is optimal.
In the previous section we have examined the problem of sorting a distributed set according to a given permutation.
This describes the common occurrence when there is some a priori ordering of the entities (e.g., of their ids), according to which the data must be sorted.
There are, however, occurrences where the interest is to sort the data with no a priori restriction on what ordering of the sites should be used.
In other words, in these cases, the goal is to sort the data according to a permutation.
Solving the unrestricted sorting problem means that we, as designers, have the choice of the permutation according to which we will sort the data.
Let us examine the impact of this choice in some details.
Notice that this cost does not depend on the size N of the distributed set, and it is less than the total additional costs of Protocol SelectSort.
This means that, with twice the additional cost of Protocol SelectSort, we can sort minimizing the necessary costs.
A key element in the functionality of distributed data is the ability to answer queries about the data as well as about the individual sets stored at the entities.
Because the data is stored in many places, it is desirable to answer the query in such a way as to minimize the communication.
We have already discussed answering simple queries such as order statistics.
In systems dealing mainly with distributed data, such as distributed database systems, distributed file systems, distributed objects systems, and so forth the queries are much more complex, and are typically expressed in terms of primitive operations.
In particular, in relational databases, a query will be an expression of join, project, and select operations.
These operations are actually operations on sets and can be re-expressed in terms of the traditional operators intersection, union, and difference between sets.
So to answer a query of the form “Find all the computer science students as well as those social science students enrolled also in anthropology but not in sociology”, we will need to compute an expressions of the form.
Clearly, if these sets are located at the entity x where the query originates, that entity can locally compute the results and generate the answer.
However, if the entity x does not have all the necessary data, x will have to involve other entities causing communication.
It is possible that each set is actually stored at a different entity, called the owner of that set, and none of them is at x.
Even assuming thatx knows which entities are the owners of the sets involved, there are many different ways and approaches that can be used to perform the computation.
For example, all those sets could be sent by the owners to x, which will then perform the operation locally and answer the query.
With this approach, call it A1, the volume of data items that will be moved is.
In some cases, for example in complete networks, the number of messages is given precisely by these sizes.
We can, however, express a lower bound on the number of data that must be moved.
As the entity x where the query originates must provide the answer, then, assuming x has none of the sets involved in the query, it must receive the entire answer.
Theorem 5.4.1 For every expression E, if the set of the entity x where the query originates is not involved in the expression, then for any strategy S.
What we will examine in the rest of this section is how we can answer queries efficiently by cleverly organizing the local sets.
In fact, we will see how the sets can be locally structured so that the computations of those subexpressions (and, thus, the answer to those queries) can be performed minimizing the volume of data to be moved.
To perform the structuring, there is need of some information at each entity; if not available, it can be computed in a prestructuring phase.
We first of all see how we can structure at each entity xi the local dataDi so to answer operations of intersections and differences with the minimum amount of communication.
The method we use to structure a local set is called Intersection Difference Partioning (IDP)
The idea of this method is to store each set Di as a collectionZi of disjoint subsets such that operations of union, intersection, and difference among the data sets can be computed easily, and with the least amount of data transfers.
In particular, the sets corresponding to the left children of level l are precisely the elements in common between Di and Sl :
By contrast, the sets corresponding to the right children of level l are precisely the elements in Di that are not part of Sj :
We are going to see now that it is possible to achieve the same goal storing at xi only the last partition Zi (i.e., the leaves of the tree)
Exactly what expressions can be answered by xi? To answer this question, observe the following:
Using these two facts and starting with Di , we can characterize the set E(xi) of all the expressions that can be answered by xi directly without communication.
An expression on k operands is sequential if it is of the form.
Notice that all this can be done by xi locally, without any communication.
Q(k), where each subquery can be answered directly by just one entity, once its local set has been stored using the partitioning method; furthermore, the answer to any two different subqueries is disjoint (Exercise 5.6.55)
This gives raise to our strategy for evaluating an arbitrary query:
To understand the advantages of this strategy, let us examine again the implications of Property 5.4.6
As the results of any two subqueries are disjoint, while the union of all results of the subqueries is precisely what we are asking for, we have that:
This optimality is with regards to the amount of data items that will be moved.
There are different possible decompositions of a queryQ into subqueries satisfying Property 5.4.6
All of them are equally acceptable to our strategy, and they all provide optimal volume costs.
To calculate the cost in terms of messages we need to take into account also the distances between the nodes in the network.
In this regard, some decompositions may be better than others.
The problem of determining the decomposition that requires less messages is a difficult one, and no solution is known till date.
An important consideration is that of the cost of setting up the final partitions at each entity.
Once in this format, we have seen how complex queries can be handled with minimal communication.
But to get it in this format requires communication; in fact each entity must somehow receive information from all the other entities about their sets.
In a complete network this can require just a single transmission of each set to a predetermined coordinator that will then compute and send the appropriate partition to each entity; hence, the total cost will be O(N ) where N is the total amount of data.
By contrast, in a line network the total cost can be as bad asO(N2), for example, if all sets have almost the same size.
It is true that this cost is incurred only once, at set-up time.
If the goal is only to answer a few queries, the cost of setup may exceed that of simply performing the queries without using the partitioned sets.
But for persistent distributed data, upon which many queries may be placed, this is an efficient solution.
Another consideration is that of the addition or removal of data from the distributed sets.
As each entity contains some knowledge about the contents of all other entities, any time an item is added to or removed from one of the sets, every entity must update its partition to reflect this fact.
Fortunately, the cost of doing this does not exceed the cost of broadcasting the added (or removed) item to each entity.
Clearly this format is more effective for slowly changing distributed data sets.
Selection among two sites was first studied by Michael Rodeh [14]; his solution was later improved by S.
Reducing the expected costs of distributed selection has been the goal of several investigations.
The even more efficient protocol ReduceSelect was later designed by Nicola Santoro and Ed Suen [19]
The sorting protocols Odd-Even Mergesort algorithm, on which Protocols OddEven-LineSort and OddEven-MergeSort are based, was developed by Kenneth Batcher [1]
The first general distributed sorting algorithm is due to Lutz Wegner [22]
There is an extensive amount of investigations on database queries, whose computation requires the use of distributed set operations like union, intersection and difference.
The entire field of distributed query processing is dedicated to this topic, mostly focusing on the estimation of the size of the output of a set operation and thus of the entire query.
The IDP structure for minimum-volume operations on distributed sets was designed and analyzed in this context by Ekow Otoo, Nicola Santoro, Doron Rotem [13]
Exercise 5.6.4 Prove that after discarding the elements greater than mx from Dx and discarding the elements greater than my from Dy , the overall lower median is the lower median of the elements still under considerations.
Exercise 5.6.5 Write protocol Halving so that it works with any two arbitrarily sized sets with the same complexity.
Exercise 5.6.6 Prove that the K-selection problem can be reduced to a medianfinding problem regardless of K and of the size of the two sets.
Write the corresponding algorithm, GeneralHalving, prove its correctness, and analyze its complexity.
Exercise 5.6.13 Prove that in the worst case, the number of iterations performed by Protocol RandomFlipSelect until termination is N.
Exercise 5.6.19 Prove that in the execution of Protocol REDUCE, Local Contraction is executed at the most three times.
Exercise 5.6.23 Prove that OddEven-LineSort performs an invariant-sized sort of an equidistribution on an ordered line.
Exercise 5.6.31 Write the set of rules of protocol OddEven-MergeSort.
Exercise 5.6.33 Prove that protocol OddEven-MergeSort correctly sorts, regardless of the storage requirement, if the initial set is equidistributed.
Exercise 5.6.36 Determine for each of the three storage requirements (invariant, equidistributed, compacted) a lower bound, in terms of n and N on the amount of necessary messages for sorting in a ring.
Exercise 5.6.40 Write the set of rules of Protocol SelectSort.
Exercise 5.6.41 Establish for each of the storage requirements the worst-case cost of protocol SelectSort to sort an equidistributed set in a ordered line.
Determine under what conditions the protocol is optimal for this network.
Exercise 5.6.42 Establish for each of the storage requirements the worst-case cost of protocol SelectSort to sort a distributed set in a ordered line.
Determine under what conditions the protocol is optimal for this network.
Exercise 5.6.43 Establish for each of the storage requirements the worst-case cost of protocol SelectSort to sort an equidistributed set in a ring.
Determine under what conditions the protocol is optimal for this network (Hint: Use result of Exercise 5.6.36)
Exercise 5.6.44 Establish for each of the storage requirements the worst-case cost of protocol SelectSort to sort a distributed set in a ring.
Determine under what conditions the protocol is optimal for this network (Hint: Use result of Exercise 5.6.36)
Exercise 5.6.45 Establish for each of the storage requirements the worst-case cost of protocol SelectSort to sort an equidistributed set in a labeled hypercube of dimension d.
Determine under what conditions the protocol is optimal for this network (Hint: Use result of Exercise 5.6.37)
Exercise 5.6.46 Establish for each of the storage requirements the worst-case cost of protocol SelectSort to sort a distributed set in a labeled hypercube of dimension d.
Determine under what conditions the protocol is optimal for this network (Hint: Use result of Exercise 5.6.37)
Show that at least 1/4 of the items are removed from consideration at each iteration.
An algorithm with decentralized control for sorting files in a network.
Bit complexity of order statistics on a distributed star network.
In the distributed computing environments we have considered so far, we have not made any assumption about time.
In fact, from the model, we know only that in absence of failure, a message transmitted by an entity will eventually arrive to its neighbor: the Finite Delays axiom.
Nothing else is specified, so we do not know for example how much time will a communication take.
In our environment, each entity is endowed with a local clock; still no assumption is made on the functioning of these clocks, their rate, and how they relate to each other or to communication delays.
For these reasons, the distributed computing environments described by the basic model are commonly referred to as fully asynchronous systems.
They represent one extreme in the spectrum of message-passing systems with respect to time.
As soon as we add temporal restrictions, making assumptions on the the local clocks and/or communication delays, we describe different systems within this spectrum.
At the other extreme are fully synchronous systems, distributed computing environments where there are strong assumptions both on the local clocks and on communication delays.
These systems are defined by the following two restrictions about time: Synchronized Clocks and Bounded Transmission Delays.
Restriction 6.1.1 Synchronized Clocks All local clocks are incremented by one unit simultaneously.
Restriction 6.1.2 Bounded Communication Delays There exists a known upper bound on the communication delays experienced by a message in absence of failures.
In a fully synchronous system, two consecutive clock ticks constitute a unit of time, and we measure the time costs of a computation in terms of the number of clock ticks elapsed from the time the first entity starts the computation to the time the last entity terminates its participation in the computation.
In other words, we can define new clock ticks, each comprising u old ones, and act accordingly.
In particular, each entity will only send messages at the beginning of.
Clearly, the entities must agree on when the new time unit starts.
After the transformation, we can still measure time costs of a computation correctly: If the execution of a protocol lastsK new time units, its time cost is uK original clock ticks.
This means that we can assume, without loss of generality, that the following restriction holds:
Restriction 6.1.3 Unitary Communication Delays In absence of failures, a transmitted message will arrive and be processed after at most one clock tick.
The main advantage of doing this redefinition of unit of time is that it greatly simplifies the design and analysis of protocols for fully synchronous systems.
In fact, it is common to find fully synchronous systems defined directly as having unitary delays.
Property 6.1.1 Bounded messages In fully synchronous systems, messages have bounded length.
In other words, there exists a constant c (depending on the system) such that each message will contain at most c bits.
Bounded messages are also called packets and the constant c is called packet size.
It could be related to other system parameters such as n (the network size) or m (the number of links)
However, it cannot depend on input values (unless they are also bounded)
It implies that if the information an entity x must transmit does not fit in a packet, that information must be “split up” and transmitted using several packets.
More precisely, the transmission of w > c bits to a neighbor actually requires the transmission of M[w] messages where.
This fact affects not only the message costs but also the time costs.
As at most one message can be sent to a neighbor at a given clock tick, the number of clock ticks required by the transmission of w > c bits is.
Fully synchronous computing environments are dramatically different from the asynchronous ones we have considered so far.
In the following we will briefly describe two situations providing an insight in the unique nature of synchronous computations.
Overcoming Lower Bounds: Different Speeds As a first example of a synchronous algorithm, we will discuss a protocol for leader election in synchronous rings.
We assume the standard restrictions for elections (IR), as well as Synch; the goal is to elect as leader the candidate with the smallest value.
The protocol is essentially AsFar with an interesting new idea.
Recall that in AsFar each entity originates a message with its own id, forwards only messages with the smallest id seen so far, and trashes all the other incoming messages.
The message with the smallest value will never be trashed; hence it will make a full tour of the ring and return to its originator; every other message will be trashed by the first entity with a smaller id it encounters.
We have seen that this protocol has an optimal message complexity on the average but uses O(n2) messages in the worst case.
However, in a synchronous system, every message transmission will take at most one time unit; so, in a sense, all messages travel at the same speed.
How can we implement variable speeds in a synchronous system? The answer is simple:
Otherwise, after holding i for f (i) clock ticks, x will forward it along the ring.
In this simple way, a we have implemented both variable speeds and the “catch-up” of slow messages by faster ones!
The correctness of this new protocol follows from the fact that again, the message with the smallest id will never be trashed and will thus return to its originator; every.
Let us assume for the moment that all entities are initially candidates and start at the same time.
For every choice of the monotonically increasing speed function f we will obtain a different cost.
As the protocol will just need an additional n messages for the final notification, we have.
To achieve this result, we have used time directly as a computational tool: to implement the variable speeds of the messages and to select the appropriate waiting function f.
The result must be further qualified; in fact, it is correct assuming that the entity values are small enough to fit into a packet.
To have a better understanding of the amount of transmissions, we can measure the number of bits:
We have assumed that all entities start at the same time.
The election messages themselves can act as “wake-up” messages, traveling at normal (i.e., unitary) speed until they reach the first spontaneous initiator, and only then traveling at the assigned speed.
In this way, we still obtain a O(n) message complexity (Exercise 6.6.3)
In fact, it is exponential not inn (a system parameter) but in the range i of the input values.
Overcoming Transmission Costs: 2-bit Communication We have seen how, in a synchronous environment, the lowerbounds established for asynchronous problems do not necessarily hold.
This is because of the additional computational power of fully synchronous systems.
The most clear and (yet) surprising example of the difference between synchronous and asynchronous environments is the one we will discuss now.
Consider an entity x that wants to communicate to a neighbor y some information, unknown to y.
Recall that in a fully synchronous system messages are bounded: If I want to transmit w bits, I will have to send.
Property 6.1.2 In absence of failures, any finite sequence of bits can be communicated transmitting two messages, regardless of the message size.
Entity y (Figure 6.6) : (a) upon receiving the “Start-Counting” message, it records the current value.
In synchronous computing there is a difference between communication and transmission.
In fact, unlike asynchronous systems where transmission of messages is the only way in which neighboring entities can communicate, in synchronous systems absence of transmission can be used to communicate information, as we have just seen.
This is the radical difference between synchronous and asynchronous computing environments.
We will investigate how to exploit it in our designs.
Summarizing, the cost of a fully synchronous protocol is both time and bits.
In general, we can trade off one for the other, transmitting more bits to use less time, or vice versa, depending on our design goals.
In a system of communicating entities, the most basic and fundamental problem is obviously the process of an entity, the sender efficiently and accurately communicating information to another entity, the receiver.
If these two entities are neighbors, this problem is called Two-Party Communication (TPC) problem.
In an asynchronous system, this problem has only one solution: The sender puts the information into messages and transmits those messages.
In fully synchronous systems, as we have already observed, transmission of bits is not the only way of communicating information; for example, in a fault-free system, if no bit is received at local time t + 1, then none was transmitted at time t.
Hence, absence of transmission, or silence, is detectable and can be used to convey information.
In fact, there are many possible solutions to the Two-Party Communication problem, called communicators, each with different costs.
In this section we will examine the design of efficient communicators.
Owing to the basic nature of the process, the choice of a communicator will greatly affect the overall performance of the higher level protocols employed in the system.
We will then discuss the problem of communicating information at a distance, that is, when the sender and the receiver are not neighbors.
We will see how this and related problems can be efficiently solved using a technique well known in very large scale integration (VLSI) and parallel systems: pipeline.
Such a transformer is a useful tool to solve problems for which an asynchronous solution is already known.
Communicators are an essential component of a transformer; in fact, as we will see, different communicators result in different costs for the generated synchronous protocol.
This is one more reason to focus on the design of efficient communicators.
In the following, we will assume that no failure will occur, that is, we operate under restriction Total Reliability.
Consider the simple task of an entity, the sender, communicating information to a neighbor, the receiver.
Clearly, there are many different ways in which we can design a protocol for the.
The problem of performing this task is called the Two-Party Communication problem, and any solution protocol is called a communicator.
A communicator must specify the operations of the sender and of the receiver.
Associated with any communicator are clearly two related cost measures: the total number of packets transmitted and the total number of clock ticks elapsed during the communication; as we will see, the study of the two-party communication problem in synchronous networks is really the study of the trade-off between time and transmissions.
This protocol, also known as C2, belongs to a class of communicators called k-bit Communicators where the number of transmitted packets is a constant k fixed a priori and known to both entities.
Thus, the total amount of time from the time the sender starts the first transmission to the time the receiver decodes the information is the quantum of silence plus the two time units used for transmitting the bits.
With this extra quantum to our disposal, consider the following strategy.
In other words, this protocol, called C3, has sublinear time complexity.
The encoding of i can be defined recursively as follows:
To obtain i = I1, the receiver will recursively compute.
We will design a general class of solution protocols and analyze their cost; we will then establish lower bounds and show that the proposed protocols achieve these bounds and are therefore optimal.
Our goal is now to design protocols that can communicate any positive integer I transmitting k + 1 packets and using as little time as possible.
Observe that with k + 1 packets the communication sequence is.
The first class of protocols are able to tolerate the type of transmission failures called corruptions.
In fact, they use packets only to delimit quanta; as it does not matter what the content of the packet is (but only that it is being transmitted), these protocols will work correctly even if the value of the bits in the packets is changed during transmission.
The second class exploits the content of the packets to convey information about I ; hence, if the value of just one of the bits is changed during transmission, the entire communication will become corrupted.
In other words, these communicators need reliable transmission for their correctness.
Clearly, the bounds and the optimal solution protocols are different for the two classes.
We will consider the first class in details; the second types of communicators will be briefly sketched at the end.
To achieve this goal we need a bijection between k-tuples and nonnegative integers.
This is not difficult to do; it is sufficient to establish a total order among tuples as follows.
The solution protocol, which we will callOrderk , thus uses the following encoding and decoding schemes.
The correctness of the protocol derives from the fact that the mapping we are using is a bijection.
Optimality We are now going to show that protocol Orderk is optimal in the worst case.
We will do so by establishing a lower bound on the amount of time required to solve the two-party communication problem using exactly k + 1 bit transmissions.
We will establish the lower bound assuming that the values I we want to transmit are from a finite set U of integers.
This assumption makes the lower bound stronger because for infinite sets, the bounds can only be worse.
Let c(w, k) denote the number of additional time units needed in the worst case to solve the two-party communication problem for Zw with k + 1 bits that can be corrupted during the communication.
In conjunction with Equation 6.8, this means that protocol Orderk is a worst case optimal.
Corruption-Free Communication ($$) If bit transmissions are error free, the value of a received packet can be trusted.
Hence it can be used to convey information about the value I the sender wants to communicate to the receiver.
In this case, the entire communication sequence, bits and quanta, is meaningful.
Other Communicators The protocols Orderk and Order+k belong to the class of k + 1-bit communicators where the number of transmitted bits is fixed a priori and known to both the entities.
In this section, we consider arbitrary communicators, where the number of bits used in the transmission might not be not predetermined (e.g., it may change depending on the value I being transmitted)
With arbitrary communicators, the basic problem is obviously how the receiver can decide when a communication has ended.
This can be achieved in many different ways, and several mechanisms are possible.
The sender uses a special pattern of bits to notify the end of communication.
As part of the communication, the sender communicates the total number of bits it will use.
For example, the sender uses the first quantum to communicate the number of bits it will use in this communication; the drawback of this approach is that the first quantum cannot be used to convey information about I.
We now show that, however ingenious the employed mechanism be, the results are not much better than those obtained just using optimal k + 1-bit communicators.
In fact, an arbitrary communicator can only improve the worst-case complexity by an additive constant.
This is true even if the receiver has access to an oracle revealing (at no cost) for each transmission the number of bits the sender will use in that transmission.
This implies that, in the worst case, communicator Order+k requires at most one time unit more than any strategy of any type which uses the same maximum number of incorruptible bits.
There exists a way of drastically reducing the time without increasing the number of bits.
This can be achieved using a well known technique called pipeline.
To understand how can an entity xj communicate an information it does not yet have, consider x2 and assume that the communicator being used is TwoBits.
It is true that x2 does not know I , so it does not know how long it has to wait.
The reasoning we just did to explain why pipeline works at x2 applies to each of the xj.
So, the answer to the question above is that each entity xj will know the information it must communicate exactly in time.
Summarizing, the entities will start staggered by one time unit and will terminate staggered by one time unit.
Each will be communicated the value I communicated by the sender.
Regardless of the communicatorC employed (the same by all entities), the overall solution protocol CommLine is composed of two simple rules:
Each communication is done using communicator C; hence the total number of bits is the same as in the nonpipelined case:
Let us stress that we use the same number of bits as a nonpipelined (i.e., sequential) communication; the improvement is in the time costs.
The time will depend on where Imax is located; in the worst case, it is x1 and the time will be.
Let us see how pipeline can be used in this case.
Again, we will make all entities in the chain start staggered by one unit of time, and each entity will start waiting a quantum of time equal to its own value.
We have described the solution using TwoBits as the communicator.
In fact, since A makes no assumptions on time, it will run under every timing condition, including the fully synchronous ones.
Its cost in such a setting would be the number of messages M(A) and the “ideal” time T (A)
Note that this presupposes that the size m(A) of the messages used byA is not greater than the packet size c (otherwise, the message must be broken into several packets, with a corresponding increasing message and time complexity)
We can actually exploit the availability of an asynchronous solution protocol A in a more clever way and with a more efficient performance than just running A in the fully synchronous system.
In fact, it is possible to transform any asynchronous protocol A into an efficient synchronous one S, and this transformation can be done automatically.
In the transformation, the transmission (and corresponding reception) of I in A is replaced by the communication of I using communicator C; this communication requires Time(C, I ) time and Packets(C, I ) packets.
As the information of each of the M(A) messages must be communicated, and the messages have size at most m(A), the total number of packets P(S) transmitted by the synchronous protocol S is just.
This simple transformation mechanism might appear to yield inefficient solutions for the synchronous case.
To dispel this false appearance, we will consider an interesting application.
Application: Election in a Synchronous Ring Consider the problem of electing a leader in a synchronous ring.
We assume the standard restrictions for elections (IR), as well as Synch.
We have seen several efficient election algorithms for asynchronous ring networks in previous chapters.
Let us choose one and examine the effects of the transformer.
This result must be compared with the bounds of the election algorithm Speed specifically designed for synchronous systems (see Figure 6.13): The transformation lemma yields bounds that are order of magnitude better than those previously obtained by specifically designed algorithm.
Once we have obtained a solution protocol using a transformer, both the bits and the time complexity of this solution depend on the communicator employed by the transformer.
Sometimes, the time complexity can be further reduced without increasing the number of bits by using pipeline.
For example, during every stage of protocol Stages and thus of protocol SynchStages, the information from each candidate must reach the neighboring candidate on each side.
This operation, as we have already seen, can be efficiently done in pipeline, yielding a reduction in time costs (Exercise 6.6.26)
Design Implications The transformation lemma gives a basis of comparison for designing efficient synchronous solutions to problems for which there already exist asynchronous solutions.
To improve on the bounds obtained by the use of the transformation lemma, it is necessary to more explicitly and cleverly exploit the availability of “time” as a computational tool.
Some techniques that achieve this goal for some specific problems are described in the next sections.
SynchStages O(n log n) O(i n log n) FIGURE 6.13: The transformer yields a more efficient ring election protocol.
When designing a protocol, our aim must be to avoid the transmission of unbounded messages; in particular, if the input values are drawn from some unbounded universe (e.g., positive integers) and the goal of the computation is the evaluation of a function of the input values, then the messages cannot contain such values.
For example, the “trick” on which the transformation lemma is based is an instance of a simple and direct way of exploiting time by counting it; in this case, the actual value is communicated but not transmitted.
Our main goal as protocol designers is to exploit the fact that in synchronous systems, time is an explicit computational tool, so as to develop efficient solutions for the assigned task or problem.
Let us consider again two problems that we have extensively studied for asynchronous networks: minimum-finding and election.
We assume the standard restrictions for minimum-finding (R), as well as Synch; in the case of election, we obviously assume Initial Distinct Values (ID) also.
We have already seen a solution protocol, Speed, designed for synchronous ring networks; we have observed how its low message costs came at the expense of a time complexity that is exponential in the range of the input values.
The Transformation Lemma provides a tool that automatically produces a synchronous solution when an asynchronous one is already available.
We have seen how the use of a transform leads to an election protocol for rings, SynchStages, with reduced bits and time costs.
The cost of minimum-finding and election can be significantly reduced by using other types of “temporal” tools and techniques.
In this section, we will describe two basic techniques that make an explicit use of time, waiting and guessing.
We will describe and use them to efficiently solve MinFinding and Election in rings and other networks.
Waiting is a technique that uses time not to transmit a value (as in the communicators), but to ensure that a desired condition is verified.
Waiting in Rings Consider a ring network where each entity x has as initial value a positive integer id(x)
Let us assume, for the moment, that the ring is unidirectional and that all entities start at the same time (i.e., simultaneous initiation)
Let us further assume that the ring size n is known.
The way of finding the minimum value using waiting is surprisingly simple.
What an entity x will initially do is nothing, but just wait.
The entity x waits for a certain amount of time f (id(x), n)
If nothing happens during this time, the entity determines “I am the smallest”
If, instead, while waiting the entity receives a “Stop” message, it determines “I am not the smallest” and forwards the message.
With the appropriate choice of the waiting function f , this surprisingly simple protocol works correctly!
To make the process work correctly, the entities with the smallest value must finish waiting before anybody else does (in this way, each of them will correctly determine “I am the minimum”)
In other words, the waiting function f must be monotonically decreasing: if id(x) < id(y) then.
In fact, it is also necessary that every entity whose value is not the smallest receives a “Stop” message while still waiting (in this way, each of them will correctly determine “I am not the minimum”)
Protocol Wait solves the minimum-finding problem, not the election: Unless we assume initial distinct values, more than one entity might have the same smallest value, and they will all correctly determine that they are the minimum.
What is the cost of such a protocol? Only an entity that becomes minimum originates a message; this message will only.
Hence the total number of messages is just n; as these messages are signals that do not contain any value, we have that Wait uses only O(n) bits.
Without Simultaneous Initiation We have derived this surprising result assuming that the entities start simultaneously.
If the entities can start at any time, it is possible that an entity with a large value starts so much before the others that it will finish waiting before the others and incorrectly determine that it is the minimum.
This problem can be taken care of by making sure that although the entities do not start at the same time, they will start not too far away (in time) from each other.
To achieve this, it is sufficient to perform a wake-up: When an entity spontaneously wants to start the protocol, it will first of all send a “Start” message to its neighbor and then start waiting.
An inactive entity will become active upon receiving the “Start” message, forward it, and start its waiting process.
The cost of the protocol is slightly bigger, but the order of magnitude is the same.
In fact, in terms of bits we are performing also a wake-up that, in a unidirectional ring, costs n bits.
As for the time, the new waiting function is just twice as the old one; hence, the time costs are at most doubled.
In other words, the costs are still those indicated in Figure 6.16
If the ring is bidirectional, the protocol requires marginal modifications, as shown in Figure 6.17
The same costs as the unidirectional case can be achieved with the same waiting functions.
If the entities have all available a value n that is, however, smaller than n, its use in a waiting function instead of n would in general lead to incorrect results.
There is, however, a range of values for n that would still guarantee the desired result (Exercise 6.6.32)
Consider the general case when the entities have available neither n nor a common value n, that is, each entity only knows its initial value id(x)
In this case, if each entity uses in the waiting function its value id(x) instead of n, the function would work in some cases, for example, when all initial values id(x) are not smaller than n.
Universal Waiting Protocol The waiting technique we have designed for rings is actually much more general and can be applied in any connected network G, regardless of its topology.
As soon as an entity x is active, it starts waiting f (id(x), n) time units.
If, nothing happens while x is waiting, x determines that “I am the minimum” and initiates a reset with message “Stop.”
If, instead, a “Stop” message arrives while x is waiting, then it stops its waiting, determines that “I am not the minimum” and participates in the reset with message “Stop.”
Again, regardless of the initiation times, it is necessary that the entities with the smallest value finish waiting before the entities with larger value and that all those other entities receive a “Stop” message while still waiting.
Applications of Waiting We will now consider two rather different applications of protocol Wait.
The first is to compute two basic Boolean functions, AND and OR; the second is to reduce the time costs of protocol Speed that we discussed earlier in this chapter.
In both cases we will consider unidirectional ring for the discussion; the results, however, trivially generalize to all other networks.
In discussing these applications, we will discover some interesting properties of the waiting function.
Once this is done, the entities with such a value know the result.
That is, if an entity has value 0, it does not wait at all.
To determine the cost of the overall protocol is quite simple (Exercise 6.6.35)
In a similar way we can use protocol Waiting to compute the OR of the input values (Exercise 6.6.36)
Reducing Time Costs of Speed The first synchronous election protocol we have seen for ring networks is Speed, discussed in Section 6.1.4
NOTE: to solve the election problem it assumes initial distinct values.
On the basis of the idea of messages traveling along the ring at different speeds, this protocol has unfortunately a terrifying time complexity: exponential in the (a priori unbounded) smallest input value imin (see Figure 6.16)
Protocol Waiting has a much better complexity, but it requires knowledge of (an upperbound on) n; on the contrary, protocol Speed requires no such knowledge.
It is possible to reduce the time costs of Speed substantially by adding Waiting as a preliminary phase.
As each entity x knows only its value id(x), it will first of all execute Waiting using 2id(x)2 as the waiting function.
Depending on the relationship between the values and n, the Waiting protocol might work (Exercise 6.6.33), determining the unique minimum (and hence electing a leader)
If it does not work (a situation that can be easily detected; see Exercise 6.6.34), the entities will then use Speed to elect a leader.
The overall cost of this combine protocol Wait+ Speed clearly depends on whether the initial Waiting succeeds in electing a leader or not.
If Waiting succeeds, we will not execute Speed and the cost will just be O(i2min) time and O(n) bits.
If Waiting does not succeed, we must also run Speed that costs O(n) messages but O(n2imin) time.
This means that the overall time cost will be only O(n2n)
In other words, whether the initial Waiting succeeds or not, protocol Wait+Speed will use O(n) messages.
Summarizing, using Waiting we can reduce the time complexity of Speed from O(n2i) to.
Application: Randomized Election If the assumption on the uniqueness of the identities does not hold, the election problem cannot be solved obviously by any minimum-finding process, including Wait.
Furthermore, we have already seen that if the nodes have no identities (or, analogously, all have the same identity), then no deterministic solution exists for the election problem, duly renamed symmetry breaking problem, regardless of whether the network is synchronous or not.
This impossibility result applies to deterministic protocols, that is, protocols where every action is composed only of deterministic operations.
A different class of protocols are those where an entity can perform operations whose result is random, for example, tossing a dice, and where the nature of the action depends on outcome of this random event.
For example, an entity can toss a coin and, depending on whether the result is “head” or “tail,” perform a different operation.
These types of protocols will be called randomized; unlike their deterministic counterparts, randomized protocols give no guarantees, either on the correctness of their result or on the termination of their execution.
So, for example, some randomized protocols always terminate but the solution is correct only with a given probability; this type of protocols is called Monte Carlo.
Other protocols will have the correct solution if they terminate, but they terminate only with a given probability; this type of protocols are called Las Vegas.
We will see how protocol Wait can be used to generate a surprisingly simple and extremely efficient Las Vegas protocol for symmetry breaking.
To make the algorithm work, we need to design a mechanism to find the minimum and detect if it is unique.
In fact, protocol Wait not only finds the minimum value but also allows an entity x with such a value to detect if it is the only one.
If x is the only minimum, its message will come back exactly after n time units; in this case, x will become leader and send a Terminate message to notify all other entities.
If there are more than one minimum, x will receive a message before n time units; it will then send a “Restart” message and start the next round.
Up to now, except for the random selection criteria, there has been little difference between Symmetry and the deterministic protocols we have seen so far.
Let us compute the number of rounds required by the protocol until termination.
The surprising thing is that this protocol might never terminate, and thus the number of rounds is potentially infinite.
In fact, with a protocol of type Las Vegas, we know that if it terminates, it solves the problem, but it might not terminate.
This is not a good news for those looking for protocols with a guaranteed performance.
The advantage of this protocol is instead in the low expected number of rounds before termination.
For n large enough, this quantity is easily bounded; in fact.
Obviously, there is no guarantee that a leader will be elected with this cost or will be elected at all, but with high probability it will and at that cost.
Guessing is a technique that allows some entities to determine a value not by transmitting it but by guessing it.
Again we will consider the minimum finding and election problems in ring networks.
Let us assume, for the moment, that the ring is unidirectional and that all entities start at the same time (i.e., simultaneous initiation)
Let us further assume that the ring size n is known.
S1: All local values are greater than p; in this case, no messages will be transmitted: There will be “silence” in the system.
Clearly, to decide, y must wait; also clearly, it cannot wait forever.
How long should y wait? The answer is simple: If a message was sent by an entity, say x, a message will arrive at y within at most d(x, y) < n time units from the time it was sent.
Hence, if y does not receive any message in the first n time units since the start, then none is coming and we are in situation S1
We will say that p is an underestimate on imin.
The condition p = imin is also considered an overestimate.
Using this fact, we can reformulate the minimum-finding problem in terms of a.
The minimum value imin is a number, previously chosen and unknown to the player, that must be guessed.
The player can ask question of type “Is the number greater than p?”
A guessing protocol will just specify which questions should be asked to discover imin.
In general, on the basis of the outcome of the execution of Decide(pi), all entities will choose a new guess pi+1
The process is repeated until the minimum value imin is unambiguously determined.
Depending on which strategy is employed for choosing pi+1 given the outcome of Decide(pi), different minimum-finding algorithms will result from this technique.
Before examining how to best play (and win) the game, let us discuss the costs of asking a question, that is, of executing protocol Decide.
In fact in situation S1, no messages will be transmitted at all.
By contrast, in situation S2, there will be exactly n messages; as the content of these messages is not important, they can just be single bits.
Summarizing, If our guess is an overestimate, we will pay n bits; if it is an underestimate, it will cost nothing.
As for the time costs, each execution of Decide will cost n time units regardless of whether it is an underestimate or overestimate.
This means that we pay n time units for each question; however, we pay n bits only if our guess is an overestimate.
Our goal must, thus, be to discover the number, asking few questions (to minimize time) of which as few as possible are overestimates (to minimize transmission costs)
As we will see, we will unfortunately have to trade off one cost for the other.
Playing the Game We will now investigate how to design a successful strategy for the guessing game.
Let us first solve the problem with k = 1, that is, we want to find the minimum with only one overestimate.
As the number (i.e., when p= imin) is already an overestimate when we find it, k = 1 means that we can never use as a guess a value greater than imin.
If M is not a perfect square, the last interval will be smaller than the others.
The second overestimate is then spent to find imin inside that interval using sequential search (as in the case k = 1)
In the worst case, we have to search all the aj and all of the last interval, that is, in the worst case the cost will be.
Notice that by allowing a single additional overestimate (i.e., using an additional n bits) we have been able to reduce the time costs from linear to sublinear.
In other words, the trade-off between bits and time is not linear.
It is easy to generalize this approach (Exercise 6.6.43) so as to find imin with a worst-case cost of.
Notice that the cost is a trade-off between questions and overestimates: The more overestimates we allow, the fewer questions we need to ask.
Furthermore, the trade-off is nonlinear: The reduction in number of questions achieved by adding a single overestimate is rather dramatic.
As every overestimate costs n bits, the total number of bits is O(n k)
The total amount of time consumed with this approach is at most O(n k M1/k)
The Optimal Solution We have just seen a solution strategy for our guessing game when the value to be guessed is in a known interval.
In the case k = 1, there is only one possible solution strategy.
Thus, as usual, to answer the above question we will establish a lower bound.
Surprisingly, in this process, we will also find the (one and only) optimal solution strategy.
To establish a lower bound (and find out if a solution is good) we need to answer the following question:
Q1: What is the smallest number of questions q needed to always win the game in an interval of size M using no more than k overestimates?
Instead of answering this question directly, we will “flip its arguments” and formulate another question:
Q2: With q questions of which at most k are overestimates, what is the largest M so that we can always win the game in an interval of that size ?
The answer will obviously depend on both q and k, that is, M will be some function h(q, k)
Before we proceed, let us summarize the problem we are facing:
We have at our disposal q questions of which only k can be overestimates.
We want to know the size h(q, k) of the largest interval in which this is possible.
Whatever the strategy be, it must start with a question.
There are two possibilities; this is either an underestimate or an overestimate.
On the basis of this, it would seem that to make the interval as large as possible, we should choose our first guess p to be as large as possible.
However, we must take into account the possibility that our first guess turns out to be an overestimate.
Solving this recurrence relation (Exercise 6.6.45), we obtain the unique solution.
If we now “flip the answer,” we can answer also question Q1 and determine a lower bound on q given M and k.
In the process of finding a lower bound, we have actually found the (one and only) optimal solution strategy to guess in the interval [1,M] with at most k overestimates.
Optimal Search Strategy To optimally search in [1,M] with at most k overestimates:
Unbounded Interval We have found the optimal solution strategy using at most k overestimates but assuming that the interval in which imin lies is known.
If this is not the case, we can always first of all establish an upperbound on imin, thus determining an interval and then search in that interval.
SynchStages O(n log n) O( i n log n ) Wait O(n) O( i n ) n known.
Recall that each question costs n time units and if it is an overestimate it also costs n bits.
Thus, the complexity of the resulting minimum-finding protocol Guess becomes O(kn) bits andO(kn ik)
This means that for any fixed k, the guessing approach yields an election protocol that is far more efficient than the ones we have considered so far, as shown in Figure 6.26
Network Topology We have described our protocol assuming that the network is a ring.
However, the optimal search strategy for the guessing game is independent of the network topology.
To be implemented, it requires subprotocol Decide(p) that has been described only for rings.
This protocol can be made universal, and can thus work in every network, by simple modifications.
Notice that each question will now cost d time units.
The number w of bits transmitted if the guess is an overestimate depends on the situation; it is, however, always bounded as follows:
Simultaneous Start We have assumed that all entities start the first execution of Decide simultaneously.
This assumption can actually be removed by simply using a wake-up procedure at the very beginning (so to bound the delays between initiation times) and using a longer delay between successive guesses (Exercise 6.6.48)
Their use has led to bitoptimal and time-efficient solutions for the minimum-finding and election problems; we have described them for ring networks, but we have seen that they are indeed universal.
Their only drawback is that they require knowledge of n (or of some upperbound on the diameter d)
In contrast, both Speed and SynchStages did not require such an a priori knowledge.
What we are going to do is to still use the waiting technique to find the smallest value; as we do not know n (nor an upperbound on it), we are going to use the guessing strategy to discover an upperbound on n.
Summarizing, in the first case, x receives its own message; in the second case, the message was originated by somebody else.
Without knowing n, how can x know whether the received message is its own?
Clearly, if each message contains the id of its originator, the problem is trivially solved.
However, the number of bits transmitted by just having such a message traveling along the ring will be O(n log i), resulting in an unbounded quantity (see Figure 6.26)
The answer is provided by understanding how transmission delays work in a synchronous ring.
Consider the delay nx(j ) from the time x transmits its message to the time a message arrives at x.
If x receives its own message, then nx(j ) = n.
By contrast, if x receives the message of somebody else, this will happen before n time units.
If x does not notice anything wrong, x will assume that indeed tx(j ) = n and will start a second Wait (with a different waiting function) to verify the guess.
If the guess is correct, x is the only entity doing so; it should thus finish waiting without receiving any message.
Furthermore, the message “Wait2” it sends now should arrive exactly after nx(j ) time units.
If x now notices something wrong (i.e., a message arrives while waiting; a message does not arrive exactly after nx(j ) time units), it will send a “Restart” message to make everybody start with a new guess g(j + 1)
Otherwise, x considers the guess verified, becomes the leader, and sends a “Terminate” message.
What we have to show now is that with the appropriate choice of waiting functions, it is impossible for an entity x to be fooled.
That is, if x does not notice anything wrong in the first and in the second waiting and becomes leader, then indeed the message x receives is its own and nobody else will become leader.
Choosing the Waiting Functions What we have to do now is to choose the two waiting functions f and h so that it is impossible for an entity x to be fooled.
To determine the waiting functions f and h we need, let us consider the situation in more details, and let us concentrate on x and see under what conditions it would be fooled.
Denote by t(x, j ) the delay between the time the first entity starts the j th iteration and the time x starts it.
Entity x starts at time t(x, j ), waits f (id(x), j ) time, and then sends its “Wait1” message; it receives one at time.
Notice that to “fool” x, this “Wait1” message must have been sent by some other entity, y.
This means that y must also have waited without receiving any message; thus it sent its message at time t(y, j ) + f (id(y), j )
At this point it becomes leader and sends a “Terminate” message.
If x has been fooled the first time, then also message “Wait2” was sent by some.
It is not difficult to verify that if x has been fooled, then there is only one fooling entity, that is, y = z (Exercise 6.6.49)
To have sent a “Wait2” message, y must have not noticed anything wrong (otherwise it would have set a “Reset” instead)
This means that similarly to x, y received a “Wait1” message ny(j ) time units after it sent one, that is, at time t(y, j ) + f (id(y), j ) + ny(j )
It waited for another h(y, j ) time units and then sent the “Wait2” message; this message thus arrived at x at time.
So, if x has been fooled, it must by accident happen that.
Summarizing, x will be fooled if and only if the condition of Equation 6.37 occurs.
Notice that this condition does not depend on the first waiting function f but only on the second one h.
What we have to do is to choose a waiting function h that makes the condition of Equation 6.37 impossible.
As the identities are distinct (because of ID restriction), this means that x = y, that is, the messages x receives are its own.
In other words, with this waiting function, nobody will be fooled.
Summarizing, regardless of the waiting function f and of the monotonically increasing guessing function g, with the appropriate choice of the second waiting function h, protocol DoubleWait correctly elects a leader.
The Cost of DoubleWait Now that we have established the correctness of the protocol, let us examine its costs.
To determine the cost, we will first examine the number of bits and then determine the time.
As we will see, we will have available many choices and, again, we will be facing a trade-off between time and bits.
Bits Each iteration consists of at most two executions of the waiting technique (with different waiting functions)
Each iteration, except the last, will be aborted and a “Restart” message will signal the start of the next iteration.
Hence, the total number of bits transmitted by DoubleWait will be at most.
Let us first of all choose the waiting functions f and h.
With these choices made, we can determine the amount of time the protocol uses until termination.
In fact, it is immediate to verify (Exercise 6.6.53) that the number of time units till termination is less than.
Again, this quantity depends solely on the choice of the guessing function g.
Trade-offs: Choosing The Guessing Function The results we have obtained for the number of bits and the amount of time are expressed in terms of the guessing function g.
This is the only parameter we have not yet chosen.
Before we proceed, let us examine what is the impact of such a choice.
To help us in the decisional process, let us restrict to a class of functions.
If we restrict ourselves to superincreasing functions, then the bit and time costs of DoubleWait become (Exercise 6.6.54)
These bounds show the existence and the nature of the trade-off between time and bits.
Examining the trade-off, we discover two important features of protocol DoubleWait:
Symmetry O(n) O(n) n known; randomized FIGURE 6.28: Summary of Election techniques for synchronous rings.
Notice that the bit complexity can be asymptotically reduced to O(n), matching the one obtained by the protocols, Wait and Guess that assume knowledge of an upperbound on n; clearly this is achieved at the expense of an exorbitant time complexity.
An exact O(n) bit complexity with a reasonable time can, however, be achieved without knowing n using DoubleWait in conjunction with other techniques (Problem 6.6.9)
A fully synchronous system is by definition highly synchronized, so it might appear strange to talk about the need for synchronization in the system and the computational problems related to it.
Regardless of the oddity, the need and the problems exist and are quite important.
There is first of all a synchronization problem related to the local clocks themselves.
We know that in a synchronous environment all local clocks tick at the same time; however, they might not sign the same value.
A synchronous system is said to be in unison if indeed all the clock values are the same.
Notice that once a system is in unison, it will remain so unless the values of some clocks are locally altered.
The unison problem is how to achieve such a state, possibly with several independent initiators.
Also, in the firing squad problem all entities must enter a special state (usually called firing), but they must do so at the same time and for the first time.
It is also stronger than unison: With unison, all entities arrive at a point where they are operating with the same clock value, and thus, in a sense, they are in the same “state” at the same time; however, the entities do not necessarily know when.
We are going to consider all three problems and examine their nature and interplay in some details.
All of them will be considered under the standard set of restriction R plus obviously Synch.
In reset, all entities must enter the same state within finite time.
One important application of reset is when a distributed protocol is only initiated by a subset of the entities in the system, and we need all entities in the system to eventually begin executing the protocol.
When reset is applied at the first step of a protocol, it is called wake-up.
The wake-up or reset problem is a fundamental problem and we have extensively examined in asynchronous systems.
In fully synchronous systems it is sometimes also called weak unison; its solution is usually a preliminary step in larger computations (e.g, Wait, Guess, DoubleWait), and it is mostly used to keep the initiation times of the main computation bounded.
For example, in protocol Wait applied to a network G (not necessarily a ring) of known diameter d , the initial wake-up ensures that all entities become awake within d time units from the start.
To see why this is true, consider any wake-up protocol W that works for any complete networks regardless of the labeling.
By contradiction, letW useo(n2) signals in every complete network of size n.
We construct now a complete networkK2n with a different labeling.
E2: Only the selected entities will start and will do so simultaneously.
From the point of view of these initiators, everything in this execution happens exactly as if they were in the other execution in the other network: Messages will be sent and received exactly from the same ports in the same steps in both executions.
In particular, none of them will send a signal outside its “little clique.” Hence, none of the other nodes will receive any signal; as those entities did not wake up spontaneously, this means that none of them will wake up at all.
However, the majority of the nodes is not awake, nor will it ever become awake, contradicting the correctness of the protocol.
In other words, there is no correct wake-up protocol for the complete networks that will always require less than O(n2) transmissions.
A synchronous system is said to be in unison if all the clock values are the same.
The unison problem is how to achieve such a state, possibly with several independent initiators.
Notice that once a system is in unison, it will remain so unless the values of some clocks are locally altered.
Let us examine a very simple protocol for achieving unison.
Each entity will execute a sequence of stages, each taking one unit of time, starting either spontaneously or upon receiving a message from another entity.
An initiator x starts by sending to all its neighbors the value of its local clock cx.
A noninitiator y starts upon receiving messages from neighbors: It increases those values by one time unit, computes the largest among these values and its own clock value, resets its clock to such a maximum, and sends it to all its neighbors.
If the value of the local clock is maximum, no message is sent; else, the local clock is set to the largest of all values, and this value is sent to all the neighbors (that sent a smaller value)
Consider the largest value tmax among the local clocks when the protocol starts.
It is not difficult to see that this value (increased by one unit at each instant of time) reaches every entity, and every entity will set its local clock to such a time value (Exercise 6.6.59)
In other words, with this simple protocol, that we shall call MaxWave, the entities are guaranteed to operate in unison within finite time.
Unison happens as soon as every entity whose initial clock value was smaller than tmax receives tmax (properly incremented)
In the worst case, only one entity z has tmax at the beginning, and this entity is the last one to start.
This value (properly incremented) has to reach every other entity in the network; this propagation will require at most a number of time units equal to the diameter d of the network; as z will start at most d time units after the first entity, this means that.
How can an entity detect termination ? How does it know whether the system is now operating in unison ? Necessarily, an entity must know d (or an upperbound on d , e.g., n) to be able to know when the protocol is over.
In fact, once an entity receives the max time, it will transmit only in this step and no more.
So the entities with the largest value will transmit to their neighbors only once; their neighbors will transmit only twice; in general, the entities at distance j from the entities with the largest value will transmit.
We also know that an entity does not send the max time to those neighbors from which it received it.
The actual cost depends on the topology of the network and the actual initiation times.
In all this discussion, we have made an implicit assumption that the clock values we are sending are bounded and fit inside a message.
In fact, clock values increase at each time unit; in our protocol, the transmitted values were increased at each time unit and the largest was propagated.
To ensure that the values are bounded, we concentrate on the definition of the problem: Our goal is to achieve unison, that is, we want all local clocks to sign the same value.
Notice that the definition does not care for what that value is, but only for that it is the same for all entities.
Armed with this understanding, we make a very simple modification to the MaxWave protocol:
It requires that all entities enter a predefined special state, firing, for the first time simultaneously.
More precisely, all the entities are initially in active state, and each active entity can at any time spontaneously become excited.
The goal is to coordinate the entities so that, within finite time from the time the first entity becomes excited, all entities become firing simultaneously and for the first time.
In its original form, the problem was described for synchronous cellular automata (i.e., computational entities withO(1) memory) placed in a line of unknown length n, and where the leftmost entity in the line is the sole initiator, known as the “general”
Note that as cellular automata only have a constant memory size, they cannot represent (nor count up to) nonconstant values such as n or d.
We are interested in solving this problem in our setting, where the entities have at least O(logn) bits of local memory, and thus they can count up to n.
Again we are looking for a protocol that can work in any network; observe that the entities need to know or to compute (an upperbound on) d to terminate.
What happens if there is no spanning tree available? Even worse, what happens if no spanning tree is constructible (e.g., in anonymous network)? The problem can still be solved.
To do so, let us explore the relationship between firing squad and unison.
First observe that as all entities become firing simultaneously, if each entity resets its local clock when it becomes firing, all local clocks will have the same value 0 at the same time.
In unison, all the local clocks will at some point sign the same value; however, the entities might not know exactly when this happens.
They might become aware (i.e., terminate) at different times; but for firing squad synchronization we need that they make a decision simultaneously, that is, with no difference in time.
Surprisingly, protocol MaxWave actually solves the firing squad problem in networks where no spanning tree is available.
If a message originated by an initiator reaches entity y at time t + w, then the value of that message (incremented by 1) is exactly w.
Regardless of whether y has already independently started or starts now, the current value of its local clock will be smaller than w; thus, y will set its clock in unison with the clocks of the initiators.
Once the clocks are in unison, unless someone resets them, they keep on being in unison.
As nobody is resetting the clocks again, this means that all entities will be in unison at time t + d.
The value of the clocks at that time is exactly d.
This means that when the reset local clock signs time d, the entity knows that indeed the entire system is in unison; if the entity enters state firing at this time, it.
Summarizing, protocol MaxWave solves the firing squad problem in d time units:
Some of the work on synchronous computing was done very early on in the context of Cellular Automata and Systolic Arrays; in particular, pipeline is a common computational tool in VLSI systems (which include systolic arrays)
This result has alerted algorithmic researchers to the existence of the field.
Subsequent improvements to bounded bit complexity and to reduced time costs were obtained by using (and combining) communicators, waiting and guessing.
The optimal kcommunicators have been designed by Una-May O’Reilly and Nicola Santoro [20]
The first combined use of communicators and pipeline is due to B.
The computations in trees using pipeline are due to Paola Flocchini [8]
The first bit-optimal election protocol for rings is due to Hans Bodlaender and Gerard Tel [5]; it does, however, require exponential time.
The simpler and more efficient protocol Symmetry has been designed by Greg Frederickson and Nicola Santoro [10]
These results have been extended to environments with ATA-synchrony by Paul Spirakis and Basil Tampakas [25]
The trade-offs for wake-up in complete graphs with chordal labeling are due to Amos Israeli, Evangelos Kranakis, Danny Krizanc, and Nicola Santoro [13]
Bounding the message size was studied by Anish Arora, Shlomi Dolev, and Mohamed Gouda [2], always in the context of self-stabilization.
The firing squad problem was originally proposed for Cellular Automata by J.
The universal protocol MaxWave is a simple extension of existing unison solutions.
Exercise 6.6.3 Modify protocol Speed so that even if the entities do not start simultaneously, a leader is elected with O(n) messages.
Use the content of the packets to convey information about the value i to be communicated.
Modify protocol R3 using the content of the packets so as to reduce the time costs.
Determine a class MonotoneOrder+k of optimal communicators that are monotonically increasing.
Write a protocol for finding the smallest value in a chain using the 2-bit communicator and pipeline.
Write a protocol for finding the sum of all the values in a chain using the 2-bit communicator and pipeline.
Add pipeline to this protocol to convey information from a candidate to a neighboring one.
Analyze its costs; in particular, determine the reduction in time with respect to the nonpipelined version.
Exercise 6.6.27 Modify protocol Wait so that it finds the minimum value only among the initiators.
Exercise 6.6.28 Determine the smallest waiting function that allows protocol Wait to work correctly without simultaneous initiation: (a) in a unidirectional ring; (b) in a bidirectional ring.
In particular, determine under what conditions the protocol would certainly work.
Exercise 6.6.35 Determine the cost of computing the AND of all input values in a synchronous ring of known size n using protocol Waiting.
Exercise 6.6.36 Describe how to efficiently use protocol Wait to compute the OR of the input values in a synchronous ring of known size n.
Exercise 6.6.37 Modify protocol Symmetry so that it works efficiently in a bidirectional square torus of known dimension.
Exercise 6.6.38 Modify protocol Symmetry so that it works efficiently in a unidirectional square torus of known dimension.
Exercise 6.6.39 Prove that with simultaneous initiation, protocol Symmetry can be modified so as to work correctly in every network of known girth.
Hint: Use the girth instead of n in the waiting function.
Exercise 6.6.41 Modify protocol Decide so as to compute the OR of the input values in a synchronous ring of known size n.
Exercise 6.6.42 Write protocol Guess and implement it; throughly test your implementation.
Exercise 6.6.47 Modify subprotocol Decide(p) so that it will work in every network, regardless of its topology.
Assume that an upperbound on the diameter of the network is known a priori.
Exercise 6.6.48 Modify subprotocol Decide(p) so that protocol Guess works correctly even if the entities do not start simultaneously.
Determine the number of bits if the time is O(n log n i)
Exercise 6.6.59 Prove that in protocol MaxWave, the largest of the local clock values (when the execution starts) will reach (properly increased) every entity, and each entity will set its local clock to such a (properly increased) time value.
Exercise 6.6.62 Determine the message cost of protocol MaxWave in a kdimensional hypercube.
Exercise 6.6.63 Determine the worst-case and average-case message costs of protocol MaxWave in a tree network.
Prove that in this way, the maximum value transmitted is at most 2d.
Exercise 6.6.65 Consider the unison protocol MinWave where instead of setting the clocks to and propagating the largest value, we set the clock to and propagate the smallest value.
Exercise 6.6.67 Determine the bit and time costs of protocol MaxWave if the content of a message is communicated using a k-bit communicator.
Exercise 6.6.70 In protocol MaxWave, let a message originated by an initiator reach another entity y at time t + w.
Prove that the value of that message (incremented by 1) is exactly w.
Exercise 6.6.71 In protocol MaxWave, let a message originated by an initiator reach another entity y at time t + w.
Prove that regardless of whether y has already independently started or starts now, the current value of its reset local clock will be smaller than w; thus, y will set its clock in unison with the clocks of the initiators.
Determine the minimum cost that can be achieved and design the corresponding protocol.
Determine the minimum cost that can be achieved and design the corresponding protocol.
Problem 6.6.4 (Size Communicator) Consider the class of communicators that use the first quantum to communicate the total number of bits that will be transmitted.
Determine the minimum cost that can be achieved and design the corresponding protocol.
Each entity has a positive integer value; they all start at the same time, but their values are not necessarily distinct.
The maximum-finding problem is the one of having all the entities with the largest value become maximum and all the other small.
Design a protocol to solve the maximum-finding problem in time linear in imax using at most O(n log n) bits.
Possibly the time should be polynomial in i or exponential in n.
Hint: Use a single iteration of DoubleWait as a preprocessing phase.
Prove or disprove that in this case the unison problem cannot be solved with explicit termination.
The goal is thus to solve the firing squad problem with the least amount of time and to do so with the least amount of memory.
The measure we use for the memory is the max number of different values that can to be stored in the memory, and it is called the number of states of the automaton.
Consider a line ofCAwith only one initiator (located at the end of the line)
Develop a solution using only five states or prove it can not be done.
Partial Answer to Exercise 6.6.11 The encoding of I can be defined recursively as follows:
Answer to Exercise 6.6.49 Letx be fooled and incorrectly become leader at the end of the j th iteration.
According to the algorithm the only way that x has for becoming leader is the following:
At time t(x, j ), x starts waiting for f (x, j )
Note that during this time x must not receive any message to become a leader later.
At time t(x, j ) + f (x, j ), x sends a “Wait1” message and becomes checking.
Note that during this time, x must not receive any message in order to become a leader later.
At time t(x, j ) + f (x, j ) + nx(j ) + h(x, j ), it sends a “Wait2” message and becomes checking-again.
The “Wait1” message is sent by y only after it successfully finished the waiting f (y, j ) time units.
That is, the “Wait1” message will be sent by y at time t(y, j ) + f (y, j )
This message requires d(y, x) unit times to reach x.
Therefore, t(x, j ) + f (x, j ) +m(x, j ) = t(y, j ) + f (y, j ) + d(y, x)
In this case, the “Wait1” message originated by y reaches z before arriving to x.
As we have assumed that this message will arrive to x, it means that z must have forwarded it; the only way it could have done so is by becoming passive, but in this case zwill not originate a Wait2 message, contradicting the assumption we have made.
Answer to Exercise 6.6.50 Let x be the entity with the smallest id, and denote this value by i.
That is, x will finish waiting before anybody else; its message will travel along the ring transforming into passive all other entities and will reach x after nx = n time units.
Thus, x will be the only entity starting the second waiting, and its “Wait2” message will reach x again after nx = n time units.
Hence, x will validate its guess, become leader, and notify all other entities of termination.
The use of a synchronizer yields maximum computation rate in distributed networks.
Unison, canon and sluggish clocks in networks controlled by a synchronizer.
Improvements in the time complexity of two message-optimal election algorithms.
New protocols for the election of a leader in a ring.
Tight bounds for synchronous communication of information using bits and silence.
Improved bounds for electing a leader in a synchronous ring.
Distributed control of updates in multiplecopy databases: a time optimal algorithm.
Optimal tradeoff between time and bit complexity in synchronous rings.
In all previous chapters, with few exceptions, we have assumed total reliability, that is, the system is failure free.
In this chapter we will examine how to compute, if possible, when failures can and do occur.
We speak of a failure (or fault) whenever something happens in the systems that deviates from the expected correct behavior.
In distributed environments, failures and their causes can be very different in nature.
In fact, a malfunction could be caused by a design error, a manufacturing error, a programming error, physical damage, deterioration in the course of time, harsh environmental conditions, unexpected inputs, operator error, cosmic radiations, and so forth.
Not all faults lead (immediately) to computational errors (i.e., to incorrect results of the protocol), but some do.
So the goal is to achieve fault-tolerant computations, that is, our aim is to design protocols that will proceed correctly in spite of the failures.
The unpredictability of the occurrence and nature of a fault and the possibility of multiple faults render the design of fault-tolerant distributed algorithms very difficult and complex, if at all possible.
In particular, the more components (i.e., entities, links) are present in the system, the greater is the chance of one or more of them being/becoming faulty.
Depending on their cause, faults can be grouped into three general classes:
Note that the same fault can occur because of different causes, and hence classified differently.
Consider, for example, a message that an entity x is supposed to send (according to the protocol) to a neighbor y but never arrives.
This fault could have been caused by x failing to execute the “send” operation in the protocol: an execution error; by the loss of the message by the transmission subsystem: a transmission error; or by the link (x, y) going down: a component failure.
Depending on their duration, faults are classified as transient or permanent.
A transient fault occurs and then disappears of its own accord, usually within a short period of time.
A bird flying through the beam of a microwave transmitter may cause lost bits on some network.
A transient fault happens once in a while; it may or may not reoccur.
If it continues to reoccur (not necessarily at regular intervals), the fault is said to be intermittent.
A loose contact on a connector will often cause an intermittent fault.
A permanent failure is one that continues to exist until the fault is repaired.
Burnout chips, software bugs, and disk head crashes often cause permanent faults.
Depending on their geographical “spread”, faults are classified as localized or ubiquitous.
Localized faults occur always in the same region of the system, that is, only a fixed (although a priori unknown) set of entities/links will exhibit a faulty behavior.
Ubiquitous faults will occur anywhere in the system, that is, all entities/links will exhibit at some point or another a faulty behavior.
Note that usually transient failures are ubiquitous, while intermittent and permanent failures tend to be localized.
Clearly no protocol can be resilient to an arbitrary number of faults.
In particular, if the entire system collapses, no protocol can be correct.
Hence, the goal is to design protocols that are able to withstand up to a certain amount of faults of a given type.
Another fact to consider is that not all faults are equally dangerous.
The danger of a fault lies not necessarily in the severity of the fault itself but rather in the consequences that its occurrence might have on the correct functioning of the system.
In particular, danger for the system is intrinsically related to the notion of detectability.
In general, if a fault is easily detected, a remedial action can be taken to limit or circumvent the damage; if a fault is hard or impossible to detect, the effects of the initial fault may spread throughout the network creating possibly irreversible damage.
For example, the permanent fault of a link going down forever is obviously more severe than if that link failure is just transient.
In contrast, the permanent failure of the link might be more easily detectable, and thus can be taken care of, than the occasional mulfanctioning.
In this example, the less severe fault (the transient one) is potentially more dangerous for the system.
With this in mind, when we talk about fault-tolerant protocols and fault-resilient computations, we must always qualify the statements and clearly specify the type and number of faults that can be tolerated.
To do so, we must first understand what are the limits to the fault tolerance of a distributed computing environment, expressed in terms of the nature and number of faults that make a nontrivial computation (im)possible.
Given the properties of the system and the types of faults assumed to occur, one would like to know the maximum number of faults that can be tolerated.
To establish the resiliency, we need to be more precise on the types of faults that can occur.
In particular, we need to develop a model to describe the failures in the system.
Faults, as mentioned before, can be due to execution errors, transmission errors, or component failures; the same fault could be caused by any of those three causes and hence could be in any of these three categories.
There are several failure models, each differing on what is the factor “blamed” for a failure.
Each failure model offers a way of describing (some of the) faults that can occur in the system.
A model is not reality, only an attempt to describe it.
Component Failure Models The more common and most well known models employed to discuss and study fault tolerance are the component failures models.
In all the component failure models, the blame for any fault occurring in the system must be put on a component, that is, only components can fail, and if something goes wrong, it is because one of the involved components is faulty.
Depending on which components are blamed, there are three types of component failure models: entity, link, and hybrid failure models.
In the entity failure (EF) model, only nodes can fail.
For example, if a node crashes, for whatever reason, that node will be declared faulty.
In this model, a link going down will be modeled by declaring one of the two incident nodes to be faulty and to lose all the message to and from its neighbor.
Similarly, the corruption of a message during transmission must be blamed on one of the two incident nodes that will be declared to be faulty.
In the link failure (LF) model, only links can fail.
For example, the loss of a message over a link will lead to that link being declared faulty.
In this model, the crash of a node is modeled by the crash of all its incident links.
The event of an entity computing some incorrect information (because of a execution error) and sending it to a neighbor, will be modeled by blaming the link connecting the entity to the neighbor; in particular, the link will be declared to be responsible for corrupting the content of the message.
In the hybrid failure (HF) model, both links and nodes can be faulty.
Although more realistic, this model is little known and seldom used.
In all three component failure models, the status faulty is permanent and is not changed, even though the faulty behavior attributed to that component may be never repeated.
In other words, once a component is marked with being faulty, that mark is never removed; so, for example, in the link failure model, if a message is lost on a link, that link will be considered faulty forever, even if no other message will ever be lost there.
That is, we focus on systems where (only) entities can fail.
Within this environment, the nature of the failures of the entities can vary.
With respect to the danger they may pose to the system, a hierarchy of failures can be identified.
With crash faults, a faulty entity works correctly according to the protocol, then suddenly just stops any activity (processing, sending, and receiving messages)
Such a hard fault is actually the most benign from the overall system point of view.
With send/receive omission faults, a faulty entity occasionally loses some received messages or does not send some of the prepared messages.
This type of faults may be caused by buffer overflows.
Notice that crash faults are just a particular case of this type of failure: A crash is a send/receive omission in which all messages sent to and and from that entity are lost.
From the point of view of detectability, these faults are much more difficult than the previous one.
With Byzantine faults, a faulty entity is not bound by the protocol and can perform any action: It can omit to send or receive any message, send incorrect.
Clearly, dealing with Byzantine faults is going to be much more difficult than dealing with the previous ones.
A similiar hierarchy between faults exists in the link as well as in hybrid failures models.
Communication Failures Model A totally different model is the communication failure or dynamic fault (DF) model; in this model, the blame for any fault is put on the communication subsystem.
More precisely, the communication system can lose, corrupt, and deliver to the incorrect neighbor.
As in this model, only the communication system can be faulty, a component fault such as the crash failure of a node, is modeled by the communication system losing all the messages sent to and from that node.
Notice that in this model, no mark (permanent or otherwise) is assigned to any component.
In the communication failure model, the communication subsystem can cause only three types of faults:
An omission: A message sent by an entity is never delivered.
An addition: A message is delivered to an entity, although none was sent.
A corruption: A message is sent but one with different content is received.
While the nature of omissions and corruptions is quite obvious, that of additions is less so.
The most obvious one is when sudden noise in the transmission channel is mistaken for transmission of information by the neighbor at the other end of the link.
The more important occurrence of additions in sytems is rather subtle, as an addition models the reception of a “nonauthorized message” (i.e., a message not transmitted by any authorized user)
In this sense, additions model messages surreptitiously inserted in the system by some outside, and possibly malicious, entity.
Spam being sent from an unsuspecting site clearly fits the description of an addition.
These three types of faults are quite incomparable with each other in terms of danger.
The hierarchy comes into place when two or all of these basic fault types can simultaneously occur in the system.
The presence of all three types of faults creates what is called a Byzantine faulty behavior.
Clearly, no protocol can tolerate any number of faults of any type.
Thus, when we talk about fault-tolerant protocols and fault-resilient computations, we must always qualify the statements and clearly specify the type and number of faults that can be tolerated.
The term “Byzantine” refers to the Byzantine Empire (330–1453 AD), the long-lived eastern component of the Roman Empire whose capital city was Byzantium (now Istanbul), in which endless conspiracies, intrigue, and untruthfulness were alleged to be common among the ruling class.
Our goal is to design protocols that can withstand as many and as dangerous faults as possible and still exhibit a reasonable cost.
What we will be able to do depends not only on our ability as designers but also on the inherent limits that the environment imposes.
In particular, the impact of a fault, and thus our capacity to deal with it and design fault-tolerant protocols, depends not only on the type and number of faults but also on the communication topology of the system, that is, on the graph G.
This is because all nontrivial computations are global, that is, they require the participation of possibly all entities.
For this reason, Connectivity is a restriction required for all nontrivial computations.
Even when initially existent, in the lifetime of the system, owing to faults, connectivity may cease to hold, rendering correctness impossible.
Hence, the capacity of the topological structure of the network to remain connected in spite of faults is crucial.
There are two parameters that directly link topology to reliability and fault tolerance:
Clearly, the higher the connectivity, the higher the resilience of the system to component failures.
Property 7.1.1 If cedge(G) = k, then for any pair x and y of nodes there are k edge-disjoint paths connecting x to y.
Property 7.1.2 If cnode(G) = k, then for any pair x and y of nodes there are k node-disjoint paths connecting x to y.
A treeT has the lowest connectivity of all undirected graphs: cedge(T ) = cnode(T ) = 1, so any failure of a link or a node disconnects the network.
For example, in a hypercube H , both connectivity parameters are log n.
Clearly the highest connectivity is to be found in the complete network K.
Note that in all connected networks G the node connectivity is not greater than the edge connectivity (Exercise 7.10.1) and neither can be better than the maximum degree:
As an example of the impact of edge connectivity on the existence of fault-tolerant solutions, consider the broadcast problem Bcast.
As an example of the impact of node-connectivity on the existence of fault-tolerant solutions, consider the problem of an initiator that wants to broadcast some information, but some of the entities may be down.
In this case, we just want the nonfaulty entities to receive the information.
In most distributed computations there is a need to have the entities to make a local but coordinated decision.
When there are no faults, reaching these agreements is possible (as we have seen in the other chapters) and often straightforward.
Interestingly, the impact that faults have on problems requiring agreement for their solution has common traits, in spite of the differences of the agreement constraints.
That is, some of the impact is the same for all these problems.
For these reasons, we consider an abstract agreement problem where this common impact of faults on agreements is more evident.
Summarizing, if there are no faults, consensus can be easily achieved (e.g., by computing the AND or the OR of the values)
Lower forms of agreement, that is, when p < n, are even easier to resolve.
In presence of faults, the situation changes drastically and even the problem must be restated.
In fact, if an entity is faulty, it might be unable to participate in the computation; even worse, its faulty behavior might be an active impediment for the computation.
In other words, as faulty entities cannot be required to behave correctly, the agreement constraint can hold only for the nonfaulty entities.
Each nonfaulty entity x has an input value v(x) and must terminally decide upon a value d(x) within a finite amount of time.
Similarly, we can define lower forms (i.e., when p < n) of agreement in presence of entity failures (EFT-Agree(p))
Reaching agreement, and consensus in particular, is strictly connected with the problem of reaching common knowledge.
Recall (from Section 1.8.1) that common knowledge is the highest form of knowledge achievable in a distributed computing environment.
In fact, any solution protocol P to the (fault-tolerant) consensus problem has the following property: As it leads all (nonfaulty) entities to decide on the same value, say d, then within finite time the value d becomes common knowledge among all the nonfaulty entities.
By contrast, any (fault-tolerant) protocol Q that creates common knowledge among all the nonfaulty entities can be used to make them decide on a same value and thus achieve consensus.
This implies that common knowledge is as elementary as consensus: If one cannot be achieved, neither can be other.
We will focus on achieving fault-tolerant consensus (problem EFT-Consensus described in Section 7.1.4), that is, we want all nonfailed entities to agree on the same value.
A first and immediate limitation to the possibility of achieving consensus in presence of node failures is given by the topology of the network itself.
This means, for example, that in a tree, if a node goes down, consensus among the others cannot be achieved.
In other words, fault-tolerant consensus cannot be achieved even under the best of conditions.
This really means that it is impossible to design fault-tolerant solutions for practically all important problems, as each could be used to achieve fault-tolerant consensus.
Before proceeding further with the consequences of this result, also called FLP Theorem (after the initials of those who first proved it), let us see why it is true.
What we are going to do is to show that no protocol can solve this problem, that is, no protocol always correctly terminate within finite time if an entity can crash.
We assume that a correct solution protocol P indeed exists and then show that there is an execution of this protocol in which the entities fail to achieve consensus in finite time (even if no one fails at all)
It does require some precise terminology and uses some constructs that will be very useful in other situations also.
We will need not only to describe the problem but also to define precisely the entire environment, including executions, events, among others.
Some of this notation has already been introduced in Section 1.6
Let us consider next the status of the system and the events being generated during an execution of the solution protocol P.
An entity reacts to external events by executing the actions prescribed by the protocol P.
Namely, when an entity x sends a message, it creates the future event of the arrival of that message; similarly, when an entity sets the alarm clock, it creates the future event of that alarm ringing.
Although an entity can reset its clock as part of its processing, we can assume, without loss of generality, that each alarm will always be allowed to ring at the time it was originally set for.
Recall from Section 1.6 that the internal state of an entity is the value of all its registers and internal storage.
Also recall that the configuration C(t) of the system at time t is a snapshot of the system at time t; it contains the internal state of each entity and the set Future(t) of the future events that have been generated so far.
A configuration is nonfaulty if no crash event has occured so far, faulty otherwise.
Particular configurations are the initial configuration, when all processes are at their initial state and Future is composed of all and only the spontaneous and crash events; by definition, all initial configurations are nonfaulty.
The output register value cannot be changed after the entity has reached a decision state, that is, once x has made a decision, that decision cannot be altered.
If a configuration is reachable from some initial configuration, it will be called accessible; we are interested only in accessible configurations.
Consider an accessible configuration C; a sequence of events applicable to C is deciding if it generates a decision configuration; it is admissible if all messages sent to nonfaulty entities are eventually received.
Proof of Impossibility Let us now proceed with the proof of Theorem 7.2.1
By contradiction, assume that there is a protocolP that correctly solves the problem.
In other words, if we consider all the possible executions of P , every admissible sequence of events is deciding.
We then prove that starting from a bivalent configuration, it is always possible to reach another bivalent configuration.
Finally, using these two results, we show how to construct an infinite admissible sequence that is not deciding, contradicting the fact that all admissible sequence of events in the execution of P are deciding.
Let us call two initial configurations adjacent if they differ only in the initial value of a single entity.
The valency of the configuration, if known, is in square brackets.
We ensure that the sequence is admissible and nondeciding in the following way.
We maintain a queue Q of entities, initially in an arbitrary order.
In any infinite sequence of such stages every entity comes to the front of the queue infinitely many times and receives every message sent to it.
As each stage starts and ends in a bivalent configuration, a decision is never reached.
Summarizing, we have shown that there is an execution in which protocol P never reaches a decision, even if no entity crashes.
It follows that P is not a correct solution to our consensus problem.
The Single-Failure Disaster result of Theorem 7.2.1 dashes any hope for the design of fault-tolerant distributed solution protocols for nontrivial problems and tasks.
Because the consensus problem is an elementary one, the solution of almost every nontrivial distributed problem can be used to solve it, but as consensus cannot be solved even if just a single entity may crash, also all those other problems cannot be solved if there is the possibility of failures.
The negative impact of this fact must not be underestimated; its main consequence is that.
This means that to have fault tolerance, the distributed computing environment.
In other words, while in general not possible (because of Theorem 7.2.1), some degree of fault tolerance might be achieved in more restricted environments.
To understand which properties (and thus restrictions) would suffice we need to examine the proof of Theorem 7.2.1 and to understand what are the particular conditions inside a general distributed computing environment that make it work.
Then, if we disable one of these conditions (by adding the appropriate restriction), we might be able to design a fault-tolerant solution.
The reason why Theorem 7.2.1 holds is that, as communication delays are finite but unpredictable, it is impossible to distinguish between a link experiencing very long communication delays and a failed link.
In our case, the crash failure of an entity is equivalent to the simultaneous failure of all its links.
So, if entity x is waiting for a reply from y and it has not received one so far, it cannot decide whether y has crashed or not.
It is this “ambiguity” that leads, in the proof, to the construction of an admissible but nondeciding infinite sequence of events.
This means that to disable that proof we need to ensure that this fact (i.e., this “ambiguity”) cannot occur.
Recall that communication delays include both transmission and processing delays.
Next observe that the reason why in a synchronous environment the ambiguity is removed is because the entities can use timeouts to reliably detect if a crash failure has occurred.
Indeed, the availability of any reliable fault detector would remove any ambiguity and thus disable that proof of Theorem 7.2.1
In other words, either restriction Link-Failure Detection or restriction Node-Failure Detection would disable that proof even if communication delays are unbounded.
Observing the proof, another point we can make is that it assumes that all initial bivalent configuration are nonfaulty, that is, the fault has not occurred yet.
This is necessary in order to give the “adversary” the power to make an entity crash when most appropriate for the proof.
It is actually sufficient that the faulty entity crashes before it sends any message, and the proof does no longer hold.
This means that it might still be possible to tolerate some crashes if they have already occurred, that is, they occur before the faulty entities send messages.
In other words, the restriction Partial Reliability stating that no faults will occur during the execution of the protocol would disable the proof, even if communication delays are unbounded and there are no reliable fault detectors.
Notice that disabling the proof we used for Theorem 7.2.1 does not imply that the Theorem does not hold; indeed a different proof could still work.
Fortunately, in those restricted environments we have just indicated that the entire Theorem 7.2.1 is no longer valid, as we will see later.
Finally, observe that the unsolvability stated by Theorem 7.2.1 means that there is no deterministic solution protocol.
It does not, however, rule out randomized solutions, that is, protocols that use randomization (e.g., flip of a coin) inside the actions.
The main drawback of randomized protocols is that they do not offer any certainty: Either termination is not guaranteed (except with high probability) or correctness is not guaranteed (except with high probability)
In fully synchronous environment, the proof of the Single-Failure Disaster theorem does not hold.
Indeed, as we will see, synchronicity allows a high degree of fault tolerance.
Recall from Chapter 6 that a fully synchronous system is defined by two restrictions: Bounded Delays and Synchronized Clocks.
We can actually replace the first restriction with the Unitary Delays one, without any loss of generality.
We consider again the fault-tolerant consensus problem EFT-Consensus (introduced in Section 7.1.4) in the complete graph in case of component failures, and more specifically we concentrate on entity failures, that is, the faults are localized (i.e., restricted) to a set of entities (eventhough we do not know beforehand which they are)
The problem asks for all the nonfaulty entities, each starting with an initial value v(x), to terminally decide on the same value in finite time, subject to the nontriviality condition: If all initial values are the same, the decision must be on that value.
We will see that if the environment is fully synchronous, under some additional restrictions, the problem can be solved even when almost one third of the entities are Byzantine.
In the case of crash failures, we can actually solve the problem tolerating any number of failures.
In a synchronous system in which the faults are just crashes of entities, under some restrictions, consensus (among the nonfailed entities) can be reached regardless of the number f of entities that may crash.
Note that an entity can crash while performing an action, that is, it may crash after sending some but not all the messages requested by the action.
If x has not received a message from neighbor y by time t + 1, it knows that y has crashed; if it receives a message from y, it will know a “report” on what y knew at time t (note that in case of Byzantine faults, this “report” could be false)
For the appropriate choice of T and with the appropriate information sent in the “report,” this mechanism enables the nonfaulty entities to reach consensus.
The actual value of T and the nature of the report depend on the types and number of faults the protocol is supposed to tolerate.
Can this situation occur in reality ? For this situation to occur, the 0 must have been sent at time f by some entity yf ;
Let us now look at the cost of protocol TellAll-Crash.
It comprises f + 1 rounds in which each nonfailed entity sends a single bit to all its neighbors.
First observe that the reason the nonfailed entities transmit in each round of protocol TellAll-Crash is only to propagate the 0 value one of them might have seen (and of which the other entities migh not yet be aware)
This means that as long as an entity sees just 1, it is not going to change the default situation.
In fact, the proof of Theorem 7.3.1, with almost no modifications, can be used to show that.
These bounds are established assuming that all entities start simultaneously.
If this is not the case, we can still solve the problem by first performing a wake-up (with possibility of crashes)
These bounds are established assuming that the network is a complete graph.
If this is not the case, and the network is a graph G, the problem can still be solved, provided.
This and related results are established under the following set BA of restrictions :
Let us see what can go wrong if we were to use the same technique in a Byzantine setting.
The first problem is not really severe; in fact, as each entity knows the identity of its neighbors (restrictions BA), when x receives a message it can detect whether the id inside is the correct one and trash the message if it is forged.
The second problem is, however, severe; as a consequence, a nonfaulty x can not simply accept any wake-up message it receives.
To see how to deal with this problem, note that what matters is not if a wake-up message was originated by a Byzantine entity, but rather if the same message was received by all nonfaulty entities.
In fact, if all nonfaulty entities accept the same information, then (regardless of its origin) they will take the same decision.
Therefore, what we need is a mechanism, to be used by the protocol, that allows x to decide whether all the other nonfaulty entities also received this wake-up message; only then, x will accept the wake-up message, even if originated by a Byzantine entity.
In other words, this mechanism must ensure that if the originator is nonfaulty, then the wake-up is accepted; if the originator is faulty, then it is accepted only if all nonfaulty entities received it.
The mechanism that we will call RegisteredMail and describe below dictates what actions must be taken when a nonfaulty entity wants to send a wake-up message, and when a nonfaulty entity receives this message.
Let us now verify that RegisteredMail is exactly the mechanism we are looking for.
Now we describe a simple binary Byzantine agreement algorithm, called TellZeroByz, that uses RegisteredMail for sending and accepting wake-up messages.
For simplicity, in the description of the protocol and in its analysis, when an entity sends a message, we will assume that it will send it also to itself (i.e., it will receive it in the next time unit)
Observe that the mechanism RegisteredMail is started only at even time steps.
Let us now analyze the correctness and complexity of the protocol.
To prove the theorem we need to show that both nontriviality and agreement conditions hold.
Summarizing, as each message contains the entity’s id, we have.
We will first consider the case n = 3 and show that it is not possible to tolerate a single faulty entity.
The entities in R do not know that the network they are in is not R.
We will now construct, starting from P , an agreement protocol for R with one Byzantine faults as follows:
We first divide the entities of Kn into three sets, A, B, and C, of size at least 1 and at most f each;
This protocol, Sim(P ), actually is a solution protocol for R.
In fact, the Byzantine failure of an entity inR corresponds to the Byzantine failure of the assigned simulated.
We have seen how to reach Boolean agreement in fully synchronous complete graphs.
In this section, we are going to examine the cases when the input values are not Boolean.
We will see that it is possible to transform any solution protocol for the Boolean case into the one that works for an arbitrary, but a priori known, set of initial values IV.
An interesting property of the protocol is that in the “second” message all the nonfaulty entities send the same value (if any) from I.
Let us now examine the correctness of protocol FromBoolean when used in conjunction with a Boolean Byzantine consensus protocol BP (e.g., TellZero-Byz)
First observe that if in the execution of protocol BP the decision is 0, then all nonfaulty entities decide v by default, and agreement holds.
In particular, let us examine the cost that FromBoolean adds to BP.
As BP is started at time 2, and the final decision in FromBoolean is taken immediately upon termination of BP , the total time overhead is two additional time steps.
Summarizing, let v denote the range of the values; then.
For example, if we use the Boolean protocol TellZero-Byz, we will have.
Until now, when discussing localized Byzantine entity failures, we have been working under the restriction that the network topology is that of a complete graph Kn.
In this section we are going to examine what happens if we remove this assumption from the list of additional restrictions BA, that is, we consider the problem of reaching consensus in a generic network G localized Byzantine entity failures.
The new set of assumptions is the same as before except that knowledge of being in a complete graph is replaced with complete topological knowledge of the graph.
In other words, we cannot expect to do better (i.e., tolerate more faults) in G than in the complete graph.
We know that there is a link between the connectivity of the network and its fault tolerance.
Indeed, in the case of Byzantine failures the limitation becomes more severe (Exercise 7.10.17):
In other words, fewer than one third of the entities must be faulty, and the graph must be more than 2f -node-connected.
The interesting thing is that we can actually design a consensus protocol that tolerates those many faults in such networks.
This fact can be used to establish reliable two-party communication mechanism as follows.
As at most f entities are faulty, at most f of those paths are dangerous and messages along them can be corrupted; in contrast, f + 1 are not faulty and the message is delivered correctly.
In other words, a majority of the copies of the message from x to y will be correct.
Observe that using Mechanism Two-Parties ByzComm any pair of nonfaulty entities x and y can simulate the existence of a direct communication link (x, y) between them (as if they were in a complete network)
In other words, we can use any solution protocol P for a complete network Kn (e.g., TellZero-Byz) and execute it in G: Whenever an entity x is requested by P to send a message to y in Kn, x will use Mechanism Two-Parties ByzComm to achieve the same goal in G.
In the simulation, we need to redefine the unit of time; this is because, while in Kn the transmission of a message from x to y requires one time unit, in G the time.
As the maximum distance between any two nodes is diam(G), we will make a time unit of protocol P correspond to diam(G) time units in the simulation of P in G (we can do that because the network is synchronous and by Restrictions GA, all nonfaulty entities start at the same time)
The resulting protocol, that we will call ByzComm(P), thus achieves the desired goal:
For example, in the case of Boolean consensus, if we use TellZero-Byz, we will have.
For example, in a ring network R, as cnode(R) = 2, not even a single Byzantine entity can be tolerated, regardless of the size of the ring!
In a torus T r we have cnode(T r) = 4; hence at most a single Byzantine entity can be tolerated.
In a general asynchronous system, as we know, it is not possible to deal with even a single crash failure in the system even if the network is fully connected.
This fact, the Single-Failure Disaster, holds for protocols where the operations performed by the entities in their actions are all deterministic.
By contrast, if we empower and allow the entities to perform random operations during their actions, then the proof of the Single-Failure Disaster theorem no longer holds.
Hence, a way to construct fault-tolerant protocols is to provide randomness to the entities.
This can be achieved by providing the entities with the ability to flip coins during their actions; entities can then use the outcome to guide their operations.
For example, an entity in a ring network may flip a two-headed coin to decide to which neighbor it will send a prepared message: To the “left” if the outcome is “head,” and to the “right” if the outcome is “tail.” In this way, the choice of the neighbor is not specified by the algorithm (i.e., it is not deterministic), but it is rather the result of a “random” event (i.e., it is randomized)
Summarizing, coin flips return “random” values according to some specified probability distribution, and they may be used by the entities to determine their next move.
There are some important consequences about using randomization in protocols.
As the outcome of a coin flip is not known a priori, the number of possible executions depends not only on time delays but also on the outcome of the coin flips (which might be different in two different executions)
This means that we must define a probability distribution on executions, assigning to executions probabilities according to the outcomes of the coin flips that generate them.
It might be possible to have executions that terminate with an incorrect result.
The existence of incorrect executions might still be acceptable and permitted, provided they all occur with very low probability.
It might be possible to have executions that never terminate.
The existence of nonterminating executions might still be acceptable and permitted, provided they all occur with very low probability.
Monte Carlo protocols are distributed algorithms such that – they always terminate; – upon termination, the problem is solved correctly with high probability.
Las Vegas protocols are distributed procedures such that – they terminate with high probability; – upon termination, the problem is always solved correctly.
Hybrid protocols are distributed procedures such that – they terminate with high probability; – upon termination, the problem is solved correctly with high probability.
In other words, with randomization we must give up either the guarantee on correctness (in the Monte Carlo case) or the guarantee on termination (in the Las Vegas), or on both (if we so design)
Thus randomization might be appropriate for situations and applications where the concern is on overall system performance rather than that of a single execution.
In the rest of this chapter, we will see how to employ randomization to achieve some level of fault tolerance.
Clearly our achievement can only be with high probability, with no other guarantee.
In this section we are going to consider an asynchronous complete graph where entities can crash.
As we know, no deterministic solution protocol exists even if only one entity may crash.
This result will be derived under the following set of restrictions:
Each round is composed of two stages, a voting stage and a ratification stage.
Although the protocol goes on forever, the decision value of an entity is unique:
As described, each entity continues to run the protocol even after the decision is made; however, the protocol can be modified so that each entity terminates its execution at most one round after first setting its output value (Exercise 7.10.18)
In the following, we assume that this is the case.
For simplicity, in the description of the protocol and in its analysis, when an entity sends a message, we will assume that it will send it also to itself.
Let us now examine the correctness of the decision process if/when the protocol terminates.
Let prefx(r) denote the value variable pref of entity x at the beginning of round r , and let foundx(r) denote the value of variable found in that round.
Lemma 7.4.1 (Nontriviality) If, at the beginning of stage r, prefx(r) = v for every correct entity x, then all correct entities decide on v in that round.
We will make use of a simple but important observation.
This means that it is impossible that two entities decide at the same round on two different values.
We are now going to prove that all nonfaulty entities, if they decide, will decide on the same value.
Lemma 7.4.2 (Agreement) Let r be the first round in which nonfaulty entities make a decision, and let x be such an entity.
If x decides on v at round r, then every nonfaulty entity decides v by round r +1
It then remains to prove that the protocol terminates with high probability.
First of all observe that in a round r the preferences are not necessarily chosen.
This means that in every round r there is a positive probability that the preferences (random or not) of all correct entities are identical, an event that we will call a success.
When this happens, by Lemma 7.4.1, every correct entity will decide on the same value within that round.
As entities flip coins independently, the probability that a success happens within the first k rounds is (Exercise 7.10.19)
The good news is that this probability goes to 1 as k goes to infinity.
In this case, this probability goes very quickly to 1 as c grows.
If the number f is rather small, a success will be achieved in a constant number of rounds.
Hacking: Reducing the Number of Rounds We can use randomization further so as to reduce the expected number of rounds from exponential to sublinear when the number of faults is nearly one third.
We will do so in an efficient simulation of protocol Rand-Omit.
The translation is accomplished by creating k committees, each composed of s entities, where the values of parameters k and s will be discussed later; note that an entity may belong to several committees.
This assignment of entities to committees creates a new “virtual” system composed of k entities: the committees.
We then simulate the execution of protocol Rand-Omit in the new system of size k: Each committee will simulate a single entity running the protocol in this new system.
To correctly simulate the execution in the virtual system, the entities in a given committee must be able to agree on the messages received and messages to be sent by the committee.
Call a committee faulty if one third or more of its members are faulty, nonfaulty otherwise.
Then a nonfaulty committee must also be able to flip a reasonably unbiased coin global to that committee and independent of the coins of other nonfaulty committees.
All these factors can indeed be taken into account, and the correctness of the resulting Protocol Committee can be ensured (Problem 7.10.2)
We must still choose the values of parameters k and s; this will be done so as to minimize the costs of Protocol Committee.
The quantity r is actually dominated by the cost of flipping a coin in each committee, which is dominated in turn by the maximum number f of faulty entities within a nonfaulty committee (Exercise 7.10.22)
Thus, to minimize the cost of Committee, we need to choose a value of k that yields an appropriate value of f.
To do so, we use the following property (Exercise 7.10.23):
Summarizing, the total number of expected rounds of protocol Committee will be O(log n)
If all entities had access to a global source of random bits (unbiased and visible to all entities), then Byzantine Agreement could be achieved in constant expected time (Exercise 7.10.24)
In the case of synchronous systems, in addition to these general results, it is also possible to implement a global source of random bits by using digital signatures and secrete sharing (Problem 7.10.5)
The proof of the Single-Failure Disaster result is based on the fact that in an asynchronous system it is impossible to distinguish a slow entity from a failed one.
This means that the availability of any reliable fault detector would remove any such ambiguity and thus disable that proof.
So, for example, the presence of restriction Link-Failure Detection and/or restriction Node-Failure Detection would disable the proof of Theorem 7.2.1 even if communication delays are unbounded.
We have seen how in fully synchronous systems the Single-Failure Disaster result does not hold.
One reason is that if messages are never lost, synchrony yields a perfect failure detector for crash failures: As the absence of an anticipated message can be detected, a missing message indicates a faulty sender.
To date, the only reliable fault detectors are those obtained in fully synchronous systems, and in a fully synchronous system we already have seen how to deal with failures.
The real problem is that in systems that are not synchronous there are only unreliable fault detectors.
At this point, we have several intriguing and important questions.
In this section we will discuss some of these questions and the connected problems.
In our discussion and solutions, we will use an additional set of assumptions:
A distributed detector of entity failures is a set of n failure-detection modules, one per entity, providing to each entity (possibly incorrect) information about the failures that occur in an execution.
In particular, each module keeps a list of entities it suspects to be faulty.
It can be consulted during any action of the protocol; upon consultation, the module returns a list of entities that the module currently suspects to have crashed.
As the failure modules can make mistakes, each module may be continually adding and removing entities from its list.
For example, an entity may use its local clock to implement a failure module based on timeouts (even if the system is asynchronous)
If x times out entity y because, for example, y failed to respond in a timely fashion to a message sent by x, it may be the case that y has not failed (e.g., the message is just slow)
If x were to receive later a message from y, then x would know it had suspected y in error and would remove y from its list of suspects.
Furthermore, the failure-detection modules of two different entities need not agree on the list of entities that are suspected to have crashed.
Failure detectors are defined in terms of the properties they satisfy, rather than in terms of the actual implementation.
In particular, two properties are considered: completeness (i.e., the assurance that faulty entities are indeed reported) and accuracy (i.e., the assurance that correct entities are not reported as faulty)
To see that both properties are necessary, note that the trivial failure detector Paranoid, in which each entity permanently suspects every other entity, will satisfy any completeness property: All faulty entities are indeed reported; however, Paranoid provides no real information about the actual failures.
Similarly, the trivial failure detector Naive, in which each entity never suspects any other entity, will satisfy any accuracy property: No correct entity will ever be incorrectly reported as faulty; however, Naive too provides no real information about the failures.
Perpetual strong accuracy is difficult (if not impossible) to achieve in many practical systems.
Actually, perpetual weak accuracy is not very weak, because it guarantees that at least one correct entity is never suspected.
Suppose that, as is frequently the case in real systems, the failure detector is implemented by a “heart beat” protocol in which entities repeatedly broadcast “I am alive” messages.
Then even weak accuracy cannot be achieved (for example, it might be foiled if the network traffic is high and messages are delayed for too long)
Thus in general (regardless of how the failure detector is implemented) even weak accuracy may be too strong a requirement for any failure detector to achieve.
Fortunately, it is not necessary that the system always behave but only that it behave eventually, and even then only long enough for the entities to reach agreement, hence, the eventual accuracy properties.
A failure detector is said to be perfect if it satisfies strong completeness and perpetual strong accuracy: Every faulty entity is detected by every correct entity, and no correct entity is ever suspected faulty.
Note that in any fully synchronous system there is a perfect failure detector enjoying simultaneously the strong completeness and strong accuracy properties.
In particular, in a fully synchronous system, every entity can broadcast “I am alive” at every time unit.
If at some time unit entity x does not receive an “I am alive” message from entity y, then x knows y has failed.
Far from perfect are those failure detectors that satisfy only weak completeness and eventual weak accuracy.
Still, any such detector is powerful enough for our purposes.
We will now answer the question of what is the “weakest” detector we need to achieve consensus in spite of crash failures.
The answer is proved by designing an algorithm that allows to reduce a failure detector to another.
A failure pattern F is a function describing the set of entities that have crashed.
Typically, for any system and for any failure pattern F occurring in this system, there may be many executions with the same failure pattern.
Let D(F ) denote the set of all failure detector histories that can occur in executions with failure pattern F and failure detector D; given a particular execution r and a variable v, let vr denote the history of that variable during that execution.
More precisely, every entity x in REDUCE executes the following:
We have seen that even in a complete graph, it is impossible to achieve consensus among the nonfaulty entities if just one entity may crash.
The proof of the Single Failure Disaster relies heavily on the fact that the “adversary” can choose which entity fails as well as the moment when the failure occurs.
In fact, if all the failure occur before the execution of the algorithm, the proof does not hold.
Indeed, it might be possible to achieve complex tasks requiring consensus in spite of multiple failures, provided they have all occurred before the task starts.
The condition that all faults, if any, occur before the computation takes place is expressed by the restriction Partial Reliability that states, “No faults will occur during the computation.” Recall that the standard set of restrictions included instead Total Reliability that states, “No faults have occurred nor will occur.”
We will construct the algorithm by adapting the protocol CompleteElect we have designed for complete networks under Total Reliability in Section 3.6
In the original protocol, a candidate entity x starts by sending a “Capture” message to a single neighbor and waits for its reply.
In our setting, this entity could have crashed, so x would never receive the reply.
To overcome this, as at most f entities have crashed, at the beginning x will send the “Capture” message to f + 1 entities, to ensure that at least one of them is alive.
As soon as x receives an “Accept” from one of them, it enters the next stage and sends its message to another entity.
In other words, at any stage (except the last), a candidate entity will have f + 1 pending requests.
In the original protocol, a candidate entity x has only one pending “Capture” message and waits for its reply; if the reply is “Reject,” x becomes passive.
In our setting, x has f + 1 pending “Capture” messages and is waiting for a reply from each one of them (we know that at least one will arrive because at most f entities are faulty)
So it may happen that while waiting for the reply from y, x receives several “Accept” messages whose effect is to increase the stage number of x.
This means that if y sends a “Reject” to x, it is based on the old stage number of x.
In particular, if the stage of y (enclosed in the received “Reject” message) is smaller than the current one of x or they are the same but the id of x is smaller than that of y (also enclosed in the received “Reject” message), x must reject the “Reject” command.
What we will do instead is to have x settle its score with y once and for all.
This settlement is achieved as follows: x will send a new “Capture” message to y with its new stage number and close all its other ports waiting for the reply from y.
That will arrive because we know that y is alive.
Note that the effect of closing the other ports is that the stage number and the status of x will not change before the reply from y arrives.
When the reply arrives, x will either increase its stage (if it is “Accept”) or become passive (if it is “Reject”) before reopening all the ports.
Note that the total number of pending “Capture” messages for x will still be f + 1: The previous one to y (no longer pending) is now replaced by the new one to y.
Few other details have to be taken care of when completing the description of the adapted protocol that we shall call FT-CompleteElect (Exercise 7.10.28)
If an entity x has an owner, then both x and its owner are nonfaulty.
A nonfaulty entity always replies to a “Warning” message; thus, when x sends a “Warning” to its owner and closes all its ports except the one to the owner, it will receive a reply from y and will thus reopen all its ports.
Lemma 7.6.2 Assume an entity x ceases to be candidate as a result of a message originated by candidate y.
Then, at any time after the time this message is processed by x, either the stage of y is greater than the stage of x or x and y are in the same stage, but id(x) < id(y)
Lemma 7.6.3 At least one entity always remains a candidate.
Lemma 7.6.4 Let x be a candidate and s be its final size.
The total number of times a “Capture” message was sent by x is at most 2s + f.
Let us now turn to the costs of Protocol FT-CompleteElect.
Let s be the final size of a candidate x that initiated the algorithm.
Entities that did not wake up spontaneously never become candidates.
Thus by Lemma 7.6.4 the total number of messages is bounded by.
We have seen the devastating impact of node failures in the general asynchronous environments.
In this section, we study the impact of link failures by means of a tale.
We then see an example of how to design an application protocol that can tolerate link failures.
Imagine a military campaign a long time ago (before the radio communication was discovered)
Two allied armies, each with its own general, are positioned on two opposite hilltops overlooking a valley from which a large enemy army will approach.
Together, the allied forces can overwhelm the enemy if they attack at the appropriate time; by contrast, attacking at the inappropriate time or separately would clearly lead to a massacre of the attackers.
Having decided beforehand not to be massacred, neither general will command his army to attack, unless sure that the allied army will also attack.
General A can see from his hilltop the enemy army.
After many observations and calculations, A determines that dawn is the only time when a simultaneous attack by the allies will be successful.
General A must now communicate this information to generalB who, from the other hilltop, is unable to perform such calculations.
In order not to alert the enemy, fires or torches cannot be used; so to communicate, a messenger must be employed.
So general A deploys a messenger to deliver the message “Let us attack at dawn” to general B.
Notice that as A does not want to be massacred, he will not attack unless he knows that general B has received this message and will attack at dawn.
To go from one hill to the other, the messenger has to traverse the valley and it takes no more than 1 hour.
The problem is that there are enemy scouts patrolling the valley, and obviously there is the risk that the messenger is caught during the trip.
In spite of the danger, the messenger safely performs the trip and delivers the message to general B.
Knowing that A will not attack unless he gets confirmation, B sends the messenger back with the message “Message received; let us attack at dawn.”
Is this transmission of messages enough for the two generals to attack at dawn? The answer is No.
In fact, if the messenger gets caught now, A will never get the message, and, not knowing whether or not B has received the first message, he will not risk being massacred by attacking alone.
Fortunately, the messenger safely performs the trip back and delivers the message to general A.
Will now the two generals attack at dawn? The answer again is No.
In fact, B does not know whether or not his confirmation message has reached A; he, however, knows that, if it did not, A will not attack; therefore, B, not wanting to risk a massacre, will not attack.
The lucky messenger gets through again, delivering the message to B.
But again, B reflects that A does not know that this message has arrived and therefore he would not attack.
Interestingly, continuing to send messages back and forth is not going to lead the two generals to attack at dawn, ever, even if the messenger is never intercepted by the enemy.
In fact, the Two Generals problem of ensuring that both A and B attack at dawn is unsolvable:
Theorem 7.7.1 The Two Generals problem has no solution even if the system is fully synchronous.
First of all observe that, in any execution of any solution protocol in which the two generals decide to attack (i.e., in which IA = 1), at least one message must be delivered.
Otherwise, B cannot distinguish this scenario from an execution in which A decides not to attack (i.e., in which IA = 0), but no messages are delivered because the link is down.
Incidentally, this means that the link must deliver at least a message before it fails for the problem to be solvable.
Let us assume that this is the case; we will see that, even so, the problem is unsolvable.
Without loss of generality, assume that the last (i.e., the kth) message delivered in E is from A to B (see Figure 7.15(a))
Note that what is important is not the fact that the link fails (i.e., the messager is cought) but the possibility that it may fail.
An important consequence is that even in a synchronous system, two neighbors cannot achieve common knowledge if the link between them may go down and no other link can be used to communicate between them.
In fact, if G is not 2-edge-connected, there is a bridge, that is, a link e whose removal will disconnect the network.
Envision the two subgraphs connected by e as the two generalsA andB in the tale, and e as the unsafe path between the two hilltops (see Figure 7.16)
It follows by Theorem 7.7.1 that the entities in the two subgraphs cannot achieve common knowledge.
Let us stress that what makes the achievement of common knowledge (and, thus, the design of a fault-tolerant solution protocol) impossible is not the fact that faults occur but rather the fact that it is possible that they occur.
An immediate consequence of Theorem 7.7.2 is that if more than two links can possibly fail, then the connectivity requirements of the network must be clearly higher:
We have seen that if the number F of faulty links is equal to or greater than the edge connectivity of the network, it is impossible to compute.
By contrast, with fewer faulty links, it is possible to achieve a reasonable level of fault tolerance.
Let us consider the case when the edge connectivity is k + 1 or higher; in this case, it is indeed possible to achieve consensus and to perform most computations when k links can fail, even if the failures are send/receive omissions and the system is asynchronous.
The reason why this is possible is that in any network G, with fewer than cedge(G) faulty links, it is always possible to broadcast.
Indeed, protocol Flood allows to broadcast with send/receive omissions even if the number of faulty links is greater than cedge(G), as long as the failures do not disconnect the network.
Furthermore, Flood is independent of F , unlike most faulttolerant protocols where, for example, the number of iterations depends on F.
Once we can broadcast in spite of link failures, we can compute simple functions (e.g., AND, OR, Min, Max) and, thus, achieve consensus.
If restriction Initial Distinct Values (ID) holds, we can perform Election: Every (initiator) entity will broadcast its value; the entity with the smallest value will become the leader.
The overall cost will obviously depend on the cost of the broadcast protocol used.
For example, broadcasting with protocol Flood, the number of messages will be no more than that in a faulty-free execution: less than 2m(G)
In special graphs, it is clearly possible to achieve better bounds both for broadcasting and for election.
In this section, we will examine this issue for the class of networks where the O(n m(G)) message cost would be the largest.
In fact, we will consider a complete network and examine how to design efficient protocols that can withstand link failures even without any synchrony.
Election in a Complete Network with Faulty Links Let us consider now the task of electing a leader in presence of faulty links.
Clearly we need to assume that each entity has a unique identifier.
Let us assume that all entities are initiators (if they are not, the first received message will act as an initiation impulse)
Each entity x broadcasts (using a F -tolerant protocol) its value id(x)
When the algorithm starts, stage(x) is set to 1 and every entity is in the active state.
Third Step: In the third step, the “Warning” messages (if any) originated by the.
The parameter r yields a trade-off between time and messages: Increasing the value of r would decrease the time while increasing the number of messages.
The best message complexity is achieved whenever r = O(1)
We have seen that, although quite negative and restrictive, the result of Lemma 7.7.1 per se is not terribly discouraging: Even if several link may fail, it is possible to achieve a reasonable level of fault tolerance by sufficiently increasing the connectivity of the network.
In other words, the price for having link fault tolerance are increased hardware costs.
We have restricted our discussion to send/receive omissions; the situation clearly changes and the results do not hold if a faulty link can also corrupt the transmitted message.
We have examined in details when and how, in synchronous system, we can cope with the presence of faulty entities.
The bounds that we have seen are tight: We have designed protocols allowing consensus to be reached among the nonfaulty entities if the number of faulty entities did not execeed the bound; the presence of one more faulty entity would make the consensus task unreachable.
All our bounds have been established in terms of the number f of faulty entities present in the system; obviously we do not know beforehand who these entities are.
It is only during the execution that their identity might become known.
We can consider this situation like that of an external observer that has available f faulty stickers to pin, one per faulty entity.
When can the external observer declare an entity to be faulty and thus pin one of the stickers to it ? As long as the entity behaves correctly (i.e., according to the protocol), it cannot be clearly declared faulty.
In contrast to this, as soon as it behaves incorrectly, it will be declared to be faulty by the observer, and the number of available stickers decreases by one.
Note that not all f stickers will necessarily be assigned in every execution.
For example, a Byzantine entity can behave correctly and never be identified by the observer.
In the setting we have established our bounds, stickers are permanent: once an entity is declared faulty, its sticker is never removed.
The consequence of this fact is that the results we have established apply only to systems where the faults are localized and permanent, that is, they are restricted to a fixed set of f entities.
Further note that an occasional transient failure of an entity (e.g., losing a single message just once) is treated by the external observer in the same way as a permanent failure of an entity: In both cases, the observer will pin a permanent faulty sticker to the entity.
This leads to undesirable conclusions: In situations where every entity will occasionally lose a message (a situation that clearly occurs in real systems), the entire system will be declared unusable for any computation, even if the system is synchronous.
In the previous section we have examined in details when and how, in synchronous system, we can cope with localized and permanent entity and link faults.
In general, the majority of failures have mostly a transient and ubiquitous nature; that is, faults can occur anywhere in the system and, following a failure, normal functioning can resume after a finite (although unpredictable) time.
In particular, failures will occur on any communication link; almost every entity will experience at one time or another send or receive failure, and so forth.
In this section we will examine how we can deal with these communication failures, also called dynamic faults or mobile faults.
For the designer of a protocol, these types of faults are much more difficult to handle than the ones that occur always in the same places.
In the latter case, once a fault is detected, we know that we cannot trust that link; with mobile faults, detection will not help us with the future events.
It is, therefore, not surprising that the number of dynamic faults that can be tolerated at each time unit is by far less than that of the localized and permanent faults we can deal with.
What is surprising is perhaps the fact that something can be done at all.
In a synchronous network, as we have already observed in Chapter 6, silences are expressive: We can have communication between entities even if no message is sent.
Let us more formally define what we mean by communication in the context of the agreement problem, and what is a faulty communication.
While the nature of omissions and corruptions is quite obvious, that of additions may appear strange and rather artificial at first.
The most obvious one is when sudden noise in the transmission channel is mistaken for a message.
However, the more important occurrence of additions in sytems is rather subtle: When we say that the received message “was not transmitted,” what we really mean is that it “was not transmitted by any authorized user.” Indeed, additions can be seen as messages surreptitiously inserted in the system by some outside, and possibly malicious, entity.
Spam being sent from an unsuspecting site clearly fits the description of an addition.
These three types of faults are quite incomparable with each other in terms of danger.
The hierarchy of faults comes into place when two or all of these basic fault types can occur in the system (see Figure 7.2)
The presence of all three types of faults creates what is called a Byzantine faulty behavior.
As in the previous section, we will concentrate on the Agreement Problem Agree(p)
The fact that dynamic faults are not localized but ubiquitous makes the problem of designing fault-tolerant software much more difficult.
The difficulty is further increased by the fact that dynamic faults may be transient and not permanent (hence harder to detect)
This results in an instance of a more general result that we will be going to derive and examine in this section.
Impossibility of Strong Majority The basic result yielding the desired impossibility results for even strong majority is obtained using a “bivalency” technique similar to the one emplyed to prove the Single-Fault Disaster.
However, the environment here is drastically different from the one considered there.
In particular, we are now in a synchronous environment with all its consequences; in particular, delays are unitary; therefore, we cannot employ (to achieve our impossibility result) arbitrarily long delays.
Furthermore, omissions are detectable! In other words, we cannot use the same arguments, the resources at our disposal are more limited, and the task of proving impossibility is more difficult.
In the actual communication, some of these messages will not be delivered or their content will be corrupted, or a message will arrive when none has been sent.
As we will see, any set of F -admissible events that is both continuous and j -adjacency preserving for some j will make any strong majority protocol fail.
To prove our impossibility result, we are going to use two properties that follow immediately from the definitions of state and of event.
First of all, if an entity is in the same state in two different configurations A and B, then it will send the same messages in both configurations.
That is, let si(C) denote the internal state of xi in C; then.
Next, if an entity is in the same state in two different configurations A and B, and it receives the same messages in both configurations, then it will enter the same state in both resulting configurations.
Given a set S of events and an agreement protocol P , let P(P, S) denote the set of all initial configurations and those that can be generated in all executions of P when the events are those in S.
Lemma 7.8.2 Every bivalent configuration in P(P, S) has a succeeding bivalent configuration.
Consequences The Impossibility of Strong Majority result provides a powerful tool for proving impossibility results for nontrivial agreement: If it can be shown that a set S of events is adjacency preserving, continuous, and F -admissible, then no nontrivial agreement is possible for the types and numbers of faults implied by S.
Obviously, not every set S of events is adjacency preserving; unfortunately, all the ones we are interested in are so.
Omission Faults We can use the Impossibility of Strong Majority result to prove that no strong majority protocol is correct in spite of deg(G) communication faults, even when the faults are only omissions.
Let Omit be the set of all events containing at most deg(G) omission faults.
Minimum number of faults per clock cycle that may render strong majority impossible.
In this sequence, each event is adjacent to the following one; furthermore, as by construction each event contains at most deg(G) omissions, it is in Omit.
Addition and Corruption Faults Using a similar approach, we can show that when the faults are additions and corruptions no strong majority protocol is correct in spite of deg(G) communication faults.
Let AddCorr denote the set of all events containing at most deg(G) addition and corruption faults.
It is not difficult to verify that AddCorr is continuous (Exercise 7.10.40)
Lemma 7.8.4 AddCorr is deg (G)-admissible, continuous, and adjacency preserving.
In this section we examine the possibility of achieving unanimity among the entities, agreement in spite of dynamic faults.
Surprisingly, unanimity can be achieved in several cases; the exact conditions depend not only on the type and number of faults but also on the edge connectivity cedge(G) of G.
In all cases, we will reach unanimity, in spite of F communication faults per clock cycle, by computing the OR of the input values and deciding on that value.
This is achieved by first constructing (if not already available) a mechanism for correctly broadcasting the value of a bit within a fixed amount of time T in spite of F communication faults per clock cycle.
The actual reliable broadcast mechanism will differ depending on the nature of the faults.
Single Type Faults: Omissions Consider the case when the communication errors are just omissions.
That is, in addition to MA we have the restriction Omission that the only faults are omissions.
Let us verify that if F < cedge(G), there are values of the timeout T (G) for which the protocol performs the broadcast.
This value for the timeout is rather high and depending on the graphG can be substantially reduced.
Using algorithm Bcast-Omit to compute the OR we have the following:
They are both estimates on how much time it takes for the broadcast to complete.
Which estimate is better (i.e., smaller) depends on the graph G.
Let us now focus on the bit costs of the protocol Consensus-Omit obtained by computing the OR of the input values by means of algorithm Bcast-Omit.
In this way, if two neighbors send messages to each other at the same time, then no more messages will be sent between them from now on.
In other words, on a link at each time unit there is only one message, except at most once when there are two.
Unanimity can be reached regardless of the number of faults in time T = diam(G) transmitting 2m(G) diam(G) bits.
Observe that, although expensive, it is no more so that what we have been able to achieve with just omissions.
Single Type Faults: Corruptions Surprisingly, if the faults are just corruptions, unanimity can be reached regardless of the number of faults.
To understand this result, first consider, that as the only faults are corruptions, there are no omissions; thus, any message transmitted will arrive, although its content may be corrupted.
Furthermore, there are no additions; thus, only the messages that are transmitted by some entity will arrive.
This means that if an entity starts a broadcast protocol, every node will receive a message (although not necessarily the correct one)
All entities with an input value 1 become initiators of WFlood, in which all nodes participate.
If there is an initial value 1, as there are no omissions, all entities will receive a message within time T (G) = diam(G)
If all initial values are 0, no broadcast is started and, as there are no additions, no messages are received; thus, all entities will detect this situation because they will not receive any message by time T (G)
The resulting protocol, Consensus-Corrupt, shown in Figure 7.20, yields the following:
Unanimity can be reached regardless of the number of faults in time T = diam(G) transmitting at most 2 m(G) bits.
Composite Faults: Omissions and Corruptions If the system suffers from omissions and corruptions, the situation is fortunately no worse than that of systems with only omissions.
As there are no additions, no unintended message is generated.
Indeed, in the computation of the OR , the only intended messages are those originated by entities with initial value 1 and only those messages (possibly corrupted) will be transmitted.
Observe that, although expensive, it is no more so that what we have been able to achieve with just omissions.
As in the case of only omissions, the factor 2 can be removed by the bit costs without any increase in time.
Composite Faults: Omissions and Additions Consider now the case of systems with omissions and additions.
To counter the negative effect of additions, each entity transmits to all their neighbors in every clock cycle.
As there are no corruptions, the content of a message can be trusted.
Composite Faults: Additions and Corruptions Consider the environment when faults can be both additions and corruptions.
In this environment messages are not lost but none can be trusted; in fact the content could be incorrect (i.e., a corruption) or it could be a fake (i.e., an addition)
For this environment, indeed we need a more complex mechanism employing several techniques, as well as an additional restriction:
Additional restriction: The network G is known to the entities.
The first technique we use is that of time splicing:
We distinguish between even and odd clock ticks; an even clock tick and its successive odd click constitute a communication cycle.
When receiving a message at an even (respective odd) clock tick, entity y will forward it only on even (respective odd) clock ticks.
For each pair of neighboring entities x, y and paths SP(x, y), every entity determines in which of these paths it resides.
To communicate a message M to neighbor y, y will send along each of the cedge(G) paths in SP(x, y) a message, containing M and the information about.
Note that incorrect path information (owing to corruptions and/or additions) in a message for y received by z is detectable and so is incorrect timing as a result of the following:
Because of local orientation, z knows the neighbor w from which it receives the message;
However, every step of the broadcast, in which every involved entity sends the bit to its neighbors, is done using the Reliable Neighbor Transmission technique.
This means that each step of the broadcast now takes t communication cycles.
As there are no omissions, any transmitted message is possibly corrupted, but, it.
Therefore, if x transmits a bit, y will eventually receive one and be able to decide the correct bit value.
We need now to choose the appropriate value of t so that y will not mistakenly interpret the arrival of bits due to additions and can decide if it was really originated by x.
Lemma 7.8.6 In t communication cycles, at most F t copies of incorrect messages arrive at y.
Consider that broadcast requires diam(G) steps, each requiring t communication cycles, each composed of two clock ticks.
Byzantine Faults: Additions, Omissions, and Corruptions In case of Byzantine faults, anything can happen: omissions, additions, and corruptions.
Not surprisingly, the number of such faults that we are able to tolerate is quite small.
Still, using a simpler mechanism than that for additions and corruptions, we are able to achieve consensus, albeit tolerating fewer faults.
Indeed, to broadcast, we use precisely the technique Reliable Neighbor Transmission described in the previous section; we do not, however, use time slicing: This time, a communication cycle lasts only one clock cycle, that is, any received message is forwarded along the path immediately.
The decision process (i.e., how y, out of the possibly conflicting received messages, determines the correct content of the bit) is according to the simple rule:
For all systems, except those where faults are just corruptions or just additions (and in which unanimity is possible regardless of faults), the bounds we have established are similar except that the possibility ones are expressed in terms of the edge connectivity cedge(G) of the graph, while the impossibility ones are in terms of the degree deg(G) of the graph.
A summary of the possibility results is shown in Figure 7.22
This means that in the case of d-connected graphs, the impossibility bounds are indeed tight:
With the number of faults (or more) specified by the impossibility bound, even strong majority is impossible;
This large class of networks includes hypercubes, toruses, rings, complete graphs, and so forth.
In these networks, the obtained results draw a precise “impossibility map” for the agreement problem in presence of dynamic communication faults, thus, clarifying the difference between the dynamic and the static cases.
For those graphs where cedge(G) < deg(G), there is a gap between possibility and impossibility.
Closing this gap is clearly a goal of future research.
Most of the work on computing with failures has been performed assuming localized entity faults, that is, in the entity failure model.
The positive effect of partial reliability on consensus in an asynchronous complete network with crash failures was proven by Michael Fisher, Nancy Lynch, and Michael Paterson [22]
An election protocol that, under the same conditions, tolerates also link crashes has been designed by N.
There is clearly need to provide the entity failure model with a unique framework for proving results both in the asynchronous and in the synchronous case.
The case of ring networks was studied by Liuba Shrira and Oded Goldreich [46]
Election protocols in presence of Byzantine links were developed for complete networks by Hasan M.
The study of ubiquitous faults has been introduced by Nicola Santoro and Peter Widmayer who proposed the communication failure model.
Most of the research on ubiquitous faults has focused on reliable broadcast in the case of omission failures.
Exercise 7.10.1 Prove that for all connected networks G different from the complete graph, the node connectivity is not larger than the edge connectivity.
Exercise 7.10.3 Prove that if we know how to broadcast in spite of k link faults, then we know how to reach consensus in spite of those same faults.
Exercise 7.10.6 Modify Protocol TellAll-Crash so as to work without assuming that all entities start simultaneously.
Exercise 7.10.8 Modify Protocol TellAll-Crash so to work when the initial values are from a totally ordered set V of at the least two elements, and the decision must be on one of those values.
Exercise 7.10.9 Modify Protocol TellAll-Crash so as to work when the initial values are from a totally ordered set V of at the least two elements, and the decision must be on one of the values initially held by an entity.
Exercise 7.10.10 Modify Protocol TellZero-Crash so as to work when the initial values are from a totally ordered set V of at the least two elements, and the decision must be on one of those values.
Exercise 7.10.18 Modify protocol Rand-Omit so that each entity terminates its execution at most one round after first setting its output value.
Ensure that your modification leaves unchanged all the properties of the protocol.
Exercise 7.10.19 Prove that with protocol Rand-Omit, the probability that a success occurs within the first k rounds is.
Exercise 7.10.22 Prove that, in protocol Committees, the number r of rounds it takes a committees to simulate a single round of protocol Rand-Omit is dominated by the cost of flipping a coin in each committee, which is dominated in turn by the maximum number f of faulty entities within a nonfaulty committee.
Exercise 7.10.24 Prove that if all entities had access to a global source of random bits (unbiased and visible to all entities), then Byzantine Agreement can be achieved in constant expected time.
Prove that Reduce satisfies the following property: Let y be any entity; if no entity suspects y in Hv before time t , then no entity suspects y in outputr before time t.
Prove that Reduce satisfies the following property: Let y be any correct entity; if there is a time after which no correct entity suspects y in Hv, then there is a time after which no correct entity suspects y in outputr.
Exercise 7.10.28 Write the complete set of rules of protocol FT-CompleteElect.
Exercise 7.10.29 Prove that the closing of the ports in protocol FT-CompleteElect will never create a deadlock.
Exercise 7.10.31 Assume that, in protocol FT-CompleteElect, an entity x ceases to be candidate as a result of a message originated by candidate y.
Prove that, at any time after the time this message is processed by x, either the stage of y is greater than the stage of x or x and y are in the same stage but id(x) < id(y)
Exercise 7.10.32 Prove that in protocol FT-CompleteElect at least one entity always remains a candidate.
Exercise 7.10.36 Prove that in protocol FT-LinkElect all the nodes in SuppressorLink(x) are distinct.
Exercise 7.10.40 Let AddCorr denote the set of all events containing at most deg(G) addition and corruption faults.
Problem 7.10.2 Complete the description of protocol Committee and prove its correctness.
Problem 7.10.3 Consider a set of asynchronous entities connected in a complete graph.
Show how the existence of both digital signatures and a trusted dealer can be used to implement a global source of random bits unbiased and visible to all entities.
Problem 7.10.4 Consider a set of asynchronous entities connected in a complete graph.
Show how the existence of both private channels and a trusted dealer can be used to implement a global source of random bits unbiased and visible to all entities.
Problem 7.10.5 Consider a set of synchronous entities connected in a complete graph.
Show how the existence of both digital signatures and secrete sharing can be used to implement a global source of random bits unbiased and visible to all entities.
Consider the Election problem and assume that all identities are known to all (nonfaulty) entities.
Show how the election can be performed using O(kf ) messages, where k is the number of initiators.
Consider now a node x different from those nodes and make that node the initiator of the broadcast.
The failure of all the xi will disconnect G making some nonfaulty nodes unreachable from x; thus, they will never receive the information.
Answer to Exercise 7.10.9 Hint: Use Min instead of AND in rep(x, t) and choose the default value appropriately.
Answer to Exercise 7.10.43 In a hypercube H , between any two nodes x and y there are log n edge-disjoint paths, each of length at most log n.
According to the protocol, x sends a message to all neighbors, thus, along all these log n paths.
Answer to Exercise 7.10.19 As the coins are flipped independently, the probability of a success is.
That means that for any round r , the probability of an insuccess is.
Answer to Exercise 7.10.32 Assume, to the contrary, that all entities cease to be candidate and consider their final stages.
Let x be the entity in the largest stage (if more than one, let it be the one among them with the smallest id)
Let y be the entity that originated the message that caused x to cease to be a candidate.
Thus domains of equal sizes (even viewed at different times) are disjoint.
A simple bivalency proof that t-resilient consensus requires t+1 rounds.
Computing input multiplicity in anonymous synchronous networks with dynamic faults.
A lower bound for the time to assure interactive consistency.
Connectivity requirements for Byzantine agreement under restricted types of failures.
Tighter bounds on broadcasting in torus networks in presence of dynamic faults.
Fault-tolerant distributed algorithm in complete networks with link and processor failures.
Distributed agreement in the presence of processor and communication faults.
Majority and unanimity in synchronous networks with ubiquitous dynamic faults.
Optimal asynchronous agreement and leader election algorithm for complete networks with Byzantine faulty links.
The types of problems we are going to discuss in this chapter arise in very different contexts and situations, and sometimes they appear to have little (if any at all) in common.
These problems arise, for example, in the context of global termination: detecting whether a computation (e.g., the execution of a protocol) has globally terminated; garbage collection: deciding whether some distributed objects (e.g., data items) are no longer needed within the system; deadlock: deciding whether a circular wait has been created within the system preventing any further progress.
All these problems do, however, share a very important trait:
We need to decide whether a certain property holds (e.g., a data object is garbage, an entity is deadlocked, all entities have terminated their execution)
The property is stable: If no external event occurs in the system, the property will continue to hold.
In the following we will examine two of these problems in detail, designing efficient solutions for them.
We will then attack the task of designing a generic solution to the problem of detecting whether a stable property holds, regardless of the specific nature of the property.
A deadlock, also known as circular wait or deadly embrace, describes a situation where a set of entities, unable to generate anything while waiting, is blocked forever, each waiting for some events that only other entities of the set can generate.
Deadlock is a dreaded occurrence in computer systems, leading to paralysis of all the entity involved, degraded performance, and possibly collapse of the entire system’s activities.
It is a dangerous subtle system failure occurring without any component suffering any fault.
The most common places where deadlock may occur are within.
Indeed, whenever some entities must suspend their activities until some event occurs, there is potential for deadlock, unless avoidance mechanisms are in place.
The fact that during a computation some entities are blocked waiting for some event to occur is dangerous but does not necessarily lead to deadlock.
For example, in the generic election protocol MegaMerger, any entity sending an Outside? message to another city with lower level number was blocked, waiting for the level of that city to increase.
Our protocol was designed (using distinctness of the edge costs) in such a way that, as we proved, no deadlock would occur.
In other words, our protocol was designed with built-in deadlock avoidance.
Unfortunately, in many applications, deadlock avoidance mechanisms are not feasible because of the costs associated with them: increased overhead, slowdown of the system, decreased performance, and so forth.
In fact, in most situations, there is no built-in mechanism to ensure that entities do not become deadlocked; thus, deadlocks may and do occur.
It is, therefore, necessary to have mechanisms to detect if a deadlock has been formed, and if so, to resolve somehow the impasse.
While the resolution phase will clearly depend on the particular application and situation, the detection task is the same and we will focus on how to efficiently perform it.
The deadlock detection problem is the one of determining if there is a deadlock in the system.
The solution protocol is started by any entity suspecting that it might be involved in a deadlock; it must terminate within finite time.
If a deadlock is found, then all entities involved must know.
Our focus in this chapter will be on the first two types; the third will be discussed in the context of continuous computations.
We will also assume Message Ordering, that is, the links are first in first out (FIFO)
Let us first of all describe the deadlock condition problem more precisely.
If these two conditions hold, the entities in the set will be waiting forever, regardless of the nature of the permission and of why they are waiting for the “permission”; for example, it could be because si needs a resource held by sj in order to complete its computation.
In other words, not only the entities in a strongly connected component (e.g., a cycle) but also those that can reach such a component are deadlocked.
However, this is not enough for personal and component detection.
Notice that an entity can be involved in several deadlock cycles at once and can be linked to more than one strongly connected component.
A crown is a directed graph formed by a single directed cycle where each entity in the cycle is the root of a (possibly empty) rooted tree; the cycle is called the core of the crown.
In the wait-for graph shown in Figure 8.2, there are three components, two of which are rooted trees, and one is a crown.
Thus, to determine if it is involved in a deadlock, an initiator x0 must just determine whether it is in a crown or in a tree.
A simple trivial solution is to collect all the information about the wait-for graph (i.e., which entities an entity is waiting for) at a single location (e.g., the initiator x0) and check if there is a cycle or not.
Let us consider the following alternate strategy, where DFlood is flooding in directed graphs:
Let k denote the number of initiators in the same component.
In the worst case, each of the k initial messages will travel to the core and go around the ring (although only one will be able to complete the tour of the core) costing possibly O(n) transmissions, for a total of O(kn) messages.
A better strategy would be to integrate into the protocol a leader-election process in each component among the initiators in that component (Problem 8.6.2)
In the previous section we considered the case when each entity waits for (at most) one permission at a time.
What happens if we remove this restriction? In general, an entity can wait for permission from several other entities and cannot proceed until all of them have been granted; this situation is sometimes called the AND-request model.
The situation in the AND-request model is much more complex than the one of the single-request model because an entity can be involved in several deadlock cycles at once and can be linked to more than one strongly connected component.
The problem is the one of determining if an initiator entity x0 is involved in one or more deadlocks, and, if so (in the case of component detection), of notifying all other entities involved in the same deadlocks.
Simple Solution: GeneralSimpleCheck A simple but inefficient solution is a generalization of SimpleCheck; it consists of the initiator x0 flooding the system.
Every internal node y receiving this message records the sender and the information in the message, adds to the message information its id, forwards it to all its outneighbors, and waits for a reply from all of them.
This message will eventually reach every sink of D; when a sink receives such a message, because it is unblocked, it will reply “No deadlock I can see.”
If all received replies are “No deadlock I can see,” the internal node y will send such a reply to the sender of the “Check” message originated by x0
Otherwise, y records the sender, adds to the message information its id, forwards it to all its out-neighbors, and waits for a reply from all of them.
As before, if all received replies are “No deadlock I can see,” y will send such a reply to all the senders of the“Check” message originated by x0
This means that, within finite time, in each one of them at least one entity will receive the message containing its own id in the list of already visited.
The cost is, however, prohibitive: The number of exchange messages could be exponential (Exercise 8.6.2)
Efficient Solution: LockGrant Let us examine how to use some of the same ideas but with a different approach to determine efficiently if the initiator x0 is deadlocked.
This observation gives us a strategy to determine all those entities that are not deadlocked:
It is not difficult to see that if there are no deadlocks in the component all entities will find out within finite time.
In fact, in this case, an entity that is deadlocked will not receive a notification from all its outneighbors.
Owing to the fact that communication delays are unpredictable, an entity waiting for a notification does not know whether the notification is encountering a delay or no notification has been sent at all.
What we need is a mechanism to ensure that the initiator x0 detects termination and whether or not it is in a deadlock.
These tasks are easily accomplished by using Shout on the wait-for graph.
Recall that the links are really bidirectional: The orientation is only logical.
However, a node will wait to send its reply to its parent until it has received a reply from all its other (inand out-) neighbors.
When a sink y receives a “Shout” message for the first time, in addition to forwarding the “Shout,” y will announce the fact that it is not deadlocked by sending a “Grant” message to all its in-neighbors.
To deal efficiently with termination, the sink y will send a reply to its parent only after it has received not only a reply from all its other neighbors but also an acknowledgment for all its own “Grant” messages (note that it will always happen)
To know that it is deadlock free, a nonsink entity z must receive a “Grant” message from all its out-neighbors; until that happens (note that it might never happen), z will send an acknowledgment to any received “Grant” message.
To deal efficiently with termination, z will acknowledge the last “Grant” message it received not immediately but only after it receives an acknowledgment for all its own “Grant” messages (note that it will always happen)
In this way, the global termination of Shout will occur only after all the transmissions of “Grants” and “Grant-Acks” have taken place.
Furthermore, the global termination of all activities coincides with the local termination of Shout at the initiator.
Summarizing, upon local termination of Shout, the initiator x0 knows its status: It is not deadlocked if and only if it has received a “Grant” message from all its out-neighbors.
However, the other entities do not even know when their own local termination has occurred.
For example, consider a nonsink entity that receives the first “Shout,” forwards it to all its other neighbors, receives from each of them a reply, and then sends its own reply to its parent; in other words, all the Shout activities for x are terminated.
Yet, it is still possible for x to receive a “Grant” message (it might come from its own parent: Exercise 8.6.3)
Thus, it is necessary for the initiator to perform a “resolution” mechanism (e.g., it notifies all other entities of termination) even if the problem to be solved is only personal detection.
Let us now prove the correctness of the protocol; first of all, let us focus on termination.
To terminate, the initiator must terminate its execution of Shout, that is, it must receive a “Reply” from all its neighbors.
As described, if an entity sends “Grant” messages, it delays sending its “Reply” to its parent until it has received a “Grant-Ack” from all its in-neighbors.
Property 8.2.6 If an entity sends a “Grant” message to a neighbor, it will receive an “Ack” from that neighbor within finite time.
Property 8.2.8 If an entity sends a “Shout” message to a neighbor, it will receive a “Reply” message from that neighbor within finite time.
The following two properties indicate that when the initiator terminates, all other.
Property 8.2.10 An entity receives a “Grant” message from all its out-neighbors if and only if it is not deadlocked.
Theorem 8.2.3 Protocol LockGrant correctly solves the personal deadlock detection problem.
To deal with the case of collective detection, it is sufficient to specify procedure RESOLVE accordingly.
The cost of protocol LockGrant is not difficult to analyze.
There are two basic activities: the shouting and the granting.
In the case of multiple initiators, the strategy AllGrant of letting every initiator run its independent execution of LockGrant will indeed work.
The bookkeeping at each entity becomes more involved but is still manageable.
A better strategy would be to integrate into the protocol a leader-election process in each component among the initiators in that component (Problem 8.6.5)
We have been examining the deadlock detection problem in the static case, that is, assuming that no other edges are added to the wait-for graph while the detection protocol is running.
It is for this reason that “being deadlocked” is a stable property.
In this dynamic situation, a deadlock can be formed during (or after) the execution.
So there can be no guarantee that if an entity is not deadlocked at the time of detection, it will remain so; indeed, “not being deadlocked” is not a stable property.
This implies that when a detection protocol correctly reports that.
For these reasons, even the definition of the (personal and collective) deadlock detection problem must be refined.
We will say that a protocol P is a solution to the personal detection problem if it has the following properties: Let entity x start protocol P at time t.
Similarly, the definition for a solution to the collective deadlock determination problem must be modified (Exercise 8.6.9)
Let us now consider solving the detection problem in this general case.
We will first start with the single-request model and then consider the more general multiplerequests model.
Dynamic Single-Request Systems Let us examine what events can take place in single-request systems.
As an entity waiting for a permission is blocked, the only entities that can act are those that are not waiting: the roots of the rooted trees.
What a root can possibly do is to grant a permission to one of its children (if any) or to request a permission from some other entity.
If y is in a crown, the entire tree rooted in r at time t is now part of a crown (see Figure 8.7(a))
If y is a descendent of r (i.e., node in the rooted tree of r), then that tree becomes a crown (see Figure 8.7(b))
In addition to these two main cases, the request from r might cause a deadlock when happening concurrently with other requests from other roots creating an overall directed cycle (Figure 8.8)
In discussing it, we can always think of this situation as happening not simultaneously but sequentially; for example, having the request from r to y occurring after the other requests have been issued.
Summarizing, rooted trees can change in size, new crowns can be created, and existing crowns can become larger in size.
Therefore, the deadlock detection protocol in the dynamic case must be able to deal with the fact that while the detection messages.
Fortunately, with simple modifications, strategy SimpleCheck will still be able to operate correctly in these more complex situations.
Let us consider first the case of a single initiator.
We know that a deadlock will persist in time; thus, if an entity is involved in a deadlock at time t, it will continue to be so involved.
In other words, if an entity x0 starting protocol SimpleCheck is at that time in a crown, it will continue to be in a crown during all the executions of the protocol.
During this time, the crown can grow; however, a crown grows only by making the fringes larger or by adding new fringes.
In other words, its core will remain unchanged (Exercise 8.6.10)
In other words, the protocol will correctly report that there is a deadlock.
With multiple initiators, protocol SimpleCheck+might have some problems owing to the fact that only the request with the smallest id is forwarded.
As id(x) < id(y), x will add y to its checker list but will not forward the message.
Thus y will be incorrectly notified that it was not involved in a deadlock when it started the detection process.
Observe that, in the case of multiple initiators, the strategy AllCheck of letting every initiator run its independent execution of SimpleCheck will indeed work.
Although using many messages, the overall cost would not be more thanO(n) messages per initiator.
The bookkeeping at each entity becomes more involved but is still manageable.
A more refined approach is provided by the strategy DelayAllCheck in which an initiator always forwards messages from other initiators with smaller ids, but it delays the decision on whether to forward a “Check” message with higher id until an answer to the previous one has arrived.
Indeed, if a “Deadlock” message arrives, then there is no longer any need to forward that delayed “Check” message.
By contrast, if a “No Deadlock” notification arrives, as the above example shows, it might not apply to the held “Check” message; in this case, it will then be forwarded.
In order of magnitude, the worst-case cost is the same as the previous solution of always forwarding the “Check” messages: O(n) messages per initiator.
In the case of multiple initiators, the strategy of letting every initiator run its independent execution of DynamicLockGrant will indeed work.
The multiple-requests system, of which the single request is a special case, is the most common request model in distributed applications, but not the only one, nor the more general one.
In this section we will briefly describe the other systems and discuss deadlock detections in those systems.
This means that loops in the wait-for graph are not meaningful with respect to deadlock.
For example, in the wait-for graph shown in Figure 8.9, entities b, c, and d are in a directed cycle but are not deadlocked because they can all reach a, which is a sink.
However, this is not enough for personal and component detection.
In fact, being in a knot is a sufficient but not necessary condition to be deadlocked; for example, in Figure 8.9, h, which is deadlocked, is not a part of a knot.
By contrast, reaching a knot is a necessary but not sufficient condition for being deadlocked; for example, in Figure 8.9, d can reach a knot but it is not deadlocked.
The personal and collective detection problems are somehow simpler in this model than in the AND-request model examined previously.
Interestingly, protocol LockGrant, with very few modifications, solves the detection problems also in these systems (Exercise 8.6.14)
Similarly, protocol DynamicLockGrant solves them in the case of a dynamic wait-for graph.
Notice that different entities may be waiting on different predicates.
At a first glance, the solution to the detection problem would appear more difficult in these systems; this is, however, not the case.
Indeed protocols LockGrant and DynamicLockGrant, with very few modifications, provide efficient solutions to the.
Among the unique aspects of distributed computations is the difficulty of determining their termination.
As we have seen throughout the book, we have to distinguish between local termination (i.e., termination of the execution of the protocol by a specified entity) and global termination (i.e., when the execution has ended in the entire system)
The importance of determining termination (local or global) derives from the fact that in real systems, entities execute not just a single protocol but several, and in some cases, there are precedence dependencies among them (i.e., the start of one cannot take place before the end of another)
Ck of computations that must be performed by the system in order.
To achieve this task, usually only local termination is really necessary: As soon as an entity determines the end of its participation in computation Ci , it can start Ci+1
There are, however, situations where global termination of Ci must be detected, at least by one entity, before the next computation may start.
This for example happens when designers of multistages protocols do not want to deal with possible problems due to concurrency (e.g., because in this way the correctness is easier to prove)
Usually it is simple to design protocols so that each entity can determine when its participation in the computation is over.
Indeed, almost all the protocols we have designed so far have a built-in local termination detection (e.g., the entity enters a terminal state)
Global termination, on the contrary, is in general much more difficult to detect, even if there is in place a local termination detection; requiring global termination detection to be part of the protocol obviously increases the overall complexity of the design.
An approach has thus been to detach the task of global termination detection from the rest of the protocol and to handle it concurrently but separately.
Indeed the problem itself can be viewed independently of the rest of the computation.
With this high-level view, the computation C will have the following simple properties:
The computation C is globally terminated if and only if all entities are passive and there are no messages in transit.
The problem is to design a protocol that determines whether the computation C is globally terminated.
First of all observe that once personal detection has been solved, collective detection is trivially achieved with an additional notification process; thus, we will focus on personal detection.
We will consider the problem under the standard assumptions: connectivity, Bidirectional Links, complete reliability, and unique identifiers.
We will also assume Message Ordering, that is, the links are FIFO.
The messages sent by the detection protocol are clearly distinguishable from those of the computation C, and they are dealt with by each entity in a separate but overlapping way, that is, each entity will be handling simultaneously both the events for C and those for the termination detection; in the following, we will call C-messages the messages sent by the computation C.
We will also assume that there is already available a subnetwork, specifically a spanning tree T, which will be used by the protocol.
Termination Query To develop our solution to the global termination detection problem, we will first consider a related simpler task, Termination Query, in which an initiator must know within finite time whether or not C is globally terminated at a given time.
As we are not making any assumptions about time, let us be more specific.
A termination query protocol Q is an algorithm that has the following three properties:
If C was terminated when the protocol started, then answer=TRUE.
If answer=FALSE then C was not terminated when the protocol started.
Answering a personal query is not difficult and can be achieved in many ways.
The solution requires a control mechanism CountAck, which is run simultaneously with C during all the executions; in this mechanism, the reception of every C-message is acknowledged:
On the basis of the existence of this control mechanism, the solution protocol TerminationQuery, which can be started at any time, is as follows: Protocol TerminationQuery:
The initiator x0 broadcasts a “Color” message (on the spanning tree)
If a white entity receives a C-message, it becomes black.
Note that status, count, white, and black are not defined in C but only in TerminationQuery.
It is easy to verify (Exercise 8.6.20) that algorithm TerminationQuery is a correct personal query protocol, that is, regardless of time delays, an execution of TerminationQuery has the following property:
To these costs we must, however, add the cost of the control mechanism, which uses one message (an acknowledgment) for each C-message.
If we treat the initiators collectively, then it is sufficient to use full saturation.
Specifically, the Color messages are sent in the wake-up phase, and the AND is computed in the saturation phase; the notification phase will tell everybody the outcome.
In this case, the saturated nodes become first aware of the result.
Repeated Queries We have discussed, solved, and analyzed the simpler task of computing a personal query.
The interest in this simpler problem is due to the fact that a solution to personal detection can be made by repeatedly executing a personal query protocol Q until global termination is detected.
The use of strategy RepeatQuery when Q is protocol TerminationQuery would lead to a global termination detection protocol whose total cost will be.
Notice that for any solution protocol Q, its cost in terms of messages will always be of the formA+ B, where A is the cost incurred to maintain the information needed by Q (e.g., the control mechanism used by TerminationQuery) and B is the cost of the actual execution of Q.
This means that the cost of strategy RepeatQuery using Q is.
This means that regardless of how efficient is the employed solution Q to the personal query problem, the strategy RepeatQuery is a cost-wise unacceptable solution to the personal detection problem.
To make it bounded we need to exercise some additional control, restarting the query only if some necessary condition holds; namely, we will restart only if all entities have become or remained passive since last iteration.
Notice that this condition alone is not sufficient as there might have been some messages in transit whose arrival has in the meanwhile transformed into active again some of these passive entities.
Let us consider again the single-initiator case and consider the first execution of Q; if successful, we are done.
If unsuccessful, x0 will wait for the next execution until it is first notified that all entities have become or remained passive since last query.
In general, if an iteration is unsuccessful, the initiator will wait until it is first notified that all entities have become passive since that iteration and will only then start a new one.
In the particular case when Q =TerminationQuery, we will have a total cost.
Let us now see how we can substantially reduce the cost of detecting global termination.
As usual we will consider first the case of a computation when there is a single initiator.
We will dynamically construct and maintain a tree rooted in the initiator; the tree will grow and shrink during the computation until only the initiator is left: When this happens and the initiator is passive, termination of C has occurred.
The initiator x0 will start the termination detection mechanism as soon as C starts.
As before, we will have an acknowledgment for every C-message sent, and we will say that an entity is white if it is passive and has received an acknowledgment for all the C-messages it sent; otherwise, we will say that it is black.
Whenever a black entity receives a C-message, it will immediately send an acknowledgment to the sender.
To see why the corresponding protocol Shrink indeed works, first of all observe that the black nodes always form a tree rooted in the initiator, as indicated by the following properties (Exercise 8.6.23):
Property 8.3.5 If all nodes are white at time t, C is globally terminated at that time.
Finally, observe that if C terminates, x will indeed detect it within finite time (Exercise 8.6.25):
What is the cost of protocol Shrink ? Each C-message generates the transmission of an acknowledgment; no other messages are transmitted during the execution.
In other words, Shrink is not only simpler but also much more efficient than all solutions we have discussed so far for detecting global termination of single-initiator computations.
What happens if C has multiple initiators? With multiple initiators, protocol Shrink will create not one dynamic tree but rather a forest of dynamic trees, each one of them rooted in one of the initiators.
Property 8.3.7 At any time t, if an entity is black, so is at least one of the initiators.
Property 8.3.8 At any time t, the black nodes form a forest of trees, each rooted in one of the initiators, and the white nodes are singletons.
Property 8.3.9 At any time t, if all initiators are white, so is every entity.
Property 8.3.10 If a saturated node is white at time t, then all nodes are white at that time.
In the delayed execution of the saturation technique, it is possible that only one entity becomes saturated (Exercise 8.6.30)
Let us call MultiShrink the corresponding protocol (see Problem 8.6.12), and let us analyze its costs.
Lower Bounds In the previous sections we have seen several protocols for global termination detection.
For all of them, the number of messages was a function not only of the number n of entities in the network but also of the number M(C) of messages of the computation C whose termination we want to detect.
While n is a system parameter, M(C) is not, and it could be arbitrarily large.
Thus, it is important to know whether this factor is at all necessary or it is possible to substantially reduce it.
In other words, it is important to establish a lower bound on the number of messages that any solution protocol must transmit to detect the global termination of a computation C.
In this regard, we have only a partial result; in fact we know only that there are computations such that to detect their global termination, any protocol needs to send at least as many messages as the computations (Problem 8.6.13):
As a consequence, protocol Shrink is worst-case optimal even constant-wise.
In the case of multiple initiators, according to Theorem 8.3.1, protocol MultiShrink is optimal in the order of magnitude as long as n = O(M(C)) or smaller.
In reality, protocol MultiShrink is always optimal in the order of magnitude.
This is because of the fact that having multiple initiators imposes a cost on solution protocols, as expressed by the following different lower bounds (Exercise 8.6.31):
Garbage Collection There is an interesting correlation between the problem of global termination detection and the one arising in the context of garbage collection.
Indeed, any garbage collection algorithm can be transformed into a global termination detection protocol (Problem 8.6.14)
In the previous sections, we have examined the problems of detecting two important properties of a computation C: deadlock and global termination.
Both properties have in common the fact that they are stable: Once they hold, they will continue to hold (unless some other computation is started)
The solutions we have developed were quite specific to the particular nature of those two properties.
What we will discuss in this section is how to detect any stable property P , regardless of its specific properties.
The goal is to develop a protocol that, for any stable property P and for any computation C, detects when P(C) holds.
Let us be more precise in terms of the requirements of the protocol, first of all the initiators: In termination detection they coincide with the initiators of C, while in deadlock detection it could be any entity.
In this section, we will impose no restrictions: Any number of entities can independently initiate and they might not coincide with the initiators of C.
As before, we will distinguish between personal and collective detection:
Observe that once personal detection has been solved, collective detection is trivially achieved with an additional notification process; thus, of the two, we will focus on personal detection.
Further observe that for the personal (and thus collective) detection problem to be solvable, property P must eventually hold in C, otherwise no detection protocol will ever terminate.
We will consider the problem under the standard assumptions: connectivity, Bidirectional Links, complete reliability, and unique identifiers.
We will also assume Message Ordering, that is, the links are FIFO.
As we did in the case of termination detection, our general solution strategy is based on the solution to a simpler problem:
Clearly a solution to personal and collective detection can be made by repeatedly executing a solution Q to the personal query problem.
The cost of this strategy will be the cost of Q times the number of times Q is invoked; as we already observed in the case of termination detection, without any control, this cost is unbounded.
Let us not worry for the moment about the number of invocations, and let us focus instead on the design of a solution protocol Q to the personal query problem.
What we need to do is to develop such a solution independently of the nature of property P (other than that it is stable)
Let us denote the status of the computation C at time t byC[t], that is,C[t] denotes the status of every entity as well as the status of every link at time t with respect to C.
We will call C[t] a perfect snapshot of C at time t.
To be able to do that, each entity must keep track of and remember all its internal states with respect to the computation C.
Assume that this is the case (an expensive assumption storage-wise)
Unfortunately, this approach not only is expensive but also does not work.
The problem is that even if an entity x knows its state at any time and remembers them all, it does not know what t0 is.
Recall, there are no global clocks, local clocks may sign different times and have different rates, and communication delays are unpredictable.
As a consequence, it is impossible to construct a perfect snapshot.
Fortunately, we do not really need to take a perfect snapshot; an imperfect one will do, as long as it is “sharp” enough to provide consistent information.
When we say that C[t] denotes the status of C at “time” t, we are referring to “time” as seen by an observer external to the system, sometimes called “real time.” Within the system, the actual value t of “real time” is not known: Each entity x has only.
Furthermore, unless the additional restrictions of full synchronicity hold, the local clocks might have different speeds, the distance between consecutive ticks of the same clock might change over time, there are no time bounds on communication delays, and so forth.
In other words, within the system, there is no common notion of time.
Fortunately, practically in all cases, although useful, a common notion of time is not needed.
To understand what is sufficient for our purposes, observe that “real time” gives a total order to all the events and the actions that occur in the system: We can say whether two events occur at the same time, whether an action is performed before an event takes place, and so forth.
In other words, given any two actions or events that occurred in the system, we (external observers) can say (using real time) whether one occurred before, at the same time as, or after the other.
The entities in the system, with just access to their local clocks, have much less knowledge about the temporal relationships of actions and events; however, they do have some.
It turns out that this knowledge is indeed sufficient for obtaining a consistent snapshot.
To see how, let us first of all generalize the notion of snapshot and introduce that of a cut.
Notice that if all ti are the same, the corresponding snapshot is perfect.
In other words, the snapshot generated by a cut is consistent if, in the cut, a message is not received before sending that message.
Summarizing, our strategy to resolve the personal query problem is to collect at the initiator x a consistent snapshot C[T ] by having each entity xj send its internal state C(xj )[tj ] to x.
We must now show that consistent snapshots are sufficient for answering a personal query.
Thus, our problem is now how to compute a consistent snapshot, which we will examine next.
Our task is to design a protocol to compute a consistent snapshot.
To achieve this task, each entity xi must select a time ti , and these local choices must be such that the snapshot generated by the resulting cut is consistent.
The difficulty is that as communication delays are unpredictable, xi does not know when its message arrives.
Fortunately, there is a very simple way to achieve our goal.
Notice that as we have assumed FIFO links, when an entity y receives a message from a neighbor x, y knows that all messages sent by x to y before transmitting this one have already arrived.
Notice that the only difference between WFlood+ and WFlood is that now a noninitiator sends a wake-up message also to the entity that woke it up.
Let t i be the time when xi becomes “awake” (i.e., it initiates WFlood+ or receives the first “wake-up” message)
Thus the problem of constructing a consistent snapshot is solved by simply executing a wake-up using WFlood+
The cost is easy to determine: In the execution of WFlood+ regardless of the number of initiators, exactly two messages are sent on each link, one in each direction.
We have just seen how to determine a consistent snapshot C[T ] (Protocol WFlood+) with multiple initiators.
Once this is done, the entities still have to determine whether or not property P holds for C[T ]
Depending on the size of the local fragments of the snapshot, the amount of information transmitted can be prohibitive.
An alternative to this centralized solution is to computeP(C) at T distributively.
This, however, requires knowledge of the nature of property P , something that we neither have nor want to require; recall: Our original goal is to design a protocol to detect a stable property P regardless of its nature.
At this point, we have a (centralized or decentralized) protocol Q for solving the personal query problem.
We can then follow strategy RepeatQuery and repeatedly execute Q until the stable property P(C) is detected to hold.
As already mentioned, the overall cost is the cost of Q times the number of times Q is invoked; as we already observed in the case of termination detection, without any control, this cost is unbounded.
Summarizing, we have seen how to solve the global detection problem for stable properties by repeatedly taking consistent snapshots of the system; such a snapshot is sometimes called a global state of the system.
This solution is independent of the stable property and thus can be applied to any.
We have also seen that the cost of the solution we have designed can be prohibitive and, without some other control, it is possibly unbounded.
The problem of distributed deadlock detection has been extensively studied and a very large number of solutions have been designed, proposed, and analyzed.
However, not all these attempts have been successful, some failing to work correctly, either detecting false deadlocks or failing to detect existing deadlocks, others exhibiting very poor performance.
As deadlock can occur in almost any application area, solutions have been developed from researchers in all these areas (from distributed databases to systems of finite state machines, from distributed operating systems to distributed transactions to distributed simulation), many times unaware of (and sometimes reproducing) each other’s efforts and results.
In addition, because of its link with cycle detection and with knot detection, some aspects of deadlock detection have also been studied by investigators in distributed graph algorithms.
Interestingly, one of the earliest algorithms, LockGrant, is not only the most efficient (in the order of magnitude) protocol for personal detection with a single initiator in a static graph but also the most general as it can be used (efficiently) in all types of request systems.
Protocol TerminationQuery for the personal termination query problem was designed by Rodney Topor [21] and used in strategy RepeatQuery for the personal termination detection problem.
The fact that Protocol WFlood+ constructs a consistent snapshot was first observed by Mani Chandy and Leslie Lamport [3]
Exercise 8.6.1 Prove that protocol GeneralSimpleCheck would solve the personal and component deadlock detection problem.
Exercise 8.6.2 Show the existence of wait-for graphs of n nodes in which protocol GeneralSimpleCheck would require a number of messages exponential in n.
Exercise 8.6.3 Show a situation where, when executing protocol LockGrant, an entity receives a “Grant” message after it has terminated its execution of Shout.
Exercise 8.6.4 Prove that in protocol LockGrant, if an entity sends a “Grant” message to a neighbor, it will receive a “Grant-Ack” from that neighbor within finite time.
Exercise 8.6.5 Prove that in protocol LockGrant, if an entity sends a “Shout” message to a neighbor, it will receive a “Reply” from that neighbor within finite time.
Exercise 8.6.7 Prove that in protocol LockGrant, if an entity receives a “Grant” message from all its out-neighbors then it is not deadlocked.
Exercise 8.6.8 Prove that in protocol LockGrant, if an entity is not deadlocked, it will receive a “Grant” message from all its out-neighbors within finite time.
Exercise 8.6.9 Modify the definition of a solution protocol for the collective deadlock detection problem in the dynamic case.
Exercise 8.6.10 Prove that in the dynamic single-request model, once formed the core of a crown will remain unchanged.
Exercise 8.6.14 Modify protocol LockGrant so that it solves the personal and the collective deadlock detection problem in the OR-Request model.
Prove the correctness and analyze the cost of the resulting protocol.
Exercise 8.6.16 Modify protocol LockGrant so that it solves the personal and the collective deadlock detection problem in the p-OF-q Request model.
Prove the correctness and analyze the cost of the resulting protocol.
Exercise 8.6.18 Modify protocol LockGrant so that it solves the personal and the collective deadlock detection problem in the Generalized Request model.
Prove the correctness and analyze the cost of the resulting protocol.
Exercise 8.6.23 Consider strategy Shrink for personal termination detection with a single initiator.
Show that at any time, all black nodes form a tree rooted in the initiator and all white nodes are singletons.
Exercise 8.6.24 Consider strategy Shrink for personal termination detection with a single initiator.
Prove that if all nodes are white at time t, then C is terminated at that time.
Exercise 8.6.26 Consider strategy Shrink for personal termination detection with multiple initiators.
Show that at any time, the black nodes form a forest of trees, each rooted in one of the initiators, and the white nodes are singletons.
Exercise 8.6.27 Consider strategy Shrink for personal termination detection with multiple initiators.
Prove that, if all nodes are white at time t, then C is terminated at that time.
Exercise 8.6.29 Consider protocol MultiShrink for personal termination detection with multiple initiators.
Prove that when a saturated node becomes white all other nodes are also white.
Exercise 8.6.30 Consider protocol MultiShrink for personal termination detection with multiple initiators.
Explain why it is possible that only one entity becomes saturated.
Problem 8.6.1 Write the set of rules of protocol Dead Check implementing the simple check strategy for personal and for collective deadlock detection in the single resource model.
Problem 8.6.2 (+) For the problem of personal deadlock detection with multiple initiators consider the strategy to integrate into the solution an election process among the initiators.
Design a protocol for the single-request model to implement efficiently this strategy; its total cost should be o(kn) messages in the worst case, where k is the number of initiators and n is the number of entities.
Problem 8.6.3 Implement protocol LockGrant, both for personal and for collective deadlock detections.
Problem 8.6.5 (++) For the problem of personal deadlock detection with multiple initiators consider the strategy to integrate into the solution an election process among the initiators.
Design a protocol for the AND-request model to implement efficiently this strategy; its total cost should be o(km) messages in the worst case, where k is the number of initiators and m is the number of links in the wait-for graph.
Prove the correctness and analyze the cost of your design.
Problem 8.6.6 (++) Modify protocol LockGrant so that, with a single initiator, it works correctly also in a dynamic wait-for graph.
Prove the correctness and analyze the cost of the modified protocol.
Problem 8.6.7 (++) For the problem of personal deadlock detection with multiple initiators consider the strategy to integrate into the solution an election process among the initiators.
Design a protocol for the OR-request model to implement efficiently this strategy; its total cost should be o(km) messages in the worst case, where k is the number of initiators and m is the number of links in the wait-for graph.
Prove the correctness and analyze the cost of your design.
Problem 8.6.8 (++) For the problem of personal deadlock detection with multiple initiators consider the strategy to integrate into the solution an election process among the initiators.
Design a protocol for the p-OF-q request model to implement efficiently this strategy; its total cost should be o(km) messages in the worst case, where k is the number of initiators and m is the number of links in the wait-for graph.
Prove the correctness and analyze the cost of your design.
Problem 8.6.9 (++) For the problem of personal deadlock detection with multiple initiators consider the strategy to integrate into the solution an election process among the initiators.
Design a protocol for the Generalized request model to implement efficiently this strategy; its total cost should be o(km) messages in the worst case, where k is the number of initiators and m is the number of links in the wait-for graph.
Prove the correctness and analyze the cost of your design.
Problem 8.6.10 (+) Write the set of rules corresponding to strategy RepeatQuery+ when Q is TerminationQuery and there are multiple initiators.
Problem 8.6.11 (+) Write the set of rules of protocol Shrink for global termination detection with a single initiator.
Problem 8.6.12 (+) Write the set of rules of protocol MultiShrink for global termination detection with multiple initiators.
Problem 8.6.14 (++) Show how to transform automatically a garbage collection algorithm GC into a termination detection protocol TD.
Problem 8.6.16 Consider a computation C that circulates k tokens among the entities in a system where tokens (but not messages) can be lost while in transit.
The problem we need to solve is the detection of whether one or more tokens are lost.
Adapt the general protocol we designed for detecting stable properties (i.e., strategy RepeatQuery using WFlood+ for personal query resolution) to solve this problem.
Use the specific nature of C to reduce the space and bit costs of each iteration, as well as the overall number of messages.
Assume that the “Grant” message from a to b is very slow.
In the meanwhile, b receives the “Shout” from a and forwards it to c and d, which will send a “Reply” to b; upon receiving these replies, b will send its “Reply” to its parent a, effectively terminating its execution of Shout.
The “Grant” message from a will then arrive after all this has occurred.
Answer to Exercise 8.6.5 Hint: First prove that in protocol LockGrant, within finite time, every entity but the initiator will send a “Reply” to its parent in the tree constructed by the Shout.
A distributed graph algorithm for the detection of local cycles and knots.
Efficient algorithms for distributed snapshots and global virtual time approximation.
The derivation of distributed termination detection algorithms from garbage collection schemes.
The derivation of graph marking algorithms from distributed termination detection protocols.
When we have been discussing computations in distributed environments, we have always considered computations that once started (by some impulse), terminate within finite time.
The termination conditions can be explicit in the protocol (e.g., the entities enter terminal states) or implicit (and hence a termination detection protocol must be run concurrently)
The key point is that, implicit or explicit, the termination occurs.
These are, for example, computations needed for the control and maintenance of the environment, and they are “on” as long as the system is “on”: The protocols composing a distributed operating system, the transaction management protocols in a distributed transaction system, the network service protocols in a data communication network, the object management functions in a distributed object system, and so forth.
Because of this nature, these computations are called continuous computations.
We have already seen one such computation in Chapter 4, when dealing with.
Heartbeat protocols form the backbone of the management of most distributed systems and networks.
It is, for example, used in most failure detection mechanisms: An entity decides that a failure has occurred if the wait for a heartbeat from a neighbor exceeds a timeout value.
In this chapter we will examine some basic problems whose solution requires continuous computations: maintaining logical clocks, controlling access to a shared resource or service, maintaining a distributed queue, and detecting and resolving deadlocks.
What this answer really points out is that we should not (because we cannot) measure the total cost of the entire execution of a continuous computation.
Which measure is most appropriate depends on the nature of the problem.
Consider the heartbeat protocol, whose total cost is infinite; The meaningful cost measure in this case is the total number of messages it uses per single beat: 2 m.
In the case of the routing table maintenance protocols, a meaningful measure is the total number of messages exchanged in the system per change in the topology.
Summarizing, we will measure a continuous computation in terms of either its cost per basic operation it implements or its cost per basic event triggering its action.
In a distributed computing environment, without additional restrictions, there is definitely no common notion of real (i.e., physical) time among the entities.
Each entity has a local clock; however, each is independent of the others.
In general this fact does not restrict our ability to solve problems or perform tasks; indeed, all the protocols we have designed, with the exception of those for fully synchronous systems, do not require any common notion of real time among the entities.
Still, there are cases when such a notion would be helpful.
Consider, for example, the situation when we need to undo some operation a (e.g., the transmission of a message) that has been erroneously performed.
In this case, we need to undo also everything (e.g., transmission of other messages) that was caused by a.
In this context, it is necessary to determine whether a certain event or action b (e.g., the transmission of some other message by some other entity) was caused (directly or indirectly) by that original action a.
If we find out that a happened after b, that is t(a) > t(b), we can exclude that b was caused by a, and we need not undo it.
So, although it would not completely solve the problem, having access to real time would be useful.
As we know, entities do not have access to real time t.
They can, however, create, using local clocks and counters, a common notion of time T among them, that would allow them to approximate real time or at least exploit some useful properties of real time.
When we talk about a common notion of time we mean a function T that assigns a value (not necessarily unique) from a partially ordered set to each event in the system; we will denote by < the partial order.
Local Events Ordering: Let a and b two events occuring both at x, with t(a) < t(b)
Send/Receive Ordering: Let a be the event at x whose reaction is the transmission of a message to neighbor y, and let b be the arrival at y of that message.
Any function T satisfying these two properties will be called virtual time.
The other desirable property is the one allowing us to simulate real time in the.
We can now formally define the property we are looking for:
Interestingly, the simultaneous presence of properties Local Events and Send/Receive ordering are enough to guarantee Causal Order (Exercise 9.6.1):
The problem is how can the entities create a virtual time T.
If we could construct virtual clocks that satisfy the Complete Causal Order property, then to identify what to undo would be easy: To completely undo a we must undo every b with T (b) > T (a)
Notice that real time is not complete with respect to causal order; in fact, t(a) < t(b) does not imply at all that a caused b! In other words, Complete Causal Order is not provided by real clocks.
This suggests that creating virtual clocks with this property is not a trivial task.
Also notice that each local clock cx , by definition, satisfies the Complete Causal Order property for the locally occurring events.
This means that as long as an entity does not interact with other entities, its local clock generates a completely consistent virtual time.
The problems clearly arise when entities interact with each another.
In the following we will design an algorithm to construct and maintain virtual clocks; we will also develop a system of virtual clocks that satisfy Complete Causal Order.
In other words, algorithm CounterClock constructs and maintains virtual clocks:
It does, however, require an additional field (the value of the local counter) in each message; the bookkeeping is minimal: limited to storing the counter and increasing its value at each event.
Notice that although the time function C created by algorithm CounterClock satisfies the causal order property like real time t , it may differ greatly from real time.
It is also possible that two independent events, occurring at diffe rent entities at different times, have the same virtual time.
It is natural to ask whether we can design virtual clocks that satisfy the much more powerful Complete Causal Order property.
Let us point out again that real time clocks do not satisfy this property.
Surprisingly, it is possible to achieve this property using solely local counters; however, we need many of them together; let us see how.
We define Vi(a) as follows: If a is the reception of a message, then Vi(a) is the value of the vector Vi after its updating when processing.
For all other events (impulses and alarm clock ringing), Vi(a) is just the value of vector Vi when event a is processed (recall that the local counter is increased as the first operation of the processing)
This system of local vectors defines a global time function V : For any event a at xi , V (a) is just Vi(a)
Notice that the values assigned to events by the time function V are vectors.
It is not difficult to see that the global time V with the partial order so defined is a virtual time, that is, it satisfies the Causal Order property.
Next observe that these local vectors satisfy also send/receive ordering (Exercise 9.6.4):
Property 9.2.3 Let a be an event in whose reaction a message is transmitted by xi , and let b be the reception of that message by xj.
Interestingly, as already mentioned, the converse is also true (Exercise 9.6.5):
For example, consider the vector clock when an entity xi reacts to an event a; the value of each component of the vector clock Vi(a) can give precise information about how many preceeding events are causally related to a.
Property 9.2.4 Let a be an event occurring at xi.
This property is useful, for example, when we do want to discard obsolete messages: If two messages are independent, both should probably be kept; by contrast, if they are causally related, only the most recent (i.e., with the greater vector) needs to be kept.
This algorithm requires that an n-dimensional vector of counters is included in each message.
By contrast, it ensures a much stronger property that not even real clocks can offer.
Indeed, the dimension n is necessary to ensure Complete Causal Order using timestamps (Problem 9.6.1)
A way to decrease the amount of additional information transmitted with each message is to include in each message not the entire vector but only the entries that have changed since last message to the same neighbor.
For large systems with frequent communication, this approach can significantly reduce the total amount of transmitted data with respect to always sending the vector.
The drawback is the increased storage and bookkeeping: Each entity xi must remember, for each neighbor xj and for each entry k in the vector, the last value of Vi[k] that xi sent to xj.
Hacking In presenting algorithm VectorClocks we have assumed that there is an a priori total ordering of the entities, and that each entity knows both its rank in the ordering and the total number n of entities.
This can be clearly obtained, for example, by performing a ranking protocol on the entities’ ids.
The cost for this operation is expensive, O(n2) messages in the worst case, even if there is already a leader and a spanning tree.
However, this cost would be incurred only once, before the creation of the clocks takes place.
Interestingly, with simple modifications to algorithm VectorClocks, it is possible to achieve the goal (i.e., to construct a virtual clock satisfying the Complete Causal Order property) without any a priori knowledge and yet without incurring in any initial cost; even more interesting is the fact that, in some cases, maintaining the clocks requires much less information inside the messages.
Bounding the Clocks The major problem with both CounterClocks and with VectorClocks is that the values of the counters are monotonically increasing: They keep on growing.
This means that these values and, hence, the bit complexity of the messages are unbounded.
A possible solution is to occasionally reset the vectors; the difficulty with this approach is clearly caused by messages in transit: The resetting of the virtual clocks will destroy any existing causal order between the arrival of these messages and the events that caused their transmission.
Any strategy to avoid this unfortunate consequence (Problem 9.6.3) is bound to be both expensive and intrusive.
In a distributed computing environment, there are many cases and situations in which it is necessary to give a single entity (or a single group of entities) exclusive control.
Another situation when exclusive control is necessary is when accessing a critical resource of a system.
This is, for example, the case when only a single resource of some type (e.g., a printer, a bus) exists in the system and that resource cannot be used concurrently.
In this case, any entity requiring the use of that resource must ensure that when it does so, it is the only one doing so.
What is important is not the nature of the resource but the fact that it must be held in mutual exclusion: only one at the time.
This means that when more than one entity may want to access the critical resource, only one should be allowed.
Any mechanism must also clearly ensure that any request is eventually granted, that is, no entity will wait forever.
The approach of using election, to select the entity to which access is granted, is unfortunately not a.
This is not (only) because of the cost but because of its unfairness: It does not guarantee that every entity wanting to access a resource will be allowed to do so (i.e., will become leader) within finite time.
This gives rise to a very interesting continuous problem, that of distributed mutual exclusion.
We will describe it more precisely using the metaphor of critical operations in a continuous computation C.
A distributed mutual exclusion mechanism is any protocol that ensures the following two properties:
Mutual exclusion: If an entity is performing a critical operation, no other entity is doing so.
Fairness: If an entity wants to perform a critical operation, it will do so within finite time.
In the rest of this section we will see how to design efficient protocols with those properties.
In the process, we will see that there is an interesting connection between the problem of distributed mutual exclusion and that of managing a distributed queue (another continuous computation)
In particular, we will see how any protocol for fair management of a distributed queue can be used to solve the problem of distributed mutual exclusion.
The problem of distributed mutual exclusion has a very simple and efficient centralized solution:
Initially, an entity is elected as leader; this entity will then coordinate the granting of permissions as follows:
The last point is achieved, for example, by having the leader keep the pending requests in a first in first out (FIFO) ordered list.
This very simple centralized protocol is not only correct but also quite efficient.
In fact, for each critical operation, there is a request from the entity to the leader, a permission (eventually) from the leader to that entity, and the notification of termination from the entity back to the leader.
Thus, there will be 3d(x, r) messages for each operation x wants to perform, where r is the leader; so, the operating cost of Central will be no more than.
This means that in a complete graph the cost will be only three messages per critical operation.
The drawbacks of this solution are those of all centralized solutions: The woarkload is not balanced; the leader might have to keep a large amount of information; the leader is a fault-tolerance bottleneck.
As we are assuming total reliability, we will not worry for the moment about the issue of fault tolerance.
The other two issues, however, are motivational enough to look for decentralized solutions.
To construct an efficient decentralized mutual-exclusion protocol, let us first reexpress the mechanism of the centralized protocol as follows: In the system there is a single “permission” token, initially held by the leader, and an entity can perform a critical operation only if in possession of such a token.
It is this fact that ensures the mutual exclusion property within protocol Central.
We can still enforce mutual exclusion using the idea of a permission token, and at the same time achieve fairness without having a leader, in a purely decentralized way.
For example, we can have the token circulate among all the entities:
If an entity needs to perform a critical operation, it will wait until it receives the token.
What is the cost per critical operation of operating such a protocol? To answer this question, consider a period of time when all entities are continuously asking for the token; in this case, almost after each move, the token will be allowing an entity to perform a critical operation.
This means that in such a situation of heavy load, the cost of EndlessTraversal is just O(1) messages per critical operation.
If the requests are few and infrequent, that is, with light load, the amount of messages per request is unpredictable as it depends on the time between successive requests and the speed of the token.
From a practical point of view, this means that the management of a seldomly used resource may result in overcharging the network with messages.
Consider now a period of time where the entities have no need to perform any critical operations; during all this time, the token will continue to traverse the network, looking for entities needing it, and finding none.
As this situation of no load can continue for an unpredictable amount of time, it follows that, in protocol EndlessTraversal, the number of messages per critical operation, is unbounded!
Let us see how this unpleasant situation can be improved.
Let us consider the virtual ring R associated to the depth-first traversal of the network; in case the network is Hamiltonian, we will use the Hamiltonian cycle as the ring.
In a traversal, the token moves along R in one direction, call it “right.” If a token reaches an entity that does not need to perform a critical operation (or just finished executing one), to cut down the number of message transmissions, instead of automatically forwarding the token along the ring, the entity will do so only if there are indeed requests for the token, that is, if there are entities wanting to perform a critical operation.
The problem is how to make the entity holding the token know if there are entities wanting it.
This problem is fortunately easy to solve: An entity needing to perform a critical operation and not in possession of the token will issue a request for the token; the request travels along the ring in the opposite direction of the token, until it reaches the entity holding the token or an entity that has also issued a request for the token.
There are many details that must be taken into account to transform this informal description into a protocol.
In our description, each link will have a color, and colors change depending on the type of message according to the following two rules:
When an entity needs to perform a critical operation and does not have the token, if its left link is white, it sends a request there and waits for the token.
When an entity receives a request (from the right link), if its left link is white, it forwards the request and waits for the token.
When an entity has received or receives the token, it will execute the following two steps: (a) if it needs to perform a critical operation, it performs it; (b) if its right link is black, it sends the token to the right.
In this way, instead of a blind endless traversal, we can have one that is fueled by requests for the token.
The worst case does not tell us the whole story.
In fact, the actual cost will depend on the frequency and the spread of the requests.
In particular, like protocol EndlessTraversal, the more frequent the requests and the larger their spread, the more protocol will OnDemandTraversal have a performance approaching O(1) messages per critical operation.
This will be so, regardless of the diameter of the topology, even in networks where protocol Central under the same conditions could require O(n) messages per request.
This point of view opens an interesting and surprising connection between the problem of distributed mutual exclusion and that of fair management of a distributed queue:
The mutual-exclusion protocol is obtained from the queue-management protocol simply as follows (see Figure 9.3):
Note that the queue does not need to be totally ordered; it is enough that every element in the queue is removed (i.e., receives the token) within finite time.
Our goal is to use this approach to design a more efficient distributed mutual-exclusion protocol.
The first idea allows an entity, once it has finished executing its critical operation, to know to which other entity it should send the token.
The second idea, of making the tree rooted in the last entity in the queue, makes reaching the end of the queue very easy: Just follow the “arrow” (i.e., the direction of the links)
These two ideas can be implemented with a simple mechanism to handle requests and token transfers.
Assume that the needed structure is already in place, that is, (i) and (ii) hold.
This means that every entity x knows which of its neighbors, last(x), is in the direction of the last entity in the queue; furthermore, if x is in the queue or holds the token, it knows the identity of the entity next(x) next in the queue (if any)
Let us consider first how to handle the token transfers.
When the entity x currently holding the token terminates its critical operation, as it knows the identity of the first entity x1 in the queue, it can send the token to it using the routing protocol; as we are assuming that the routing protocol is correct, this message will be delivered to x1 within finite time.
In other words, the handling of the token is done independently of the handling of the requests and is implemented using a correct routing protocol; thus, as long as every entity in the queue knows the identity of the next, token transfers pose no problems.
Notice that when the request message from y reaches the old root, that entity will know that y is now after it in the queue.
Summarizing, if the needed structure and information is in place, a single request for the token can be easily and simply handled, correctly maintaining and updating the structure and information.
If there are several concurrent requests for the token, the handling of one could interfere with the handling of another, for example, when trying to root the tree in the “last” entity in the queue: Indeed, which of them is going to be the last? Fortunately, concurrency is not a problem: The set of rules to handle a single request will correctly work for any number of them!
Let us first of all write down more precisely the set of rules:
Handling requests – When entity x needs the token, it sends a “Request(x)” message containing.
Handling the token: An entity x holding the token, upon termination of a.
If two or more “Request” messages are issued concurrently, only one will reach the current root: The others will be diverted to one of the entities issuing one of the messages.
The token is at node d that is executing a critical operation, no entities are in the queue, and the tree is rooted in d.
A request for the token is made by b and concurrently by c; both b and c set last to themselves and send their request following the direction of the arrow.
When f receives the request from c, it will forward it to b (following the arrow) and flip the direction of the link to c setting last(f ) = c.
In other words, the request from b is forwarded to d , while that from c is forwarded to b.
As a result, at the end, next(d) = b and next(b) = c, that is, b is ahead of c in the queue.
Had the message from c arrived at f before that of b, the outcome would have been reversed.
Notice that at the end the tree is rooted in c.
The correctness of the protocol is neither obvious nor immediate.
Observe that if there is a request in transit on a link, then the link is not directed toward any of the two entities connected by it.
Property 9.3.3 In L[t], from any nonterminal node there is a directed path to exactly one terminal entity.
We are now ready to prove the main correctness property.
Theorem 9.3.1 InL[t] any terminal path leads to either the entity holding the token or a waiter.
We need to show that, within finite time, every message will stop traveling.
Call target(v)[t] the terminal node at the end of the terminal path of v at time t ; if a “Request” is traveling from u to v at time t , then the target of the message is target (v)[t]
Theorem 9.3.2 Every request will be delivered to its target.
This means that the number of messages in the worst case is always better that that of protocol OnDemandTraversal.
In situations of high load (i.e., frequent requests from many entities), the improvement (if any at all) strongly depends on the structure of the spanning tree (see Exercise 9.6.17)
In the centralized solution, every entity needing to perform a critical operation would ask for permission to do so; as there was one entity capable of granting such a permission, the request would be sent just there.
We can still use the same idea, of asking for permission and waiting to obtain it, in a decentralized way: If an entity x needs to perform a critical operation, it asks all the other entities for their permission and it will perform the critical operation when it receives permission from all entities.
An entity y, upon receiving such a request, will grant its permission if not performing a critical operation or waiting itself to receive permission.
In other words, if no other entity is currently performing a critical operation or waiting itself to receive permission, x will be allowed to perform its critical operation.
What happens, however, if some entity y is performing a critical operation or waiting for permissions? We must specify some rules that ensure that even in this case x will indeed receive permission from all entities within finite time.
Assuming this is done, the general rulers are as follows:
When an entity y receives a request r(x) for permission, – if y is not executing a critical operation nor it is waiting for permission, it.
When an entity z finishes performing its critical operation, it removes fromQ(z) one request at the time and grants to it its permission.
In fact, its correctness relies on the total order imposed on the requests, but not every total order will do.
To understand what can go wrong, consider first the following example.
In the example we just examined, the problem was due to the nature of the specific total order.
Example Suppose that y sends its request and that no entity is performing a critical operation or is requesting a permission; thus, every entity receiving the request r(y) will grant its permission.
After it has granted permission to r(y), entity x now needs to perform a critical operation; x then broadcasts its request r(x)
It is indeed possible that r(x) arrives at y before y has received all the permission for its request; in this case, as r(x) < r(y), y will grant its permission to r(x)
As a consequence, within finite time both y and x will perform a critical operation, violating the mutual exclusion condition!
These two examples show that the total order imposed on the requests cannot be arbitrary; rather, it must satisfy some temporal constraints.
Temporal Constraint: If a request r(x) is generated after x has granted its permission to a request r(y) generated by y, then r(x) > r(y)
By contradiction, let two entities x and y perform a critical operation at the same time; this means that each must have granted permission to the request of the other.
Under this condition, the only reason why x would grant its permission to r(y) is that r(y) arrives before x issued r(x) (otherwise, as r(x) < r(y), x would not have granted the permission)
In this case, however, according to the Temporal Constraint, r(x) > r(y): a contradiction.
To prove fairness, we need to show that each convergecast is eventually completed; this is left as an exercise (Exercise 9.6.18)
Observe that this property can be easily achieved using the virtual clocks discussed in Section 9.2.2
What an entity x needs to do is to advance the value of its virtual clock whenever it wants to issue a request and use that value as r(x)
The properties of virtual clocks ensure that Temporal Constraint holds (Exercise 9.6.19)
We will call AskAllClocks the resulting protocol employing logical clocks in its rules (Exercise 9.6.20)
In the case of a complete network, the approaches that we have described so far become rather straightforward.
New ones can actually be devised that would be not efficient in other networks.
In the following we will consider how to render the permission-based approach of AskAll more efficient in the case of a complete network.
In protocol AskAllClocks, each request must be granted a permission from every entity.
A way to reduce the amount of messages is to reduce the number of permissions needed before an entity can perform its critical operation.
In this protocol, if an entity neither has issued a request nor is performing a critical region, it will “accept” (i.e., grant its permission to) all incoming requests, that is, it treats requests in a nonexclusive way.
If the entity has made its own request, until it is granted permission from all entities, it will accept all incoming requests with smaller value, that is, it treats requests with smaller value in a nonexclusive way.
In this way, the number of messages for a request by x will be twice the size of the subset associated to x.
So what we would like to do is to select these sets as small as possible.
However, we still must ensure that mutual exclusion is preserved; to do so, it is necessary that, for any two entities x and y, their subsets have at least an entity z in common; in this way, if z grants permission to x, it will not do so to y concurrently.
The collection of such sets is called a coterie, and the set S(x) is called the quorum of x.
To remedy this, we will make permissions “contestable” and “revocable”, that is, an entity that has granted its permission, it might under some conditions to revoke its permission and grant it to some other request.
Consider the case when z has granted permission to a request r(y) (still pending), and it receives a request r(x)
If r(x) > r(y), then z will locally enqueue the request and notify x (by sending a on hold message)
If instead r(x) < r(y), then z must decide whether or not to revoke the permission given to y and to transfer it to x.
It does so by sending a probe message to y and waiting for an answer; during this time, z will not send any other probe to y.
If the reply is that it is all right to revoke the permission (i.e., it receives a revoke message from y), then z will grant its permission to x.
Let us see under what conditions y will tell z that it is all right to revoke the granted permission.
The general rule will be that if an entity receives an on hold answer to its request, then it must reply with revoke to any probe message it receives, that is, if its request is put on hold, an entity will give up on all permissions that are being contested.
Therefore, when the probe from z arrives at y, if y has already received a on hold message, it will send a revoke message to z; otherwise, y waits to see what the replies to its request will be: Should one of them be on hold, y will send revoke to z.
By contrast, if none of the replies to its request has been on hold, then y has received permission from everybody in its quorum; hence y will start its critical operation; upon termination, it will notify all its quorum (including z), that the request is no longer pending (by sending a completed message)
Summarizing, if r(x) < r(y), then z will receive as a reply to its probe to y either a revoke or a completed message.
In either case, z will then grant its permission to x.
The cost of the protocol is not difficult to determine: Each request message can cause a probe message, replied by a revoke message that will cause a grant and (eventually) the corresponding release.
Alternatively, each request can cause just an on hold reply followed eventually by a grant and eventually by the corresponding release.
As the request of an entity x is sent to its quorum S(x), the total number of messages per request by x is at most 5|S(x)|
That means that, in the worst case, the number of messages per critical operation is.
If we want just to minimize this quantity, we can choose the coterie where each quorum is composed of a single entity, the same for all quorums, that is,S(x) = S(y) = r for all x, y.
Notice that in this case, we have exactly the centralized approach to distributed mutual exclusion: r is the central controller ensuring both fairness and mutual exclusion; the cost will be just five messages per critical operation, which is actually worse than that of protocol Central when used in a complete graph.
The drawback with this choice of coterie is the same as with any centralized solution.
In particular, the load is unbalanced: r receives and must handle almost all the traffic in the system.
Indeed, a desirable requirement for a coterie is that it is balanced.
To formalize this concept, let the load of x in a coterie C be the number load(x) of quorums in which x appears.
Further notice that to be usable, the coterie must be constructed before the mutual-exclusion protocol takes place.
In those cases, to compare protocols , the bit complexity should be used instead.
The number of messages (or of bits) per critical operation is not the only factor to be taken into account.
In particular, in addition to the amount of work (i.e., message transmissions), we are interested in how this work is distributed among the entities, on whether or not it is balanced.
All decentralized protocols are better in this respects than protocol Central; they all, however, have drawbacks either in terms of poorer performance or because requiring additional tools and mechanisms.
Fault Tolerance The issue of fault tolerance is relevant not only for centralized solutions but also for decentralized ones as well.
None of the protocols we have considered considers the possibility of faults, and indeed these protocols would not work correctly, should a failure occur.
The task of designing fault-tolerant protocols (e.g., of making the proposed ones fault tolerant) can be achieved, as we know, by adding capabilities to the system (e.g., synchrony, fault detection)
The goal is in particular to ensure that the protocol satisfies the following consequence of the fairness requirement, which is explicit in the classical (i.e., nondistributed) mutual-exclusion problem:
Any entity not performing a critical operation must not prevent an entity wanting to perform a critical operation from doing so.
This means that if an entity fails when not performing a critical operation, this failure should not prevent other entities from performing their critical operations.
To achieve this goal we assume reliable fault detection and that if/when faults occur, there will be enough time for both detection and restorative action.
For some protocols, the transformation into a fault-tolerant one is not difficult.
Consider for example protocol Central and consider crash failures of entities.
If the leader fails, on the other hand, a new leader must be elected and all entities must resubmit their requests to the new leader before normal functioning can resume.
For other protocols, however, the requirement of being able to tolerate crash failures requires major modifications.
We have examined the problem distributed deadlock detection in some detail in Section 8.2
There, we considered two terminating versions, the personal detection and the collective detection problems, both with static and with dynamic requests.
In real systems, however, the requests are only dynamic and the main problem, is a continuous one: Requests are continuously and unpredictably generated, possibly creating deadlocks, and every deadlock in the system must be resolved within finite time from its occurrence.
Resolving a deadlock means that some requests must be aborted so that the resulting wait-for graph is free of deadlock.
Clearly, to be resolved, a deadlock must first be detected.
The mechanism, specifying how to detect the occurrence of deadlock and how to decide which requests to abort, forms the core to the solution of this continuous problem that we will call system detection and resolution.
Clearly, in this mechanism, we can employ the protocols for personal or collective detection we designed and discussed in Section 8.2
To solve this problem, we will use the general strategy of repeatedly having one or more entities start a personal detection protocol; to work correctly, this strategy must ensure that if a deadlock occurs, at least one entity involved in it will sooner or later start a personal detection.
Whenever an entity waits for a permission longer than a predefined timeout value, the entity will initiate the detection process.
Thus, any entity requesting a permission, after waiting “too long,” will start a personal detection.
Once a deadlock has been detected, in the strategy the resolution mechanism takes place, ensuring that only the necessary requests are aborted and that the deadlock is indeed resolved.
In this strategy, when using personal detection, we do not actually need the initiator to be notified of the result.
In fact, if it is detected that there is no deadlock, then there is nothing to be resolved; hence, the entity making the discovery need not notify the initiator and the detection process can terminate.
Similarly, if there is a deadlock, the entity making the discovery can start the resolution mechanism.
Hence, in our strategy we will use this scaled down version of the personal detection protocols.
Notice that there might be several deadlocks in the system at the same time, some being formed while the computation is taking place, and that several entities can independently detect the presence of the same deadlock.
Analogously to the other continuous computations, we will measure the cost of a solution as the total number of messages transmitted per request.
We will consider the problem under the standard assumptions: connectivity, Bidirectional Links, complete reliability, and unique identifiers.
We will also assume message ordering, that is, the links are FIFO.
Let us consider the problem in systems where a nonwaiting entity can ask for only one permission at a time.
In this case, we know that there is a deadlock in the system if and only if there is a cycle in the wait-for graph.
We can use protocol SimpleCheck for detection without the final notification to the initiator: The message will travel until it reaches the root of the component (if there is no deadlock in the component) or the closest entity in the crown (if there is deadlock in the component)
In the first case, the root will just discard the message.
In the second case, that entity in the crown will detect the existence of the deadlock when it receives the message for the second time; the resolution mechanism must be started now.
Notice that, to resolve a deadlock in the single-request model, it is necessary and also sufficient to abort a single request in the core (i.e., remove one edge from the cycle)
This means that we must first of all know which edges in the wait-for graph form the cycle.
Further notice that, as mentioned earlier, it is possible that several entities in the same component start detection independently.
In this case, the same deadlock (assuming one exists) will be detected by several entities.
As we need to remove only one link, to decide which one, we will run an election among the entities of the core that have detected the deadlock: The leader will then abort its own request, destroying the cycle and resolving the deadlock.
Observe that, once this is done, the component becomes a tree rooted in the leader.
For example, to run the ring-election protocol on the core, the entities initiating that protocol must know which of its possibly many links are those in the cycle (Exercise 9.6.25)
Also, once the election process is started, any subsequent detection message should be stopped from circulating in the ring.
In systems where a nonwaiting entity can ask for many permission at a time (i.e., AND, OR, p-OF-q, generalized systems), the solution strategy will be the same as for single-request systems:
Any entity after waiting too long for permission(s) starts a personal detection protocol for dynamic wait-for graphs.
Whenever an entity detects the existence of a deadlock, it becomes a candidate and starts an election process to elect a leader among the candidates in its component; during this process, further detection messages in the component are discarded.
Once elected, the leader chooses which link(s) must be removed (i.e., which request(s) must be aborted) and ensures that this is done.
The cost of this strategy clearly depends on the types of requests allowed in the system and on the protocols used for personal detection and for election.
In the case of AND requests, for example, if we use protocol DynamicDeadGrant for personal detection and protocol MegaMerger for election in the component, the cost will be at most.
The notions of virtual clocks and causal order were first explicitly stated and analyzed by Leslie Lamport to whom Algorithm CounterClocks is due.
Protocol PseudoVectorClocks (Problem 9.6.2) has been designed by Mark Wineberg [unpublished]
The fact that n-dimensional timestamps are needed to ensure Complete Causal Order has been proven by Bernadette Charron-Bost [5]
In the context of distributed mutual exclusion, circular token-based control (i.e., protocol EndlessTraversal) is the most commonly used mechanism in practical systems, ranging from token rings to bus networks and to hub polling systems; it is indeed at the basis of several IEEE standards.
Although its cost is theoretically unbounded, an extensive amount of literature exists on the performance of the resulting system.
Most of the research work on distributed mutual exclusion has focused on complete graphs.
The idea of using just a majority of (instead of all) the permissions had been introduced much earlier by Bob Thomas [35] together with the idea of using timestamps to tag requests.
The notion of majority was extended by Dave Gifford, who proposed weighted voting [10]
The more general continuing problem of system detection and resolution is the one really occurring in systems.
The solution strategy considered in this chapter has been extensively investigated and experimentally analyzed under all types of systems, for example by Natalija Krivokapic, Alfons Kemper, and Ehud Gudes [14]
The issue of how long an entity should wait before using timeout has been studied by Micha Hofri [12] and extensively discussed by Philip A.
Exercise 9.6.2 Let C be the virtual time constructed by algorithm CounterClocks.
For each of the following situations, provide a small example showing its occurrence:
Exercise 9.6.3 Let C be the virtual time constructed by algorithm CounterClocks.
For each of the following situations, provide a small example showing its occurrence:
Exercise 9.6.4 Let V be the global time constructed by algorithm VectorClocks.
Exercise 9.6.6 Modify algorithm VectorClocks so as to include in each message not the entire vector but only the entries that have changed as last message to the same neighbor.
Compare experimentally the amount of information transmitted by VectorClocks+ with that of VectorClocks.
Compare experimentally the amount of information transmitted by PseudoVectorClocks with that of VectorClocks.
Modify protocol Central so that with three messages per critical operation, the leader needs only to keep one item of information, instead of the entire set of pending requests.
Exercise 9.6.11 Prove that protocol OnDemandTraversal is correct, ensuring both mutual exclusion and fairness.
Exercise 9.6.12 Prove that in protocol OnDemandTraversal, a request message and the token cannot cross each other on a link.
Exercise 9.6.13 Prove that at any time t during the execution of protocol Arrow, L[t] is acyclic.
Exercise 9.6.14 Prove that at any time t during the execution of protocol Arrow, from any nonterminal node there is a directed path to exactly one terminal entity.
Exercise 9.6.15 An entity is said to be a waiter at time t if it has requested a token and it has not yet received it at time t.
Prove that at any time t during the execution of protocol Arrow, in L[t] any terminal path leads either to the entity holding the token or to a waiter.
Exercise 9.6.16 Prove that during the execution of protocol Arrow, every request will be delivered to its target within finite time.
Exercise 9.6.17 Compare experimentally the performance of protocols Arrow and OnDemandTraversal under different load conditions.
Investigate the impact of the structure of the spanning tree on their performace.
Exercise 9.6.18 Prove that under Temporal Constraint, in protocol AskAll every request receives permissions from all entities within finite time.
Exercise 9.6.19 Show how to use using virtual clocks to ensure that property Temporal Constraint holds.
Exercise 9.6.20 (,) Write the set of rules corresponding to Strategy AskAll using logical clocks to impose total order among requests.
Exercise 9.6.21 (,) Write the set of rules of protocol AskQuorum and prove its correctness.
Exercise 9.6.25 Devise a method so that the entities in the core can execute the ring-election protocol without sending any message to noncore entities.
Problem 9.6.1 (,,) Prove that any timestamp-based virtual clock that satisfies property Complete Causal Order must use vectors of size at least n.
Problem 9.6.2 (,) Modify algorithm VectorClocks, so as to construct and maintain virtual clocks satisfying the Complete Causal Order property without any a priori knowledge, without incurring in any initial cost, without any additional messages, and with no more information in each message that that required by VectorClocks.
Problem 9.6.3 (,,) Design an algorithm to occasionally reduce the values of the vector clocks.
Your protocol should not destroy any causal relationship between the events occurring after the reduction.
A distributed algorithm for mutual exclusion in an arbitrary network.
On timeout for global deadlock detection in decentralized database systems.
Deadlock detection in distributed database systems: A new algorithm and a comparative performance analysis.
Time, clocks, and the ordering of events in a distributed system.
A log n distributed mutual exclusion algorithm based on path reversal.
A class of deadlock-free Maekawa-type algorithms for mutual exclusion in distributed systems.
A majority consensus approach to concurrency control for multiple copy databases.
