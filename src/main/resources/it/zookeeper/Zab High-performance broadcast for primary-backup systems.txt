Abstract—Zab is a crash-recovery atomic broadcast algorithm we designed for the ZooKeeper coordination service.
ZooKeeper implements a primary-backup scheme in which a primary process executes clients operations and uses Zab to propagate the corresponding incremental state changes to backup processes1
Due the dependence of an incremental state change on the sequence of changes previously generated, Zab must guarantee that if it delivers a given state change, then all other changes it depends upon must be delivered first.
Since primaries may crash, Zab must satisfy this requirement despite crashes of primaries.
Applications using ZooKeeper demand high-performance from the service, and consequently, one important goal is the ability of having multiple outstanding client operations at a time.
Zab enables multiple outstanding state changes by guaranteeing that at most one primary is able to broadcast state changes and have them incorporated into the state, and by using a synchronization phase while establishing a new primary.
Before this synchronization phase completes, a new primary does not broadcast new state changes.
Finally, Zab uses an identification scheme for state changes that enables a process to easily identify missing changes.
Experiments and experience so far in production show that our design enables an implementation that meets the performance requirements of our applications.
Our implementation of Zab can achieve tens of thousands of broadcasts per second, which is sufficient for demanding systems such as our Web-scale applications.
Atomic broadcast is a commonly used primitive in distributed computing and ZooKeeper is yet another application to use atomic broadcast.
ZooKeeper is a highly-available coordination service used in production Web systems such as the Yahoo! crawler for over three years.
Such applications often comprise a large number of processes and rely upon ZooKeeper to perform important coordination tasks, such as storing configuration data reliably and keeping the status of running processes.
Given the reliance of large applications on ZooKeeper, the service must be able to mask and recover from failures.
ZooKeeper is a replicated service, and it requires that a majority (or more generally a quorum) of servers has not crashed for progress.
With ZooKeeper, a primary process receives all incoming client requests, executes them, and propagates the resulting non-commutative, incremental state changes in the form of transactions to the backup replicas using Zab, the ZooKeeper atomic broadcast protocol.
Upon primary crashes, processes execute a recovery protocol both to agree upon a common consistent state before resuming regular operation and to establish a new primary to broadcast state changes.
To exercise the primary role, a process must have the support of a quorum of processes.
As processes can crash and recover, there can be over time multiple primaries and in fact the same process may exercise the primary role multiple times.
To distinguish the different primaries over time, we associate an instance value with each established primary.
A given instance value maps to at most one process.
Note that our notion of instance shares some of the properties of views of group communication [8], but it presents some key differences.
With group communication, all processes in a given view are able to broadcast, and configuration changes happen when any process joins or leaves.
With Zab, processes change to a new view (or primary instance) only when a primary crashes or loses support from a quorum.
Critical to the design of Zab is the observation that each state change is incremental with respect to the previous state, so there is an implicit dependence on the order of the state changes.
State changes consequently cannot be applied in any arbitrary order, and it is critical to guarantee that a prefix of the state changes generated by a given primary are delivered and applied to the service state.
State changes are idempotent and applying the same state change multiple times does not lead to inconsistencies as long as the application order is consistent with the delivery order.
Consequently, guaranteeing at-least once semantics is sufficient and simplifies the implementation.
As Zab is a critical component of the ZooKeeper core, it must perform well.
Some applications of ZooKeeper encompass a large number of processes and use ZooKeeper extensively.
We designed ZooKeeper to have high throughput and low latency, so that applications could use it extensively on cluster environments: data centers with a large number of wellconnected nodes.
Multiple outstanding transactions: It is important in our setting that we enable multiple outstanding ZooKeeper operations and that a prefix of operations submitted concurrently by a ZooKeeper client are committed according to FIFO order.
Traditional protocols to implement replicated state machines, like Paxos [2], do not enable such a feature directly, however.
If primaries propose transactions individually, then the order of learned transactions might not satisfy the order dependencies and consequently the sequence of learned transactions cannot be used unmodified.
One known solution to this problem is batching multiple transactions into a single Paxos proposal and having at most one outstanding proposal at a time.
Such a design affects either throughput or latency adversely depending on the choice of the batch size.
Figure 1 illustrates a problem we found with Paxos under our requirements.
It shows a run with three distinct proposers that violates our requirement for the order of generated state changes.
Such a run is not acceptable because the state change represented by B causally depends upon A, and not C.
Consequently, B can only be chosen for sequence number i+1 if A has been chosen for sequence number i, and C cannot be chosen before B, since the state change that B represents cannot commute with C and can only be applied after A.
Efficient recovery: One important goal in our setting is to recover efficiently from primary crashes.
For fast recovery, we use a transaction identification scheme that enables a new primary to determine in a simple manner which sequence of transactions to use to recover the application state.
In our scheme, transaction identifiers are pairs of values: an instance value and the position of a given transaction in the sequence broadcast by the primary process for that instance.
Under this scheme, only the process having accepted the transaction with the highest identifier may have to copy transactions to the new primary, and no other transaction requires recovery.
This observation implies that a new primary is able to decide which transactions to recover and from which process simply by collecting the highest transaction identifier from each process in a quorum.
For recovery with Paxos, having the last sequence number for which a process accepted a value is not sufficient, since processes might accept different values (with different ballot numbers) for every sequence number.
Consequently, a new primary has to execute Phases 1 of Paxos for all previous.
Summary of contributions: We describe here the design of Zab, an atomic broadcast protocol for the ZooKeeper coordination service.
Zab is a high-performance atomic broadcast protocol for primary-backup systems.
Compared to previous atomic broadcast protocols, Zab satisfies a different set of correctness properties.
In particular, we propose a property called primary order that is important for primary-backup systems.
This property is critical to enable the correct ordering of state changes over time as different processes exercise the primary role while allowing multiple outstanding transactions.
Primary order is different from causal order, as we discuss in Section III-B.
Given our use of the primary order property, we say that Zab is a PO atomic broadcast protocol.
Finally, our scheme for identifying transactions enables faster recovery compared to classic algorithms such as Paxos, since Zab transaction identifiers map to at most one transaction and processes accept them in order.
More precisely, the channel cij between processes pi and pj is such that each of the processes has a pair of buffers: an input buffer and an output buffer.
A call to send a message m to process pj is represented by an event send(m, pj), which inserts m into the output buffer of pi for cij.
Messages are transmitted following the order of send events, and they are inserted into the input buffer.
A call to receive the next message m in the input buffer is represented by an event recv(m, pi)
Single iteration: The input buffer of a process pj for channel cij contains messages from at most one iteration.
Implementation of channels: To implement the properties we state for channels and ensure liveness, it is sufficient to assume fair-lossy links (a precise definition of fair-lossy in the crash-recovery model appears in the work of Boichat and Guerraoui [12])
At the beginning of a new iteration, we establish a connection between pi and pj.
By doing so, we guarantee that only messages sent are received (Integrity), a prefix of the sequence of messages sent from a process pi to a process pj are received, and once we close a connection and establish a new one, we guarantee that a process only has messages from a single iteration.
ZooKeeper uses a primary-backup scheme to execute requests and propagate state changes to backup processes using.
To guarantee that the transactions a primary broadcast are consistent, we need to make sure that a primary only starts generating state updates once the Zab layer indicates that recovery has completed.
For this purpose, we assume that processes implement a ready(e) call, which the Zab layer uses to signal to the application (primary and backup replicas) that it can start broadcasting state changes.
A call to ready(e) also sets the value of the variable instance that a primary uses to determine its instance.
The primary uses the value of instance to set the epoch of transaction identifiers when broadcasting, and we assume that the value of e is unique for different primary instances.
Note that the statement of agreement is different compared to previous work.
We instead state agreement as a safety property, which guarantees that the state of two processes do not diverge.
Finally, a primary has to guarantee that the state updates generated are consistent.
Comparison with causal atomic broadcast PO atomic broadcast is designed to preserve the causal.
In this section, we compare causal atomic broadcast and PO atomic broadcast, and argue that they are not comparable.
It follows directly from the core properties that PO atomic broadcast implements PO causal order and strict causality [15]
There are two roles Zab process can perform according to the protocol: leader and follower.
A leader concurrently executes the primary role and proposes transactions according to the order of broadcast calls of the primary.
Followers accept transactions according to the steps of the protocol.
Each process implements a leader oracle, and the leader oracle provides the identifier of the prospective leader `
If the leader oracle of a process determines that it is the leader, then it executes the leader steps of the protocol.
Being selected the leader according to its oracle, however, is not sufficient to establish its leadership.
To establish leadership, a process needs to complete the synchronization phase (Phase 2)
In the phase description of Zab, and later in the analysis, we use the following notation:
The following section discusses the Zab protocol in more detail along with some implementation aspects.
In our implementation of Zab, a Zab process can be looking for a leader (ELECTION state), following (FOLLOWING state), or leading (LEADING state)
While in this state the process tries to elect a new leader or become a leader.
If the process finds an elected leader, it moves to the FOLLOWING state and begins to follow the leader.
If the process is elected leader, it moves to the LEADING state and becomes the leader.
Given that a process that leads also follows, states LEADING and FOLLOWING are not exclusive.
A follower transitions to ELECTION if it detects that the leader has failed or relinquished leadership, while a leader transitions to ELECTION once it observes that it no longer has a quorum of followers supporting its leadership.
Co-locating the leader and the primary on the same process has practical advantages.
The primary-backup scheme we use requires that at most one process at a time is able to generate updates that can be incorporated into the service state.
A primary propagates state updates using Zab, which in turn requires a leader to initiate proposals.
Leader and primary correspond to different functionality, but they share a common requirement: election.
By co-locating them, we do not need separate elections for primary and leader.
Also important is the fact that calls to broadcast transactions are local when they are co-located.
Figure 5 shows the events for both the leader and followers when establishing a new leader.
An elected leader does not become established for a given epoch e until it completes Phase 2, in which it successfully achieves consensus on the proposal history and on itself as the leader of e.
We define a established leader and a established epoch as follows:
It initially collects the latest epoch of a quorum of followers Q, proposes a later epoch, and collects the latest epoch and highest zxid of each of the followers in Q.
The leader completes Phase 1 once it selects the history from a follower f with latest epoch and highest zxid in a ACK-E(f.a, hf )
These steps are necessary to guarantee that once the prospective leader completes Phase 1, none of the followers in Q accept proposals from earlier epochs.
Given that the history of a follower can be arbitrarily long, it is not efficient to send the entire history in a ACK-E(f.a, hf )
The last zxid of a follower is sufficient for the prospective leader to determine if it needs to copy transactions from any given follower, and only copies missing transactions.
In Phase 2, the leader proposes itself as the leader of the new epoch and sends a NEWLEADER(e, Ie) proposal, which contains the initial history of the new epoch.
As with ACK-E(f.a, hf ), it is not necessary to send the complete initial history, but instead only the transactions missing.
A leader becomes established once it receives the acknowledgments to the new leader proposal from a quorum of followers, at which point it commits the new proposal.
One interesting optimization is to use a leader election primitive that selects a leader that has the latest epoch and has accepted the transaction with highest zxid among a quorum of processes.
Such a leader provides directly the initial history of the new epoch.
A leader proposes operations by queuing them to all connected followers.
To achieve high throughput and low latency, the leader has a steady stream of proposals to the followers.
By the channel properties, we guarantee that followers receive proposals in order.
In our implementation, we use TCP connections to exchange messages between processes.
If a connection to a given follower closes, then the proposals queued to the connection are discarded and the leader considers the corresponding follower down.
Detecting crashes through connections closing was not a suitable choice for us.
Timeout values for a connection can be of minutes or even hours, depending on the operating system configuration and the state of the connection.
To mutually detect crashes in a fine-grained and convenient manner, avoiding operating system reconfiguration, leader and followers exchange periodic heartbeats.
If the leader does not receive heartbeats from a quorum of followers within a timeout interval, the leader renounces leadership of the epoch, and transitions to the ELECTION state.
When a follower emerges from leader election, it connects to the leader.
To support a leader, a follower f acknowledges its new epoch proposal, and it only does so if the new epoch proposed is later than f.p.
A follower only follows one leader at a time and stays connected to a leader as long as it receives heartbeats within a timeout interval.
If there is an interval with no heartbeat or the TCP connection closes, the follower.
Figure 5 shows the protocol a follower executes to support a leader.
The follower sends its current epoch f.a in a current epoch message (CEPOCH(f.a)) to the leader.
The leader sends a new epoch proposal (NEWEPOCH(e)) once it receives a current epoch message from a quorum Q of followers.
The new proposed epoch e must be greater than the current epoch of any follower in Q.
A follower acknowledges the new epoch proposal with its latest epoch and highest zxid, which the leader uses to select the initial history for the new epoch.
In Phase 2, a follower acknowledges the new leader proposal (NEWLEADER(e, Ie)) by setting its f.a value to e and accepting the transactions in the initial history.
Note that once a follower accepts a new epoch proposal for an epoch e, it does not send an acknowledgement for any other new epoch proposal for the same epoch e.
This property guarantees that no two processes can become established leaders for the same epoch e.
In Phase 3, the follower receives new proposals from the leader.
A follower adds new proposals to its history and acknowledges them.
It delivers these proposals when it receives commit messages from the leader.
Note that a follower and a leader follow the recovery protocol both when a new leader is emerging and when a follower connects to an established leader.
If the leader is already established, the NEWLEADER(e, Ie) proposal has already been committed so any acknowledgements for the NEWLEADER(e, Ie) proposal are ignored.
Zab requires the presence of a leader to propose and commit operations.
To sustain leadership, a leader process ` needs to be able to send messages to and receive messages from followers.
In this section, we present an argument for the correctness of Zab.
A more detailed proof appear in a technical report [15]
We present initially a list of definitions followed by a set of invariants that the protocol must satisfy.
The following properties are invariants that the protocol maintains at each step, and that can be verified against the protocol of Section IV in a straightforward manner.
We use them when proving the core properties of Zab.
During the broadcast phase of epoch e, a follower f such that f.a = e accepts proposals and delivers transactions according to zxid order.
The initial history Ie of an epoch e is the history of some follower.
We now present proof sketches for the properties we introduced in Section III.
Note that we use in some statements the terms follower, leader, and primary, instead of process to better match our definitions and the algorithm description.
We have written our implementation of Zab and the rest of the ZooKeeper server in Java.
Because Zab is not separable from ZooKeeper, we wrote a special benchmark wrapper that hooks into the internals of ZooKeeper to interact directly with Zab.
When Java first starts there is class loading and incremental compilation that takes place that adversely affects the initial runs.
We also allocate files for logging transactions in the initial runs that are reused in later runs.
To avoid these startup effects we run some warmup batches and then run approximately 10 batches sequentially.
We ran our benchmark with 1024-byte operations, which represents a typical operation size.
Throughput of 1K messages as the number of servers increases.
The error bars show the range of the throughput values from the runs.
Figure 6 shows how throughput varies with the number of servers.
One line shows the throughput when nothing is written to disk.
This isolates the performance of just the protocol itself and the network.
Because we have a single gigabit network interface we have a cap on outgoing bandwidth.
We also show the theoretical maximum replication throughput given this cap.
Although we sync requests to disk using the fdatasync system call, this call only forces the request to the disk, and not necessarily to the disk media.
By default, disks have a write cache on the disk itself and acknowledge the write before it is written to the disk media.
In the event of a power failure, writes can be lost if they have not reached the disk media.
As shown in this figure and the next, there is a high price to pay when the disk cache is turned off.
When running with a disk cache, or with a battery backed cache, such as those in raid controllers, the performance with the disk is almost identical to network only and both are saturating the network.
When we turn the disk write cache off, Zab becomes I/O bound and the throughput is roughly constant with the number of servers.
With more than seven servers, throughput decreases with more servers, since the same network-only bottlenecks are present when the transactions are logged to disk.
As we scale the number of servers we saturate the network card of the leader which causes the throughput to decrease as the number of servers increases.
We can use a broadcast tree or chain replication [7] to broadcast the proposals to avoid this saturation, but our performance is much higher than we need in production, so we have not explored these alternatives.
Using ping we measured the basic latency between servers to be 100 microseconds.
Latency of 1K operations as the number of servers increases.
As with throughput, turning off the disk write cache causes a significant performance impact.
We use preallocated files, write sequentially, and use the fdatasync to only sync the data to disk.
Unfortunately, the Linux kernel does not recognize the “only sync data” flag until version 2.6.26
When we sync, the performance penalty should be no more than a rotational delay and a seek (around 20 milliseconds)
This extra access time affects both the latency and the throughput.
In the first phase, called read phase, the new leader contacts all other processes to read any possible value that has been proposed by previous leaders and committed.
In the second phase, called write phase, the new leader proposes its own value.
Compared to Paxos, one important difference with Zab is the use of an additional phase: synchronization.
A new prospective leader ` tries to become established by executing a read phase, that we call discovery, followed by a synchronization phase.
During the synchronization phase, the new leader ` makes sure that a quorum of followers delivers chosen transactions proposed by established leaders of prior epochs.
This synchronization phase prevents causal conflicts and ensures that this property is respected.
In fact, it guarantees that all processes in the quorum deliver transactions of prior epochs before transactions of the new epoch e are proposed.
Once the synchronization phase completes, Zab executes a write phase that is similar to the one of Paxos.
For each instance of consensus, a Zab leader chooses a value that is anchored, it tries to get a quorum of agents (followers) to accept it, and it finishes by recording the value on a quorum of agents.
In Zab, determining which value is anchored for a consensus instance is simple because we grant the right to propose a value for a given consensus instance to exactly one leader, and, by the algorithm, a leader proposes at most one value to each instance.
Consequently, the anchored value is either the single value the leader proposed or no value (no-op)
Zab splits the sequence of consensus instances into epochs, and to the consensus instances of an epoch, only one leader can propose values.
Lampson observes that the Viewstamped replication protocol has a consensus algorithm embedded [19]
The approach proposed by Viewstamped replication combines transaction processing with a view change algorithm [20]
The view change algorithm guarantees that events known to a majority of replicas (or cohorts in their terminology) in a view survive into subsequent views.
Like Zab, the replication algorithm guarantees the order of events proposed within an epoch.
With passive replication, a single process executes clients operations and propagates state changes to the remaining replicas.
Primary-backup is also a special case of Vertical Paxos [21], which is a family of Paxos variants that enable reconfigurations over time and requires fewer acceptors.
Vertical Paxos relies upon a configuration master for configuration changes.
Each configuration is associated to a ballot number, which increases for every new configuration, and the proposer of each configuration uses the corresponding ballot number to propose values.
Vertical Paxos is still Paxos, and each instance of consensus can have multiple values proposed over time under different ballots, thus causing the undesirable behavior for our setting we discuss previously in the Introduction.
Rodrigues and Raynal propose a crash-recovery atomic broadcast protocol using a consensus implementation [3]
To avoid duplicates of delivered messages, they use a call A-deliver-sequence to obtain the sequence of ordered messages.
Mena and Schiper propose to add a commit primitive to the specification of atomic broadcast [4]
Messages that have not been committed can be delivered twice.
With Zab, messages can be delivered twice as long as they respect the order agreed upon.
Boichat and Guerraoui propose a modular and incremental approach to total-order broadcast, and their strongest algorithm corresponds to Paxos [12]
The general idea is to guarantee that all processes observe the same events in the same order.
This guarantee applies not only to message delivery events, but also to failures, recoveries, group membership changes, etc.
Although atomic broadcast is important for virtually synchronous environments, other weaker forms of broadcast, such as causal broadcast, also enable applications to obtain the property of virtual synchrony.
Different from such a programming model, Zab assumes a static ensemble of processes and does not perform view or epoch changes upon failures of processes other than the leader, unless the leader has no quorum supporting it.
Also, different from the ABCAST protocol of Birman and Joseph, Zab uses a sequencer to disseminate messages because it naturally matches the ZooKeeper application.
They present three total order properties: strong total order, weak total order, and reliable total order.
Reliable total order is the strongest property, and guarantees that a prefix of messages totally ordered by a timestamp function are delivered in a view.
Zab properties match more closely this property, with one key difference: each view has at most one process broadcasting.
Having a single process broadcasting in a view simplifies the implementation of the property, since the ordering is established directly by the process broadcasting.
COReL is an atomic broadcast protocol for partitionable environments [24]
It relies upon Transis, a group communication layer [25] and enables processes in a primary component to totally order messages.
Like Zab, upon a configuration change, COReL does not introduce new messages until recovery ends to guarantee a.
COReL, however, assumes that all processes can initiate messages, leading to different ordering guarantees.
Two key requirements in our design were efficient recovery upon primary changes and state consistency.
We observed that primary order was a necessary property for guaranteeing correct recovery in our use of primary-backup.
We considered protocols in the literature like Paxos, but even though Paxos is a popular choice for implementing replicated systems, we found that it does not satisfy this property when there are multiple outstanding transactions without batching.
Our implementation of Zab has been able to provide us excellent throughput performance while guaranteeing these properties.
To guarantee primary order despite primary crashes, Zab implements three phases.
One particular phase critical to guarantee that the property is satisfied is synchronization.
Upon a change of primary, a quorum of processes has to execute a synchronization phase before the new primary broadcasts new transactions.
Executing this phase guarantees that all transactions broadcast in previous epochs that have been or will be chosen are in the initial history of transactions of the new epoch.
Zab uses a scheme for assigning transaction identifiers that guarantees at most one value for each identifier.
This scheme enables efficient recovery of primary crashes by allowing correct transaction histories to be chosen by simply comparing the last transaction identifier accepted by a process.
Zab is in production as part of ZooKeeper and has met the demands of our workloads.
The performance of ZooKeeper has been key for its wide adoption.
Raynal, “Atomic broadcast in asynchronous crashrecovery distributed systems and its use in quorum-based replication,” IEEE Transactions on Knowledge and Data Engineering, vol.
Redstone, “Paxos made live: An engineering perspective,” in PODC ’07: Proceedings of the twenty-sixth annual ACM symposium on Principles of distributed computing.
Guerraoui, “Reliable and total order broadcast in the crash-recovery model,” J.
Lamport, “Time, clocks, and the ordering of events in a distributed system,” Communications of the ACM, vol.
Toueg, “The weakest failure detector for solving consensus,” in PODC ’92: Proceedings of the eleventh annual ACM symposium on Principles of distributed computing.
Zhou, “Omega meets Paxos: Leader election and stability without eventual timely links,” in DISC’05: Proceedings of the International Conference on Distributed Computing, vol.
Lampson, “The ABCD’s of Paxos,” in PODC ’01: Proceedings of the twentieth annual ACM symposium on Principles of distributed computing.
Liskov, “Viewstamped replication: A new primary copy method to support highly-available distributed systems,” in PODC ’88: Proceedings of the seventh annual ACM Symposium on Principles of distributed computing.
Joseph, “Reliable communication in the presence of failures,” ACM Trans.
Dolev, “Efficient message ordering in dynamic networks,” in PODC ’96: Proceedings of the fifteenth annual ACM symposium on Principles of distributed computing.
