The publisher offers discounts on this book when ordered in quantity.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in the book, and Manning Publications was aware of a trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning’s policy to have the books we publish printed on acid-free paper, and we exert our best efforts to that end.
Recognizing also our responsibility to conserve the resources of our planet, Manning books are printed on paper that is at least 15 percent recycled and processed without the use of elemental chlorine.
Familiar with a whole gamut of programming languages and techniques, he is an expert in highperformance systems, build tools, type theory, and many other areas.
He is also a gifted teacher, and all that combined is what makes Scala in Depth special.
This book provides in-depth coverage of several of the more intricate areas of Scala, including advanced aspects of its type system, implicits, composition techniques with traits, collections, actors, functional categories.
But this is not a dry recollection of language and library concepts.
The book is full of practical advice on how to apply these lesser known parts of Scala in useful ways, and what the best practices are.
The explanations and examples demonstrate Joshua’s great experience constructing largescale systems in Scala.
Scala in Depth is not a beginner’s introduction; it should primarily appeal to competent Scala programmers who want to become experts.
The techniques that are taught are handy for constructing flexible and type-safe library abstractions.
I am particularly happy about one other thing: The book fills a gap in that it explains.
Scala is one of few languages that actually has a specification.
That specification consists mainly of definitions written in highly stylized prose and mathematical formulas; so it’s not everybody’s piece of cake.
Joshua’s book manages to be both authorative and understandable as it explains these concepts.
During that first conversation Michael and I discussed the Scala ecosystem and what kind of a book would best serve the community.
I believed Scala needed a “practical Scala” book to help guide those new to the language.
Scala is a beautiful language, but it brings many new concepts to the table.
I had watched as the community slowly discovered best practices and a code style that was wholly “Scala.” But I wasn’t sure whether I was the right person to write such a book.
When it came down to it, I was passionate about the topic, had enough free time to do the research, and had the support of the magnates of the community to help achieve what you are reading today—so I decided to go ahead.
One reason it took so long was the evolving nature of Scala and the emergence of new best practices.
To all aspiring authors out there, I will tell you that writing a book makes you an expert.
You may think you are one before you start, but true expertise grows from the blood, sweat, and tears of teaching, of trying to convey complex concepts to your readers with clarity.
Working on this book was a journey that I never could have completed without a very supportive and loving wife, a great publisher, and an amazing community of Scala developers and readers willing to read my manuscript in various stages, point out my typos and misspellings, and offer advice on how to make Scala in Depth a much better book than I could have achieved alone.
While I’m going to try to list them all, I’m sure I’ll miss a few as there were just too many for my tiny brain to remember.
This book showed me that I have a lot of high quality friends, coworkers, and family.
The biggest thank you is for my wife and children, who had to deal with a husband/father who was constantly hiding in a corner, writing, when he should have been helping out.
There’s no way an author can write a book without the support of immediate family, and mine was no exception.
Next, I’d like to thank Manning Publications and all the work the staff did to ensure I became a real author.
Not only did they review and lay out the book, they also helped improve my technical writing skills for clear communication.
I can’t give enough thanks to the whole team, but I’d especially like to thank Katherine Osborne for putting up with my missed deadlines, Pennsylvania-Dutch sentence structures, and xiv.
Katherine was instrumental to the voice of this book, and those who’ve been reading the MEAPs will notice the improvement.
The next group that deserves thanks are the Scala experts and nonexperts who helped me improve my technical material and descriptions.
Justin Wick was a reviewer and collaborator on a lot of the content, and definitely helped me reach a wider audience than I had initially attempted to attract.
He also reviewed the final manuscript and code one last time, just before the book went into production.
Adriaan Moors, as usual, pointed out all my mistakes when.
Eric Weinberg was an old coworker of mine who helped provide guidance for reaching non-Scala developers in the book.
Viktor Klang reviewed the “Actors” chapter (and the whole book) and offered improvements.
Thank you also to Martin Odersky for his endorsement and kind words on the final product that you will read in the foreword, Josh Cough for being a guy I can bounce ideas off when needed, and Peter Simanyi for an email with a very detailed, thorough, complete, and awesome review of the entire book.
Manning also contacted the following reviewers, who read the manuscript at various stages of its development, and I would like to thank them for their invaluable insights and comments: John C.
Finally, I’d like to thank all of the MEAP reviewers.
I received great feedback from them and appreciate the support and good reviews this book received before it was even in print.
You guys had to bear with lots of typos and errors and deserve credit for persevering through my rough initial cuts and making it to this final version.
This book, picking up where introductory books drop off, enables readers to write idiomatic Scala code and understand trade-offs when making use of advanced language features.
In particular, this book covers Scala’s implicit and type systems in detail before discussing how these can be used to drastically simplify development.
The book promotes the “blended style” of Scala, where paradigms are mixed to achieve something greater.
Scala in Depth is for new or intermediate Scala developers who wish to improve their skills with the language.
While this book covers very advanced concepts in Scala, it attempts to pull along those new to Scala.
This book was written for readers who know Java or another object-oriented lanxvi.
Scala in Depth begins with a philosophical discussion of the “xen” of Scala—that Scala is a blending of concepts that achieve a greater whole when combined.
In particular, three dichotomies are discussed: static typing versus expressiveness, functional programming versus object-oriented programming, and powerful language features versus dead simple Java integration.
These are the things that every Scala developer should be aware of and make use of in daily development.
This chapter is for every Scala developer and covers the basics that make Scala a great language.
Chapter 3 is a digression in code style and associated issues.
Scala brings a few new players to the table, and any Scala style guide should reflect that.
Some common conventions from popular languages like Ruby and Java can actually be deterrents to good Scala code.
Chapter 4 covers new issues arising in object-oriented design due to Scala’s mixin inheritance.
One topic of interest to any Scala developer is the early initializer coverage, which gets little coverage in other books.
After object orientation, the book moves into the implicit system.
In chapter 5, rather than simply discussing best practices, a deep dive is taken into the mechanics of implicits in Scala.
This chapter is a must for all Scala developers who wish to write expressive libraries and code.
The discussion covers all the ways types appear in Scala and how to utilize the type system to enforce constraints.
The chapter moves into a discussion of higher-kinded types and finishes with a dive into existential types.
Chapter 7 discusses the most advanced usage patterns in the language, the intersection of types and implicits.
This intersection is where a lot of interesting and powerful abstractions occur, the epitome of which is the type class pattern.
Having covered the most advanced aspects of Scala, in chapter 8 we move into a discussion of Scala’s collection library.
This includes the design and performance of Scala’s collections as well as how to deal with the powerful type mechanisms.
Chapter 9 kicks off the discussion on actors in Scala.
Actors are a concurrency mechanism that can provide great throughput and parallelism when used appropriately.
The chapter dives into issues of designing actor-based systems and finishes with a demonstration of how the Akka actors library provides best practices by default.
While Scala is more compatible with Java than most other JVM languages, there’s still a mismatch in features between the two.
It’s at these corners that issues arise in Scala-Java integration and this chapter provides a few simple rules that help avoid these issues.
Chapter 11 takes concepts from category theory and makes them practical.
In pure functional programming, a lot of concepts from category theory have been applied to code.
These are akin to object-oriented design patterns, but far more abstract.
While they have terrible names, as is common in mathematics, these concepts are immensely useful in practice.
No coverage of functional programming would be complete without a discussion of some of these abstractions, and Scala in Depth does its best to make these concepts real.
All source code in the book is in a fixed-width font like this, which sets it off from the surrounding text.
I have tried to format the code so that it fits within the available page space in the book by adding line breaks and using indentation carefully.
To run the examples, readers should have Scala installed and, optionally, SBT (http://scalasbt.org)
Longer listings appear under clear listing headers; shorter listings appear between lines of text.
Purchase of Scala in Depth includes free access to a private web forum run by Manning Publications where you can make comments about the book, ask technical questions, and receive help from the authors and from other users.
This page provides information on how to get on the forum once you’re registered, what kind of help is available, and the rules of conduct on the forum.
Manning’s commitment to our readers is to provide a venue where a meaningful dialog between individual readers and between readers and the authors can take place.
It’s not a commitment to any specific amount of participation on the part of the authors, whose contribution to the AO forum remains voluntary (and unpaid)
We suggest you try asking the author some challenging questions lest his interest stray.
The Author Online forum and the archives of previous discussions will be accessible from the publisher’s website as long as the book is in print.
He started his professional career as a software developer in 2004, cutting his teeth with C++, STL, and Boost.
Around the same time, Java fever was spreading and his interest was migrating to web-hosted distributed Java-delivered solutions to aid health departments in the discovery of disease outbreaks.
He introduced Scala into his company code base in 2007, and soon after he was infected by Scala fever, contributing to the Scala IDE, maven-scala-plugin and Scala itself.
Today, Josh is the author of several open source Scala projects, including the Scala automated resource management library and the PGP sbt plugin, as well as contributing to key components in the Scala ecosystem, like the maven-scala-plugin.
His current work at Typesafe Inc., has him doing everything from building MSIs to profiling performance issues.
He likes short walks on the beach and dark beer.
The book includes finely colored illustrations of figures from different regions of Croatia, accompanied by descriptions of the costumes and of everyday life.
While the caption for the illustration on the cover does not tell us the town or region of origin, the blue woolen trousers and richly embroidered vest and jacket that the figure is wearing are typical for the mountainous regions of central Croatia.
Dress codes and lifestyles have changed over the last 200 years, and the diversity by region, xix.
It’s now hard to tell apart the inhabitants of different continents, let alone of different hamlets or towns separated by only a few miles.
Perhaps we have traded cultural diversity for a more varied personal life—certainly for a more varied and fast-paced technological life.
Manning celebrates the inventiveness and initiative of the computer business with book covers based on the rich diversity of regional life of two centuries ago, brought back to life by illustrations from old books and collections like this one.
Scala was developed with the premise that you could mix together object orientation, functional programming, and a powerful type system and still keep elegant, succinct code.
It was hoped that this blending of concepts would create something that real developers could use and that could be studied for new programming idioms.
It was such a large success that industry has started adopting Scala as a viable and competitive language.
Scala attempts to blend three dichotomies of thought into one language.
Functional programming is programming through the definition and composition of functions.
Object-oriented programming is programming through the definition and composition of objects.
Programs can be constructed through both the definition and composition of objects or functions.
This gives Scala the ability to focus on “nouns” or “verbs” in a program, depending on what is the most prominent.
Mainstream statically typed languages tend to suffer from verbose type annotations and boilerplate syntax.
Scala takes a few lessons from the ML programming language and offers static typing with a nice expressive syntax.
Code written in Scala can look as expressive as dynamically typed languages, like Ruby, while retaining type safety.
Finally, Scala offers a lot of advanced language features that are not available in Java.
But Scala runs on the Java virtual machine (JVM) and has tight integration with the Java language.
This means that developers can make direct use of existing Java libraries and integrate Scala into their Java applications while also gaining the additional power of Scala.
This integration makes Scala a practical choice for any JVMbased project.
Let’s take a deeper look at the blending of paradigms in Scala.
Functional programming and object-oriented programming are two different ways of looking at a problem.
Functional programming puts special emphasis on the “verbs” of a program and ways to combine and manipulate them.
Object-oriented programming puts special emphasis on “nouns” and attaches verbs to them.
The two approaches are almost inverses of each other, with one being “top down” and the other “bottom up.”
It approaches software by dividing code into nouns or objects.
Each object has some form of identity (self/this), behavior (methods), and state (members)
After identifying nouns and defining their behaviors, interactions between nouns are defined.
The problem with implementing interactions is that the interactions need to live inside an object.
Modern object-oriented designs tend to have service classes, which are a collection of methods that operate across several domain objects.
Service classes, although objects, usually don’t have a notion of state or behavior independent of the objects on which they operate.
A good example is a program that implements the following story: “A cat catches a bird and eats it.” An object-oriented programmer would look at this sentence and see two nouns: cat and bird.
The cat has two verbs associated with it: catch and eat.
In the example, when a Cat catches a Bird, it converts the bird to a type of Food, which it can then eat.
Functional programming approaches software as the combination and application of functions.
It tends to decompose software into behaviors, or actions that need to be performed, usually in a bottom-up fashion.
Functions are viewed in a mathematical sense, purely operations on their input.
Functional programming attempts to defer all side effects in a program as long as possible.
Removing side effects makes reasoning through a program simpler, in a formal sense.
It also provides much more power in how things can be abstracted and combined.
In the story “A cat catches a bird and eats it,” a functional program would see the two verbs catch and eat.
A program would create these two functions and compose them to create the program.
In the example, the catch method takes a Cat and a Bird and returns a new value of type Cat with Catch.
The eat method is defined as taking a CatWithPrey (a cat needs something to eat) and returns a FullCat (because it’s no longer hungry)
Functional programming makes more use of the type system to describe what a function is doing.
The catch and eat methods use the type signatures to define the expected input and output states of the function.
The with keyword is used to combine a type with another.
In this example, the traits Catch and FullTummy are used to denote the current state of a Cat.
The methods eat and catch return new instances of Cat attached to different state types.
The story value is created by composing the functions catch and eat.
This means that the catch method is called and the result is fed into the eat method.
Finally, the story function is called with a Cat and a Bird and the result is the output of the story: a full cat.
Functional programming and object orientation offer unique views of software.
It’s these differences that make them useful to each other.
In the example, the functional version was built by composing a set of functions that encompassed a story and then feeding the initial data into these functions.
For the object-oriented version, a set of objects was created and their internal state was manipulated.
Object orientation can focus on the nouns of the system and functional programming can compose the verbs.
In fact, in recent years, many Java developers have started moving toward splitting nouns and verbs.
The Enterprise JavaBeans (EJB) specification splits software into Session beans, which tend to contain behaviors, and Entity beans, which tend to model the nouns in the system.
Stateless Session beans start looking more like collections of functional code (although missing most of the useful features of functional code)
This push of functional style has come along much further than the EJB specifications.
The Spring Application Framework promotes a functional style with its Template classes, and the Google Collections library is very functional in design.
Let’s look at these common Java libraries and see how Scala’s blend of functional programming with object orientation can enhance these Application Program Interfaces (APIs)
Many modern API designs have been incorporating functional ideas without ascribing them to functional programming.
For Java, things such as Google Collections or the Spring Application Framework make popular functional concepts accessible to the Java developer.
Scala takes this further and embeds them into the language.
To illustrate, you’ll do a simple translation of the methods on the popular Spring JdbcTemplate class and see what it starts to look like in Scala.
Now for a simple translation into Scala, you’ll convert the interface into a trait having the same method(s):
Table 1.1 Attributes commonly ascribed to object-oriented and functional programming.
The simple translation makes a lot of sense but it’s still designed with a distinct Java flair.
This method takes a JDBC connection and returns a PreparedStatement.
This feature lets us change the JdbcTemplate query method so that it takes functions instead of interfaces.
These functions should have the same signature as the sole method defined on the interface.
The RowMapper argument can be replaced by a function that takes a ResultSet and an integer and returns some type of object.
The updated Scala version of the JdbcTemplate interface would look as follows:
This technique involves some controlling entity (the JdbcTemplate) creating a resource and delegating the use of it to another function.
In this case, there are two functions and three resources.
Also, as the name implies, JdbcTemplate is part of a template method in which pieces of the behavior were deferred for the user to implement.
In a more functional approach, these behavioral pieces become arguments to the controlling function.
This provides more flexibility by allowing mixing/matching arguments without having to continually use subclasses.
You may be wondering why you’re using AnyRef for the second argument’s return value.
Because Scala has supported generics, even when compiling for 1.4 JVMs, we should modify this interface further to remove the AnyRef and allow users to return specific types.
With a few simple transformations, you’ve created an interface that works directly against functions.
This is a more functional approach because Scala’s function traits allow composition.
By the time you’re finished reading this book, you’ll be able to approach the design of this interface completely differently.
Functional programming also shines when used in a collections library.
The Ruby and Python programming languages support some functional aspects directly in their standard library collections.
For Java users, the Google Collections library bring practices from functional programming.
The Google Collections API adds a lot of power to the standard Java collections.
Primarily it brings a nice set of efficient immutable data structures, and some functional ways of interacting with your collections, primarily the Function interface and the Predicate interface.
These interfaces are used primarily from the Iterables and Iterators classes.
Besides equality, it contains an apply method that returns true or false against its argument.
It returns a new collection containing only elements that pass the predicate apply method.
The find method looks in a collection for the first element passing a Predicate and returns it.
The filter and find method signatures are shown in the following code.
There also exists a Predicates class that contains static methods for combining predicates (ANDs/ORs) and standard predicates for use, such as “not null.” This simple interface creates some powerful functionality through the potential combinations that can be achieved with terse code.
Also, because the predicate itself is passed into the filter function, the function can determine the best way or time to execute the filter.
The data structure may be amenable to lazily evaluating the predicate, making the iterable returned a “view” of the original collection.
It might also determine that it could best optimize the creation of the new iterable through some form of parallelism.
This has been abstracted away, so the library could improve over time with no code changes on our part.
The Predicate interface is rather interesting, because it looks like a simple function.
This function takes some type T and returns a Boolean.
Let’s rewrite the filter/find methods in Scala and see what their signatures would look like:
You’ll immediately notice that in Scala we aren’t using any explicit ? super T type annotations.
This is because Scala defines type variance at declaration time.
For this example, that means that the variance annotation is defined on the Function1 class rather than requiring it on every method that used the class.
What about combining predicates in Scala? We can accomplish a few of these quickly using some functional composition.
Let’s make a new Predicates module in Scala that takes in function predicates and provides commonly used function predicates.
The predefined predicates should also have a type T => Boolean.
We’ve now started to delve into the realm of functional programming.
We’re defining first-class functions and combining them to perform new behaviors.
Playing with functions also makes more extensive use of generics and the type system.
Scala has put forth a lot of effort to reduce the overhead for generics in daily usage.
Functional programming is more than combining functions with other functions.
The essence of functional programming is delaying side effects as long as possible.
This predicate object defines a simple mechanism to combine predicates.
The predicate isn’t used to cause side effects until passed to the Iterables object.
Complex predicates can be built from simple predicates using the helper methods defined on the object predicates.
Functional programming grants the means to defer state manipulation in a program until a later time.
It provides a mechanism to construct verbs that delay side effects.
These verbs can be combined in a fashion that makes reasoning through a program simpler.
Eventually the verbs are applied against the nouns of the system.
A common misconception among developers is that static typing leads to verbose code.
This myth exists because many of the languages derived from C, where types must be explicitly specified in many different places.
As software has improved, along with compiler theory, this is no longer true.
Scala uses some of these advances to reduce boilerplate in code and keep things concise.
Scala made a few simple design decisions that help make it expressive:
Let’s look at how Scala changes the sides of type annotations.
Scala places type annotations on the right-hand side of variables.
In some statically typed languages, like Java or C++, it’s common to have to express the types of variables, return values, and arguments.
When specifying variables or parameters, the convention, drawn from C, is to place type indicators on the left-hand side of the variable name.
For method arguments and return values, this is acceptable, but causes some confusion when creating different styles of variables.
C++ is the best example of this, as it has a rich set of variable styles, such as volatile, const, pointers, and references.
Table 1.2 shows a comparison of C++ variables   and Scala variables.
The more complicated a variable type, the more annotations are required directly on the type of the variable.
In C++, this is maximized in the usage of a pointer, because a pointer can be constant.
Scala defines three variable types on the left-hand side, like var, val, and lazy val.
In all instances, the type of the name x is Int.
Mutable integer variable int x int x var x: Int.
Immutable integer value const int x final int x val x: Int.
In addition to separating the concerns of how a variable behaves from the variable type, the placement of types on the right allows type inference to determine the type of the variables.
Type inference is when the compiler determines what the type annotation should be, rather than forcing the user to specify one.
The user can always provide a type annotation, but has the option to let the compiler do the work.
This feature can drastically reduce the clutter found in some other typed languages.
Scala takes this even further to do some level of inference on arguments passed into methods, specifically with first-class functions.
If a method is known to take a function argument, the compiler can infer the types used in a function are literal.
Scala syntax takes the general approach that when the meaning of a line of code is straightforward, the verbose syntax can be dropped.
This feature can confuse users first using Scala but can be rather powerful when used wisely.
Let’s show a simple refactoring from the full glory of Scala syntax into the simplistic code that’s seen in idiomatic usage.
This code accepts a list whose type, T, is able to be implicitly converted into a variable of type Ordered[T] (T <% Ordered[T])
We’ll discuss type parameters and constraints in great detail in chapter 6, so don’t focus too much on these.
We’re requiring that the list contain elements that we have some notion of ordering for, specifically a less than function (<)
If it’s empty, or Nil, then we return a Nil list.
If it encounters a list, we extract the head (x) and tail (xs) of the list.
We use the head element of the list to partition the tail into two lists.
We then recursively call the Quicksort method on each partition.
In the same line, we combine the sorted partitions and the head element into a complete list.
You may be thinking, “Wow, Scala looks ugly.” In this case you would be right.
There’s a lot of syntactic noise preventing the meaning of the code from being clear.
Let’s pull out our surgical knife and start cutting out cruft.
The compiler will assume that the end of a line is the end of an expression, unless you leave some piece of syntax hanging, like the.
But removing semicolons isn’t quite enough to reduce the clutter.
This is the name Scala gives to its ability to treat methods as operators.
A method of no arguments can be treated as a postfix operator.
A method of one argument can be treated as an infix operator.
There’s also the special rule for certain characters (for example, :) at the end of a method name that reverses the order of a method call.
Scala also provides placeholder notation when defining anonymous functions (aka, lambdas)
This syntax uses the _ keyword as a placeholder for a function argument.
If more than one placeholder is used, each consecutive placeholder refers to consecutive arguments to the function literal.
This notation is usually reserved for simple functions, such as the less-than (<) comparison in our Quicksort.
We can apply this notation paired with operator notation to achieve the following on our quick sort algorithm:
Scala offers syntactic shortcuts for simple cases, and it provides a mechanism to bend the type system via implicits conversions and implicits arguments.
Scala implicits are a new take on an old concept.
The first time I was ever introduced to the concept of implicit conversions was with primitive types in C++
C++ allows primitive types to be automatically converted as long as there is no loss of precision.
For example, we can use an int literal when declaring a long value.
The types double, float, int, and long are different to the compiler.
It does try to be intelligent and “do the right thing” when mixing these values.
Scala provides this same mechanism, but using a language feature that’s available for anyone.
The scala.Predef object is automatically imported into scope by Scala.
It’s a handy mechanism for providing convenience functions to users, like directly writing println instead of Console.
These are a set of implicit conversions that automatically migrate from lowerprecision types to higher precision types.
The following listing shows the set of methods defined for the Byte type.
The implicit before the method means the compiler may attempt to apply this method to a type Byte, if it’s required for correct compilation.
This means if we attempt to pass a Byte to a method requiring a Short, it will use the implicit conversion defined as byte2short.
Scala also takes this one step further and looks for methods via implicit conversions if the current type doesn’t have the called method.
This comes in handy for more than just primitive conversions.
Scala also uses the implicit conversion mechanism as a means of extending Java’s base classes (Integer, String, Double, and so on)
This allows Scala to make direct use of Java classes, for ease of integration, and provide richer methods that make use of Scala’s more advanced features.
Implicits are a powerful feature and are mistrusted by some.
The key to implicits in Scala are knowing how and when to use them.
They’re primarily used to automatically convert from one type to another as needed, but can also be used to limited forms of compiler time metaprogramming.
To use, implicits must be associated with a lexical scope.
This can be done via companion objects or by explicitly importing them.
The implicit keyword is used in two different ways in Scala.
First it’s used to identify and create arguments that are automatically passed when found in the scope.
This can be used to lexically scope certain features of an API.
As implicits also have a lookup policy, the inheritance linearization, they can be used to change the return type of methods.
This allows some advanced APIs and type-system tricks such as that used in the Scala collections API.
The implicit keyword can also be used to convert from one type to another.
This occurs in two places, the first when passing a parameter to a function.
If Scala detects that a different type is needed, it will check the type hierarchy and then look for an implicit conversion to apply to the parameter.
An implicit conversion is a method, marked implicit, that takes one argument and returns something.
The second place where Scala will perform an implicit conversion is when a method is called against a particular type.
If the compiler can’t find the desired method, it will apply implicit.
These features combine an expressive syntax with Scala, despite its advanced type system.
Creating expressive libraries requires a deep understanding of the type system, as well as thorough knowledge of implicit conversions.
The type system also interoperates well with Java, which is a critical design for Scala.
One of Scala’s draws is its seamless integration with Java and the JVM.
Scala provides a rich compatibility with Java, such that Java classes can be mapped directly to Scala classes.
The tightness of this interaction makes migrating from Java to Scala rather simple, but caution should be used with some of Scala’s advanced feature sets.
Scala has some advanced features not available in Java, and care was taken in the design so that seamless Java interaction can be achieved.
For the most part, libraries written in Java can be imported into Scala as is.
Using Java libraries from Scala is seamless because Java idioms map directly into Scala idioms.
Java classes become Scala classes; Java interfaces become abstract Scala traits.
This combined with Scala’s package import mechanism and method access make Java libraries feel like natural Scala libraries, albeit with more simplistic designs.
For example, the following listing shows a Java class that has a constructor, a method, and a static helper method.
This mapping is rather natural and makes using Java libraries a seamless part of using Scala.
Even with the tight integration, Java libraries usually have a form of thin Scala wrapper that provides some of the more advanced features a Java API could not provide.
These features are apparent when trying to use Scala libraries inside Java.
Scala attempts to map its features to Java in the simplest possible fashion.
For the most part, simple Scala features map almost one-to-one with Java features (for example, classes, abstract classes, methods)
Scala has some rather advanced features that don’t map easily into Java.
Although Java statics map to Scala objects, Scala objects are instances of a singleton class.
This class name is compiled as the name of the object with a $ appended to the end.
A MODULE$ static field on this class is designed to be the sole instance.
All methods and fields can be accessed via this MODULE$ instance.
Scala also provides forwarding static methods when it can; these exist on the companion class (that is, a class with the same name as the object)
Although the static methods are unused in Scala, they provide a convenient syntax when called from Java.
Scala promotes the use of function as object, or first-class functions.
As of Java 1.6, there is no such concept in the Java language (or the JVM)
When the compiler encounters the need for passing a method as a function object, it creates an anonymous subclass of an appropriate function trait.
As traits don’t map into Java, the passing of first-class functions from Java into Scala is also inhibited but not impossible.
We’ve created an abstract class in Scala that Java can implement more easily than a function trait.
Although this eases the implementation in Java, it doesn’t make things 100% simple.
There’s still a mismatch between Java’s type system and Scala’s encoding of types that requires us to coerce the type of the function when making the Scala call, as you can see in the following listing.
It’s possible to use first-class functions and with them a more functional approach when combining Scala and Java.
As you can see, Scala can integrate well with existing Java programs and be used side by side with existing Java code.
Java–Scala interaction isn’t the only benefit of having Scala run inside the JVM; the JVM itself provides a huge benefit.
As alluded to earlier, the JVM provides many of the benefits associated with Java.
Through bytecode, libraries become distributable to many differing platforms on an as is basis.
The JVM has also been well tested in many environments and is used for large-scale enterprise deployments.
It has also been a big focus on performance of the Java platform.
The HotSpot compiler can perform various optimizations on code at runtime.
This also enables users to upgrade their JVM and immediately see performance improvements, without patches or recompiling.
The primary benefit of Scala running on the JVM is the HotSpot runtime optimizer.
This allows runtime profiling of programs, with automatic optimizations applied against the JVM bytecode.
Scala acquires these optimization “for free” by nature of running against the JVM.
Every release of the JVM improves the HotSpot compiler, and this improves the performance of Scala.
Method inlining is HotSpot’s ability to determine when it can inline a small method directly at a call-spot.
This was a favorite technique of mine in C++, and HotSpot will dynamically determine when this is optimal.
On Stack Replacement refers to HotSpot’s ability to determine that a variable could be allocated on the stack versus the heap.
I remember in C++ the big question when declaring a variable was whether to place it on the stack or the heap.
HotSpot performs escape analysis to determine if various things “escape” a certain scope.
This is primarily used to reduce locking overhead when synchronized method calls are limited to some scope, but it can be applied to other situations.
It’s the ability to determine whether an optimization did not improve performance and undo that optimization, allowing others to be applied.
These features combine into a pretty compelling picture of why new and old languages (for example, Ruby) desire to run on the JVM.
In this chapter, you’ve learned a bit about the philosophy of Scala.
Scala was designed with the idea of blending various concepts from other languages.
Scala blends functional and object-oriented programming, although this has been done in Java as well.
Scala made choices about syntax that drastically reduced the verbosity of the language and enabled some powerful features to be elegantly expressed, such as type inference.
Finally, Scala has tight integration with Java and runs on top of the Java virtual machine, which is perhaps the single most important aspect to make Scala relevant to us.
It can be utilized in our day-to-day jobs with little cost.
As Scala blends various concepts, users of Scala will find themselves striking a balance among functional programming techniques, object orientation, integration with existing Java applications, expressive library APIs, and enforcing requirements through the type system.
Often the best course of action is determined by the requirements at hand.
It’s the intersection of competing ideas where Scala thrives and also where the greatest care must be taken.
This book will help guide when to use each of these techniques.
Let’s start looking at a few key concepts every Scala developer needs to know when coding Scala.
You’ll learn about the Read Eval Print Loop and how you can use this to rapidly prototype software.
From this, we’ll spring into immutability and why it can help to greatly simplify your programs, and help them run better concurrently.
But the single most important thing Scala provides is a Read Eval Print Loop (REPL)
The REPL is an interactive shell that The core rules.
This chapter covers a few topics that every newcomer to Scala needs to know.
Not every topic is covered in depth, but we cover enough to allow you to explore the.
Learn to use the Read Eval Print Loop (REPL) compiles Scala code and returns results/type immediately.
The Scala REPL is instantiated by running scala on the command line, assuming you have Scala installed on your machine and your path is set correctly.
From now on, in code examples I’ll use the scala> prompt to imply that these were entered into the REPL.
Let’s do a few quick samples in the REPL and see what it shows us.
The first part of this expression is a variable name for the expression.
The next part of the result expression (after the :) is the static type of the expression.
The last part of the result expression is the stringified value of the result.
This normally comes from calling the toString method defined on all classes within the JVM.
As you can see, the REPL is a powerful way to test the Scala language and its type system.
Most build tools also include a mechanism to start the REPL with the same classpath as your current working project.
This means libraries and compiled classes from your project are available within the REPL.
You can make API calls and remote server hits inside the REPL.
This can be a great way to test out a web service or REST API in a quick manner.
This leads to what I refer to as experiment-driven development.
Experiment-driven development is where you, the developer, first spend some time experimenting with a live interpreter or REPL before writing tests or production code.
This gives you time to fully understand the external pieces of software you’re interacting with and get a feel for the comings and goings of data within that API.
It’s a great way to learn about a new web service or RESTful API that has just been published, that latest Apache library, or even learn about something one of your coworkers have written.
After determining the workings of the API, you can then better write your own code.
If you also ascribe to test-driven development, this means that you would then write your tests.
Scala provides the REPL tool so every developer can toy around in the language before committing any final code.
It’s by far the most useful tool in the Scala ecosystem.
There has been a big push for developers to embrace test-driven development (TDD)
This is an approach to development where one writes the unit tests first, and then any implementation of those classes.
You don’t always know what your API should be before you write the tests.
Part of TDD is defining the API through the tests.
It allows you to see your code in context and get a feel for whether it’s something you would want to use.
Strongly typed languages can present more issues than dynamic languages with TDD because of expressiveness.
Using the REPL, experiment-driven development brings this API definition phase before test generation, allowing a developer to ensure an API is possible in the type system.
Scala is a strongly typed language with flexible syntax, and as such sometimes requires some finagling with the type system to attain the API you desire.
Because a lot of developers don’t have strong type theory backgrounds, this often requires more experimentation.
Experiment-driven development is about experimenting in the REPL with the type system to utilize types as effectively as possible in your API.
Experiment-driven design is more about adding larger features or domains into your code, rather than new methods or bug fixes.
Experiment-driven design can also help drastically when defining domain-specific languages (DSLs)
A DSL is a pseudo programming language that deals with a particular domain.
This language is specific to the domain at hand—for example, querying for data from a database.
A DSL may be either internal, as seen in many Scala libraries, or external like SQL.
In Scala, it is popular among library developers to create DSLs covering the same domain as the library.
For example, the Scala actors library defines a DSL for sending and receiving messages in a thread-safe manner.
One of the challenges when defining a DSL in Scala is to make effective use of the type system.
A good type-safe DSL can be expressive and easy to read and can catch many programming errors at compiler time rather then runtime.
Also having static knowledge of types can drastically improve performance.
Learn to use the Read Eval Print Loop (REPL) iment with how to express a particular domain and make sure that expression will compile.
When developing Scala, one finds himself adopting the following creative flow:
When used effectively, experiment-driven development can drastically improve the quality of your API.
It will also help you become more comfortable with Scala syntax as you progress.
The biggest issue remaining is that not every possible API in Scala is expressible in the REPL.
This is because the REPL is interpreted on the fly, and it eagerly parses input.
The Scala REPL attempts to parse input as soon as it possibly can.
This, and a few other limitations, means that there are some things that are hard to impossible to express with the REPL.
One important function to express are companion objects and classes.
A companion object and class are a set of object and class definitions that use the same name.
This is easy to accomplish when compiling files; declare the object and class like so:
These statements will also evaluate in the REPL, but they won’t function as companions of each other.
To prove this, in the following listing let’s do something that a companion object can do, that a regular object can’t: access private variables on the class.
To fix this issue, we need to embed these objects in some other accessible scope within the interpreter.
In the following listing, let’s place them inside some scope so we can interpret/compile the class and companion object at the same time:
This gives us our accessible scope, and defers the REPL’s compilation until the close of the holder object.
We then have to import Foo from the holder object.
This allows us to test/define companion objects within the REPL.
Even working around eager parsing, there are still some language features that the REPL can’t reproduce.
Most of these issues revolve around packages, package objects, and package visibility restrictions.
In particular, you’re unable to effectively create a package or package object in the REPL the same way you can within a source file.
This also means that other language features dealing with packages, particularly visibility restrictions using the private keyword, are also inexpressible.
Usually packages are used to namespace your code and separate it from other libraries you might use.
This isn’t normally needed inside the REPL, but there may be times when you’re toying with some advanced feature of Scala—say, package objects and implicit resolutionand you would like to do some experiment-driven development.
In this case, you can’t express what you want solely in the REPL; see the following listing.
As stated before, most build utilities allow you to create a Scala REPL session against your current project.
As a last resort you can toy with some concept in a Scala file, recompile, and restart your REPL session.
The JRebel team has graciously provided free licenses when used with Scala.
This tool, combined with some form of continuous compilation, available in most Scala build tools, will allow you to modify your project files and have the changed behavior be immediately available within you REPL session.
The Simple Build Tool (http://mng.bz/2f7Q) provides the cc target for continuous compilation.
Whatever build tool you use to start a REPL session must be integrated with a JRebel classloader so that dynamic class reloading can happen.
This technique is a bit detailed and prone to change, so please check your build tool’s documentation or the JRebel website for help.
The REPL will allow you to try out Scala code and get a real feel for what you’re doing before attempting to create some large complicated system.
It’s often important in software development to get a slightly more than cursory knowledge of a system before tackling a new feature.
The Scala REPL should allow you to do so with a minimal amount of time, allowing you to improve your development skills.
This entire book is enriched with examples of code from the REPL, as it’s the best tool to teach and learn Scala.
I often find myself running sample programs completely via the REPL before I even create some kind of “main” method, or a unit test, as is standard within Java development.
To help encourage this, the book favors demonstrating concepts in the REPL using a few simple scripts.
Please feel free to follow along with a REPL of your own.
All the major IDEs have support for running the Scala REPL and most of the major build tools.
Consult the documentation of the Scala integration for your IDE or build system for details on how to ensure a good REPL experience.
For bonus points, use the REPL in combination with a graphical debugger.
The REPL is also a great way to begin learning how to use expressions rather than statements.
In Scala, a lot of code can be written as small methods of one expression.
This style is not only elegant, but helps in code maintenance.
What’s the difference between an expression and a statement? A statement is something that executes, but an expression is something that evaluates.
Expressions are blocks of code that evaluate to a value.
This means that if the control were to branch, each of these branches must evaluate to a value as well.
The if clause is a great example; this checks a conditional expression and returns one expression or another, depending on the value of the conditional expression.
As you can see, in Scala an if block is an expression.
Our first if block returns "true string", the true expression.
The second if block returns hello, the result of the false expression.
To accomplish something similar in Java, you would use the ?: syntax as shown in the following:
String x = true ? "true string" : "false string"
An if block in Java is therefore distinct from a ?: expression in that it doesn’t evaluate to a value.
You can’t assign the result of an if block in Java, but Scala has unified the concept of ?: with its if blocks.
Scala has no ?: syntax; you merely use if blocks.
In fact, Scala has few statements that do not return values from their last expression.
One of the keys to using expressions is realizing that there’s no need for a return statement.
An expression evaluates to a value, so there’s no need to return.
While programming in Java, there was a common practice of having a single point of return for any method.
This meant that if there was some kind of conditional logic, the developer would create a variable that contained the eventual return value.
As the method flowed, this variable would be updated with what the method should return.
The last line in every method would be a return statement.
As you can see, the result variable is used to store the final result.
The code falls through a pattern match, assigning error strings as appropriate, then returns the result variable.
The type of the value is determined as a common super type from all case statement returns.
Pattern matching also throws an exception if no pattern is matched, so we’re guaranteed a return or error here.
First, we changed the result variable to a val and let the type inferencer determine the type.
This is because we no longer have to change the val after assignment; the pattern match should determine the unique value.
Therefore, we reduced the size and complexity of the code, and we increased immutability in the program.
Immutability refers to the unchanging state of an object or variable; it’s the opposite of mutability.
Mutability is the ability of an object or variable to change or mutate during its lifetime.
The second thing we’ve done is remove any kind of assignment from the case statements.
The last expression in a case statement is the “result” of that case statement.
We could have embedded further logic in each case statement if necessary, as long as we eventually had some kind of expression at the bottom.
The compiler will also warn us if we accidentally forget to return, or somehow return the wrong type.
The code is looking a lot more concise, but we can still improve it somewhat.
In fact, for the createErrorMessage method, we can remove the intermediate result variable altogether.
Note how we haven’t even opened up a code block for the method? The pattern match is the only statement in the method, and it returns an expression of type String.
Note how much more concise and expressive the code is.
Also note that the compiler will warn us of any type infractions or unreachable case statements.
This is because code utilizing mutable objects tends to be written in an imperative style.
Imperative coding is a style that you’re probably used to.
Many early languages such as C, Fortran, and Pascal are imperative.
Imperative code tends to be made of statements, not expressions.
Then statements are executed that “mutate” or change the state of an object.
In the case of languages that don’t have objects, the same mechanisms apply, except with variables and structures.
Note how a vector is constructed and then mutated via the magnify method.
In the case of object mutation, what value should be returned? One option is to return the object that was just mutated, as in the following listing.
This may seem a great option but has some serious drawbacks.
In particular, it can get confusing determining when an object is being mutated, especially when combined with immutable objects.
See if you can determine what values should print at the end.
Assume that the - method defined on Vector2D follows the mathematical definition.
This means that the operations are modifying the variables in the order they’re used.
Next, we subtract y from x to give x the value (2.0,4.0)
Why? Because the right-hand side of the - method must be evaluated next, and (x-y) is the first part of this expression.
Finally we subtract x from itself, bringing the resulting value to (0.0,0.0)
Why? Because the expression on the left-hand side of the - and the right-hand side of the minus both start with the x variable.
Because we’re using mutability, this means that each expression returns x itself.
This is particularly the case with operator overloading, as with the previous example.
Code has a common task where you need to look up values on an object based on some value.
Let’s consider a simple example of looking up the action to perform based on a Menu button click.
When we click the Menu button, we receive an event from our event system.
This event is marked with the identifier of the button pressed.
We want to perform some action and return a status.
Note how we’re mutating the objects in place and then returning our result.
Instead of an explicit return statement, we state the expression we wish to return.
You can see the code here is more succinct than creating a variable to hold the result variable.
You’ll also notice that mixing mutation statements with our expressions has reduced some of the clarity of the code.
This is one of the reasons why it’s better to prefer immutable code—the topic of our next section.
You can now reduce clutter and increase expressiveness within your code.
Immutability is a term to denote that something doesn’t change, in this case the state of an object, once constructed.
Immutability, in programming, refers to the unchanging state of objects after construction.
This is one of the capstones of functional programming and a recommended practice for object-oriented design on the JVM.
Scala is no exception here and prefers immutability in design, making it the default in many cases.
In this section, you’ll learn how immutability can help when dealing with equality issues or concurrent programs.
Creating immutable classes drastically reduces the number of potential runtime issues.
The most important thing to realize in Scala is that there’s a difference between an immutable object and an immutable reference.
Defining a variable as a val means that it’s an immutable reference.
All method parameters are immutable references, and class arguments default to being immutable references.
The only way to create a mutable variable is through the var syntax.
The immutability of the reference doesn’t affect whether the object referred to is immutable.
You can have a mutable reference to an immutable object and vice versa.
This means it’s important to know whether the object itself is immutable or mutable.
In general, it’s safe to assume that if the documentation states an object is immutable, then it is;
The Scala standard library helps make the delineation obvious in its collections classes by having parallel package hierarchies, one for immutable classes and one for mutable classes.
In Scala immutability is important because it can help programmers reason through their code.
If an object’s state doesn’t change, then you can determine where objects are created to see where state changes.
It can also simplify methods that are based on the state of an object.
This benefit is particularly evident when defining equality or writing concurrent programs.
One critical reason to prefer immutability is the simplification of object equality.
If an object won’t change state during its lifetime, one can create an equals implementation that is both deep and correct for any object of that type.
This is also critical when creating a hash function for objects.
A hash function is one that returns a simplified representation of an object, usually an integer, that can be used to quickly identify the object.
A good hash function and equals method are usually paired, if not through code, then in logical definition.
If state changes during the lifetime of an object, it can ruin any hash code that was generated for the object.
This in turn can affect the equality tests of the object.
The following listing shows a simple example of a twodimensional geometric point class.
It consists of x and y values, corresponding to locations on the x and y axes.
It also has a move method, which is used to move the point around the two-dimensional plane.
Imagine we want to tie labels to particular points on this Point2D to string values.
For efficient lookup, we’re going to use a hashing function and a HashMap.
Let’s try the simplest possible thing, hashing with the x and y variables directly, in the following listing.
Things appear to be working exactly as we want—until we attempt to construct a new point object with the same values as point x.
This point should hash into the same section of the map, but the equality check will fail because we haven’t created our own equality method.
By default, Scala uses object location equality and hashing, but we’ve only overridden the hash code.
Object location equality is using the address in memory for an object as the only factor to determine if two objects are equal.
In our Point2 case, object location equality can be a quick check for equality, but we can also make use of the x and y locations to check for equality.
But Scala also abstracts primitives such that they appear as full objects.
The compiler will box and unbox the primitives as needed for you.
As the hashCode and equals methods are defined on AnyRef, Scala provides the methods ## and == that you can use for both AnyRef and AnyVal.
Let’s implement our own equality method in the following listing and see what the results are.
The implementation of equals may look strange, but will be covered in more detail in section 2.5.2
For now, note that the strictEquals helper method compares the x and y values directly.
This means that two points are considered equal if they are in the same location.
We’ve now tied our equals and hashCode methods to the same criteria, the x and y values.
Let’s throw our x and y values into a HashMap again, only this time we’re going to move the x value, and see what happens to the label attached to it.
What happened to the label attached to x? We placed it into the HashMap when x has a value of (1,1)
Well, what if we try to look up the value using a new point, z, that still has a hash code of 32? It also fails, because x and z aren’t equal according to our rules.
You see, a HashMap uses the hash at the time of insertion to store values but doesn’t update when an object’s state mutates.
This means we’ve lost our label for x when using hash-based lookup, but we can still retrieve the value when traversing the map or using traversal algorithms:
As you can see, this behavior is rather confusing, and can cause no end of strife when debugging.
As such, it’s generally recommended to ensure the following constraints when implementing equality:
As you can see, the second constraint implies that all criteria used in creating a hashCode should not change with the life of an object.
The last statement, when applicable, means that an object’s hash and equals method should be computed using its own internal state.
Combine this with the first statement, and you find that the only way to satisfy these requirements is through the use of immutable objects.
If the state of an object never changes, it’s acceptable to use it in computing a hash code or when testing equality.
You can also serialize the object to another JVM and continue to have a consistent hash code and equality.
You may be wondering, why do I care about sending objects to other JVMs? My software will never run on more than one JVM.
In fact, my software runs on a mobile device, where resources are critical.
The problem with that thinking is that serializing an object to another JVM need not be done in real time.
I could save some program state to disk and read it back later.
This is effectively the same as sending something to another JVM.
Although you may not be directly sending it over the network, you’re sending it through time, where the JVM of today is the writer of data, and the JVM started tomorrow is the user of the data.
In these instances, having a hash code and equals implementation is critical.
Remove this constraint, and there are only two simple ways to satisfy the first two constraints:
As you can see, this means that something in the object must be immutable.
Making the entire object immutable simplifies this whole process greatly.
Immutability doesn’t merely simplify object equality; it can also simplify concurrent access of data.
Programs are becoming increasingly parallelized, and processors are splitting into multiple cores.
The need to run concurrent threads of control in programs is growing across all forms of computing.
Traditionally, this meant using creative means to protect access to shared data across these various threads of control.
Immutability can help share state while reducing the need for locking.
Threads that wish to read data can’t do so unless the lock is available to obtain.
Even using read-write locks can cause issues, because a writer may be slow in preventing readers from accessing the data they desire.
On the JVM, there are optimizations in the JIT to attempt to avoid locks when they aren’t necessary.
In general, you want to have as few locks in your software as possible, but you want enough to encourage a high degree of parallelism.
The more you can design your code to avoid locking the better.
For instance, let’s try to measure the effect of locking on an algorithm and see if we can design a new algorithm that reduces the amount of locking.
We’ll create an index service that we can query to find particular items by their key.
The service will also allow users to add new items into the index.
We expect to have many users looking up values and a smaller amount of users adding additional content to the index.
The service is made up of two methods: lookUp, which will look up values in the index by the key, and insert, which will insert new values into the service.
Let’s implement this using a locking and a mutable HashMap.
The first is the currentIndex, which is a reference to the mutable HashMap that we use to store values.
The lookUp and insert methods are both surrounded by a synchronized block, which synchronizes against the MutableService.
You’ll notice that all operations on a MutableService require locking.
But given what was stated about the usage of this service, the lookUp method will be called far more often than the insert method.
A read-write lock could help in this situation, but let’s look at using immutability instead.
We’ll change the currentIndex to be an ImmutableHashMap that get overwritten when the insert method is called.
The lookUp method can then be free of any locking, as shown in the following code:
The first thing to notice is that the currentIndex is a mutable reference to an immutable variable.
We update this reference every time there’s an insert operation.
The second thing to notice is that this service isn’t completely immutable.
All that’s happened is the reduction of locking by utilizing an immutable HashMap.
This simple change can cause a drastic improvement in running time.
I’ve set up a simple micro-performance benchmark suite for these two classes.
We construct a set of tasks that will write items into the service and a set of tasks that will attempt to read items from the index.
We then interleave the two sets of tasks and submit them to a queue of two threads for execution.
We time the speed that this entire process takes and record the results.
The y-axis is the execution time of running a test.
The x-axis corresponds to the number of insert/lookUp tasks submitted to the thread pools.
You’ll notice that the mutable service’s execution time grows faster than the immutable service’s execution time.
This graph certainly shows that extra locking can severely impact performance.
But note that the execution times of this test can greatly vary.
Due to the uncertainty of parallelism, this graph could look anywhere from the one shown above to a graph where the immutable service and mutable service execution times track relatively the same.
In general, the MutableService implementation was slower than the ImmutableService, but don’t judge performance from one graph or on execution alone.
Figure 2.3 shows another graph where you can see, for one particular test, the MutableService had all of its stars align and ran with a drastically reduced locking overhead.
You can see in the preceding run where a single test case had all its timing align so that the MutableService could outperform the ImmutableService.
Though possible for this specific case, the general case involved the ImmutableService outperforming the MutableService.
Prefer immutability it appears that the ImmutableService will perform better in the general case and not suffer from random contention slowdowns.
The most important thing to realize is that immutable objects can be passed among many threads without fear of contention.
The ability to remove locks, and all the potential bugs associated with them, can drastically improve the stability of a codebase.
Combined with the improved reasoning one can get, as seen with the equals method, immutability is something to strive to maintain within a codebase.
Immutability can ease concurrent development by reducing the amount of protection a developer must use when interacting with immutable objects.
Scala does its best to discourage the use of null in general programming.
It does this through the scala.Option class found in the standard library.
An Option can be considered a container of something or nothing.
This is done through the two subclasses of Option: Some and None.
None denotes an empty container, a role similar to what Nil plays for List.
While it was habit in Java to initialize values to null, Scala provides an Option type for the same purpose.
Option is self-documenting for developers and, used correctly, can prevent unintended null pointer exceptions when using Scala.
In Java, and other languages that allow null, null is often used as a placeholder to denote a nonfatal error as a return value or to denote that a variable isn’t yet initialized.
In Scala, one can denote this through the None subclass of Option.
Conversely, one can denote an initialized, or nonfatal variable state through the Some subclass of Option.
Let’s look at the usage of these two classes in the following listing.
An Option containing no value can be constructed via the None object.
An Option that contains a value is created via the Some factory method.
Option provides many differing ways of retrieving values from its inside.
The get method will attempt to access the value stored in an Option and will throw an exception if it’s empty.
This is similar to accessing nullable values within other languages.
The getOrElse method will attempt to access the value stored in an Option, if one exists; otherwise it will return the value supplied to the method.
Scala provides a factory method on the Option companion object that will convert from a Java style reference, where null implies an empty variable, into an Option where this is more explicit.
The Option factory method will take a variable and create a None object if the input was null, or it will create a Some if the input was initialized.
This makes it rather easy to take inputs from an untrusted source—that is, another JVM language—and wrap them into Options.
You might be asking yourself why you would want to do this.
Isn’t checking for null just as simple in code? Option provides advanced features that make it far more ideal than using null and if checks.
The greatest feature of Option is that you can treat it as a collection.
This means you can perform the standard map, flatMap, and foreach methods, as well as utilize it inside a for expression.
This helps ensure a nice concise syntax, and it opens a variety of different methods to handling uninitialized values.
Let’s look at some common issues solved using null and their solutions using Option, starting with creating an object or returning a default.
You’ll have many times in code when you’ll need to construct something if some other variable exists, or supply some sort of default.
Let’s pretend that we have an application that requires some kind of temporary file storage for its execution.
The application is designed so that a user may optionally specify a directory to store temporary files on the command line.
If the user doesn’t specify a new file, if the argument provided by the user is not a real directory, or if they didn’t provide a directory, then we want to return a sensible default temporary directory.
The following listing shows a method that will return this temporary directory:
The first thing we do is use the map method on Option to create a java.io.File if there was a parameter.
Next, we make sure that this newly constructed file object is a directory.
This will check whether the value in an Option abides by some predicate, and if not, convert to a None.
Finally, we check to see if we have a value in the Option; otherwise we return the default temporary directory.
This enables a powerful set of checks without resorting to nested if statements or blocks.
Sometimes we would like a block, such as when we want to execute a block of code based on the availability of a particular parameter.
Option can be used to execute a block of code if the Option contains a value.
This is done through the foreach method, which, as expected, iterates over all the elements in the Option.
As an Option can only contain zero or one value, this means the block either executes or is ignored.
As you can see, this looks like a normal “iterate over a collection” control block.
The syntax remains similar when we need to iterate over several variables.
Let’s look at the case where we have some kind of Java servlet framework, and we want to be able to authenticate users.
If authentication is possible, we want to inject our security token into the HttpSession so that later filters and servlets can check access privileges for this user, as in the following listing.
Note that you can embed conditional logic in a for expression.
This helps keep less nested logical blocks within your program.
Another important consideration is that all the helper methods do not need to use the Option class.
Use None instead of null front-line defense for potentially uninitialized variables, but it doesn’t need to pollute the rest of your code.
In Scala, Option as an argument implies that something may not be initialized.
If a method takes a value that is not labeled as an Option, you should not pass it null or uninitialized parameters.
Scala’s for expression syntax is rather robust, even allowing you to produce values, rather then execute code blocks.
This is handy when you have a set of potentially uninitialized parameters that you want to transform into something else.
Sometimes we want to transform a set of potentially uninitialized values so that we have to deal with only one.
To do this, we need to use a for expression again, but this time using a yield.
The following listing shows a case where a user has input some database credentials, or we attempted to read them from an encrypted location, and we want to create a database connection using these parameters.
We don’t want to deal with failure in our function, as this is a utility function that won’t have access to the user.
In this case, we’d like to transform our database connection configuration parameters into a single option containing our database.
What if we wanted to abstract this such that we can take any function and create one that’s option-friendly in the same manner? The following listing shows what we’ll call the “lift” function.
The lift3 method looks somewhat like our earlier createConnection method, except that it takes a function as its sole parameter.
The Function3 trait represents a function that takes three arguments and returns a result.
The lift3 function takes a function of three arguments as input and outputs a new function of three arguments.
As you can see from the REPL output, we can use this against existing functions to create option-friendly functions.
This technique works well when used with the “encapsulation” of uninitialized variables.
You can write most of your code, even utility methods, assuming that everything is initialized, and then lift these functions into Option-friendly variants when needed.
One important thing to mention is that Option derives its equality and hashCode from what it contains.
In Scala, understanding equality and hashCode, especially in a polymorphic setting, is very important.
Let’s discuss how to properly implement an equals and hashCode function in Scala.
This can be tricky in a polymorphic language, but can be done by following some basic rules.
In general, it’s best to avoid having multiple concrete levels with classes that also need equality stronger then referential equality.
In some cases, classes only need referential equality, the ability to differentiate two objects to determine if they’re the same instance.
But if the equality comparison needs to determine if two differing instances are equivalent and there are multiple concrete hierarchies, then things get a bit more tricky.
To understand this issue, we’ll look at how to write a good equality method.
We’d like to construct a time line, or calendar, widget.
This widget needs to display dates, times, and time ranges as well as associated events with each day.
The fundamental concept in this library is going to be an InstantaneousTime.
InstantaneousTime is a class that represents a particular discrete time within the time series.
We could use the java.util.Date class, but we’d prefer something that’s immutable, as we’ve just learned how this can help simplify writing good equals and hashCode methods.
We’ll assume that all other times can be formatted into this representation and that time zones are an orthogonal concern to representation.
We’re also going to make the following common assumptions about our equality usage in the application:
When equals is called and it will return true, it’s because both objects are the same reference.
Most calls to equals result in a return of false.
Let’s take a first crack at the class and a simple equals and hashCode method pair in the following listing, and see what this looks like.
As this is the only data value in the class, and it’s immutable, equals and hashCode will be based on this value.
When implementing an equals method within the JVM, it’s usually more performant to test referential equality before doing any sort of deep equality check.
For a sufficiently complex class, it can drastically help performance, but this class doesn’t need it.
The next piece to a good equals method is usually using the hashCode for an early false check.
Given a sufficiently sparse and easy to compute hashCode, this would be a good idea.
Once again, in this class it’s not necessary, but in a sufficiently complex class, this can be performant.
In Scala, when calling the equals or hashCode method it’s better to use ## and ==
But the equals and hashCode method are used when overriding the behavior.
This split provides better runtime consistency and still retains Java interoperability.
This class helps us illustrate two principles: the importance of a good equality method and always challenge the assumptions of your code.
In this case, the “best practice” equality method, while great for a sufficiently complex class, provides little benefit for this simple class.
Our implementation of equals suffers from yet another flaw, that of polymorphism.
In general, it’s best to avoid polymorphism with types requiring deep equality.
Scala no longer supports subclassing case classes for this very reason.
But there are still times in code where this is useful or even necessary.
To do so, we need to ensure that we’ve implemented our equality comparisons correctly, keeping polymorphism in mind and utilizing it in our solution.
Let’s create a subclass of InstantaneousTime that also stores labels.
This is the class we’ll use to save events in our timeline, so we’ll call it Event.
We’ll make the assumption that events on the same day will hash into the same bucket, and hence have the same hashCode, but equality will also include the name of the event.
Let’s take a crack at an implementation in the following listing.
We’ve dropped the hashCode early exit in our code, as checking the repr member is just as performant in our particular class.
The other thing you’ll notice is that we’ve changed the pattern match so that only two Event objects can be equal to each other.
Let’s try to use this in the REPL in the following listing.
What’s happened? The old class is using the old implementation of the equality method, and therefore doesn’t check for the new name field.
We need to modify our original equality method in the base class to account for the fact that subclasses may wish to modify the meaning of equality.
In Scala, there’s a scala.Equals trait that can help us fix this issue.
The Equals trait defines a canEqual method that’s used in tandem with the standard equals method.
The canEqual method allows subclasses to opt out of their parent classes’ equality implementation.
This is done by allowing the other parameter in the equals method an opportunity to cause an equality failure.
To do so, we override canEqual in our subclass with whatever rejection criteria our overridden equals method has.
Let’s modify our classes to account for polymorphism using these two methods in the following listing.
The first thing to do is implement canEqual on InstantaneousTime to return true if the other object is also an InstantaneousTime.
Next let’s account for the other object’s canEqual result in the equality implementation.
Finally, an overridden canEqual in the Event class will only allow equality with other Events.
This allows a subclass to do so without the usual dangers associated with a parent class equals method returning true while a subclass would return false for the same two objects.
Let’s look at our earlier REPL session and see if the new equals methods behave better, as in the following listing.
We can now write a general equals method that performs well with general assumptions about our programs, and we can handle the case where our classes are also polymorphic.
In this chapter, we looked at the first crucial items for using Scala.
Leveraging the REPL to do rapid prototyping is crucial to any successful Scala developer.
Option can also help improve reasonability of the code by clearly delineating where uninitialized values are accepted.
Also, writing a good equality method in the presence of polymorphism can be difficult.
All of these practices can help make the first steps in Scala successful.
For continued success, let’s look at code style and how to avoid running into issues with Scala’s parser.
This chapter presents style suggestions that will help you avoid compiler or runtime errors.
Style issues are usually a “holy war” among developers, who each has her own opinions.
But there are certain things that Scala allows from a style perspective that can cause logic or runtime issues in your programs.
This chapter doesn’t try to proselytize you into whether you should place spaces between parenthesis or what the best number of spaces for indentation is.
This chapter merely presents a few styles that will cause real issues in Scala, and why you should modify your preferred style accordingly, if needed.
We discuss why placing opening braces for block expressions can convey different meanings to the compiler.
Operator notation can cause issues if the compiler Modicum of stylecoding conventions.
Also, when naming variables in Scala, there are some names that are syntactically valid but will cause compiler or runtime errors.
Finally, we discuss the benefits of compile-time warnings and how you can use annotations to increase the helpfulness of the Scala compiler.
I’ve found that my style when writing in a new language tends to borrow heavily from styles I use in other languages until I’ve learned the language well.
A lot of users come from Java or Ruby languages and you can see this influence in the syntax.
Over time, this style will change and adjust to accommodate the new language as certain guidelines are found to cause issues in the new language.
As such, it’s important to understand exactly where your style is coming from and whether that style makes sense in the new language.
In fact, it’s not just the language itself that dictates style.
You must consider many human social interactions, especially if you work in a company with a large developer base.
One thing that always frustrated me when using C++ was coding conventions that were developed before the ready availability of cheap mature C++ IDEs.
An IDE can negate the need for a lot of coding conventions by visually altering code based on a good semantic parse.
IDEs can also allow developers to click-through method calls into method definitions or declarations to quickly get a feel for what’s going on in code.
A good modern IDE makes a lot of “standard practice” coding conventions unnecessary.
But this doesn’t erase the need for any coding conventions.
Coding conventions do serve a few purposes, which can be boiled down into three categories: code discovery, uniformity, and error prevention.
Error prevention conventions are style rules that help avoid bugs in production code.
This could be anything from marking method arguments as final in Java to marking all single argument constructors as explicit in C++
The goal of these style rules will be obvious to any experienced developer of that language.
Uniformity rules are about keeping the look of code the same across a project.
These are a necessary evil in development workshops and the cause of style wars.
Without them, version control history can become misaligned as developers fight to push their own personal style, or lack thereof.
With them, moving between source files requires little “readability” mental adjustments.
These rules are things like how many spaces to put between parentheses.
Code discovery rules are about enabling engineers to easily reason through code and figure out what another developer intended.
These rules usually take the form of variable naming rules, such as placing m_ in front of member variables or prefixing interfaces with a capital I.
Code discovery should align with the development environments that are expected in a team.
If a team is using vanilla VI for editing, it will be more useful to add more code discovery guidelines than another project.
Avoid coding conventions from other languages power users, it would need less code discovery rules, as the IDE will provide many alternative means of improving discovery.
The way you should develop coding conventions for a team is to:
These will usually be copied from other projects in the same language, but you may need to create new rules.
Develop discovery related rules, such as how to name packages and where to place source files.
These should match the development environments used by team members.
Follow up the rules defined above with any uniformity related rules required for the team.
These rules vary from team to team and can be fun to agree upon.
When creating uniformity guidelines, you should keep in mind automated tool support.
This can help new engineers on the project save time until they become accustomed to the style.
For Scala, you should check out the Scalariform project http://mng.bz/78G9, which is a tool to automatically refactor Scala code given a set of style rules.
The issue nowadays is that most developers have a set of coding guidelines they prefer and pull them from project to project regardless of the language or the team.
When starting a new project and developing new coding standards, make sure you don’t just pull conventions from previous languages.
Scala syntax isn’t a direct C-clone; there are some pitfalls that certain coding styles will create in the language.
We show an example with defining code blocks in Scala.
In many languages, the choice between same line and next line opening brace doesn’t matter.
This is not the case in Scala, where semicolon inference can cause issues in a few key places.
The easiest way to show the issue is with the definition of methods.
The more idiomatic Scala convention for a function as simple as triple is to define it on one line with no code block.
This is a toy example though, so we’ll assume you have a good enough reason to use a code block, or your coding convention specifies you always having code blocks.
Now let’s try to make a function that returns Unit using the convenience syntax:
This method will compile fine when used from the interpretive session, however it fails utterly when used inside a class, object, or trait definition in Scala 2.7.x and below.
To reproduce the behavior in Scala 2.8, we add another line between the method name and the opening brace.
In many C-style languages, including Java, this change is acceptable.
In Scala, we see the issue, as shown in the following listing:
Inside of the FooHolder class definition block, Scala sees the def foo() line of code as an abstract method.
This is because it doesn’t catch the opening brace on the next line, so it assumes def foo() is a complete line.
When it encounters the block expression, it assumes it found a new anonymous code block that should be executed in the construction of the class.
A simple solution to this problem exists: Add a new style guideline that requires the = syntax for all method definitions.
This should solve any issues you might experience with opening brackets on the next line.
The = added after the def foo(): Unit tells the compiler that you’re expecting an expression that contains the body of the foo function.
The compile will then continue looking in the file for the code block.
In Scala, if statements don’t require code blocks at all.
This means the same kind of behavior could occur on an if statement if not properly structured.
Luckily in that case, the compiler will catch and flag an error.
In an interpretive session, you can’t enter this code because it will compile at the end of the first code block (the if statement) because it assumes the statement is complete.
Most good tools allow you to automatically start an interpretive session against a compiled instance of your project.
This means you wouldn’t have to cut and paste code from your project into the session; however, in practice I find that sometimes my project isn’t compiling and I want to test out a feature.
In this case, I have to edit the files before pasting into the interpretive session.
Make sure when you’re setting up a project, especially in a language you haven’t used extensively before, that you rethink your style guidelines and choose ones that fit the new language and the environment you will be developing in.
Don’t merely pull what worked before in Language Foo and assume it will work well in Scala.
A collaborative effort is in place to create a “good enough” style guide for Scala.
This style guide should act as a good starting point and is currently located at http:// mng.bz/48C2
One style adjustment that can drastically help in Scala is to dangle operators at the end of lines.
A dangling operator is an operator, such as + or - that’s the last nonwhitespace character in a line of code.
Dangling operators will help the compiler determine the true end of a statement.
Earlier, we described how this is important for block expressions.
The concept works just as well with other types of expressions in Scala.
The Test class has a foo method that’s attempting to create a large string.
Rather than having dangling aggregation operators, the + operator is found on the next line.
A simple translation of this to Scala will fail to compile.
Again, this is because the compiler is inferring the end of line before it should.
To solve this issue, we have two options: dangling operators or parentheses.
A dangling operator is an operator that ends a line, letting the compiler know there’s more to come, as shown in the following listing:
Dangling operators have the advantage of maintaining a minimal amount of syntax.
An alternative to dangling operators is wrapping expressions in parentheses.
You wrap any expression that spans multiple lines in parentheses.
This has the advantage of allowing potentially arbitrary amount of whitespace between members of the expression.
Whichever one of these style guidelines you choose is up to you and your development shop.
I prefer dangling operators, but both options are valid Scala syntax and will help you avoid parsing issues.
Now that we’ve discussed working around inference in the compiler, let’s discuss another way to avoid issues in the compiler: the naming of variables.
One of the most common adages in any programming language is to use meaningful argument or variable names.
Code clarity is a commonly ascribed benefit of meaningful argument names.
Meaningful names can help take an arcane piece of code and turn it into something a new developer can learn in moments.
Some variables exist for which it is hard to determine appropriate names.
In my experience this usually comes when implementing some kind of mathematical algorithm, like fast Fourier transforms, where the domain has well-known variable names.
In this case, it’s far better to use the standard symbols rather than invent your own names.
In the case of Fourier transforms, the equation is shown in figure 3.1
When implementing a Fourier transform, using a variable named N to represent the size of the input data, n to represent the index of a summing operation and k to represent the index to an output array is acceptable, as it’s the notation used in the function.
In many of languages, you end up “spelling” symbols because the language doesn’t support mathematical symbols directly.
In Scala, we can directly write ? rather than PI if we desire.
In this section well look at “reserved” characters that you shouldn’t use for variable names, as well as using named and default parameters effectively.
Reserved characters are characters the compiler reserves for internal use, but it doesn’t warn you if you use them.
This can cause issues at compile time or, even worse, runtime.
These issues could be anything from a warning message on code that’s perfectly valid, or exceptions thrown at runtime.
Scala provides a flexible naming scheme for variables and methods.
You use extended characters, if you desire to code mathematical equations directly.
This allows you to write functions that look like mathematical symbols if you’re writing some form of advanced mathematics library.
My recommendation here is to ensure that whatever characters you use in your variable and method names, make sure that most developers in your shop know how to input them on their keyboards or ensure there’s a direct key for it.
Nothing is worse than having to copy and paste special characters into a program because you desire to use them.
An example of Scala’s flexible naming is the duality of => and ? for defining closures and pattern matching.
To even use the ? character in this book, I had to look it up and paste it into my editor.
The best example of unicode and non-unicode operators comes from the Scalaz library.
Let’s look at one of the examples from the Scalaz source code:
Although random unicode characters can be frustrating for developers, there’s one character that’s easy to type that can cause real issues in code: the dollar sign ($)
Scala allows naming to be so flexible, you can even interfere with its own name mangling scheme for higher level concepts on the JVM.
Name mangling refers to the compiler altering, or mangling, the name of a class or method to translate it onto the underlying platform.
This means that if I looked at the classfile binaries Scala generates, I may not find a class with the same name as what I use in my code.
This was a common technique in C++ so that it could share a similar binary interface with C but allow for method overloading.
For Scala, name mangling is used for nested classes and helper methods.
As an example, let’s create a simple trait and object pairing and look at how Scala names the underlying JVM classes and interfaces.
When Scala has to generate anonymous function or classes, it uses a name containing the class it was defined in—the string anonfun and a number.
These strings are all joined using the $ character to create an entity.
Let’s compile a sample and see what the directory looks like afterwards.
This sample will be a simple main method that computes the average of a list of numbers, as shown in the following listing:
Let’s play a game called “break Scala’s closures.” This game will help outline the issues with using $ in parameter names, something useful for those who are interested in adding plugin functionality to Scala, but not for general developers.
Feel free to skip to section 3.3.2 if you’re not interested in this.
What happens if we define our own class that has the same mangled name as the anonymous function? Either our class or the anonymous function class will be used at runtime.
Let’s create a new Average.scala file in the following listing and use the ` syntax to create a new mischievous class and see what happens:
This compiles fine, so we know the compiler won’t catch our mischievousness.
Let’s see what happens when we try to use it in an interactive interpreted session:
The mischievous class is instantiated, as seen by the “O MY!” output.
The mischievous class is even passed into the foldLeft method as seen in the stack trace.
It isn’t until the foldLeft function attempts to use the class instance that it realizes that this class isn’t a closure.
Well, what are the odds that someone would name a class the same kind of arcane string that occurs from name mangling? Probably low, but the $ character still gives Scala some issues.
When defining nested classes, Scala also uses the $ character to mangle names, similar to Java inner classes.
We can cause similar errors by defining mischievous inner classes, as shown in the following listing:
In general then, it’s best to avoid the $ character altogether in your naming schemes.
It’s also best to avoid making an inner class with the name anonfun or $anonfun that has its own numbered inner classes, although I have no idea why you would desire to do so.
For completeness, it’s best to totally avoid the mangling schemes of the compiler.
Luckily in this case the compiler will warn that the method avg$default$1 is a duplicate.
This isn’t the most obvious error message, but then again, the method name isn’t exactly common.
So, although it’s possible to use $ in method names and class names, it can get you into trouble.
The examples I’ve posted are somewhat extreme, but illustrate that name mangling issues can be rather tricky to track down.
Scala 2.8.x brings with it the ability to use named parameters.
This means that the names you give parameters of methods become part of the public API.
Your parameter names become part of the API, and changing them can and will break clients.
Also, Scala allows users to define different parameter names in subclasses.
In Scala, parameter names are part of the API and should follow all the coding conventions used for method and variable names.
Defining named parameters in Scala is easy, it’s required syntax.
Whatever name you declare for a parameter is the name you can use when calling it.
Let’s define a simple Foo class with a single method foo, but with several parameters.
First, notice that the foo method declares defaults for all of its parameters.
This allows us to call the method without passing any arguments.
Things are more interesting when we pass arguments using their names, like when we write x.foo(two = "not two")
Scala still allows argument placement syntax, where the order of the parameters is the same in the definition site and the call site.
On this call, 0 is the first parameter and is referred to in the function as the argument one.
Mixed mode is where you can use argument placement syntax for some arguments, and named parameters for the rest.
So, why all the fuss over argument naming? Argument names become confusing with inheritance in the mix.
Scala uses the static type of a variable to bind parameter names, however the defaults are determined by the runtime type.
Say it to yourself: Names are static; values are runtime.
Child extends the foo method, but notice the naming difference.
We’ve purposely reused the same names in differing orders to be confusing, but we’ve left the implementation of the method the same.
The interesting part comes when we instantiate the Child class with the static type of Parent.
What happened? In the Child class, we defined the argument names in the reverse order of the Parent class.
The unfortunate circumstance of named parameters in Scala is that they use the static type to determine ordering.
Remember our earlier mantra: values are runtime; names are static.
Renaming arguments in a child class isn’t a warning in the compiler.
As of Scala 2.8.0, there’s no way to make this warning without writing your own compiler plugin.
This naming issue may not be a huge deal when you’re the author of an entire type hierarchy, but it might be when working on a larger team where others are consuming classes from others and are unhappy with parameter naming schemes from other developers.
This is to be done with an annotation on the parameter itself, declaring the old name.
Clients of your library can then use both names, albeit the one will issue a warning.
As the specifics may change, please follow the Scala mailing list and check the release notes of 2.8.1 for the mechanics of this.
For some shops, particularly ones I’ve worked in, developers were allowed to disagree on method naming conventions because they never mattered before.
Ensure that your developers are aware of naming in general, and of this particular surprising change (at least surprising when coming from a language without named parameters)
Remember that naming variables, classes, and parameters are all important in Scala.
Misnaming can wind up in anything from a compile-time error to a subtle and hard-to-fix bug.
This is one area where the compiler can’t offer much assistance besides helpful error messages.
Scala did the world a great service when it introduced the override keyword.
This keyword is used to demarcate when a method is intended to override vs.
If you neglect the keyword and the compiler finds you’re overriding a superclass method, it will emit an error.
If you add the override keyword and no superclass has the defined method, the compiler will warn you.
One scenario remains where override isn’t required but can cause issues: purely abstract methods.
Scala has no abstract modifier: A purely abstract method is one that has no implementation.
In Scala, while the override keyword is optional in some situations, it’s safe to always mark methods with override.
We want to define a business service for an application.
We’ll allow them to log in, change their password and log out as well as validate that someone is still logged in.
We’re going to make an abstract interface for users of our service.
We define a login method that takes the user’s credentials and returns a new session for that user.
We also define a logout method that takes a UserSession object and invalidates it and performs any cleanup that may be needed.
The isLoggedIn method will check to see if a UserSession is valid, meaning the user is logged in.
The changePassword method will change the user’s password but only if the new password is legal and the UserSession is valid.
Now let’s make a simple implementation that assumes any credentials are okay for any user and that all users are valid.
The method still compiles, so that means the override keyword wasn’t needed.
Why? Scala doesn’t require the override keyword if your class is the first to define an abstract method.
It also comes into play when using multiple inheritances, but we’ll look into this in a moment.
For now, let’s see what happens in the following listing if we change the method signature in the parent class:
Notice we’ve changed the changePassword method in the UserService trait.
The new method compiles fine, but the UserServiceImpl class won’t compile.
Because it’s concrete, the compiler will catch the fact that changePassword defined in the UserService isn’t implemented.
What happens if instead of an implementation, we’re providing a library with partial functionality? Let’s change UserServiceImpl to a trait, as shown in the following listing:
When we migrate UserServiceImpl to a trait, compilation now succeeds.
This is an issue primarily when providing a library with no concrete implementations, or some form of DSL that’s expected to be extended.
Therefore, only users of the library will notice this easy-to-prevent issue.
All that’s required is to use the override modifier before any overridden method, as shown in the following listing:
Because this is such an easy error for the compiler to catch, there’s no reason to run into the issue.
What about the multiple inheritance we mentioned earlier? It’s time to look into how override interacts with multiple inheritance.
Scala doesn’t require the override keyword when implementing abstract methods.
A deadly diamond occurs by creating a class that has two parent classes.
Both of the parent classes must also be subclasses of the same parentparent class.
If you were to draw a picture of the inheritance relationship, you would see a diamond.
Let’s start our own diamond by creating two traits, Cat and Dog, that extend a common base trait Animal.
The Animal trait defines a method talk that’s also defined in Cat and Dog.
Now imagine some mad scientist is attempting to combine cats and dogs to create some new species, the KittyDoggy.
How well could they do this using the override keyword? Let’s define our three classes in the following listing and find out:
We define the talk method on the Animal trait to return a String.
We then create the Cat and Dog traits with their own implementation of the talk method.
Let’s pop open the REPL and try to construct our KittyDoggy experiment.
Remember to cackle when typing, as shown in the following listing:
This results in the talk operation picking up the Dog behavior and ignoring the Cat behavior.
That’s not quite what our mad-scientist experiment wants to accomplish, so instead we try to combine a Dog with a Cat.
This ends up pulling in the Cat behavior and ignoring the Dog behavior! In Scala, the last trait “wins” when it comes to class linearization and method delegation, so this isn’t unexpected.
Class linearization refers to the order in which parent calls occur for a particular class.
In the preceding example, for the type Cat with Dog, the parent calls would first try the Dog trait, then Cat and then Animal.
Class linearization will be covered in more detail in the section 4.2
What happens now if we remove the override keyword from the Cat and Dog traits? Let’s find out in the following listing:
The definitions of Animal, Cat, and Dog are the same as before except that no override keyword is used.
Let’s put on our evil lab coat again and see if we can combine our cats and dogs in the following listing:
When we attempt to construct our Cat with Dog, the compiler issues an error that we’re trying to override a method without the override keyword.
The compiler is preventing us from combining two different concrete methods that aren’t explicitly annotated with override.
This means if I want to prevent mixing overrides of behavior, from mad scientist programmers, then I must not use the override modifier.
This feature makes more sense when the traits don’t share a common ancestor, as it requires the mad scientist to manually override the conflicting behaviors of two classes and.
But in the presence of the base class, things can get strange.
Let’s see what happens in the following listing if Cat defines its talk method with override, but Dog does not.
Mixing a Cat with a Dog is still bad, because Dog doesn’t mark talk method as being able to override.
But extending a Dog with a Cat is acceptable because the Cat’s talk can override.
We can’t use the compiler to force users to pick a talk implementation every time they inherit from Dog and another Animal.
In the case where any of the Animals defines a talk method with an override, we lose our error message.
This reduces the utility of the feature, specifically for the inheritance case.
In practice, the utility of not using the override keyword for subclass method overrides is far outweighed by the benefits of doing so.
As such, you should annotate your objects with the override keyword.
When it comes to multiple inheritance and overridden methods, you must understand the inheritance linearization and it consequences.
We discuss traits and linearization in detail in section 4.2
Another area where the compiler can drastically help us out is with error messages for missed optimizations.
The Scala compiler provides several optimizations of functional style code into performant runtime bytecodes.
The compiler will optimize tail recursion to execute as a looping construct at runtime, rather than a recursive function call.
Annotate for expected optimizations when a method calls itself as the last statement, or its tail.
Tail recursion can cause the stack to grow substantially if not optimized.
Tail call optimization isn’t as much about improving speed as preventing stack overflow errors.
The compiler can also optimize a pattern match that looks like a Java switch statement to act like a switch statement at runtime.
The compiler can figure out if it’s more efficient and still correct to use a branch lookup table.
The compiler will then emit a tableswitch bytecode for this pattern match.
The tableswitch bytecode is a branching statement that can be more efficient than multiple comparison branch statements.
The switch and tail recursion optimizations come with optional annotations.
The annotations will ensure the optimization is applied where expected or an error is issued.
The first optimization we’ll look at is treating pattern matching as a switch statement.
What this optimization does is try to compile a pattern match into a branch table rather than a decision tree.
This means that instead of performing many different comparisons against the value in the pattern match, the value is used to look up a label in the branch table.
The JVM can then jump directly to the appropriate code.
This whole process is done in a single bytecode, the tableswitch operation.
In Java, the switch statement can be compiled into a tableswitch operation.
In Scala, the compiler can optimize a pattern match into a single tableswitch operation if all the stars align, or at least the right conditions apply.
For Scala to apply the tableswitch optimization, the following has to hold true:
The expression must also have its value available at compile time: The value of the expression must not be computed at runtime but instead always be the same value.
There should be more than two case statements, otherwise the optimization is unneeded.
Let’s take a quick look at some successfully optimized code and what operations will break it.
First let’s start off with a simple switch on an integer.
We’re going to switch on an integer to handle three cases: the integer is one, the integer is two, and all other possible integer values.
As previously stated, we’re explicitly looking at the case when an integer is either one or two.
The compiler is able to optimize this to a tableswitch, as you can see in the bytecode.
What you’re seeing here are the bytecode instructions for the unannotated method.
The tableswitch instruction is made up of mappings of integer values to bytecode instruction labels (or line numbers)
If the compiler is going to create this tableswitch instruction, it needs to know the values of each case statement expression in order to do the right thing.
First, we can include a type check in the pattern match.
This can be surprising, as you might expect the type check would be superfluous, and not change the compiled code.
Let’s take our original function and add a type check for Int on one of the case statements.
The difference between this example and the previous one is the type check on the third case statement: i : Int.
Although the type of the variable is already known to be an Int, the compiler will still create a type check in the pattern match and this will prevent it from using a tableswitch bytecode.
The first thing you’ll notice is that there’s an if_icmpne comparison bytecode instead of the tableswitch.
Truncating the output, you’ll see that the method is compiled as a sequence of such comparison bytecodes.
The truncated section is the remaining bytecode to throw the error.
The compiler has inferred that our match was not complete, so it created a default case that will result in a runtime error.
The instanceof bytecode is how the JVM performs typechecks for classes, traits, and objects.
But an integer is a primitive on the JVM, so to perform a type check, Scala must “box” the integer primitive into an object form of that integer.
This is Scala’s standard mechanism for checking the type of any primitive on the JVM; therefore, try not to let any extraneous type checks in your code.
As you can see, it was fairly easy to construct a case where we thought the compiler would optimize our code, and yet it could not.
A simple solution to this problem is to annotate your expressions so the compiler will warn you if it can’t make an optimization.
As of Scala 2.8.0, the compiler currently provides two annotations that can be used to prevent compilation if an optimization isn’t applied.
Let’s look at the following listing to see how the switch annotation could have helped us earlier:
The compiler has given us the warning statement we desired.
We’re unable to compile because our pattern match can’t be optimized.
The merits of using a tableswitch are debatable, and not nearly as universal as the next annotation.
The @tailrec annotation is used to ensure that tail call optimization (usually abbreviated TCO) can be applied to a method.
Tail call optimization is the conversion of a recursive function that calls itself as the last statement into something that won’t absorb stack space but rather execute similarly to a traditional while or for loop.
The JVM doesn’t support TCO natively, so tail recursive methods will need to rely on the Scala compiler performing the optimization.
By annotating tail recursive methods, we can guarantee expected runtime performance.
To optimize tail calls, the Scala compiler requires the following:
Let’s see if we can create a good tail recursive method.
My first tail recursive function was working with a tree structure, so let’s do the same: Implementing a breadth first search algorithm using tail recursion.
The breadth first search algorithm is a way to inspect a graph or tree such that you inspect the top-level elements, then the nearest neighbors of those elements, and then the nearest neighbors of the nearest neighbors, and so on, until you find the node you’re looking for.
The first thing we do is define a Node class that allows us to construct a graph or tree.
This class is pretty simple and doesn’t allow the creation cycles, but it’s a good starting point for defining the algorithm.
It takes in a starting point, a Node, and a predicate that will return true when the correct node is found.
The algorithm itself is fairly simple and involves maintaining a queue of nodes to inspect a mechanism to determine if a node has already been seen.
Let’s create a helper function that will use tail recursion and search for the node.
This function should take the queue of nodes to inspect and a set of nodes that have already been visited.
The search method can then call this helper function, passing in List(start) for the initial queue of nodes and an empty set for the list of visited nodes.
The help method, called loop, is implemented with a pattern match.
The first case pops the first element from the queue of nodes to inspect.
It then checks to see if this is the node we are looking for and returns it.
The next case statement also pops the first element from the work queue and checks to see if it hasn’t already been visited.
If it hasn’t, the node is added to the list of visited nodes and its edges are added to the end of the work queue of nodes.
The next case statement is hit when a node has already been visited.
This case will continue the algorithm with the rest of the nodes in the queue.
The last case statement is hit if the queue is empty.
In that case, None is returned indicating no Node was found.
The interesting part of this algorithm is what the compiler does to it.
Let’s look at the bytecode for the loop helper method for any sort of function call.
We find none, so the tail call optimization must have kicked in for this method.
Let’s check for any kind of branching in the method to see what happened.
These lines are the final bytecodes in each of our case statements.
The compiler has converted the tail recursion into a while loop.
This technique of converting a tail recursive function into a while loop can help prevent stack overflow issues at runtime for many recursive algorithms.
It’s perhaps the most important optimization to require of the compiler when writing code.
No one wants an unexpected stack overflow in production code! So, once again requiring the optimization is as simple as annotating the tail recursive method with @tailrec.
Great! Now you can ensure that expected optimizations appear in programs when needed.
Remember that these annotations aren’t asking the compiler to provide an optimization, but rather requiring that the compile do so or issue a warning.
In the switch example, if the compiler had been unable to provide a tableswitch instruction, the code would still have failed to compile.
This doesn’t indicate that the code would have performed slowly.
In fact, with only two case statements, it would possibly be slower to use a tableswitch bytecode.
Therefore, make sure you use these annotations only when you require an optimization.
Unlike the switch optimization, it’s always a good idea to annotate tail recursion.
In this chapter, you’ve learned or refreshed your memory on coding conventions, their utility, and why you should look at them with fresh eyes when coming to Scala.
This is a modern programming language, with an interesting twist to C style languages.
As such, Scala requires adjustments to syntax and coding styles.
These rules should help you avoid simple syntax-related programming errors and be productive in your efforts.
With Scala, the syntax was designed in a “scalable” way.
This means that if you attempt to write concise code and run into issues, try to use the less concise, more formal syntax until you resolve the issue.
This graceful degradation is helpful in practice as it lets users “grow” into their understanding of the syntax rules.
Syntax is something that shouldn’t get in the way of development but instead become a vehicle for programmers to encode their thoughts into programs.
Therefore, know the syntax and how to avoid compilation or runtime problems from poor use of the language.
Now that we’ve looked at how to use Scala syntax, it’s time to dig into some of its more advanced features, starting with its object orientation.
Even operators are method calls against the class of an object.
Objects are core to everything in Scala, and understanding the details of how they work is important for using Scala.
Object, class, and traits are used to define public APIs for libraries.
The initialization, comparison, and composition of objects are the bread and butter of Scala development.
Initialization is important because of mixin inheritance and the way objects get instantiated in various locations.
Comparing two objects for equality is critical and can be made trickier when inheritance gets in the mix.
Finally, composition of functionality is how code reuse is accomplished, and Scala provides a few new ways to compose objects.
A common starting point for most developers learning Scala is the standard “Hello, World” program.
You’ll see many examples on the internet with the following code:
Although elegant, this code sample is misleading in its simplicity.
The Application trait uses a nifty trick to simplify creating a new application but comes with a price.
Let’s look at a simplified version of the Application trait in the following listing.
That one empty method is all that’s needed for the Application trait.
The interface is for JVM interoperability and the implementation is a set of static methods that can be used by classes implementing the trait.
When compiling the Test object, a main method is created that forwards to the Application implementation class.
Although this method is empty, the logic inside the Test object is placed in the Test object’s constructor.
One of these static forwarder methods will be the main method, in the signature the JVM expects.
The static forwarder will call the method on the singleton instance of the Test object.
Code inside a static initialization block isn’t eligible for HotSpot optimization.
In fact, in older versions of the JVM, methods called from a static initialization block wouldn’t be optimized either.
In the most recent benchmarks, this has been corrected, such that only the static block itself isn’t optimized.
Scala 2.9 provides a new mechanism for dealing with constructors.
When implementing a class that extends DelayedInit, the entire constructor is wrapped into a function and passed to the delayedInit method.
As stated before, this method has a function object passed to it.
This function contains all the regular constructor logic, which provides a clean solution to the Application trait.
The delayedInit method is overridden to store the constructor logic in the x variable.
The main method is defined so that it will execute the constructor logic stored in the x variable.
Now that the trait is created, let’s try it in the REPL.
The first line creates a new anonymous subclass of the App trait.
This subclass prints the string "Now I'm initialized" in its constructor.
The next line calls the main method on the App trait.
This calls the delayed constructor, and the string "Now I'm initialized" is printed.
The DelayedInit trait can be dangerous because it delays the construction of the object until a later time; methods that expect a fully initialized object may fail subtly at runtime.
The DelayedInit trait is ideal for situations where object construction and initialization are delayed.
For example, in the Spring bean container, objects are constructed and then properties are injected before the object is considered complete.
The DelayedInit trait could be used to delay the full construction of an object until after all properties have been injected.
A similar mechanism could be used for objects created in Android.
The DelayedInit trait solves the problem where construction and initialization of objects are required, due to external constraints, to happen at different times.
This separation isn’t recommended in practice but sometimes is necessary.
Another initialization problem exists in Scala, and this occurs with multiple inheritance.
Scala traits provide the means to declare abstract values and define concrete values that rely on the abstract.
For example, let’s create a trait that stores property values from a config file.
The Property trait defines an abstract member name which stores the current name of the Property.
The toString method is overridden to create a string using the name member.
Using abstract values in traits requires special care with object initialization.
While early initializer blocks can solve this, lazy val can be a simpler solution.
Even better is to avoid these dependencies by using constructor parameters and abstract classes.
The val x is defined as an anonymous subclass of the Property trait.
But when the REPL prints the value of toString, it shows the value null for name.
When the toString method looks for the value of name, it hasn’t been initialized yet, so it finds the value null.
After this, the anonymous subclass is constructed and the name property is initialized.
The first is to define the toString method as lazy.
Although this delays when the toString method will look for the value of the name property, it doesn’t guarantee that the initialization order is correct.
Part of this was the creation of early member definitions.
This is done by creating what looks like an anonymous class definition before mixing in a trait.
The class X is defined such that it extends the Property trait.
But before the Property trait is an anonymous block containing the early member definition.
This is a block containing a definition of the val name.
When constructing the class X, the toString method correctly displays the name HI.
The anonymous block after the new keyword is the early member definition.
This defines the members that should be initialized before the Property trait’s constructor is initialized.
The REPL prints the correct toString from the Property value.
Early member definitions solve issues that occur when a trait defines an abstract value and relies on it in other concrete values.
For any complicated trait hierarchies, early member initializations provide a more elegant solution to the problem.
Because the members that need early initialization can be buried behind several layers of inheritance, it’s important to document these throughout a type hierarchy.
Scala makes multiple inheritance simpler and provides mechanisms for dealing with the complicated situation.
Some things can be done to help prevent issues in the future such as providing empty implementations for abstract methods.
I needed to be able to create managed objects, including physical servers, network switches, and so on.
The system needed to emulate the real world and create realistic-looking data that would be fed through our application’s processing stream.
We used this simulation shown in the following figure 4.1 to test “maximum throughput” of the software.
We want this system to model real-world entities as best as possible.
We also want the ability to mix in different behaviors to our entities, where certain base traits could provide default behavior.
Starting out, we want to model network switches and network servers, including Windows and Linux servers, along with some form of agent that runs on these services and provides additional functionality.
This is a simple trait that contains a handleMessage method.
This method takes in a message and a context and performs some behavior.
The design of the simulation is such that each entity will communicate through a simulation context via messages.
When an entity receives a message, it updates its current state and sends messages appropriate to that state.
The context can also be used to schedule behavior for later in the simulation.
Let’s define a simple NetworkEntity trait with simple NetworkEntity behavior.
Remember that in a chain-of-command pattern, we want to define a base set of functionality and defer the rest to a parent class.
Scala traits have a handy property of not defining their super class until after they have been mixed in and initialized.
This means that an implementer of a trait doesn’t necessarily know which type super will be until a process called linearization occurs.
Because of linearization, the NetworkEntity trait could be using super correctly, or it might not, as the compilation output implies:
This means we have to do one of two things: define a self-type or make the abstract method have a default “do nothing” implementation that would get called.
The self-type approach could work, but it limits how your trait could be mixed in.
We would be defining an alternative “base” that the trait had to be mixed into.
This base would then need to have some kind of implementation for handleMessage.
This provides too much restriction for the aims of the application.
Message may or may not point to an implementation function.
Class linearization Linearization is the process of specifying a linear ordering to the superclasses of a given class.
In Scala, this ordering changes for each subclass and is reconstructed for classes in the hierarchy.
This means that two subclasses of some common parent could have different linearizations and therefore different behaviors.
The right way to approach this is to implement the method in the SimulationEntity trait.
This gives all our mixed-in traits the ability to delegate to super, which is a common theme when using traits as mixins.
You must select some point in an object hierarchy where traits may start being mixed in.
In our simulation, we desire to start directly at the top with SimulationEntity.
But if you’re attempting to use traits with a Java hierarchy, this might not be the case.
You may desire to start mixing into some lower-level abstraction.
The point is that you need to select the right location to ensure that your mixin-delegate behavior will work correctly.
Sometimes with real-life libraries you can’t find default behaviors to delegate into.
In this case, you might think that you could provide your own “empty implementation” trait.
Let’s see if we can do that on your network simulation example.
This code looks like a perfectly reasonable class hierarchy and compiles correctly, but it doesn’t work in practice.
The reason is that the linearization of a concrete entity (Router in this case) doesn’t work with the MixableParent trait; things aren’t ordered as we’d like.
The issue arises when we try to create a Router with NetworkEntity class.
This class compiles fine but fails to handle the Test message at runtime, because this is how the linearization works.
The following figure 4.2 shows the class hierarchy for a Router with NetworkEntity class and numbering classes/traits in their linearization.
This order determines what super means for each trait in the hierarchy.
As you can see, the MixableParent class is being called directly after NetworkEntity but before Router.
This means that the behavior in NetworkEntity is never called because the MixableParent doesn’t call its super! Therefore, we have to find a way of getting MixableParent earlier in the linearization.
Because things linearize right to left in Scala, we want to try creating a MixableParent with Router with NetworkEntity.
That first requires turning the Router class into a trait.
This might not be feasible in real life, but let’s continue the exercise.
We’ll see what this looks like in a Scala REPL session:
In Scala, trait linearization means that super calls within a trait may be different depending on how an object is linearized.
To provide full flexibility, each composable trait should be able to call a super method, even in that super method doesn’t do anything.
As you can see, the behavior is now correct, but it isn’t quite intuitive that you have to use the MixableParent first in every entity creation.
Also, the Router trait suffers from the same issues as MixableParent.
It doesn’t delegate to its parent class! This is okay because Router is an entity that other behavior is mixed into, but in some cases.
Also, there are cases where you can’t convert your classes into traits.
When creating a hierarchy of mixable behaviors via trait, you need to ensure the following:
An aphorism among the Java community is “favor composition over inheritance.” This simple advice means that it’s usually best in object-oriented Java to create new classes that “contain” other classes, rather than inherit from them.
This advice also has other benefits, including creating more selfcontained “do one thing well” classes.
Interestingly, Scala blurs this aphorism with its addition of traits.
You can decide the ordering of polymorphic behavior by adjusting the order trait inheritance is declared.
These features combine to make traits a viable mechanism of composing functionality.
Trait composability isn’t all roses; there are still some issues that aren’t addressed.
Let’s look at the issues associated with composing behavior via inheritance in Java in table 4.1
Let’s look at the complaints associated with composing behavior via inheritance in Java and see how they stack up against  Scala.
Scala traits immediately solve the problem of having to reimplement behavior in subclasses.
Scala traits still suffer from two major issues, breaking encapsulation and needing access to a constructor.
Suppose we have a class that represents a data access service in the system.
This class has a set of query-like methods that look for data and return it.
Suppose also that we want to provide a logging ability so we can do postmortem analysis on a system if it runs into an issue.
Let’s see how this would look with classic composition techniques:
The current method of composition means that the DataAccess class must be able to instantiate the Logger class.
An alternative would be to pass a logger into the constructor of the DataAccess class.
In either case, the DataAccess trait contains all logging behavior.
One point about the preceding implementation is that the logging behavior is nested in your DataAccess class.
If we instead wanted to also have the ability to use DataAccess with no Logger, then we need to create a third entity that composes behavior from the first two.
Listing 4.8 Composition of Logger and DataAccess into third class.
Now we have standalone classes Logger and DataAccess that are minimal in implementation.
This implementation has all the benefits of DataAccess and Logger being encapsulated and doing only one thing.
The issue here is that LoggedDataAccess doesn’t implement the DataAccess interface.
These two types can’t be used interchangeably in client code via polymorphism.
Let’s see what this would look like with pure inheritance:
Notice how the LoggedDataAccess class is now polymorphic on DataAccess and Logger.
This means you could use the new class where you would expect to find a DataAccess or Logger class, so this class is better for later composition.
Something is still strange here: LoggedDataAccess is also a Logger.
This seems an odd dichotomy to have for a DataAccess class.
In this simple example, it seems Logger would be an ideal candidate for member-composition into the LoggedDataAccess class.
The hierarchy will have three logger types, one for local logging, one for remote logging, and one that performs no logging.
The next thing we do is create what I’ll call an abstract member-composition class.
We can then create subclasses matching all the existing Logger subclasses.
The HasLogger trait does one thing: contains a logger member.
This class can be subclassed by other classes who want to use a Logger.
It gives a real “is-a” relationship to make inheritance worthwhile to Logger users.
The answer comes with the ability to override members as you would methods in Scala.
This allows you to create classes that extend HasLogger and then mixin the other HasLogger traits later for different behavior.
In the following listing, let’s look at using the HasLogger trait to implement our DataAccess class.
In the unit test, we don’t want to be logging output; we want to test the behavior of the function.
To do so, we want to use the NullLogger implementation.
We now have the ability to change the composition of the DataAccess class when we instantiate it.
As you can see, we gain the benefits of member-composition and inheritance composition at the cost of more legwork.
Let’s see if Scala has something that could reduce this legwork.
This is because the trait contains the environment needed for another class to function.
In the case of classic Java-like inheritance, we can try to compose using constructor arguments.
This reduces the number of parent classes to one, as only abstract/concrete classes can have arguments, and they can only be singly inherited.
But Scala has two features that will help you out:
In the following listing, let’s recreate the DataAccess class, but this time as a full up class where the logger is a constructor argument.
We’ll promote this argument to be an immutable member on the DataAccess class.
It defaults to a particular logger at instantiation time and lets you supply your own (via constructor) if desired.
Composition can include inheritance extend this class, provide users with a mechanism to supply a logger to the subclass and use the same default as the DataAccess class.
To do so, we’ll have to understand how the compiler collects default arguments.
When a method has default arguments, the compiler generates a static method for obtaining the default.
Then when user code calls a method, if it doesn’t supply an argument, the compiler calls the static method for the default and supplies the argument.
In the case of a constructor, these arguments get placed on the companion object for the class.
The companion object will have methods for generating each argument.
These argumentgenerating methods use a form of name mangling so the compiler can deterministically call the correct one.
The mangling format is method name followed by argument number, all separated with $
Let’s look at what a subclass of DataAccess would have to look like for our requirements:
First, the constructor is pickled with a method name of init.
This is because in the JVM bytecode, constructors are called <init>
The second is the use of the backtick (`) operator.
In Scala, this method is used to denote “I’m going to use an identifier here with potentially nonstandard characters that could cause parsing issues.” This is handy when calling methods defined in other languages that have different reserved words and identifiers.
We’ve finally created a method of simplifying composition using constructor arguments.
The method certainly suffers from ugliness when trying to also include inheritance in your classes.
Let’s look at the pros and cons of each compositional method in the following table:
Many “new” methods of doing object composition are possible within Scala.
When it comes to inheritance, I prefer “is-a” or “acts-as-a” relationships for parents.
If you have single-class hierarchies and no “is-a” relationships, your best option is composition using constructors with default arguments.
Scala provides the tools you need to solve the problem you have at hand.
Make sure you understand it fully before deciding on an object-composition strategy.
In section 11.3.2, we show an alternative means of composing objects using a functional approach.
Although the concepts behind this approach are advanced, the approach offers a good middle ground between using constructors with default arguments and abstract member composition.
It’s possible to mix implementation and interface with traits, but it is still a good idea to provide a pure abstract interface.
This can be used by either Scala or Java libraries.
It can then be extended by a trait which fills in the implementation details.
Modern object-oriented design promotes the use of abstract types to declare interfaces.
In Java, these use the interface keywords and can’t include implementation.
In C++ the same could be accomplished by using all pure virtual functions.
A common pitfall among new Scala developers was also an issue with C++: With the new power of traits, it can be tempting to put method implementations into traits.
Be careful when doing so! Scala’s traits do the most to impact binary compatibility of libraries.
In the following listing, let’s look at a simple Scala trait and a class that uses this trait to see how it compiles:
The following listing shows the javap output for the Main class:
As you can see, with some adjustment to reading JVM bytecode, the Main class is given a delegate class from the compiler.
One obvious issue with binary compatibility is that if the Foo trait is given another method, the Main class won’t be given a delegate method without recompiling it.
It will allow you to link (think binary compatibility) even if a class doesn’t fully implement an interface.
It errors out only when someone tries to use a method on the interface that’s unimplemented.
Let’s take it for a test toast in the following listing.
We’ll change the Foo trait without modifying the Main class.
We should still be able to use the compiled Main to instantiate a Foo at runtime.
You’ll notice we’re making a new Main object and coercing its type to be a Foo.
The most interesting piece of this class is that it compiles and runs.
Notice that the classes link fine; it even runs the first method call! The issue comes when calling the new method from the Foo trait.
The confusing part to a Scala newcomer is that the trait provides a default implementation! Well, if we want to call the default implementation, we can do so at runtime.
Let’s look at the modified ScalaMain in the following listing:
You’ll see we’re looking up and using the new method via reflection.
This points out an interesting side of the JVM/Scala’s design; methods added to traits can cause unexpected runtime behavior.
Therefore it’s usually safe to recompile all downstream users, to be on the safe side.
The implementation details of traits can throw off new users, who expect new methods with implementations to automatically link with precompiled classes.
Not only that, but adding new methods to traits will also not break binary compatibility unless someone calls the new method!
When creating two different “parts” of a software program, it’s helpful to create a completely abstract interface between them that they can use to talk to each other.
This middle piece should be relatively stable, compared to the others, and have as few dependencies as possible.
One thing you may have noticed from earlier is that Scala’s traits compile with a dependency on the ScalaObject trait.
It’s possible to remove this dependency, something that’s handy if the two pieces of your software wanted to use differing Scala-library versions.
The key to this interaction is that each module depends on the common interface code and no artifacts from each other.
This strategy is most effective when there are different developers on module A and module B, such that they evolve at different rates.
Preventing any kind of dependencies between the modules allows the new module systems, such as OSGi, to dynamically reload module B without reloading module A so long as the appropriate framework hooks are in place and all communications between the modules A and B happen via the core-api module.
To create a trait that compiles to a pure abstract interface, similar to a Java interface don’t define any methods.
You’ll notice the PureAbstract trait doesn’t have a dependency on ScalaObject.
This is a handy method of creating abstract interfaces when needed; it becomes important when used with module systems like OSGi.
In fact, this situation is similar to the one faced when interfacing two C++ libraries using C interfaces.
Although this rule may seem contradictory to the “Provide empty implementations for abstract methods,” the two are used to solve differing problems.
Use this rule when trying to create separation between modules, and provide implementations for abstract methods when creating a library of traits you intend users to extend via mixins.
Pure abstract traits also help explicitly identify a minimum interface.
Some designers prefer “rich” APIs, and others prefer “thin,” where a thin API would be the minimum necessary to implement a piece of functionality, and a rich API would contain a lot of extra helper methods to ease usage.
Scala traits bring the power to add lots of helper methods, something lacking in Java’s interfaces.
This kind of power was common in C++, which also suffered many more issues with binary compatibility.
In C++, binary compatibility issues forced the creation of a pure “C” integration layer for libraries.
This layer wrapped a rich C++ hierarchy inside the library.
Clients of the library would then implement wrappers around the C layer, converting back from classless world to OO and providing the “rich” API.
In my experience, these classes usually were thin wrappers around the C layer and mostly lived in header files, such that users of the library could gain binary compatibility without having to write their own wrapper.
In Scala, we can provide our rich interface via a simple delegate trait and some mixins.
The “thin” interface should be something that we can reasonably expect someone to implement completely.
This way the users of the “abstract interface” can grow their rich interface as needed for their project, assuming the “thin” interface is complete.
When you have two pieces of software that will be interacting but were developed by diverse or disparate teams, you should promote abstract interfaces into their own traits and lock those traits down as best as possible for the life of that project.
When the abstract interface needs to be modified, all dependent modules should be upgraded against the changed traits to ensure proper runtime linkage.
However, for a human reading  a nontrivial method implementation, infering the return type can be troubling.
In addition, letting Scala infer the return type can allow implementation details to slip past an interface.
It’s best to explicitly document and enforce return types in public APIs.
This library contains a MessageDispatcher interface that users of your library can use to send messages.
A Factory method also takes various configuration parameters and returns a MessageDispatcher.
As a library designer, we decide that we want to rework existing implementation to create different MessageDispatcher implementations based on the parameters to the Factory method.
Let’s start with a MessageDispatcher trait in the following listing:
The trait is rather simple; it provides a mechanism to send messages.
The actor dispatcher will transmit messages to an actor in the Scala actors library.
This method looks standard but has one issue: The return type isn’t a MessageDispatcher but an ActorDispatcher.
This may be okay in a small project, but it lends itself to issues if others rely on receiving ActorDispatcher instances from this method instead of a MessageDispatcher.
We can easily change this by refactoring your API slightly to return more than one type.
We also change the createDispatcher method to take in any type of object and return appropriate dispatchers for each.
If we don’t have a useful dispatcher, we’ll use the NullDispatcher.
This slight change has made the compiler reinfer a different return type.
The resulting API has inferred MessageDispatcher as the return type.
This could silently break code that was relying on receiving an ActorDispatcher.
It’s easy enough to annotate the return type for a public API.
To help avoid confusion or leaking implementation details, it’s best to provide explicit return types on public methods in your API.
This can also help speed compilation slightly, as the type inferences don’t need to figure out a return type, and it gives a chance for your implicit conversions to kick in, coercing things to the desired type.
The only time it would be okay to not specify return types is in the case of sealed single-class hierarchy, a private method, or when overriding a method in a parent that explicitly declares a return type.
Ironically, when coding in a functional style, you find that you tend not to use inheritance as much as you would think.
I find this rule generally applies to my domain model and perhaps my UI library, but not the more functional aspects of my code.
The body of code in a class definition defines the constructor of a class.
For top-level objects, this means that code in the body should avoid expensive operations and other non-construction behavior.
But when defining methods, only methods marked with override can override an existing implementation in the hierarchy.
Adding override can help ease mixin inheritance and avoid method typos.
Mixin inheritance also provides a new way to compose software.
Mixins can mark objects as having a value and allow new values to be mixed in via inheritance.
This technique provides the most flexibility when pure abstract interfaces are used for the API systems.
Finally, type inference can change an API as the object model expands.
For public methods, it’s best to explicitly annotate return types on critical interfaces.
This leads to the best use of objects in Scala.
The implicit system in Scala allows the compiler to adjust code using a well-defined lookup mechanism.
A programmer in Scala can leave out information that the compiler will attempt to infer at compile time.
In both of these situations, the compiler follows a set of rules to resolve missing data and allow the code to compile.
When the programmer leaves out parameters, it’s incredibly useful and is done in advanced Scala libraries.
The implicit system is one of the greatest assets of the Scala programming language.
Using it wisely and conservatively can drastically reduce the size of your code base.
It can also be used to elegantly enforce design considerations.
Scala provides an implicit keyword that can be used in two ways: method or variable definitions, and method parameter lists.
If this keyword is used on method or variable definitions, it tells the compiler that those methods or variable definitions can be used during implicit resolution.
Implicit resolution is when the compiler determines that a piece of information is missing in code, and it must be looked up.
The implicit keyword can also be used at the beginning of a method parameter list.
This tells the compiler that the parameter list might be missing, in which case the compiler should resolve the parameters via implicit resolution.
Let’s look at using the implicit resolution mechanism to resolve a missing parameter list:
The findAnInt method declares a single parameter x of type Int.
This function will return any value that’s passed into it.
The parameter list is marked with implicit, which means that we don’t need to use it.
If it’s left off, the compiler will look for a variable of type Int in the implicit scope.
The findAnInt method is called without specifying any argument list.
The compiler complains that it can’t find an implicit value for the x parameter.
Since this is in the REPL, the test value will be available in the implicit scope for the rest of the REPL session.
The call to findAnInt succeeds and returns the value of the test value.
The compiler was able to successfully complete the function call.
Because the method call is complete, the compiler doesn’t need to look up a value using implicits.
Remember this, as implicit method parameters can still be explicitly provided.
To understand how the compiler determines if a variable is available for implicit resolution, it’s important to dig into how the compiler deals with identifiers and scope.
Before delving into the implicit resolution mechanism, it’s important to understand how the compiler resolves identifiers within a particular scope.
This section references chapter 2 of the Scala Language Specification (SLS), I highly recommend reading through the SLS after you have an understanding of the basics.
Identifiers play a crucial role in the selection of implicits, so let’s dig into the nuts and bolts of identifiers in Scala.
Scala defines the term entity to mean types, values, methods, or classes.
These are the things we use to build our programs.
But we’ve given this class the name Foo, which is the binding.
If we declare this class locally within the REPL, we can instantiate it using the name Foo because it’s locally bound.
Here we can construct a new variable, named y, of type Foo using the name Foo.
Again, this is because the class Foo was defined locally within the REPL and the name Foo was bound locally.
The Foo class is now a member of the package test.
If we try to access it with the name Foo, it will fail on the REPL:
Trying to call new Foo fails because the name Foo isn’t bound in our scope.
The import statement takes test.Foo entity and binds it in the local scope with the name Foo.
This allows us to construct a new test.Foo instance by calling new Foo.
This concept should be familiar from Java’s import statement or C++’s using statement.
The import statement can be used anywhere in the source file and it will only create a binding in the local scope.
This feature allows us to control where imported names are used within our file.
This feature can also be used to limit the scope of implicits views or variables.
We’ll cover this aspect in more detail in section 5.4
Binding entities allows us to name them within a particular scope.
But it’s important to understand what constitutes a scope and what bindings are found in a scope.
This means I can construct a new scope inside another scope.
When creating a new scope, the bindings from the outer scope are still available.
The Foo class is defined with the constructor parameter x.
We then define the tmp method with a nested scope.
We can still access the constructor parameter inside this scope with the name x.
This nested scope has access to bindings in its parent scope, however we can create new bindings that shadow the parent.
In this case, the tmp method can create a new binding called x that does not refer to the constructor parameter x.
The Foo class is defined the same as before, but the tmp method defines a variable named x in the nested scope.
Shadowing means that the local binding is visible and the constructor parameter is no longer accessible, at least using the name x.
In Scala, bindings of higher precedence shadow bindings of lower precedence within the same scope.
Also, bindings of higher or the same precedence shadow bindings in an outer scope.
Definitions and declarations that are local, inherited, or made available by a package clause in the same source file where the definition occurs have highest precedence.
Definitions made available by a package clause not in the source file where the definition occurs have lowest precedence.
This file defines a package test with the x object inside it.
The x object overrides the toString method so we can easily call toString on it.
This means that for the purposes of our test, the x object should have the lowest binding precedence with binding rules.
Now, let’s create a file that will test the binding rules:
First, we declare the contents of the file to be in the same test package as our earlier definition.
Next, we define a main method that will call four testing methods, one for each binding precedence rule.
Because the Test object is defined within the test package, the x object created earlier is available and used for this method.
To prove this, look at the output of this method:
Calling the testSamePackage method produces the string we defined for the object x.
Now let’s see what happens if we add a Wildcard import:
The Wildcard object is a nested object used to contain the x entity so that it can later be imported.
The entity x is defined as a method that returns the string "Wildcard Import x"
This is a wildcard import that will bind all the names/entities from the Wildcard object into the current scope.
Because wildcard imports have higher precedence than resources made available from the same package but in a different source file, the Wildcard.x entity will be used instead of the test.x entity.
When calling the testWildcardImport method, the string Wildcard Import x is returned—exactly what we expect from the binding precedence.
Once again, the Explicit object is used to create a new namespace for another x entity.
The testExplicitImport method first imports this entity directly and then uses the wildcard import against the Wildcard object.
Although the wildcard import is after the explicit import, the binding precedence rules kick in and the method will use the x binding from the Explicit object.
As expected, the returned string is the one from Explicit.x.
This precedence rule is important when dealing with implicit resolution, but we’ll get to that in section 5.1.3
The final precedence rule to test is for local declarations.
Let’s modify the testExplicitImport method to define a local binding for the name x:
The next two lines explicitly import and implicitly import x bindings from the Explicit and Wildcard objects, as we saw earlier.
Finally, we call println(x) and see which binding is selected.
Again, even though the import statements come after the val x statement, the local variable is chosen based on the binding priorities.
Why all the emphasis on name resolution within the compiler? Implicit resolution is intimately tied to name resolution, so these intricate rules become important when using implicits.
The Scala Language Specification declares two rules for looking up entities marked as implicit:
The implicit entity binding is available at the lookup site with no prefix—that is, not as foo.x but only x.
If there are no available entities from this rule, then all implicit members on objects belong to the implicit scope of an implicit parameter’s type.
The first rule is intimately tied to the binding rules of the previous section.
The second rule is a bit more complex and we’ll look into it in section 5.1.4
In this example, the name x is bound in an outer scope.
The name x is also imported from the test package in a nested scope.
The value x from the outer scope isn’t eligible to shadow within the nested scope, and the imported value x doesn’t have high enough precedence to shadow.
First, let’s look at our earlier example of implicit resolution:
The findAnInt method is declared with an implicit parameter list of a single integer.
The next line defines a val test with the implicit marker.
This makes the identifier, test, available on the local scope with no prefix.
When we write this method call, findAnInt, the compiler will rewrite it as findAnInt(test)
The second rule for implicit lookup is used when the compiler can’t find any available implicits using the first rule.
In this case, the compiler will look for implicits defined within any object in the implicit scope of the type it’s looking for.
The implicit scope of a type is defined as all companion modules that are associated with that type.
This means that if the compiler is looking for a parameter to the method def foo(implicit param : Foo), that parameter will need to conform to the type Foo.
If no value of type Foo is found using the first rule, then the compiler will use the implicit scope of Foo.
The implicit scope of Foo would consist of the companion object to Foo.
The holder object is used so we can define a trait and companion object within the REPL, as described in section 2.1.2
Inside, we define a trait Foo and companion object Foo.
The companion object Foo defines a member x of type Foo that’s available for implicit resolution.
Next we import the Foo type from the holder object into the current scope.
This step isn’t necessary, it’s done to simplify the method definition.
When called with no argument lists, the compiler will use the implicit val x defined on the companion.
Because the implicit scope is looked at second, we can use the implicit scope to store default implicits while allowing users to import their own overrides as necessary.
As stated previously, the implicit scope of a type T is the set of companion objects for all types associated with the type T—that is, there’s a set of types that are associated with T.
All of the companion objects for these types are searched during implicit resolution.
The Scala Language Specification defines association as any class that’s a base class of some part of type T.
If type T is defined as A with B with C, then A, B, and C are all parts of the type T and their companion objects will be searched during implicit resolution for type T.
If T is parameterized, then all type parameters and their parts are included in the parts of type T.
For example, an implicit search for the type List[String] would look in List’s companion object and String’s companion object.
If T is a singleton type T, then the parts of the type p are included in the parts of type T.
This means that if the type T lives inside an object, then the object itself is inspected for implicits.
Singleton types are covered in more detail in section 6.1.1
If T is a type projection S#T, then the parts of S are included in the parts of type T.
This means that if type T lives in a class or trait, then the class or trait’s companion objects are inspected for implicits.
Type projections are covered in more detail in section 6.1.1
The implicit scope of a type includes many different locations and grants a lot of flexibility in providing handy implicit resolution.
Let’s look at a few of the more interesting cases of implicit scope.
The Scala language defines the implicit scope of a type to include the companion objects of all types or subtypes included in the type’s parameters.
This means, for example, that we can provide an implicit value for List[Foo] by including it in the type Foo’s companion object.
The holder object is used, again, to create companion objects in the REPL.
The holder object contains a trait Foo and its companion object.
The companion object contains an implicit definition of a List[Foo] type.
We can use this function to look up a type using the current implicit scope.
It uses the type parameter T to allow us to reuse it for every type we’re looking for.
We’ll cover type parameters in more detail in section 6.2
The call to implicitly for the type List[holder.Foo] returns the implicit list defined within Foo’s companion object.
This mechanism is used to implement type traits sometimes called type classes.
Type traits describe generic interfaces using type parameters such that implementations can be created for any type.
This trait can be implemented for a given type to describe how it should be serialized into a binary format.
This method takes in an instance of the type parameter and returns an array of bytes representing that parameter.
Code that needs to serialize objects to disk can now attempt to find a BinaryFormat type trait via implicits.
We can provide an implementation for our type Foo by providing an implicit in Foo’s companion object, as follows:
Its companion object is defined with an implicit val that holds the implementation of the BinaryFormat.
Now, when code that requires a BinaryFormat sees the type Foo, it will be able to find the BinaryFormat implicitly.
The details of this mechanism and design techniques are discussed in detail in section 7.2
Implicit lookup from type parameters enables elegant type trait programming.
Nested types provides another great means to supply implicit arguments.
Implicit scope also includes companion objects from outer scopes if a type is defined in an inner scope.
This allows us to provide a set of handy implicits for a type in the outer scope.
The Foo object also defines an implicit method that creates an instance of the Bar trait.
This technique is similar to placing implicits directly in a companion object.
Defining implicits for nested types is convenient when the outer scope contains several subtypes.
We can use this technique in situations where we can’t create an implicit on a companion object.
Because of this, implicits associated with the object’s type, that are desired on the implicit scope of that object’s type, must be provided from an outer scope.
The object Foo also defines an implicit that returns Bar.type.
An additional case of nesting that may surprise those not used to it is the case of package objects.
As of Scala 2.8, objects can be defined as package objects.
A package object is an object defined using the package keyword.
It’s convention in Scala to locate all package objects in a file called package.scala in a directory corresponding to the package name.
Any class that’s defined within a package is nested inside the package.
Any implicits defined on a package object will be on the implicit scope for all types defined inside the package.
This provides a handy location to store implicits rather than defining companion objects for every type in a package, as shown in the following example:
The package object foo is declared with a single implicit that returns a new instance of the Foo class.
Next, the class Foo is defined within the package foo.
In Scala, packages can be defined in multiple files and the types defined in each source file is aggregated to create the complete package.
There can only be one package object defined in all source files for any given package.
The Foo class has an overridden toString method that will print the string "Foo!"
Let’s compile the foo package and use it in the REPL, as follows:
Without importing the package object or its members, the compiler can find the implicit for the foo.Foo object.
It’s common in Scala to find a set of implicit definitions within the package object for a library.
Usually this package object also contains implicit views, a mechanism for converting between types.
An implicit view is an automatic conversion of one type to another to satisfy an expression.
The previous conversion would implicitly convert a value of OriginalType to a value of ViewType if available on the implicit scope.
Let’s look at a simple example attempting to convert an integer to a string:
The foo method is defined to take a String and print it to the console.
The call to foo using the value 5 fails, as there’s a type mismatch.
It takes a single value of type Int and returns a String.
This method is the implicit view, and is commonly referred to as the view Int => String.
The compiler detected that the types did not conform and that there was a single implicit view that could correct the situation.
The implicit scope used for implicit views is the same as for implicit parameters.
But when the compiler is looking for type associations, it uses the type it’s attempting to convert from, not the type it’s attempting to convert to.
The test object is a scoping object used so we can create a companion object in the REPL.
This contains the Foo and Bar traits as well as a companion object to Foo.
The companion object to Foo contains an implicit view from Foo to Bar.
Remember that when the compiler is looking for implicit views, the type it’s converting from defines the implicit scope.
This means the implicit views defined in Foo’s companion object will be inspected only when attempting to convert an expression of type Foo to some other expression.
Let’s try this out by defining a method that expects the type Bar.
The bar method takes a bar and prints the string bar.
Let’s try to call it with a value of foo and see what happens:
The expression bar(x) triggers the compiler to look for an implicit view.
Finding the fooToBar view, the compiler inserts the necessary transformation and the method compiles successfully.
This style of implicits allows us to adapt libraries to other libraries, or add our own convenience methods to types.
It’s a common practice in Scala to adapt Java libraries so that they work well with the Scala standard library.
This module is a set of implicit views that can be imported into the current scope to allow automatic conversion between Java collections and Scala collections and to “add” methods to the Java collections.
Adapting Java libraries, or third party libraries, into your project using implicit views is a common idiom in Scala.
We’d like to write a wrapper around the java.security package for easier usage from Scala.
The AccessController class provides the static method doPrivileged, which allows us to run code in a privileged permission state.
The trait is similar to Scala’s Function0 trait, and we’d like to be able to use an anonymous function when calling the doPrivileged method.
Let’s create an implicit view from a Function0 type to a doPrivileged method:
Next, the call to doPrivileged passed the anonymous function () => println("this is privileged")
Again, the compiler sees that the anonymous function doesn’t match the expected type.
It’s common to write a wrapper class for existing Java libraries that add more advanced Scala idioms.
Scala implicits can be used to convert from the original type into the wrapped type and vice versa.
For example, let’s look at adding some convenience methods onto the java.io.File class.
We’d like to provide a convenience notation for java.io.File so that the / operator can be used to create new file objects.
Let’s create the wrapper class that will provide the / operator:
It provides one new method / that takes a string and returns a new FileWrapper object.
The newly returned FileWrapper object points to a file with the name specified to the / method inside the directory of the original file.
For example, if the original FileWrapper, called file, pointed at the /tmp directory, then expression file / "mylog.txt" will return a FileWrapper object that points at the /tmp/mylog.txt file.
We’d like to use implicits to automatically convert between java.io.File and FileWrapper, so let’s add an implicit view to FileWrapper’s companion object:
The FileWrapper companion object defines one method, wrap, which takes a java .io.File and returns a new FileWrapper.
The next line creates a new java.io.File object with the string "."
This string denotes that the file object should point to the current directory.
The last line calls the / method against a java.io.File.
The compiler doesn’t find this method on a standard java.io.File and looks for an implicit view that would enable this line to compile.
Finding the wrap method in scope, the compiler wraps the java.io.File into a FileWrapper and calls the / method.
This mechanism is a great way to append methods onto existing Java classes, or any library.
We have the performance overhead of the wrapper object instantiation, but the HotSpot optimizer may mitigate this.
Enhancing existing classes with implicit views microbenchmarks this will occur.
Again, it’s best to profile an application to determine critical regions rather than assuming HotSpot will take care of allocations.
One issue with the FileWrapper is that calling its / method will return another FileWrapper object This means we can’t pass the result directly into a method that expects a vanilla java.io.File.
The / method could change to instead return a java.io.File object, but Scala also provides another solution.
When passing a FileWrapper to a method that expects a java.io.File type, the compiler will begin a search for a valid implicit view.
As stated earlier, this search will include the companion object for the FileWrapper type itself.
Let’s add an unwrap implicit view to the companion object and see if this works:
The FileWrapper companion object now contains two methods: wrap and unwrap.
The unwrap method takes an instance of FileWrapper and returns the wrapped java.io.File type.
The next line construct a java.io.File object pointing to the current directory.
This method expects an input of type java.io.File and will print the path to the file.
The last line calls the useFile method with the expression: cur / "temp.txt"
Again, the compile sees the / method call and looks for an implicit view to resolve the expression.
The resulting type of the expression is a FileWrapper, but the useFile method requires a java.io.File.
This search finds the unwrap implicit view on FileWrapper’s companion object.
The types are now satisfied and the compiler has completed the expression.
Notice that utilizing the unwrap implicit view doesn’t require an import, as needed for the wrap method.
This is because the wrap implicit view was used when the compile did not know the required type to satisfy the cur / "temp.txt" expression; therefore it looked for only local implicits, as java.io.File has no companion object.
This feature allows us to provide a wrapper object with additional functionality and nearinvisible conversions to and from the wrapper.
Take care when providing additional functionality to existing classes using implicit views.
This mechanism makes it much harder to determine if there’s a name conflict across differing implicit views of a type.
It also has a performance penalty that may not be mitigated by the HotSpot optimizer.
Finally, for folks not using a modern Scala IDE, it can be difficult to determine which implicit views are providing methods used in a block of code.
While they seem like a good idea in a lot of situations, Scala provides better alternatives in most cases.
Using too many implicit views can greatly increase the ramp-up time of new developers on a code base.
While useful, they should be limited to situations where they are the right solution.
Scala implicit views provide users with the flexibility to adapt an API to their needs.
Using wrappers and companion object implicit views can drastically ease the pain of integrating libraries with varied but similar interfaces or can allow developers to add functionality to older libraries.
Implicit views are a key component in writing expressive Scala code, and should be handled with care.
Implicits also have an interesting interaction with another Scala feature—default parameters.
Implicit arguments provide a great mechanism to ensure that users don’t have to specify redundant arguments.
In the event that no parameter is specified and no implicit value is found using implicit resolution, the default parameter is used.
This allows us to create default parameters that remove redundant ones while still allowing users to provide different parameters.
For example, let’s implement a set of methods designed to perform matrix calculations.
These methods will utilize threads to parallelize work when performing calculations on matrices.
But as a library designer, we don’t know where these methods will be called.
They may be operating within a context where threading isn’t allowed, or they may already have their own work queue set up.
We want to allow users to tell us how to use threads in their context but provide a default for everyone else.
The Matrix class takes an array of double values and provides two similar methods: row and col.
These methods take an index and return an array of the values for a given matrix row or column respectively.
The Matrix class also provides rowRank and colRank values which return the number of rows and columns in the matrix respectively.
Finally the toString method is overridden to create a prettier output of the matrix.
The Matrix class is complete and ready for a parallel multiplication algorithm.
Let’s start by creating an interface we can use in our library for threading:
This method takes a function that returns a value of type A.
It also returns a function that returns a value of type A.
The returned function should return the same value as the passed-in function, but could block the current thread until the function is calculated on its desired thread.
Let’s implement our matrix calculation service using this ThreadStrategy interface:
The method takes two Matrix classes, assumed to have the correct dimensions, and will return a new matrix that’s the multiplication of the passed-in matrices.
Matrix multiplication involves multiplying the elements in Matrix a’s rows by the elements in Matrix b’s columns and adding the results.
This multiplication and summation is done for every element in the resulting matrix.
A simple way to parallelize this is to compute each element of the result matrix on a separate thread.
Wrap the buffer in a Matrix class and return it.
The initial assert statement is used to ensure that the Matrix objects passed in are compatible for multiplication.
By definition, the number of columns in Matrix a must equal the number of rows in Matrix b.
We then construct an array of arrays to use as the buffer.
The resulting matrix will have the same number of rows as Matrix a and the same number of columns as Matrix b.
Now that the buffer is ready, let’s create a set of closures in the following listing that will compute the values and place them in the buffer:
The computeValue helper method takes a row and a column attribute and computes the value in the buffer at that row and column.
The first step is matching the elements of the row of a with the elements of the column of b in a pairwise fashion.
Scala provides the zip function which, given two collections, will match their elements.
Next, the paired elements are multiplied to create a list of the products of each element.
The final calculation takes a sum of all the products.
This final value is placed into the correct row and column in the buffer.
The next step is to take this method and construct a function for every row and column in the resulting matrix and pass these functions to the threading strategy, as follows:
This for expression loops every row and column in the resulting matrix and passes a function into the ThreadStrategy parameter threading.
After farming out the work to threads, the multiply method must ensure that all work is complete before returning results.
We do this by calling each method returned from the ThreadStrategy.
The last portion of the multiple method ensures all work is completed and returns the result Matrix built from the buffer object.
Let’s test this in the REPL, but first we need to implement the ThreadStrategy interface.
Let’s create a simple version that executes all work on the current thread:
The SameThreadStrategy ensures that all passed-in work operates on the calling thread by returning the original function.
Let’s test out the multiply method in the REPL, as follows:
The first line is creating an implicit ThreadStrategy that will be used for all remaining calculations.
Everything appears to be working correctly with a single thread, so let’s create a multithreaded service, as in the following listing:
The thread pool is constructed with the number of threads equal to the number of available processors.
The execute method takes the passed-in function and creates an anonymous Callable instance.
The Callable interface is used in Java’s concurrent library to pass work into the thread pool.
This returns a Future that can be used to determine when the passedin work is completed.
The last line of execute returns an anonymous closure that will call get on future.
This call blocks until the original function executes and returns the value returned by the function.
Also, every time a function is executed inside the Callable, it will print a message informing which thread it’s executing on.
Now what if we wanted to provide a default threading strategy for users of the library, and still allow them to override if desired? We can use the default parameter mechanism to provide a default.
This will be used if no value is available in the implicit scope, meaning that our users can override the default in a scope by importing or creating their own implicit ThreadStrategy.
Users can also override the behavior for a single method call by explicitly passing the ThreadStrategy.
The multiply method now defines the SameThreadStrategy as the default strategy.
Now when we use this library, we don’t have to provide our own implicit ThreadStrategy:
This means we get the elegance of implicit parameters with the utility of default parameters.
This allows users of the MatrixService to decide when to parallelize computations performed with the library.
They can do this for a particular scope by providing an implicit or for a single method call by explicitly passing the ThreadStrategy.
This technique of creating an implicit value for a scope of computations is a powerful, flexible means of using the strategy pattern.
The strategy pattern is an idiom where a piece of code needs to perform some operation, but certain behaviors, or execution “strategy,” can be swapped into the method.
The ThreadPoolStrategy is such a behavior that we’re passing into our MatrixUtils library methods.
This same ThreadPoolStrategy could be used across different subsections of components in our system.
It provides an alternative means of composing behavior than using inheritance, as discussed in section 4.3
Another good example of implicits with default parameters is reading the lines of a file.
This can be done by providing an implicit argument for the line ending strategy and providing a default value of “don’t care.”
Implicits provide a great way to reduce boilerplate in code, such as repeated parameters.
The most important thing to remember when using them is be careful, which is the topic of the next section.
The most important aspect of dealing with implicits is ensuring that programmers can understand what’s happening in a block of code.
Programmers can do this by limiting the places they must check to discover available implicits.
As seen in section 1.1.3, Scala will look in the companion objects of associated types for implicits.
Companion and package objects should be considered part of the API of a class.
When investigating how to use a new library, check the companion and package objects for implicit conversions that you may use.
Because implicit conflicts require explicit passing of arguments and conversions, it’s best to avoid them.
This can be accomplished by limiting the number of implicits that are in scope and providing implicits in a way that they can overridden or hidden.
At the beginning of every compiled Scala file there’s an implicit import scala.Predef._
It also contains implicits that will convert between Java’s boxed types and Scala’s unified types for primitives.
When coding in Scala, it’s a good idea to know the implicits are available in the scala.Predef object.
The last possible location for implicits are explicit import statements within the source code.
Because these are the only form of implicits that require an explicit import statement in every source file they’re used, they require the most amount of care.
When defining a new implicit view or parameter that’s intended to be explicitly imported, you should ensure the following:
Because Scala uses scope resolution to look up implicits, if there’s a naming conflict between two implicit definitions it can cause issues.
These conflicts are hard to detect because implicit views and parameters can be defined in any scope and imported.
The scala.Predef object has its contents implicitly imported into every Scala file so that conflicts become immediately apparent.
This defines a Time object that contains a TimeRange class.
You can use this method to construct time range objects.
This to method returns a Range object that can be used in for expressions.
Imagine a scenario where someone is using this TimeRange implicit to construct time ranges, and then desires the original implicit defined in Predef for a for expression.
One way to solve this is to import the Predef implicit at a higher precedence level in a lower scope where it’s needed.
This can be confusing, as shown in the following example:
The Time implicits are imported and the expression is again printed.
Next, in a lower scope, the Predef longWrapper is imported and the expression is printed.
Finally, in yet a lower scope, the Time longWrapper is imported and the expression is again printed.
The second TimeRange result is after the Time implicit conversion is imported.
The next NumericRange result is from the nested scope in method x() and the final TimeRange result is the result of the statement in the deeply nested y() method.
The best way is to avoid conflicts across implicit views, but sometimes this is difficult.
In those cases, it’s better to pick one conversion to be implicit and use the other explicitly.
Making implicits discoverable also helps make code readable, as it helps a new developer determine what is and should be happening in a block of code.
Limiting the scope of implicits implicits discoverable is important when working on a team.
Within the Scala community, it’s common practice to limit importable implicits into one of two places:
Package objects make a great place to store implicits because they’re already on the implicit scope for types defined within the package.
Users need to investigate the package object for implicits relating to the package.
Placing implicit definitions that need explicit import on the package object means that there’s a greater chance a user will find the implicits and be aware of them.
When providing implicits via package object, make sure to document if they require explicit imports for usage.
A better option to documenting explicit import of implicits is to avoid import statements altogether.
Their secondary lookup rules, which inspect companion objects of associated types, allow the definition of implicit conversions and values that don’t require explicit import statements for these implicit values.
With some creative definitions, expressive libraries can be defined that make the full use of implicits without requiring any imports.
Let’s look at an example of this: a library for expressing complex numbers.
Complex numbers are numbers that have a rational and imaginary part to them.
The imaginary part is the part multiplied by the square root of -1, also known as i (or j for electrical engineers)
This is simple to model using a case class in Scala:
The ComplexNumber class defines a real component of type Double called real.
The ComplexNumber class also defines an imaginary component of type Double called imaginary.
This class represents complex numbers using floating point arithmetic for the component parts.
Addition,+, is defined such that the real/imaginary component of the sum of two complex numbers is the sum of the real/imaginary components of two numbers.
The first line multiplies a real component by an imaginary component and the resulting complex number is imaginary.
The second line adds a real component to an imaginary component, resulting in a complex number with both real and imaginary parts.
The operators * and + work as desired, but calling the ComplexNumber factory method is a bit verbose.
This can be simplified using a new notation for complex numbers.
In mathematics, complex numbers are usually represented as a sum of the real and imaginary parts.
This notation would make an ideal syntax for the complex number library.
Let’s define the symbol i to refer to the square root of –1
This defines the val i on the package object for complexmath.
This places the name i available within the complexmath package and allows it to be imported directly.
This name can be used to construct complex numbers from their component parts.
But a piece is missing, as shown in the following REPL session:
Attempting to multiply the imaginary number i by a Double fails because the ComplexNumber type only defines multiplication on ComplexNumber types.
Limiting the scope of implicits considered a complex number that has no imaginary component.
This property can be emulated in Scala using an implicit conversion from Double to ComplexNumber:
The complexmath package object now contains the definition for the value i as well as an implicit conversion from Double to ComplexNumber called realToComplex.
We’d like to limit the usage of this implicit conversion so that it’s only used when absolutely needed.
Let’s try using the complexmath package without explicitly importing any implicit conversions:
The important thing to note here is that only the name i is imported from complexmath.
The rest of the implicit conversions are all trigged from the i object when the compiler first sees the expression i*5
The value i is known to be a ComplexNumber and defines a * method that takes another ComplexNumber.
The literal 5.0 isn’t of the type ComplexNumber, but Double.
This search finds the realToComplex conversion on the package object and applies it.
The compiler finds a + method defined on ComplexNumber that accepts a ComplexNumber.
Notice how the value i is used to trigger complex arithmetic.
Once a complex number is seen, the compiler can accurately find implicits to ensure that expressions are compiled.
The syntax is elegant and concise, and no implicit conversions were needed to make this syntax work.
The downside is that the value i must be used to begin a ComplexNumber expression.
Let’s look at what happens when i appears at the end of the expression:
The compiler complains about the expression because it can’t find a + method defined for the type Double that takes a ComplexNumber.
This issue could be solved by importing the implicit view of Double => ComplexNumber into scope:
The downside is that there’s now an additional implicit view in scope for the type Double.
This can cause issues if other implicit views are defined that provide similar methods to ComplexNumber.
Let’s define a new implicit conversion that adds an imaginary method to Double.
Note that implicit conversions are not applicable because they are ambiguous: both method doubleToReal in object $iw of type.
The first statement defines an implicit view on the Double type that adds a new type containing a real method.
The real method returns a string version of the Double.
The next statement attempts to call the real method and is unable to do so.
The issue here is the ComplexNumber type also defines a method real, and so the implicit conversion from Double => ComplexNumber is getting in the way of our doubleToReal implicit conversion.
This conflict can be avoided by not importing the Double => ComplexNumber conversion:
The example starts a new REPL session that only imports complexmath.i.
Now the expression 5.0 real successfully compiles because there’s no conflict.
You can use this idiom to successfully create expressive code without all the dangers of implicit conflicts.
Define an entry point into the library such that implicit conversions are disambiguated after the entry point.
In the complexmath library, the value i is the entry point.
In the complexmath library, the entry point i allows certain types of expressions but not others that intuition would suggest should be there.
In this situation, it’s acceptable to provide implicit conversions that can be imported from a well-known location.
Following these guidelines helps create expressive APIs that are also discoverable.
In this chapter, we discussed the implicit lookup mechanism of Scala.
Scala supports two types of implicits: implicit value and implicit views.
Implicit values can be used to provide arguments to method calls.
Implicit views can be used to convert between types or to allow method calls against a type to succeed.
Both implicit values and implicit views use the same implicit resolution mechanism.
The first stage looks for implicits that have no prefix in the current scope.
The second stage looks in companion objects of associated types.
They can also be used with default parameters to reduce the noise for method calls and tie behavior to the scope of an implicit value.
Most importantly, implicits provide a lot of power and should be used responsibly.
Limiting the scope of implicits and defining them in well-known or easily discoverable locations is key to success.
You can do this by providing unambiguous entry points into implicit conversions and expressive APIs.
Implicits also interact with Scala’s type system in interesting ways.
We’ll discuss these in chapter 7, but first let’s look at Scala’s type system.
The type system allows us to create all sorts of interesting walls around ourselves, known as types.
These walls help prevent us from accidentally writing improper code.
This is done through the compiler tracking information about variables, methods, and classes.
The more you know about Scala’s type system, the more information you can give the compiler, and the type walls become less restrictive while still providing the same protection.
When using a type system, it’s best to think of it as an overprotective father.
It will constantly warn you of problems or prevent you from doing things altogether.
The better you communicate with the type system, the less restrictive it becomes.
But if you attempt to do something deemed inappropriate, the compiler will warn The type system.
The compiler can be a great means of detecting errors if you give it enough information.
In his book Imperfect C++, Matthew Wilson uses an analogy of comparing the compiler to a batman.
This batman isn’t a caped crusader but is instead a good friend who offers advice and supports the programmer.
In this chapter, you’ll learn the basics of the type system so you can begin to rely on it to catch common programming errors.
The next chapter will cover more advanced type system concepts, as well as utilizing implicits with the type system.
This chapter will cover the basics of the type system, touching on definitions and theory.
The next chapter covers more practical applications of the type system and the best practices to use when defining constraints.
Feel free to skip this information if you’re already comfortable with Scala’s type system.
Understanding Scala’s type system begins in first understanding what a type is and how to create it.
A type is a set of information the compiler knows.
This could be anything from “what class was used to instantiate this variable” to “what collection of methods are known to exist on this variable.” The user can explicitly provide this information, or the compiler can infer it through inspection of other code.
When passing or manipulating variables, this information can be expanded or reduced, depending on how you’ve written your methods.
To begin, let’s look at how types are defined in Scala.
If the user called a substring on a variable of type String the compiler would allow the call, because it knows that it would succeed at runtime (move above when passing or manipulating variables)
Defining a class, trait, or object automatically creates an associated type for the class, trait, or object.
This type can be referred to using the same name as the class or trait.
For objects we refer to the type slightly differently due to the potential of classes or traits having the same name as an object.
Let’s look at defining a few types and referring to them in method arguments:
As seen in the example, class and trait names can be referenced directly when annotating types within Scala.
When referring to an object’s type, you need to use the type member of the object.
This syntax isn’t normally seen in Scala, because if you know an object’s type, you can just as easily access the object directly, rather than ask for it in a parameter.
Types within Scala are referenced relative to a binding or path.
As discussed in chapter 5, a binding is the name used to refer to an entity.
A path isn’t a type; it’s a location of sorts where the compiler can find types.
When a type name is used directly, there’s an implicit empty path preceding it.
Using the this keyword directly in a class C is shorthand for the full path C.this.
This path type is useful for referring to identifiers defined on outer classes.
The path p.x where p is a path and x is a stable identifier of x.
A stable identifier is an identifier that the compiler knows for certain will always be accessible from the path p.
For example, the path scala.Option refers to the Option singleton defined on the package scala.
The formal definition of stable members are packages, objects, or value definitions introduced on nonvolatile types.
A volatile type is a type where the compiler can’t be certain its members won’t change.
The type definition could change depending on the subclass and the compiler doesn’t have enough information to compute a stable identifier from this volatile type.
The path C.super or C.super[P] where C refers to a class and P refers to a parent type of class C.
Use this path to disambiguate between identifiers defined on a class and a parent class.
The dot operator can be thought of doing the same for types as it does for members of an object.
It refers to a type found on a specific object instance.
When a method is defined using the dot operator to a particular type, that type is bound to a specific instance of the object.
This means that you can’t use a type from a different object, of the same class, to satisfy any type constraints made using the dot operator.
The best way to think of this is that there’s a path of specific object instances connected by the dot operator.
For a variable to match your type, it must follow the same object instance path.
The hash operator (#) is a looser restriction than the dot operator.
It’s known as a type projection, which is a means of referring to a nested type without requiring a path of object instances.
This means that you can reference a nested type as if it weren’t nested.
In the preceding example, the Outer class defines a nested trait Inner along with two methods that use the Inner type.
Method foo uses a path dependent type and method bar uses a type projection.
Variables x and y are constructed as two different instances of the Outer class.
This type displays with the variable instance of Outer, which is x.
To access the correct type Y, you must travel the path through the x variable.
If we call the foo method on x using the Inner instance from the same x variable, then the call is successful.
But using the Inner instance from the y variable causes the compiler to complain with a type error.
The type error explicitly states that it’s expecting the Inner type to come from the same instance as the method call----the x instance.
The instance restriction isn’t in place as it was for the foo method.
When calling the bar method on the x instance using the inner type from the y instance, the call succeeds.
This shows that although path-dependent types ( foo.Bar ) require the Bar instances to be generated from the same foo instance, type projections (Foo#Bar ) match any Bar instances generated from any Foo instances.
Both path-dependent and type projection rules apply to all nested types, including those created using the type keyword.
A path-dependent type foo.Bar is rewritten as foo.type#Bar by the compiler.
The expression foo.type refers to the singleton type of Foo.
This singleton type can only be satisfied by the entity referenced by the name foo.
The path-dependent type ( foo.Bar ) requires the Bar instances to be generated from the same foo instance, while a type projection Foo#Bar would match any Bar instances generated from any Foo instances, not necessarily the entity referred to by the name Foo.
In Scala, all type references can be written as projects against named entities.
There can be some confusion when using path-dependent types for classes that have companion objects.
For example, if the trait bar.Foo has a companion object bar.Foo, then the type bar.Foo (bar.type#Foo) would refer to the trait’s type and the type bar.Foo.type would refer to the companion object’s type.
Scala also allows types to be constructed using the type keyword.
This can be used to create both concrete and abstract types.
Concrete types are created by referring to existing types, or through structural types which we’ll discuss later.
Abstract types are created as place holders that you can later refine in a subclass.
This allows a significant level of abstraction and type safety within programs.
We’ll discuss this more later, but for now let’s create our own types.
The type keyword can only define types within some sort of context, specifically within a class, trait, or object, or within subcontext of one of these.
It consists of the keyword itself, an identifier, and, optionally, a definition or constraint for the type.
If no constraints or assignments are provided, the type is considered abstract.
We’ll get into type constraints a little later; for now let’s look at the syntax for the type keyword:
Notice that concrete types can be defined through combining other types.
This new type is referred to as a compound type.
The new type is satisfied only if an instance meets all the requirements of both original types.
The compiler will ensure that these types are compatible before allowing the combination.
As an analogy, think of the initial two types as a bucket of toys.
Each toy in a given bucket is equivalent to a member on the original type.
When you create a compound type of two types using the with keyword, you’re taking two buckets, from two of your friends, and placing all their toys into one larger, compound bucket.
When you’re combining the buckets, you notice that one friend may have a cooler version of a particular toy, such as the latest Ninja Turtle action figure, while the other friend, not as wealthy, has a ninja turtle that’s bright yellow and has teeth marks.
In this case, you pick the coolest toy and leave it in the bucket.
Given a sufficient definition of cool, this is how type unions work in Scala.
A type is more refined if Scala knows more about it.
You may also have situations where you discover that both friends have broken or incomplete toys.
In this case, you would take pieces from each toy and attempt to construct the full toy.
For the most part, this analogy holds for compound types.
It’s a simple combination of all the members from the original types, with various override rules.
Type unions are even easier to understand when looking at them through the lens of structural types.
In Scala, a structural type is created using the type keyword and defining what method signatures and variable signatures you expect on the desired type.
This allows a developer to define an abstract interface without requiring users to extend some trait or class to meet this interface.
One common usage of structural typing is in the use of resource management code.
Reflection isn’t always available on every platform and it can lead to performance issues.
It’s best to provide named interfaces rather than use structural types in the general case.
However for nonperformance sensitive situations, they can be very useful.
Some of the most annoying bugs, in my experience, are resource-related.
We must always ensure that something acquired is released, and something created is eventually destroyed.
As such, there’s a lot of boilerplate code common when using resources.
I would love to avoid boilerplate code in Scala, so let’s see if structural types can come to the rescue.
Let’s define a simple function that will ensure that a resource is closed after some block of code is executed.
There’s no formal definition for what a resource is, so we’ll try to define it as anything that has a close method.
The first thing we do is define a structural type for resources.
We define a type of the name Resource and assign it to an anonymous, or structural, resource definition.
The resource definition is a block that encloses a bunch of abstract methods or members.
In this case, we define the Resource type to have one member, named close.
Finally, in the closeResource method, you can see that we can accept a method parameter using the structural type and call the close member we defined in definition.
Then we attempt to use our method against System.in, which has a close method.
You can tell the call succeeds by the exception that’s thrown.
In general, you shouldn’t close the master input or output streams when running inside the interpreter! But it does show that structural types have the nice feature of working against any object.
This is nice for dealing with libraries or classes we don’t directly control.
Structural typing also works within nested types and with nested types.
Let’s try implementing a simple nested abstract type and see if we can create a method that uses this type.
We then implement a real object Foo that meets this structural type.
Now we try to create a test method that should return the result of calling the x method on an instance of type T.
We expect this to return an integer, as the x method on T returns the X type, and this is aliased to Int.
Why? Scala doesn’t allow a method to be defined such that the types used are path-dependent on other arguments to the method.
In this case, the return value of test would be dependent on the argument to test.
We can prove this by writing out the expected return type explicitly:
If instead of using a path-dependent type, we wanted the type project against X we can modify our code.
The compiler won’t automatically infer this for us, because the inference engine tries to find the most specific type it can.
T#X, on the other hand, is valid in this context, and it’s also known to be an Int by the compiler.
Let’s see what signature the compiler creates for something returning the type T#X.
As you can see, the method is defined to return an Int, and works correctly against our Foo object.
What does this code look like if we have it use the abstract type Y instead? The compiler can make no assumptions about the type Y, so it only allows you to treat it as the absolute minimum type, or Any.
Let’s create a method that returns a T#Y type to see what it looks like:
As a quick aside, notice the types of the x and y methods.
The x and y methods have return values that are path-dependent on this.
When we defined the x method, we specified only a type of X, and yet the compiler turned the type into this.X.
Because the X type is defined within the structural type T, you can refer to it via the identifier X; it refers to the path-dependent type this.X.
Understanding when you’ve created a path-dependent type and when it’s acceptable to refer to these types is important.
When you reference one type defined inside another, you have a path-dependent type.
Using a path-dependent type inside a block of code is perfectly acceptable.
The compiler can ensure that the nested types refer to the exact object instance through examining the code.
But to escape a path-dependent type outside this original scope, the compiler needs some way of ensuring the path is the same instance.
This can sometimes boil down to using objects and vals instead of classes and defs.
First we set up the nested types T and U.
We then create an instance of type Foo.T labeled baz.
Being a val member, the compiler knows that this instance is unchanging throughout the lifetime of the program and is therefore stable.
Finally, we create a method that takes the type Foo.baz.U as an argument.
We accept this because the path-dependent type U is defined on a path known to be stable:Foo.baz.
When running into path-dependent type issues, we can fix things by finding a way for the compiler to know that a type is stable---that the type will also be well defined.
This can usually be accomplished by utilizing some stable reference path.
Let’s look at a more in-depth example of path-dependent types by designing an Observable trait that you can use as a generic mechanism to watch for changes or notify others of a change.
The Observable trait should provide two public methods: one that allows observers to subscribe and another that unsubscribes an observer.
Observers should be able to subscribe to an Observable instance by providing a simple function callback.
Types an observer can unsubscribe from change events on the observer at a future date.
With path-dependent types, we can enforce that this handle is valid only with the originating Observable instance.
The first thing to notice is the abstract Handle type.
We’ll use this type to refer to registered observer callback functions.
The observe method is defined to take a function of type this.type => Unit and return a handle.
The callback is a function that takes something of this.type and returns a Unit.
The type this.type is a mechanism in Scala to refer to the type of the current object.
This is similar to calling Foo.type for a Scala object with one major difference.
Unlike directly referencing the current type of the object, this.type changes with inheritance.
In a later example, we’ll show how a subclass of Observable will require callbacks to take their specific type as their parameter.
The unobserve function takes in a handle that was previously assigned to a callback and removes that observer.
This handle type is path-dependent and must come from the current object.
This means even if the same callback is registered to different Observable instances, their handles can’t be interchanged.
The next thing to notice is that we use a function here that isn’t yet defined: createHandle.
This method should be able to construct handles to callbacks when they’re registered in an observe method.
I’ve purposely left this abstract so that implementers of the observable pattern can determine their own mechanism for differentiating callbacks with handles.
The DefaultHandles trait extends Observable and provides a simple implementation of Handle: It defines the Handle type to be the same type as the callbacks.
In the case of Scala’s Function object equality and hash code are instance-based, as is the default for any user-defined object.
Now that there’s an implementation for the handles, let’s define an observable object.
Let’s create a IntHolder class that will hold an integer.
The IntHolder will notify observers every time its internal value changes.
The IntHolder class should also allow a mechanism to get the currently held integer and set the integer:
The IntStore class extends the Observable trait from the previous lines of code and mixes in the DefaultHandles implementation for handles.
The get method returns the value stored in the IntStore.
The set method assigns the new value and then notifies observers of the change.
The toString method has also been overridden to provide a nicer printed form.
Next an observer is registered that will print the IntStore to the console on changes.
The handle to this observer is saved in the handleval.
Notice that the type of the handle uses a path-dependent x.type.
The observer is notified and IntStore(2) is printed on the console.
Next the handle variable is used to remove the observer.
Now, when the value stored in x is changed to 4, the new value isn’t printed to the console.
What happens if we construct multiple IntStore instances and attempt to register the same callback to both? If it’s the same callback, using the DefaultHandles trait means that the two handles should be equal.
First we create separate instances, x and y of IntStore.
Next we need to create a callback we can use on both observers.
Now let’s register the callback on both the x and y variable instances and check to see if the handles are equal:
The result is that the handle objects are exactly the same.
Note that the == method does a runtime check of equality and works on any two types.
This means that theoretically the handle from y could be used to remove the observer on x.
Let’s look at what happens when attempting this on the REPL:
The path-dependent typing restricts our handles from being generated from the same method.
Even though the handles are equal at runtime, the type system has prevented us from using the wrong handle to unregister an observer.
This is important because the type of the Handle could change in the future.
If we implemented the Handle type differently in the future, then code that relied on handles being interchangeable between IntStores would be broken.
Path-dependent types have other uses, but this should give you a good idea of their use and utility.
What if, in the Observable example, we had wanted to include some kind of restriction on the Handle type? This is where we can use type constraints.
Type constraints are rules associated with a type that must be met for a variable to match the given type.
A type can be defined with multiple constraints at once.
Each of these constraints must be satisfied when the compiler is type checking expressions.
This is where the type selected must be equal to or a supertype of the lower bound restriction.
The first thing we do is define type B inside class A to have a lower bound of List[Int]
Then we instantiate a variable x as an anonymous subclass of A, such that type B is stabilized at Traversable[Int]
This doesn’t issue any warning, because Traversable is a parent class of List.
The interesting piece here is that we can call our foo method with a Set class.
A Set isn’t a supertype of the List class; it’s a subtype of Traversable! Just because the type restriction on type B requires it to be a superclass of List doesn’t mean that arguments matching against type B need to be within List’s hierarchy.
They only need to match against the concrete form of type B, which is Traversable.
What we can’t do is create a subclass of A where the type B is assigned as a Set[Int]; a Set could be polymorphically referred to as Iterable or Traversable.
Because Scala is a polymorphic object-oriented language, it’s important to understand the difference between the compile-time type constraints and runtime type constraints.
In this instance, we are enforcing that type B’s compile-time type information must come from a superclass of List or List itself.
Polymorphism means that an object of class Set, which subclasses Traversable, can be used when the compile-time type requires a Traversable.
When doing so, we aren’t throwing away any behavior of the object; we’re merely dropping some of our compile-time knowledge of the type.
It’s important to remember this when using with lower bound constraints.
An upper bound restriction states that any type selected must be equal to or a lower than the upper bound type.
In the case of a class or trait, this means that any selected type must subclass from the class or trait upper bound.
In the case of structural types, it means that whatever.
Type constraints type is selected must meet the structural type, but can have more information.
First we create a type B that has a lower bound of Traversable[Int]
When we use an unrefined type B, we can use any method defined in Traversable[Int] because we know that any type satisfying B’s type restriction needs to extend Traversable[Int]
Later we can refine type B to be a List[Int], and everything works great.
Once refined, we can’t pass other subtypes of Traversable[Int], such as a Set[Int]
The parameters to the count method must satisfy the refined type B.
We can also create another refinement of type B that’s a Set[Int], and this will accept Set[Int] arguments.
As you can see, upper bounds work the opposite way from lower bounds.
Another nice aspect of upper bounds is that you can utilize methods on the upper bound without knowing the full type refinement.
If the compiler ever warns about incompatible type signatures that include Nothing or Any, without your code referring to them, it’s a good bet that you have an unbounded type somewhere the compiler is trying to infer.
An interesting side note about Scala is that all types have an upper bound of Any and a lower bound of Nothing.
This is because all types in Scala descend from Any, while all types are extended by Nothing.
If the compiler ever warns about incompatible type signatures that include Nothing or Any, without you having specified them, it’s a good bet that you have an unbounded type somewhere the compiler is trying to infer.
If a method accepts an argument of type Any, it can be passed an expression of type Int.
When enforcing type constraints on method parameters it may not be necessary to use a type constraint but instead accept the subtype.
Because the type T doesn’t occur in the resulting value, the method could be written as.
They help us define generic methods that can retain whatever specialized types they’re called with.
They help design generic classes that can interoperate with all sorts of code.
The standard collection library uses them extensively to enable all sorts of powerful combinations of methods.
The collections, and other higher-kinded types, benefit greatly from using both upper and lower bounds in code.
To understand how and when to use them, we must first delve into type parameters and higher-kinded types.
Type parameters and higher-kinded types are the bread and butter of the type system.
A type parameter is a type definition that’s taken in as a parameter when calling a method, constructing a type, or extending a type.
Higher-kinded types are those that accept other types and construct a new type.
Just as parameters are key to constructing and combining methods, type parameters are the key to constructing and combining types.
Type parameters are defined within brackets ([]) before any normal parameters are defined.
Normal parameters can then use the types named as parameters.
Let’s look at a simple method defined using type parameters in figure 6.1:
This type parameter is then used in the method parameter list.
The randomElement method takes a List of some element type, named A, and returns an instance of that element type.
When calling the method, we can specify the type parameter as we wish:
You can see that when we specify Int for the type parameter, the method will accept lists of integers but not lists of strings.
But we can specify String as a type parameter and allow lists of strings but not lists of integers.
In the case of methods, we can even leave off the type parameter, and the compiler will infer one for us if it can:
If there are several arguments to a function, the compiler will attempt to infer a type parameter that matches all the arguments.
When passed an integer and a string, the parameter to List is inferred as Any.
This is the lowest possible type that both parameters will conform to.
Any also happens to be the top type, the one that all values conform to.
If we choose different parameters, say a String and a Seq, the compiler infers a lower type, that of java.lang .Object, otherwise known in Scala as AnyRef.
The compiler’s goal, and yours as well, is to preserve as much type information as possible.
This is easier to accomplish using type constraints on type parameters.
It’s possible to specify constraints in line with type parameters.
These constraints ensure that any type used to satisfy the parameter needs to abide by the constraints.
Specifying lower bound constraints also allows you to utilize members defined on the lower bound.
Upper bound constraints don’t imply what members might be on a type but are useful when combining several parameterized types.
Type parameters are like method parameters except that they parameterize things at compilation time.
It’s important to remember that all type programming is enforced during compilation and all type information must be known at compile time to be useful.
Type parameters also make possible the creation of higher-kinded types.
Higher-kinded types are those that use other types to construct a new type.
This is similar to how higher-order functions are those that take other functions as parameters.
A higher-kinded type can have one or more other types as parameters.
In Scala, you can do this using the type keyword.
The Callback type takes a type parameter and constructs a new Function1 type.
The type Callback isn’t a complete type until it’s parameterized.
Higher-kinded types can be used to make a complex type---for example, M[N[T, X], Y] look like a simpler type, such as F[X]
The Callback type can be used to simplify the signature for functions that take a single parameter and return no value.
This first statement constructs a Callback[Int] named x that takes an integer, adds it with the value 2 and prints the result.
The type Callback[Int] is converted by the compiler into the full type (Int) => Unit.
Higher-kinded types are used to simplify type signatures for complex types.
They can also be used to make complex types fit the simpler type signature on a method.
The foo method is defined as taking a type M that’s parameterized by an unknown type.
The _ keyword is used as a placeholder for an unknown, existential type.
Existential types are covered in more detail in section 6.5
The next statement calls the method foo with a type parameter of Callback and an argument of x, defined in the earlier example.
Higher-kinded types are used to simplify type definitions or to make complex types conform to simple type parameters.
Variance is an additional complication to parameterized types and higher-kinded types.
Variance refers to the ability of type parameters to change or vary on higher-kinded types, like T[A]
Variance is a way of declaring how type parameters can be changed to create conformant types.
A higher-kinded type T[A] is said to conform to T[B] if you can assign T[B] to T[A] without causing any errors.
The rules of variance govern the type conformance of types with parameters.
If we want to make use of covariance or contravariance, stick to immutable classes, or expose your mutable class in an immutable interface.
Invariance refers to the unchanging nature of a higher-kinded type parameter.
A higher-kinded type that’s invariant implies that for any types T, A, and B if T[A] conforms to T[B] then A must be the equivalent type of B.
Covariance refers to the ability to substitute a type parameter with its parent type: For any types T, A and B if T[A] conforms to T[B] then A <: B.
The Mammal and Cat relationship is such that the Cat type conforms to the Mammal type; if a method requires something of type Mammal, a value of type Cat could be used.
If a type T were defined as covariant, then the type T[Cat] would conform to the type T[Mammal]: A method requiring a T[Mammal] would accept a value of type T[Cat]
Type lambdas were discovered and popularized by Jason Zaugg, one of the core contributors to the Scalaz framework.
Notice that the direction of conformance arrows is the same.
The conformance of T is the same (co-) as the conformance of its type parameters.
The easiest example of this is a list, which is higher-kinded on the type of its elements.
You could have a list of strings or a list of integers.
Because Any is a supertype of String, we can use a list of strings where a list of Any is expected.
Creating a Covariant parameter is as easy as adding a + symbol before the type parameter.
Let’s create a covariant type in the REPL and try out the type conformance it creates.
First we construct a higher-kinded class T that takes a covariant parameter A.
Next we create a new value with the type parameter bound to AnyRef.
Now, if we try to assign our T[AnyRef] to a variable of type T[Any], the call succeeds.
This is because Any is the parent type of AnyRef, and our covariant constraint is satisfied.
But when we attempt to assign a value of type T[AnyRef] to a variable of type T[String], the assignment will fail.
The compiler has checks in place to ensure that a covariant annotation doesn’t violate a few key rules.
In particular, the compiler tracks the usage of a higher-kinded type and ensures that if it’s covariant, that it occurs only in covariant positions in the compiler.
We’ll cover the rules for determining variance positions soon, but for now, we’ll look at what happens if we violate one of our variance positions:
As you can see, the compiler gives us a nice message that we’ve used our type parameter A in a position that’s contravariant, when the type parameter is covariant.
We’ll cover the rules shortly, but for now it’s important to know that full knowledge of the rules isn’t needed to utilize variance correctly if you have a basic understanding of the.
You can reason in your head whether a type should be covariant or contravariant and then let the compiler tell you when you’ve misplaced your types.
You can use tricks to avoid placing types in contravariant positions when you require covariance, but first let’s look at contravariance.
Figure 6.3 shows the same conformance relationship between Mammal and Cat types.
If the type T is defined as contravariant, then a method expecting a type of T[Cat] would accept a value of type T[Mammal]
Notice that the direction of the conformance relationship is opposite (contra-) that of the Mammal--Cat relationship.
Contravariance can be harder to reason through but makes sense in the context of a Function object.
A Function object is covariant on the return type and contravariant on the argument type.
You can take the return value of a function and cast it to any supertype of that return value.
As for arguments, you can pass any subtype of the argument type.
First we create a foo method of type Any => String.
Next we define bar a method of type String => Any.
As you can see, we can implement bar in terms of foo, and both calls to bar and foo with strings return the same value, as the functions are implemented the same.
We can’t pass an Int variable to the bar method, as it will fail to compile, but we can pass that Int variable to the foo method.
Now if we want to construct an object that represents a function, we’d like this same behavior---that is, we’d like to be able to cast the function object as flexibly as possible.
The first type parameter is for the argument to the function and the second is for the return type.
We then construct a new value with an argument of type Any and a return value of type String.
Return values are actual values, and we know that we can always take a variable and cast it to a supertype.
Once again, we declare our Function trait; but this time we declare the return value type covariant.
We construct a new value of the Function trait, and again attempt to cast it.
But we’re able to cast our return value type from String to Any.
Once again we construct a Function trait, only this time the argument type is contravariant and the return type is covariant.
We instantiate an instance of the trait and attempt our cast, which succeeds! Let’s extend our trait to include a real implementation and ensure things work appropriately:
We create our Function trait, but this time it has an abstract apply method that will hold the logic of the function object.
Now we construct a new function object foo with the same logic as the foo method we had earlier.
We attempt to construct a bar Function object using the foo object directly.
Notice that no new object is created; we’re merely assigning one type to another similar to polymorphically assigning a value of a child class to a reference of the parent class.
Now we can call our bar function and receive the expected output.
But there are some situations you may encounter when you need to tweak your code for appropriate variance annotations.
When designing a higher-kinded type, at some point you’ll you wish it to have a particular variance, and the compiler won’t let you do this.
When the compiler restricts variance but you know it shouldn’t, there’s usually a simple transform that can fix your code to compile and keep the type system happy.
The easiest example of this is in the collections library.
The Scala collections library provides a mechanism for combining two collections types.
In the actual collections library, the method signature is pretty complicated due to the library’s advanced features, so for this example we’ll use a simplified version of the ++ signature.
Let’s attempt to define an abstract List type that can be combined with other lists.
We’d like to be able to convert, for example, a list of strings to a list of Any, so we’re going to annotate the ItemType parameter as covariant.
The compiler is complaining that we’re using the ItemType parameter in a contravariant position! This statement is true, but we know that it should be safe to combine two lists of the same type and still be able to cast them up the ItemType hierarchy.
Is the compiler too restrictive when it comes to variance? Perhaps, but let’s see if we can work around this.
We’re going to make the ++ method take a type parameter.
We can use this new type parameter in the argument to avoid having ItemType in a contravariant position.
The new type parameter should capture the ItemType of the other List.
Adding the OtherItemType lets the creation of the List trait succeed.
We implement an EmptyList class that’s an efficient implementation of a List of no elements.
The combination method,++, should return whatever is passed to it, because it’s empty.
When we define the method, we get a type mismatch.
The issue is that OtherItemType and ItemType aren’t compatible types! We’ve enforced nothing about OtherItemType in our ++ method and therefore made it impossible to implement.
Well, we know that we need to enforce some kind of type constraint on OtherItemType and that we’re combining two lists.
We’d like OtherItemType to be some type that combines well with our list.
Because ItemType is covariant, we know that we can cast our current list up the ItemType hierarchy.
As such, let’s use ItemType as the upper bound constraint on OtherItemType.
We’ll also need to change the return type of the ++ method to return the OtherItemType, as OtherItemType may be higher up the hierarchy than ItemType is.
Let’s take it for a test drive through the REPL and ensure that combining empty lists of various types returns the types we desire.
First we try to combine our list of Strings and Ints.
You can see that the compiler infers Any as a common superclass to String and Int and gives you a list of Any.
This is exactly what we wanted! Next we combine our list of strings with a list of anys and we get another list of anys.
If we combine the list of strings with a list of AnyRefs, the compiler infers AnyRef as the lowest possible type, and we retain a small amount of type information.
If we combine the list of strings with another list of strings, we’ll retain the type of a List[String]
We now have a list interface that’s very powerful and type-safe.
The variance annotation can affect a lot of the mechanics in Scala, including type inference.
The safest bet when working with variance is to start with everything invariant and mark variance as needed.
In general, when running into covariance or contravariance issues in class methods, its usually the case of introducing a new type parameter and using that for the signature of the method.
In fact, the final ++ method definition is far more flexible, and still type-safe; therefore, when you face issues with variance annotations, step back and try to introduce a few new type parameters.
Existential types are a means of constructing types where portions of the type signature are existential, where existential means that although some real type meets that portion of a type signature, we don’t care about the specific type.
Existential types were introduced into Scala as a means to interoperate with Java’s generic types, so we’ll start by looking at a common idiom found in Java programs.
In Java, generic types were added to the language later and done so with backward compatibility in mind.
As such, the Java collections API was enhanced to utilize generic types but still supports code that’s written without generic types This was done using a combination of erasure and a subdued form of existential types.
Scala, seeking to interoperate as closely as possible with Java, also supports existential types for the same reason.
Scala’s existential types are far more powerful and expressive than Java’s, as illustrated next.
The interface has a type parameter E which is used to specify the type of elements in the list.
The get method is defined using this type parameter for its return value.
This setup should be familiar from the earlier discussion of type parameters.
The strangeness begins when we look at the backward compatibility.
The older List interface in Java was designed without generic types.
Code written for this old interface is still compatible with Java Generics.
The generic parameter for the List interface is never specified.
In this example, Java is using existential types when type parameters aren’t specified.
This allows the older code, where List had no generic type parameter and the get method returned the type Object, to compile directly against the new collections library.
Is creating a list without any type parameters equivalent to creating a List<Object>? The answer is no; they have a subtle difference.
When compiling the following code, you’ll see unchecked warnings from the compiler:
Java doesn’t consider List and List<Object> the same type, but it will automatically convert between the two.
This is done because the practical difference between these two types, in Java, is minimal.
In Scala, existential types take on a slightly different flavor.
Let’s create an existential type in a Java class and see what it looks like in the Scala REPL.
The Test class provides a single makeList method that returns a List with existential type signature.
Let’s start up the Scala REPL and call this method:
Scala provides a convenience syntax for creating existential types that uses the underscore in the place of a type parameter.
We’ll cover the full syntax shortly; but this shorthand is more commonly found in production code.
This _ can be considered a place holder for a single valid type.
The _ is different from closure syntax because it isn’t a placeholder for a type argument, but rather is a hole in the type.
The compiler isn’t sure what the specific type parameter is, but it knows that there is one.
You can’t substitute a type parameter later; the hole remains.
This means we can’t add new values to the list without some form of casting.
In Java, this operation would compile and an Unchecked warning would be issued.
Existential types can also have upper and lower bounds in Scala.
This can be done by treating the _ as if it were a type parameter.
The foo method is defined to accept a List of some parameter that has a lower bound of Int.
This parameter could have any type; the compiler doesn’t care which type, as long as it’s Int or one of its super types.
We can call this method with a value of type String, because Int and String share a parent, Any.
When calling the foo method with a List("Hi") the call succeeds, as expected.
Scala’s formal syntax for existential types uses the forSome keyword.
Here’s the excerpt explaining the syntax from the Scala Language Specification:
In the preceding definition, the Q block is a set of type declarations.
Type declarations, in the Scala Language Specification could be abstract type statements or abstract val statements.
The compiler knows that there’s some type that meets these definitions, but doesn’t remember what that type is specifically.
The type declared in the T section can then use these existential identifiers directly.
This is easiest to see by converting the convenient syntax into the formal syntax.
Remember that all type declarations in the forSome blocks are treated as existential that can be used in the left hand side type.
The forSome block could be used for any kind of type declaration, including values or other existential types.
And now it’s time for a more complex example involving existential types.
Remember the Observable trait from the dependent type section? Let’s take another look at the interface for the Observable trait:
Imagine we wanted some generic way to interact with the Handle types that are returned from this object.
You can declare a type that can represent any Handle using existential types.
This means that the value x is existential: the compiler doesn’t care which Observable value, only that there is one.
On the left-hand side of the type declaration is the pathdependent type x.Handle.
Notice that it isn’t possible to create this existential type using the convenience syntax, as the _ can only stand in for a type declaration.
Let’s create a trait that will track all the handles from Observables using the Ref type.
We’d like this trait to maintain a list of handles such that we can appropriately clean up the class when needed.
The handles member is defined as a List of Ref types.
An addHandle method is defined, which takes a handle, the Ref type, and adds it to the list of handles.
The removeDependencies method loops over all registered handles and calls their remove method.
Finally, the observe method is defined such that it registers a handler with an observable and then registers the Handle returned using the addHandle method.
You may be wondering how we’re able to call remove on the Handle type.
This is invalid code using the Observable trait as defined earlier.
The code will work this minor addition to the Observable trait:
The Observable trait is now defined such that the Handle type requires a method remove() : Unit.
Now you can use the Dependencies trait to track handlers registered on observables.
The first line creates a new VariableStore which is a subclass of Observable.
The next statement constructs a Dependencies object, d, to track registered observable handles.
The next two lines register the function println with the Observablex and add the handle to the Dependencies object d.
This invokes the observers with the current value and the registered observer println is called to print VariableStore(1) on the console.
After this, the removeDependencies method is used on the object d to remove all tracked observers.
The next line again changes the value in the VariableStore and no values are output on the console.
Existential types provide a convenient syntax to represent these abstract types and interact with them.
Although the formal syntax isn’t frequently used, this uncommon situation is the most common case where it’s needed.
It’s a great tool to understand and use when running into nested types that seem inexpressible, but it shouldn’t be needed in most situations.
In this chapter, you learned the basic rules governing Scala’s type system.
We looked at structural typing and how you can use it to emulate duck typing in dynamic languages.
We learned how to create generic types using type parameters and how to enforce upper and lower bounds on types.
We looked at higher-kinded types and type lambdas and how you can use them to simplify complex types.
We also looked into variance and how to create flexible parameterized classes.
Finally we explored existential types and how to create truly abstract methods.
These basic building blocks of the Scala type system are used to construct advanced models of behavior and interactions within a program.
In the next chapter, we use these fundamentals to implement implicit resolution and advanced type system features.
The type system and the implicit resolution mechanism provide the tools required to write expressive, type-safe software.
Implicits can encode types into runtime objects and can enable the creation of type classes that abstract behavior from classes.
Implicits can be used to directly encode type constraints and to construct types recursively.
Combined with some type constructors and type bounds, implicits and the type system can help you encode complex problems directly into the type system.
Most importantly, you can use implicits to preserve type information and delegate behavior to type-specific implementations while preserving an abstract interface.
The ultimate goal is the ability to write classes and methods that can be reused anytime they’re needed.
Scala supports two types of type constraint operators that aren’t type constraints but are implicit lookups—context bounds and view bounds.
These operators allow us to define an implicit parameter list as type constraints on generic types.
This syntax can reduce typing in situations where implicit definitions must be available for lookup but don’t need to be directly accessed.
View bounds are used to require an available implicit view for converting one type into another type.
This constraint means that the parameter x is of type A and there must be an implicit conversion function A => B available at any call site.
This foo method is also defined with a type parameter A that has no constraints.
Two parameter lists exist: one that accepts an A type and one that requires the implicit conversion function.
Although this second form requires more typing, it places a userdefined label on the implicit conversion function.
Context bounds, similar to view bounds, declare that there must be an implicit value available with a given type.
This constraint means that the parameter x is of type A and there must be an implicit value B[A] available when calling method foo.
To use implicit type constraints or not When should you choose to use view/context bounds vs.
The code within the method doesn’t need to access the implicit parameter directly, but relies on the implicit resolution mechanism.
In this situation, we must require an implicit parameter be available to call another function that requires that implicit parameter, or the implicit is automatically used.
The meaning conveyed by the type parameter is more clear using context/view bounds than an implicit parameter list.
See section 7.3 on type classes for examples of this scenario.
This foo method defines two parameter lists, with the implicit parameter list accepting a value of type B[A]
The key difference between the two foo versions is that this one gives an explicit label to the B[A] parameter that can be used within the function.
Context bounds are extremely useful in helping provide implicit values in companion objects.
This naturally leads to type classes, shown in section 7.3
Type classes are a means of encoding behavior into a wrapper or accessor for another type.
They are the most common use of context bound constraints.
Scala’s implicit views are often used to enrich existing types, where enrich means adding additional behavior that doesn’t exist on the raw type (see Odersky’s paper “Pimp My Library”: http://mng.bz/86Qh)
Implicit type constraints are used when we want to enrich an existing type while preserving that type in the type system.
For example, let’s write a method that will return the first element of a list and the list itself:
The method defines a type parameter T for the element of the collection.
It then returns the head of the traversable and the traversable itself.
When calling this method with an array, the resulting type is a Traversable[Int] and the runtime type is WrappedArray.
This method has lost the original type information about the array.
Context bounds and view bounds allow developers to enforce complex type constraints in a simple fashion.
They are best applied when we do not need to access the captured implicit by name, but the method requires the implicit to be available in its scope.
A better polymorphism One of the key tenants of object-oriented programming is polymorphism: the ability of a complex type to act as a simple type.
In the absence of generic types and bounds, polymorphism usually results in a loss of type information.
One of the biggest benefits of using Scala is the ability to preserve specific types across generic methods.
The entire collections API is designed such that methods defined in the lower level types preserve the original types of the collections as much as possible.
For example, a method that works with types that could be serialized but doesn’t serialize them might look something like this:
The method implementation calls send on each receiver, passing the message to each.
But the implementation of the send method on the Receiver type would need to use the Serializable value, so it’s better to explicitly specify the implicit argument list in that method.
Context bound and view bound constraints are used to clarify the intent of an implicit argument.
Implicit arguments can be used to capture a relationship from the type system.
Scala 2.8 formalized the ability to encode type information into implicit parameters.
It does this through two mechanisms: Manifests and implicit type constraints.
A Manifest for a type is generated by the compiler, when needed, with all the known information for that type at that time.
Manifests were added specifically to handle arrays and were generalized to be useful in other situations where the type must be available at runtime.
Implicit type constraints are direct encoding of supertype and equivalence relationships between types.
These can be useful to restrict a generic type further within a method.
These constraints are static—that is, they happen at compile time.
As stated earlier, Manifests were first introduced into Scala to help deal with arrays.
In Scala, an array is a class with a parameter.
On the JVM, however, there are different types of arrays for every primitive type and one for objects.
The Java language distinguishes between these types and requires programs to do so as well.
Scala allows code to be written for generic Array[T] types.
But because the underlying implementation must know whether the original array was an int[], a double[], or one of the other array types, Scala needed a way to attach this information to the type so that generic array implementations would know how to treat the array, and so the birth of Manifests.
ClassManifest provides the smallest runtime overhead for the greatest benefit.
While Manifests are extremely useful, any advanced type checking should still be performed by the compiler and not at runtime.
Usually ClassManifest is good enough when reified types are required.
You can use Manifests to pass along more specific type information with a generic type parameter.
In Scala, all methods involving Arrays require an appropriate Manifest for the array’s type parameter.
This was done because although Scala treats Arrays as generic classes, they are encoded differently by type on the JVM.
Rather than encode these differently in Scala, for example int[] and double[], Scala chose to hide the runtime behavior behind the Array[T] class and associated methods.
Because different bytecode must be emitted for Array[Int] and Array[Double], Scala uses Manifests to carry around the type information about arrays.
Manifest—This Manifest stores a reflective instance of the class associated with a type T as well as Manifest values for each of T’s type parameters.
This allows reflective invocations of methods defined on that class.
If there’s one available, then the OptManifest instance will be the Manifest subclass.
If none can be provided, then the instance will be the NoManifest class.
ClassManifest—This class is similar to Manifest except that it only stores the erased class of a given type.
This erased class of a type is associated with the type without any type parameters.
For example, the type List[Int] would have the erased class List.
For a type with a lot of nesting, this can be quite deep, and methods on the Manifest may traverse the entire depth.
The ClassManifest is designed for scenarios where type parameters don’t need to be captured.
The OptManifest is designed for situations where a Manifest isn’t needed for runtime behavior, but can be used for runtime improvements if it’s available.
Manifests are useful in Scala to create abstract methods whose implementations diverge based on the types they work on, but the resulting outputs don’t.
A good example of this is any method using generic arrays.
Because Scala must use different bytecode instructions depending on the runtime array type, it requires a ClassManifest on the Array element.
The first method is defined as taking a generic Array of type A.
It attempts to construct a new array containing only the first element of the old array.
But because we haven’t captured a Manifest, the compiler can’t figure out which runtime type the resulting Array should have.
Now the A type parameter also captures the implicit ClassManifest.
When called with an Array[Int], the compiler constructs a ClassManifest for the type Int, and this is used to construct a runtime array of the appropriate type.
This could be used directly instead of delegating to Scala’s generic Array factory method.
Using Manifests requires capturing the Manifest when a specific type is known before passing to a generic method.
If the type of an array were “lost,” the array couldn’t be passed into the first method.
The value x is constructed as an Array with existential type.
The first method can’t be called with the value x because a ClassManifest can’t be found for the array’s type.
Although this example is contrived, the situation itself occurs when working with Arrays in nested generic code.
We solve this by attaching Manifests to types all the way down the generic call stack.
This type can then be inspected and used at runtime, but the Manifest can only capture the type available when the Manifest is looked for on the implicit scope.
Manifests are useful tools for capturing runtime types, but they can become viral in code, needing to be specified in many different methods.
Use them with caution and in situations where they’re required.
Don’t use them to enforce type constraints when the type is known by the compiler; instead these can be captured with another implicit at compile time.
The intersection of type inference and type constraints sometimes causes issues where you need to use reified type constraints.
What are reified type constraints? These are objects whose implicit existence verifies that some type constraint holds true.
The <:< class is the reification of the upper bound constraint.
It’s an object that you can call methods on at runtime.
Why do we need reified type constraints? Sometimes it can help the type inferencer automatically determine types for a method call.
One of the neat aspects of the type inferencing algorithm is that implicits can defer the resolution of types until later in the algorithm.
Scala’s type inferencer works in a left-to-right fashion across parameter lists.
This allows the types inferred from one parameter list to affect the types inferred in the next parameter list.
A great example of this left-to-right inference is with anonymous functions using collections.
The foo method defines two parameter lists with one type parameter: one that takes a list of the unknown parameter and another that takes a function using the unknown parameter.
When calling foo without a type parameter, the call succeeds because the compiler is able to infer the type parameter A as equal to String.
This type is then used for the second parameter list, where the compiler knows that the _ should be of type String.
This doesn’t work when we combine the parameter lists into one parameter list.
In this case, the compiler complains about the anonymous function _.isEmpty because it has no type.
The compiler has not inferred that A = String yet, so it can’t provide a type for the anonymous function.
The compiler is unable to infer all the parameters in one parameter list, so the implicit parameter list is used to help the type inferencer.
Let’s create a method peek that will return a tuple containing a collection and its first element.
This method should be able to handle any collection from the Scala library, and it should retain the original type of the method passed in:
The method will have two type parameters: one to capture the specific type of the collection, called C, and another to capture the type of elements in the collection, called A.
The type parameter C has a constraint that it must be a subtype of Traversable[A], where Traversable is the base type of all collection in Scala.
The method returns types A and C, so specific types are preserved.
But the type inferencer can’t detect the correct types without annotations.
The call to peek with a List[Int] type fails because the type inferencer is unable to find both types C and A using only one parameter.
To solve this, let’s make a new implementation of peek that defers some type inference for the second parameter list:
This peek method also has two type parameters but no type constraints on the C parameter.
The first argument list is the same as before, but the second argument list takes an implicit value of type C <:< Traversable[A]
Just as methods named after operators can be used in operator notation, so can types with type parameters.
Let’s look at the code for the <:< type found in scala.Predef:
This means that <:< can be used in any context where Java serialization may be used.
Next, the conforms method takes a type parameter A and returns a new <:< type such that it converts from type A to A.
Now when using the peek method without type parameters, the type inferencer succeeds:
Calling the peek method with a List[Int] correctly returns an Int and a List[Int] type.
Capturing type relationships can also be used to restrict an existing type parameter at a later time.
Sometimes a parameterized class allows methods if the parameter supports a set of features, or extends a class.
I call these specialized methods—that is, the method is for a specialized subset of a generic class.
These methods use the implicit resolution system to enforce the subset of the generic class for which they’re defined.
For example, the Scala collections have a specialized method sum that works only for numerical types.
This type is any supertype of the type of elements in the collection.
The parameter num is an implicit lookup for the Numeric type class.
This type class provides the implementation of zero and plus, as well as other methods, for a given type.
The sum method uses the methods defined on numeric to fold across the collection and “plus” all the elements together.
This method can be called on any collection whose type supports the Numeric type class.
This means that if we desired, we could provide our own type class for types that aren’t normally considered numeric.
For example, let’s use sum on a collection of strings:
The first REPL line constructs an implicitly available Numeric[String] class.
The zero method is defined to return an empty string and the plus method is defined to aggregate two strings together.
Next, when calling sum on a List of strings, the result is that the strings are all appended together.
The sum method used the Numeric type class we provided for the String type.
Methods can also be specialized using the <:< and =:= classes.
For example, a method that compressed a Set, if that set was of integers, could be written like this:
The Set trait is defined with a type parameter representing the elements contained in the set.
The compress function will take the current set and return a compressed version of it.
But the only implementation of a compressed set available is the CompressedIntSet, is a Set[Int] that optimizes storage space using compression techniques.
The implicit ev parameter is used to ensure that the type of the original set is exactly Set[Int] such that a CompressedIntSet can be created.
Specialized methods are a great way to provide a rich API and enforce type safety.
They help smooth out some rough edges between generalized classes and specific use cases for those generalized classes.
In the Scala core library, they’re used to support numerical operations within the collection framework.
They pair well with type classes, which provide the most flexibility for users of a class.
A type class is a mechanism of ensuring one type conforms to some abstract interface.
The type class pattern became popular as a language feature within the Haskell programming language.
In Scala, the type class idiom manifests itself through higher-kinded types and implicit resolution.
We’ll cover the details of defining your own type class, but initially let’s look at a motivating example for why you should use type classes.
Let’s design a system that synchronizes files and directories between various locations.
These files could be local, remote, or located in some kind of version control system.
We’d like to design some form of abstraction that can handle synchronizing between all the different location possibilities.
Using our object-oriented hats, we start with trying to define an abstract interface that we can use of.
Let’s call our interface FileLike, and define what methods we need for synchronization.
To start, we know that we need a method of determining whether a FileLike object is an aggregate of other FileLike objects.
When the FileLike object is a directory, we need a mechanism to retrieve the FileLike objects it contains.
We need some way of determining if a directory contains a file we see in another directory.
To do this, we’ll provide a child method that attempts to discover a FileObject contained in the current FileLike object with a given relative filename.
That is, a FileLike object that is a placeholder for a real file.
We can use the null object to write data into new files.
We’d like a mechanism to check whether the FileLike object exists, so we’ll make a method called exists.
In the case of directories, we add the mkdirs method to create a directory at the path defined by a null FileLike object.
Next we supply the content and writeContent methods as a mechanism to retrieve content from FileLike objects and to write content to FileLike objects.
For now, we’ll assume that we always write file contents from one side to another, and there’s no optimizations for files that might exist on both sides and be equivalent.
The FileLike interface is defined with the methods described earlier.
The method name returns the relative name of the FileLike object.
The method exists returns whether or not the file has been created on the filesystem.
The method isDirectory returns whether the class is an aggregate of other files.
The children method returns a sequence containing all the FileLike objects that are contained in the current FileLike object, if it is a directory.
The child method returns a new FileLike object for a child file below the current file.
This method should throw an exception if the current file isn’t a directory.
The mkdirs method creates directories required to ensure the current file is a directory.
The content method returns an InputStream containing the contents of the file.
Finally, the writeContent method accepts an InputStream and writes the contents to the file.
The synchronize function contains two helper methods, one for directory like objects and another for FileLike objects.
The synchronize method then delegates to these two helper functions appropriately.
Let’s try to capture the fromFileLike type separate from the toFileLike type.
These can ensure the method arguments have the correct order.
The synchronize method now captures the from type in the type parameter F and the to type in the type parameter T.
Great! Now there’s a compilation failure on the synchronize call.
In fact, if the arguments are reordered, the exception remains.
The FileLike interface doesn’t preserve the original type when getting children! One fix would be to modify the FileLike interface to be higher-kinded, and use the type parameter to enforce the static checks.
Let’s modify the original FileLike interface to take a type parameter:
This new definition of FileLike uses a recursive type constraint in its type parameter.
The captured type T must be a subtype of FileLike.
This interface works great for the synchronization method, except it suffers from one problem: the need to create FileLike wrappers for every FileLike object passed to the method.
When synchronizing java.io.File and java.net.URL instances, a wrapper must be provided.
Instead of defining the type FileLike[T <: FileLike[T]], we can define FileLike[T]
This new trait would allow interacting with any T as if it were a file and doesn’t require any inheritance relationship.
The FileLike type class trait looks similar to the higher-kinded FileLike trait, except for two key points.
The FileLike type class works for a particular type T and against it.
This brings us to the second difference: All the methods take a parameter of type T.
The FileLike type class isn’t expected to be a wrapper around another class, but instead it’s an accessor of data or state from another class.
It allows us to keep a specific type, while treating it generically.
Let’s look at what the synchronization method becomes using the FileLike type class trait.
Notice the use of the context bounds syntax for FileLike.
As described in section 7.1, this is equivalent to defining an implicit parameter for the FileLike on a given type.
The next thing to notice is the implicitly method lookup of the FileLike parameters.
Finally, every call made that utilizes type F or T uses the FileLike type class.
The synchronize method can now work across many different types.
Let’s see what happens when we use it on two java.io.File objects.
The compiler now complains that there’s no implicit value for FileLike [java.io.File]
This is the error message provided if we attempt to use a type that doesn’t have a corresponding type trait in the implicit scope.
The error message isn’t quite what we want, and may be improved later, but it’s important to understand what this message means.
The synchronize method requires a type trait implementation for java.io.File.
The conventional way to provide default implicit values for a set of types is through a companion object to the type class trait.
The writeClient method is more complex, so you can find the implementation in the source for this book.
Remember that the companion object is one of the last places checked for implicit values.
Separation of abstractions—Type classes create new abstractions and allow other types to adapt, or be adapted, to the abstraction.
This is helpful when creating an abstraction that works with preexisting types, and those types can’t be changed.
Composability—You can use the context bound syntax to specify multiple types.
This means you can easily require the existence of several type classes when writing your methods.
This is far more flexible than expected for some abstract interface, or a combination of abstract interfaces.
Type classes can also use inheritance to compose two type classes together into one implicit variable that provides both.
Sometimes this may make sense, but in general type classes retain the most flexibility by avoiding inheritance.
Overridable—Type classes allow you to override a default implementation through the implicit system.
By putting an implicit value higher in the lookup chain, you can completely replace how a type class is implemented.
This can be helpful when providing several type classes with various behaviors because the user can select a nondefault type class when needed.
Type safety—You could use several mechanisms, such as reflection, instead of type classes.
The primary reason to prefer type classes over these methods is the guaranteed type safety.
When requiring a specific behavior using a type class, the compiler will warn if that behavior isn’t found, or isn’t yet implemented.
Although reflection could be used to find methods on any class and call them, its failure occurs at runtime and isn’t guaranteed to occur during testing.
Type classes are a powerful design tool and can greatly improve the composability and reusability of methods and abstractions.
These abstractions can also compose into higher-level type classes that are combinations of the lower-level ones.
The Serializable type class is defined such that it can serialize a given type T.
The method Tuple2 accepts two type parameters, T and V, as well as implicit Serializable type classes associated with these parameters.
The Tuple2 method returns a Serializable type class for (T,V) tuples.
Now any Tuple2 of types that support the Serializable type class also supports the Serializable class.
Type classes start to show some of the power and complex constraints that can be encoded into the type system.
This can be further extended to encode significantly complex type dependent algorithms and type level programming.
There comes a time in an algorithm’s life when it needs to do something rather clever.
This clever behavior encodes portions of the algorithm into the type system so that it can execute at compile time.
The sort algorithm can be written against the raw Iterator interface.
But if I call sort against a vector, then I’d like to be able to utilize vector’s natural array separation in my sorting algorithm.
Traditionally this has been solved with two mechanisms: overloading and overriding.
Using overloading, the sort method is implemented in terms of Iterable and another is implemented in terms of Vector.
The downside to overloading is that it prevents you from using named/default parameters, and it can suffer at compile time due to type erasure.
The types used in parameters are erased at runtime into a lower type.
This means that functions that operate on parameterized types can erase to the same bytecode on the JVM causing conflict.
This is one of the reasons to avoid overloading in Scala.
Using overriding, the sort method is implemented against a base class.
Each subclass that wishes to specialize the sort method should override the base class implementation with its own custom sort mechanism.
In the case of Iterable and Vector, both would need to define the same sort method.
The downside to overriding is that the type signatures must be the same and there must be an inheritance relationship between the classes owning a method.
Overriding seems like a better option than overloading but imposes some strict limitations, especially the inheritance relationship.
The inheritance restriction prevents external methods from using overriding behavior, limiting them to overloading and its drawbacks.
The solution is to use the implicit system to associate a type class with the external types.
For the sort method, it can be modified to accept an implicit parameter of type Sorter, where the Sorter class contains all the sort logic, as follows:
The Sorter class is defined with a single method sort.
The sort method accepts a value of type A and returns type B.
The sort method is constructed such that it accepts a collection of type A and an implicit Sorter object and sorts the collection.
The sort algorithm selection has been turned into a type system problem.
Each algorithm has been converted into a type and the selection has been encoded into the implicit system.
This premise can be generalized to encode other types of problems into the type system.
It’s simple to encode conditional logic into the type system.
This can be done by encoding Boolean types into the type system.
The TBool trait is defined having one type constructor If.
This type constructor can be considered a method working inside the type system with types as its arguments and types as its results.
The If type constructor takes three arguments: the type to return if the TBool is true, the type to return if the TBool is false, and an upper bound for the return values.
Now let’s encode the true and false types into the type system.
Its If type constructor is overridden to return the first type passed in.
Its If type constructor is overridden to return the second type passed in.
The X type constructor is created to accept an encoding Boolean type and return either the type String or the type Int.
Conditional execution using the type system type of X[TTrue], but because the X type constructor is designed to return the type String when passed the TTrue type, compilation fails because the value is of type Int.
The next definition of x succeeds because the X type constructor evaluates to String and the value is of type String.
This mechanism of encoding logic into the type system can be useful at times.
One feature that’s lacking in the Scala standard library but that’s available in the MetaScala library is a heterogeneous typed list—that is, a type-safe list of values with unbounded size.
This is similar to Scala’s TupleN classes, except that a heterogeneous typed list supports append operations to grow the list with additional types.
The key to a type-safe list is encoding all the types of the list into the type system and preserving them throughout the usage of the list.
The preceding line constructs a heterogeneous list comprising of a string, an integer, and a Boolean value.
It contains each of the types in the list embedded within HCons types, ending with HNil.
The structure of the heterogeneous list is shown in the type.
It’s a linked list of cons cells, holding a single value type and the rest of the list.
There’s a special list called HNil, which represents the termination of a list or an empty list.
They also carry around the current type of the head and the remaining type of the list.
HCons is a linked list both in physical memory and in the type system.
The HNil type represents the termination of the list and is similar to using Nil to terminate reference/pointer based linked lists.
This is the same for creating if/else type constructs or heterogeneous lists.
This HList trait is a marker trait for constructing HLists.
The HCons type encodes a link in a linked list.
The value for the head is parameterized and can be any type.
The tail is another HList but is parameterized as T.
This is how the type system can capture the complete types of the heterogeneous list.
The types are encoded in a linked list of HCons types as the values are stored in a linked list of HCons values.
The HNil class also extends HList and represents an empty list or the end of list.
Finally, the object HList is used to provide convenience aliases for the HCons and HNil types.
The answer is the full type of the list is required when constructing a new HCons cell.
If you placed this definition on HList, the captured type T in any new HCons cell would always be only HList.
This negates the desired effect of preserving the type information in the list.
The :: and HNil types are encoded as a class with corresponding value because they must be used in type signatures and expressions.
The class types allow them to be directly referenced in type signatures, and the values allow them to be used as expressions.
The val x is defined with type String :: Int :: Boolean :: HNil and the expression "Hi" :: 5 :: false :: HNil.
If we made HNil an object, the type would instead be String :: Int :: Boolean :: HNil.type.
Combined with the HNil value, this enables us to extract typed values from a list using pattern matching.
Let’s pull the values out of the x list constructed earlier:
The first line is a pattern match value assignment from list x.
The resulting types of one, two, and three are String, Int, and Boolean respectively, and the values are extracted correctly.
You can also use this extraction to pull out portions of the list; for example, let’s pull the first two elements from the x list:
This line extracts the first and second value into variables called first and second.
The rest of the list is placed into a variable called rest.
Notice the types of each: first and second have the correct types from the portion of the list, and the rest variables is of type ::[Boolean,HNil] or Boolean :: HNil.
This mechanism of extracting typed values from the list is handy, but it’d be nice to have an indexing operation.
The indexing operation can be encoded directly into the type system using functions.
The indexAt2of3 method takes a heterogeneous list of three elements and returns the second element.
The next call shows that the method works and will infer the types from the heterogeneous list.
This direct encoding of indexing operations is less than ideal.
An explosion of methods is required to index elements into lists of various sizes.
The heterogeneous list would also have support methods like insert into index and remove from index.
These operations would have to be duplicated if we used this style of direct encoding.
Let’s construct a type that looks at a particular index and can perform various operations like adding, retrieving, or removing the element at the index.
This type is called an IndexedView as it represents a view of the heterogeneous list at a given index into the list.
To be able to append or remove elements from the list, the view must have access to the types preceding the current index and the types after the current index.
Before is the types of all the elements in the list before the current index.
After is the types of all elements in the list after the current index.
At is the type at the current index of the list.
Fold is used to look at the entire list and return a given value.
Fold takes a function that will look at the before, at and after portions of the list.
This allows us to use fold to perform operations centered at this current index.
The get method is implemented in terms of fold to return the value at the current index.
At this index, the Before type would be String :: Int ::HNil.
Notice that the Before type isn’t exactly the same as the previous HCons cell, because it’s terminated with HNil after the previous two types.
The important aspect of the IndexedView is that it gives us direct access to the type of the current value—that is, we can name the current type using the type parameter At.
It also preserves the types preceding and following the current type such that we can use them with aggregate functions.
Constructing an IndexedView at an index in the list is done recursively.
Let’s start with the base case of defining an IndexedView at the first index of a list.
The class HListView0 accepts a list of head type H and a tail type of T.
The Before type is an empty list, as there are no elements before for the first index.
The After type is the same as the captured type of the list’s tail, T.
The At type is the type of the current head of the list, H.
The fold method is implemented such that it calls the function f with an empty list, the head and the tail of the list.
Let’s create an instance of IndexedView that delegates to another IndexedView.
The HListViewN class has two type parameters: H and NextIdxView.
NextIdxView is the type of the next IndexedView class used to construct an IndexedView.
The Before type is the current type parameter H appended to the next indexer’s HList.
The After type is also deferred to the next indexer.
The side effect of this is that the At and After types will be determined by an HListView0 and carried down the recursive chain by the HListViewN classes.
Finally, the fold operation calls fold on the next IndexedView and wraps the before list with the current value.
To construct an IndexedView at the third element of an HList requires two HListViewN classes linked to an HListView0 class.
The HListView0 class points directly at the cons cell, which holds the third element of the HList.
Each instance of the HListViewN class appends one of the previous types of the list to the original HListView0 class.
One important piece to mention about the HListViewN classes is that they retain references to the elements of the list and recursively rebuild portions of the list in.
You can see this in the diagram with the arrows labeled “h”
The runtime performance implications are that the farther down a list an index goes, the more recursion required to perform operations.
Now that there’s a mechanism to construct IndexedView classes at arbitrary depths of the HList, there must be a method of constructing these classes.
The first will be a mechanism that takes a list and builds a type for the view at index N of that list.
The second mechanism is a way of constructing the recursive IndexedView types if the final type is known.
For the first mechanism, let’s add a type to the HList class that will construct an IndexedView at a given index value for the current list.
The ViewAt type constructor is defined as constructing a subclass of IndexedView.
The full value will be assigned in the HCons and HNil classes respectively.
The ViewAt type constructor takes a parameter of type Nat.
Nat is a type that we create for this purpose representing natural numbers encoded into the type system.
Nat is constructed the same way naturally numbers are constructed in mathematical proofs, by building from a starting point.
The trait Nat is used to mark natural number types.
The trait _0 is used to denote the starting point for all natural numbers, zero.
The Succ trait isn’t directly referenced but is used to construct the rest of the natural number set (or at least as many as we wish to type)
The next step is to use these index values to construct the IndexedView type for an HList at that index.
To do so, let’s construct a mechanism to pass type lambdas into natural numbers and build complete types.
The Nat trait is given a new type called Expand.
The first is a type lambda that’s applied against the previous natural number if the Nat.
The second is the type returned if the natural number is _0
The third type is an upper bound for the first two types to avoid compilation type inference issues.
Let’s implement this type on the _0 and Succ traits:
The _0 trait defines its Expand type to be exactly the second parameter.
This is similar to a method call that returns its second parameter.
The Succ trait defines its expand method to call the type constructor passed into the first parameter against the previous Nat type.
This can be used to recursively build a type by providing a type that uses itself in the NonZero type attribute.
Let’s use this trick and define the ViewAt type on HList.
The ViewAt type is defined as an expansion against the natural number parameter N.
The first type parameter to Expand is the recursive type constructor.
Deconstructing, the type is an HListViewN comprised of the current head type and the tail’s ViewAt type applied to the previous natural number (or N-1)
If a Nat index is passed into the ViewAt type that goes beyond the size of the list, it will fail at compile time with the following message:
The compiler will issue an illegal cyclic reference in this instance.
Although not exactly the error message desired in this situation, the compiler prevents the invalid index operation.
Now that the indexed type can be constructed for a given index and a given HList, let’s encode the construction of the IndexedView into the implicit system.
We can do this with a recursive implicit lookup against the constructed IndexedView type.
The IndexedView companion object is given two implicit functions: index0 and indexN.
The function indexN takes an HList and an implicit conversion of the tail of the HList into an IndexedView and returns a new IndexedView of the complete list.
The type parameters on indexN preserve the types of the head and tail of the list as well as the full type of the IndexedView used against the tail of the list.
The full implicit value is found, and a constructor of an IndexedView from a HList is available.
The viewAt method is defined as taking a type parameter of the Nat index and an implicit function that constructs the IndexedView from the current list.
The first line in the example constructs a heterogeneous list and the second shows how to use a natural number to index into the list (assuming _0 is the first element of the list)
The heterogeneous list demonstrates the power of Scala’s type system.
It encodes an arbitrary sequence of types and allows type-safe indexing of this sequence.
This type-level programming is the most advanced usage of the Scala type system that may be required for general development.
The simple build tool (SBT) is used to build Scala code that utilizes a different form of the HList presented earlier.
Although HLists are complicated, the SBT tool introduces them in a way that’s simple and elegant for the user.
It’s worth going to http://mng.bz/Cdgl and reading how they’re used.
In this chapter, you learned the advanced techniques for utilizing Scala’s type system.
Implicits allow the capturing of runtime and compile time type constraints.
You can use type classes as a general purpose abstraction to associate types with functionality.
They are one of the most powerful forms of abstraction within Scala.
Finally, we explored in depth the conditional execution and type level programming.
This advanced technique tends to be used in core libraries and not as much in user code.
The main theme in all of these sections is that Scala allows developers to preserve type information while writing low-level generic functions.
The more type information that can be preserved, the more errors the compiler can catch.
For example, the synchronize method defined in section 7.3 was able to prevent accidental argument reversal by capturing the from and to types.
The HList class allows developers to create arbitrarily long typed lists of elements that can be modified directly rather than passing around a List[Any] and having to determine the types of each element at runtime.
This also prevents users from placing the wrong type at a given index.
The less a method or class assumes about its arguments or types, the more flexible it is and the more often it can be reused.
The next chapter covers the Scala collections library, and we make heavy use of the concepts defined in this chapter.
In particular, the collections library attempts to return the most specific collection type possible after any method call.
The Scala collections library is the single most impressive library in the Scala ecosystem.
It’s used in every project and provides myriad utility functions.
The Scala collections provide many ways of storing and manipulating data, which can be overwhelming.
Because most of the methods defined on Scala collections are available on every collection, it’s important to know what the collection types imply in terms of performance and usage patterns.
Changing the execution semantics of a collection from sequential to parallel and back.
Sometimes parallel execution can drastically improve throughput, and sometimes delaying the evaluation of a method can improve performance.
The Scala collections library provides the means for developers to choose the attributes their collections should have.
The biggest difficulty with all the new power from the collections library is working generically across collections.
We discuss a technique to handle this in section 8.5
Let’s look at the key concepts in the Scala collection library and when to use each.
With all the new choices in the Scala collections library, choosing the right collection is important.
Each collection has different runtime characteristics and is suited for different styles of algorithms.
For example, Scala’s List collection is a single linked-list and is suited for recursive algorithms that operate by splitting the head off the rest of the collection.
In contrast, Scala’s Vector class is implemented as a set of nested arrays that’s efficient at splitting and joining.
The key to utilizing the Scala collections library is knowing what the types convey.
In Scala, there are two places to worry about collection types: creating generic methods that work against multiple collections and choosing a collection for a datatype.
Creating generic methods that work across collection types is all about selecting the lowest possible collection type that keeps the generic method performant, but isn’t so high up the collections hierarchy that it can’t be used for lots of different collections.
In fact, the type-system tricks we discuss in section 7.3 can allow you to use type-specialized optimizations generically.
Choosing a collection for a datatype is done by instantiating the right collection type for the use case of the data.
The core abstractions in the collections library illustrate different styles of collections.
Each level in the hierarchy represents a new set of abstract functions that can be implemented to define a new collection or add performance goals onto the parent class.
The collections hierarchy starts with the Traversable abstraction and works it way toward Map, Set, and IndexedSequence abstractions.
Let’s look at the abstract hierarchy of the collections library.
This trait represents a collection that can be traversed at least once.
An Iterator is a stream of incoming items where advancing to the next item consumes the current item.
A Traversable represents a collection that defines a mechanism to traverse the entire collection but can be traversed repeatedly.
The Iterable trait is similar to Traversable but allows the repeated creation of an Iterator.
The hierarchy branches out into sequences, maps (also known as dictionaries), and sets.
The generic variants of collections offer no guarantees on serial or parallel execution, while the traits discussed here enforce sequential execution.
The principles behind each collection are the same, but traversal ordering isn’t guaranteed for parallel collections.
Let’s look at when to use each of the collection types.
A rich set of collections The Scala collections library is rich in choices.
The core set of abstractions has variants that allow one or more of these differentiators to be true.
The Traversable trait is defined in terms of the foreach method.
This method is an internal iterator---that is, the foreach method takes a function that operates on a single element of the collection and applies it to every element of the collection.
Traversable collections don’t provide any way to stop traversing inside the foreach.
To make certain operations efficient, the library uses preinitialized exceptions to break out of the iteration early and prevent wasted cycles.
This technique is somewhat efficient on the JVM, but some simple algorithms will suffer greatly.
The index operation, for example, has complexity O(n) for Traversable.
When using Traversables, it’s best to utilize operations that traverse the entire collection, such as filter, map, and flatMap.
Traversables aren’t often seen in day-to-day development, but when they are, it’s common to convert them into another sort of collection for processing.
For example, we’ll define a Traversable that opens a file and reads its lines for every traversal.
Internal versus external iterators Iterators can either be internal or external.
An internal iterator is one where the collection or owner of the iterator is responsible for walking it through the collection.
An external iterator is one where the client code can decide when and how to iterate.
Scala supports both types of iterators with the Traversable and Iterable types.
The Traversable trait provides the foreach method for iteration, where a client will pass a function for the collection to use when iterating.
The Iterable trait provides an iterator method, where a client can obtain an iterator and use it to walk through the collection.
The downside is that any collections that only support internal iterators must extend Traversable and nothing else.
The foreach method is overridden to open the file and read lines from the file.
The method uses a try-finally block to ensure the file is closed after iteration.
This implementation means that every time the collection is traversed, the file is opened and all of its contents are enumerated.
Finally, the toString method is overridden so that when it’s called within the REPL, the entire file’s contents aren’t enumerated.
The second line iterates over all the lines in the file and splits this line into words before constructing a new list with the words.
The result is another Traversable of String that has all the individual words of the file.
The return type is Traversable even though the runtime type of the resulting list of words is a scala.List.
The starting type in the for expression was a Traversable, so the resulting type of the expression will also be a Traversable without any outside intervention.
Although we can’t create efficient random element access, the traversable can be terminated early if necessary.
The foreach method has been modified with logging statements in three places: The first is when the file is opened, the second is when the file has reached its termination state, and the third is when the file is closed.
But now when trying to extract all the individual words, the logging statements are printed.
The file is opened, the iteration is completed, and the file is closed.
Now what happens if only the top two lines of the file need to be read?
The take method is used to limit a collection to the first n elements, or in this case the first two elements.
Now when extracting the lines of the file, the Opening file and Closing file logging statements print, but not the Done iterating file statement.
This is because the Traversable class has an efficient means of terminating foreach early when necessary.
This preallocated exception can be efficiently thrown and caught on the JVM.
The downside to this approach is that the take method will read three lines of the file before the exception is thrown to prevent continued iteration.
Traversable is one of the most abstract and powerful traits in the collections hierarchy.
The foreach method is the easiest method to implement for any collection type, but it’s suboptimal for many algorithms.
It doesn’t support random access efficiently and it requires one extra iteration when attempting to terminate traversal early.
The next collection type, Iterable, solves the latter point by using an external iterator.
The Iterable trait is defined in terms of the iterator method.
This returns an external iterator that you can use to walk through the items in the collection.
This class improves slightly on Traversable performance by allowing methods that need to parse a portion of the collection stop sooner than Traversable would allow.
External iterators are objects you can use to iterate the internals of another object.
The Iterable trait’s iterator method returns an external iterator of type Iterator.
The hasNext method returns true if there are more elements in the collection and returns false otherwise.
The next method returns the next element in the collection or throws an exception if there are none left.
In the worst case, the file may remain open for the entire life of an application.
Because of this issue, subclasses of Iterable tend to be standard collections.
The major benefit of the Iterable interface is the ability to coiterate two collections efficiently.
The first is a list of names and the second is a list of addresses.
We can use the Iterable interface to iterate through both lists at the same time efficiently.
The value n is created as an external iterator on the name strings.
The value a is created as an external iterator on the address strings.
The while loop iterates over both the a and n iterators simultaneously.
When joining information between two collections, requiring the Iterable trait can greatly improve the efficiency of the operation.
But we still have an issue with using external iterators on mutable collections.
The collection could change without the external iterator being aware of the change:
Next, the value i is constructed as an iterator over the array.
Now let’s remove all the elements from the mutable structure and see what happens to the i instance:
Because the iterator is external, it isn’t aware that the underlying collection has changed and returns true, implying there’s a next element.
We should use the Iterable trait when explicit external iteration is required for a collection, but random access isn’t required.
The Seq trait is defined in terms of the length and apply method.
We can use the apply method to index into the collection by its ordering.
The Seq trait offers no guarantees on performance of the indexing or length methods.
We should use the Seq trait only to differentiate Sets and Maps from sequential collections---that is, if the order in which things are placed into a collections is important and duplicates should be allowed, then the Seq trait should be required.
A good example of when to use a Sequence is when working with sampled data, such as audio.
Audio data is recorded at a sampling rate and the order in which it occurs is important in processing that data.
Using the Seq trait allows the computation of sliding windows.
Let’s instantiate some data and compute the sum of elements in sliding windows over the data.
The tails method returns an iterator over the tail of an existing collection.
This means, each successive collection in the tails iterator has one less element.
These collections can be converted into sliding windows using the take method, which ensures only N elements exist (in this case two)
Next, we use the filter method to remove windows that are less than the desired size.
Finally, the sum method is called on these windows and the resulting collection is converted to a list.
Sequence tends to be used frequently in abstract methods when the algorithms are usually targeted at one of its two subclasses: LinearSeq and IndexedSeq.
The LinearSeq trait is used to denote that a collection can be split into a head and tail component.
The trait is defined in terms of three “assumed to be efficient” abstract methods: isEmpty, head, and tail.
The isEmpty method returns true if the collection is empty.
The head method returns the first element of the collection if the.
The sliding method Scala defines a sliding method on collections that can be used rather than the tails method.
The tail method returns the entire collection minus the head.
This type of collection is ideal for tail recursive algorithms that split collections by their head.
A Stack is a collection that operates like a stack of toys.
It’s easy to get the last toy placed on the stack, but it could be frustrating to continually remove toys to reach the bottom of the stack.
A LinearSeq is similar in that it can be decomposed into the head (or top) element and the rest of the collection.
Let’s look at how we can use a LinearSeq as a stack in a tree traversal algorithm.
The trait BinaryTree is defined as covariant on its type parameter A.
It has no methods and is sealed to prevent subclasses outside of the current compilation unit.
It’s defined with the type parameter specified to Nothing, which allows it to be used in any BinaryTree.
The Branch class is defined such that it has a value and a left-hand tree and right-hand tree.
Finally, the Leaf type is defined as a BinaryTree that contains only a value.
The traverse method is defined to take a BinaryTree of content elements A and a function that operates on the contents and returns values of type U.
The traverse method uses a nested helper method to implement the core of its functionality.
The traverseHelper method is tail recursive and is used to iterate over all the elements in the tree.
Use the right collection a nextLinearSeq, which contains the elements of the binary tree that it should look at later.
The traverseHelper method does a match against the current tree.
If the current tree is a branch, it’ll send the value at the branch to the function f and then recursively call itself.
When it does this recursive call, it passes the left-hand tree as the next node to look at and appends the right-hand tree to the front of the LinearSeq using the +: method.
Appending the right-hand tree to the LinearSeq is a fast operation, usually O(1), due to the requirements of the LinearSeq trait.
If the traverseHelper method encounters a Leaf, the value is sent to the function f.
But if the next stack isn’t empty, the stack is decomposed using the head and tail method.
These methods are defined as efficient for the LinearSeq, usually O(1)
The head is passed into the traverseHelper method as the current tree and the tail is passed as the next stack.
Finally, if the traverseHelper method encounters a NilTree it operates similarly to when it encounters a Leaf.
Because a NilTree doesn’t contain data, only the recursive traverseHelper call is needed.
Now, let’s construct a BinaryTree and see what traversal looks like:
First, a BinaryTree is created with two branches and two leaves.
This technique of manually creating a Stack on the Heap and deferring work onto it is a common practice when converting a general recursive algorithm into a tail recursive algorithm or an iterative algorithm.
When using a functional style with tail recursion, the LinearSeq trait is the right collection to use.
The IndexedSeq trait is similar to the Seq trait except that it implies that random access of collection elements is efficient---that is, accessing elements of a collection should be constant or near constant.
This collection type is ideal for most generalpurpose algorithms that don’t involve head-tail decomposition.
Let’s look at some of the random access methods and their utility.
An IndexedSeq can be created using the factory method defined on the IndexedSeq object.
By default, this will create an immutable Vector, described in section 8.2.1
IndexedSeq collections have an updated method that takes an index and a new value and returns a new collection with the value at the index updated.
Indexing into an IndexedSeq is done with the apply method.
In Scala, a call to an apply method can be abbreviated so that indexing looks like the following:
The result of the expression is the value at index 2 of the collection x.
In Scala, indexing into any type of collection, including arrays, is done with an apply method rather than specialized syntax.
Sometimes it’s more important to check whether or not a collection contains a particular item than it is to retain ordering.
The Set trait denotes a collection where each element is unique, at least according to the == method.
A Set is the ideal collection to use when testing for the existence of an element in a collection or to ensure there are no duplicates within a collection.
Scala supports three types of immutable and mutable sets: TreeSet, HashSet, and BitSet.
The TreeSet is implemented as a red black tree of elements.
A red black tree is a data structure that attempts to remain balanced, preserving O(log2n) random access to elements.
We find elements in the tree by checking the current node.
If the current node is greater than the desired value, we check the left subbranch.
If the current node is less than the desired value, then we check the right subbranch.
If the elements are equal, then the appropriate node was found.
To create a TreeSet, an implicit Ordering type class is required so that the less than and greater than comparisons can be performed.
The HashSet collection is also implemented as a tree of elements.
The biggest difference is that the HashSet uses the hash of a value to determine which node in the tree to place an element.
This means that elements that have the same hash value are located at the same tree node.
If the hashing algorithm has a low chance of collision, HashSets generally outperform TreeSets for lookup speed.
The BitSet collection is implemented as a sequence of Long values.
A BitSet stores an integer value by setting the bit corresponding to that value to true in the underlying Long value.
BitSets are often used to efficiently track and store in memory a large set of flags.
One of the features of Sets in Scala is that they extend from the type (A) => Boolean—that is, a Set can be used as a filtering function.
Let’s look at mechanisms to limit the values in one collection by another.
Note that any collection can be converted into a set (with some cost) using the toSet method.
Because a Set is also a filtering function, it can be passed directly to the filter function on the range.
Although Scala’s Set collection provides efficient existence checking on collections, the Map collection performs a similar operation on key value pairs.
The Map trait denotes a collection of key value pairs where only one value for a given key exists.
Map provides an efficient lookup for values based on their keys:
The first statement constructs a map of error codes to error messages.
The second statement accesses the value at key 1 in the errorcodes map.
These implementations are similar to the HashSet and TreeSet implementations.
The basic rule of thumb is that if the key values have an efficient hashing algorithm with low chance of collisions, then HashMap is preferred.
Scala’s Map provides two interesting use cases that aren’t directly apparent from the documentation.
The first is that, similarly to Set, a Map can be used as a partial function from the key type to the value type.
For each element in the list, a value is searched for in the errorcodes map.
The result is a list of error messages corresponding to the previous list of error values.
Scala’s Map also provides the ability to specify a default value to return in the event a key doesn’t yet exist:
The addresses map is constructed as the configuration of what mailing address to use for a username.
The Map is given a default value corresponding to a local company address where most users are assumed to be located.
When looking up the address for user josh, a specific address is found.
When looking up the address for the user john, the default is returned.
In idiomatic Scala, we usually use the generic Map type directly.
This may be due to the underlying implementation being efficient, or from the convenience of threeletter collection names.
Regardless of the original reason, the generic Map type is perfect for general purpose development.
Now that the basic collection types have been outlined, we’ll look at a few specific immutable implementations.
Immutable collections are the default in Scala and have many benefits over mutable collections in general purpose programming.
In particular, immutable collections can be shared across threads without the need for locking.
Scala’s immutable collections aim to provide both efficient and safe implementations.
Many of these collections use advanced techniques to ‘share’ memory across differing versions of the collection.
Let’s look at the three most commonly used immutable collections: Vector,List, and Stream.
In the absence of hard performance constraints, you should make Vectors the default collection.
Let’s look at its internal structure to see what makes it an ideal collection.
Vectors are composed as a trie on the index position of elements.
A trie is a tree where every child in a given path down the tree shares some kind of common key as shown in figure 8.2:
Each node of the tree contains two values and a left and right branch.
The path to any index in the trie can be determined from the binary representation of the number.
This gives us a well known path down the trie for any index given the binary number.
The cost of indexing any random element in the binary index trie is log2(n), but this can be reduced by using a higher branching factor.
For smaller collections, due to the ordering of the trie, the access time is less.
The random access time of elements scales with the size of the trie.
The other property of the trie is the level of sharing that can happen.
If nodes are considered immutable, then changing values at a given index can reuse portions of the trie.
Here’s an updated trie where the value at index 0 is replaced with the value new.
To create the new trie, two new nodes were created and the rest were reused as is.
Immutable collections can benefit a great deal from sharing structure on every change.
This can help reduce the overhead of changing the collections.
The key difference is that a Vector represents the branches of a trie as an array.
This turns the entire structure into an array of arrays.
Finding an element in the collection involves determining the depth of the tree and indexing into the appropriate display array the same way that the binary trie was indexed.
Because Scala’s Vector is branched 32 ways, it provides several benefits.
As well as scaling lookup times and modification times with the size of the collection, it also provides decent cache coherency because elements that are near each other in the collection are likely to be in the same array in memory.
Essential, like C++’s vector, Scala’s Vector should be the preferred collection for general-purpose computational needs.
Its efficiency combined with the thread-safety gained from immutability make it the most powerful sequence in the library.
Vector is the most flexible, efficient collection in the Scala collections library.
Its indexing performance is excellent, as are append and prepend.
When unsure of the runtime characteristics of an algorithm, it’s best to use a Vector.
This can have decent performance if you’re always appending or removing from the front of the list, but it can suffer with more advanced usage patterns.
Most users coming from Java or Haskell tend to use List as a go-to default from habit.
Although List has excellent performance in its intended use case, it’s less general than the Vector class and should be reserved for algorithms where it’s more efficient.
List is comprised of two classes: An empty list called Nil and a cons cell, sometimes referred to as a linked node.
The cons cell holds a reference to a value and a reference to the rest of the list.
Creating a list is as simple as creating cons cells for all the elements in the list.
In Scala, the cons cell is called :: and the empty list is called Nil.
A list can be constructed by appending elements to the empty list Nil.
If an operator ends with the : character, it’s considered right associative.
The preceding line is equivalent to the following method calls:
In this version, it’s easier to see how the list is constructed.
Scala’s treatment of the : character in operator notation is a general concept designed to handle cases like this where left associativity is not as expressive as right associativity.
The List collection extends from LinearSeq, as it supports O(1) head/tail decomposition and prepends.
Lists can support large amounts of sharing as long as you use prepends and head/tail decomposition.
But if an item in the middle of the list needs to be modified, the front half of the list needs to be generated.
This is what makes List less suited to general-purpose development than Vector.
The head and tail components of a list are known when the list is constructed.
Scala provides a different type of linked list where the values aren’t computed until needed.
A stream can lazily evaluate its members and persist them over time.
A stream could represent an infinite sequence without overflowing memory constraints.
Streams remember values that were computed during their lifetime, allowing efficient access to previous elements.
This has the benefit of allowing backtracking but the downside of causing potential memory issues.
A Stream is composed of cons cells and empty streams.
The biggest difference between Stream and List is that Stream will lazily evaluate itself.
Rather than storing elements, a stream stores functionobjects that can be used to compute the head element and the rest of the Stream (tail)
This allows Stream to store infinite sequences: a common tactic to join information with another collection.
For example, this can be used to join indexes with elements in a sequence.
The from method on Stream creates an infinitely incrementing stream starting at the passed in number.
The zip method does pairwise join at each index element of two sequences.
The result is that the elements of the original list are joined with their original indices.
Even though the stream is infinite, the code compiles successfully because the stream is generated only for the indices required by the list.
Constructing a stream can be done similarly to list, except the cons (::) cell is constructed with the #:: method and an empty stream is referred to as Stream.empty.
These three members are all prepended to an empty stream.
When accessing the first element in the stream, the head is returned without touching the rest of the stream.
But when the second index is accessed, the stream needs to compute the value.
When computing the value, the string HI is printed to the console.
Next, when indexing into the third value, it must be computed and the BAI string is printed.
Now when printing the value of the stream to the console, it displays all three elements, because they’re persisted.
The stream won’t recompute the values for indices it has already evaluated.
The Haskell language has lazy evaluation by default while Scala has eager evaluation by default.
When looking for something from a lazily evaluated list, like Haskell’s list, use Scala’s Stream, not its List.
One excellent use of Streams is computing the next value of the stream using previous values.
This is evident when calculating the Fibonacci sequence, which is a sequence where the next number is calculated using the sum of the previous two numbers.
The helper function f is defined to take two integers and construct the next portion of the Fibonacci sequence from them.
The #:: method is used to prepend the first input number to the stream and recursively call the helper function f.
The recursive call puts the second number in place of the first and adds the two numbers together to send in as the second number.
Effectively, the function f is keeping track of the next two elements in the sequence and outputs one every time it’s called, delaying the rest of the calculation.
This method call drops the first three values of the sequence and takes five values from it and converts them to a list.
The resulting portion of the Fibonacci sequence is displayed to the screen.
Notice that now the fibs sequence prints out the first eight elements.
This is because those eight elements of Stream were evaluated.
Streams don’t work well when the eventual size of the stream won’t fit into memory.
In these instances, it’s better to use a TraversableView to avoid performing work until necessary while allowing memory to be reclaimed.
If you need arbitrary high indices into a Fibonacci sequence, the collection could be defined as follows:
The fibs2 collection is defined as a Traversable of integers.
The foreach method is defined in terms of a helper method next.
The next method is almost the same as the helper method for the fibs stream except that instead of constructing a Stream, it loops infinitely, passing Fibonacci sequence values to the traversal function f.
This Traversable is immediately turned into a TraversableView with the view method to prevent the foreach method from being called immediately.
Now, let’s use this version of a lazily evaluated collection.
Similarly to Stream, the Fibonacci sequence is calculated on the fly and values are inserted into the resulting list.
But when reinspecting the fibs2 sequence after operating on it, none of the calculated indices are remembered on the TraversableView.
This means indexing into the TraversableView repeatedly could be expensive.
It’s best to save the TraversableView for scenarios where a Stream would not fit in memory.
Stream provides an elegant way to lazily evaluate elements of a collection.
This can amortize the cost of calculating an expensive sequence or allow infinite streams to be used.
Although mutability should and can be avoided in general development, it’s necessary in situations and can be beneficial.
Let’s look at how to achieve mutability in Scala’s collection library.
Mutable collections are collections that can conceptually change during their lifetime.
The individual elements of the array could be modified at any point during the array’s lifetime.
In Scala, mutability isn’t the default in the collections API.
For example, the map, flatMap, and filter methods defined on mutable collections will create new collections rather than mutate a collection in place.
The mutable collections library provides a few collections and abstractions that need to be investigated over and above the core abstractions:
The ArrayBuffer collection is a mutable Array that may or may not be the same size as that of the collection.
This allows elements to be added without requiring the entire array to be copied.
Internally, an ArrayBuffer is an Array of elements, as well as the stored current size.
When an element is added to an ArrayBuffer, this size is checked.
If the underlying array isn’t full, then the element is directly added to the array.
The key is that the new array is constructed larger than required for the current addition.
Although the entire array is copied into the new array on some append operations, the amortized cost for append is a constant.
Amortized cost is the cost calculated over a long time---that is, during the lifetime of an ArrayBuffer, the cost of appending averages out to be linear across all the operations, even though any given append operation could be O(1) or O(n)
This property makes ArrayBuffer a likely candidate for most mutable sequence construction.
The main difference between the two is that Java’s ArrayList attempts to amortize the cost of removing and appending to the front and back of the list whereas Scala’s ArrayBuffer is only optimized for adding and removing to the end of the sequence.
The ArrayBuffer collection is ideal for most situations where mutable sequences are required.
In Scala, it’s the mutable equivalent of the Vector class.
Let’s look at one of the abstractions in the mutable collections library, mixin mutation event publishing.
Scala’s mutable collection library provides three traits, ObservableMap, ObservableBuffer, and ObservableSet, that can be used to listen to mutation events on collections.
Mixing one of these traits into the appropriate collection will cause all mutations to get fired as events to observers.
These events are sent to observers, and the observers have a chance to prevent the mutation.
The object x is created as an ArrayBuffer that mixes in the ObservableBuffer.
In the constructor, a subscriber is registered that prints events as they happen:
Adding the element 1 to the collection causes the Include event to be fired.
This event indicates that a new element is included in the underlying collection.
The result is a Remove event, indicating the index and value that was removed.
The API is designed for advanced use cases such as data binding.
Data binding is a practice where one object’s state is controlled from another object’s state.
That way, any changes to the underlying ArrayBuffer would update the display of the UI element.
Scala’s mutable collection library also allows mixins to be used to synchronize operations on collections.
A Synchronized* trait can be used on its corresponding collection to enforce atomicity of operations on that collection.
Let’s look at an alternative solution to parallelizing and optimizing collection usage: Views and Parallel collections.
Strict evaluation is when operations are performed immediately when they’re defined.
This is in contrast to lazy evaluation where operations can be deferred.
Sequential evaluation is when operations are performed sequentially without parallelism against a collection.
As shown in figure 8.6, this is in contrast to parallel evaluation where evaluation could happen on multiple threads across portions of the collection.
The collections library provides two standard mechanisms to migrate from the default evaluation semantics into either parallel or lazy evaluation.
These take the form of the view and par methods.
The view method can take any collection and efficiently create a new collection that will have lazy evaluation.
The force method is the inverse of the view method.
It’s used to create a new collection that has strict evaluation of its operations.
In a similar fashion, the par method is used to create a collection that uses parallel execution.
The inverse of the par method is the seq method, which converts the current collection into one that supports sequential evaluation.
One important thing to note in the diagram is what happens when calling the par method on a View.
As of Scala 2.9.0.1, this method call converts a sequential lazy collection into a parallel strict collection rather than a parallel lazy collection.
This is due to how the encoding of types work.
The collections in the collections library default to strict evaluation---that is, when using the map method on a collection, a new collection is immediately created before continuing with any other function call.
Method calls on a view will only be performed when they absolutely have to.
This line constructs a list and iterates over each of its elements.
During this iteration, the value is printed and the value is modified by one.
The result is that each element gets printed to the string and the resulting list is created.
Let’s do the same thing, except this time we’ll change from using a List to a ListView.
This expression is the same as the previous one, except the view method is called on List.
The view method returns a view or window looking at the current collection that delays all functions as long as possible.
The map function call against the view is not performed.
Let’s modify the result so that the values are printed.
The toList method is called against the view from the previous example.
Because the view must construct a new collection, the map function that was deferred before is executed and the value of each item of the original list is printed.
Finally, the value of the new list, with each element incremented by one, is returned.
This class opens a file and iterates over the lines for every traversal.
This can be combined with a view to create a collection that will load and parse a file when it’s iterated.
Traversable views provide the best flexibility between simplicity in implementation, runtime cost, and utility for ephemeral streams of data.
If the data is only streamed once, a TraversableView can get the job done.
Let’s imagine that a system has a simple property configuration file format.
Every line of a config file is a key-value pair separated by the = character.
Also, any line that doesn’t have an = character is assumed to be a comment.
The expression matches lines that have a single = sign with content on both sides.
Finally, a for expression is used to parse the file.
The regular expression P is used to extract key-value pairs and return them.
The parsedConfigFile method is called with a sample config file.
The config contains two attribute value pairs that are numbered.
Now, when the configuration file should be read, the force method can be used to force evaluation of the view.
The file is opened and iterated, pulling out all the parsable lines; these values are returned into the result.
Using views with Traversables is a handy way to construct portions of a program and delay their execution until necessary.
For example, in an old Enterprise JavaBeans application, I constructed a parsable TraversableView that would execute an RMI call into the ApplicationServer for the current list of “active” data and perform operations on this list.
Although there were several operations to whittle down and transform the data after it was returned from the ApplicationServer, these could be abstracted behind the TraversableView, similarly to how the config file example exposed a TraversableView of key value pairs rather than a TraversableView of the raw file.
Let’s look at another way to change the execution behavior of collections: parallelization.
Parallel collections are collections that attempt to run their operations in parallel.
A Splitable iterator is an iterator that can be efficiently split into multiple iterators where each iterator owns a portion of the original iterator.
In the example, the Splitable iterator is split into two iterators.
These split iterators can be fed to different threads for processing.
The Parallel collection operations are implemented as tasks on Splitable iterators.
Each task could be run using a parallel executor, by default a ForkJoinPool, initialized with a number of worker threads equal to the number of processors available on the current machine.
Tasks themselves can be split, and each task defines a threshold it can use to determine whether it should be split further.
Let’s look at figure 8.8 to see what might happen in parallel when calculating the sum of elements in a parallel collection.
The sum method on the collection of one to eight is split into seven tasks.
Each task computes the sum of the numbers below it.
If a task contains more numbers than the threshold, assumed to be two in this example, the collection is split and more sum tasks are thrown on the queue.
These tasks are farmed out to a ForkJoinPool and executed in parallel.
A ForkJoinPool is like a thread pool, but with better performance for tasks that fork (split) first and join (combine) later.
When using parallel collections, there are two things to worry about:
The collections library does its best to reduce the cost of the first.
The library defines a ParArray collection that can take Arrays, and Array-based collections, and convert them into their parallel variant.
The library also defines a ParVector collection that can efficiently, in 0(1) time, convert from a Vector to a ParVector.
In addition to these, the library has a mechanism to parallelize Set, Map, and the Range collections.
The collections that aren’t converted efficiently are those that subclass from LinearSeq.
In this example, a List of three numbers is converted to a parallel collection using the par method.
The runtime complexity of this operation is O(N) because the library has to construct a Vector from the List.
If this overhead, combined with the overhead of parallelizing, is less than the runtime of an algorithm, it can still make sense to parallelize LinearSeq.
But in most cases, it’s best to avoid LinearSeq and its descendants when using parallel collections.
The second thing to worry about with parallel collections is the parallelizability of a task.
This is the amount of parallelism that can be expected from a given task.
For example, the map method on collections can have a huge amount of parallelism.
The map operation takes each element of a collection and transforms it into something else, returning a new collection.
The preceding constructs a parallel vector of integers and converts all the elements to their string representation.
One method that doesn’t have any parallelism is the foldLeft method.
The foldLeft method on collections takes an initial value and a binary operation and performs the operation over the elements in a left-associative fashion.
The association requires that the operations be performed in sequence.
If this were used on a parallel collection to compute the sum of elements in the collection, it would not execute in parallel.
The foldLeft method is called with an initial value of 0 and an operation of +
The result is the correct sum, but there’s no indication of whether this was parallelized.
Let’s use a cheap trick to figure out if the foldLeft is parallelized.
This time the foldLeft method is called with an empty Set of strings.
The binary operation appends the current executing thread to the set.
But if the same trick is used with the map operation, more than one thread will be displayed on a multicore machine:
The map operation converts each element to the current thread it’s running in, and the resulting list of threads is converted to a Set, effectively removing duplicates.
The resulting list on my dual-core machine has two threads.
Notice that unlike the previous example, the threads are from the default ForkJoinPool.
So, using parallel collections for parallelism requires using the parallelizable operations.
With all these different collection types, it can be difficult to write generic methods that need to work against many collection types.
The new collections library goes to great lengths to ensure that generic methods, like map, filter, and flatMap will return the most specific type possible.
If you start with a List, you should expect to retain a List for the duration of computations unless you perform some sort of transformation.
You can do this property through a few type system tricks.
Let’s look at implementing a generic sort algorithm against collections.
The NaiveQuickSort object is defined with a single method sort.
It takes in an Iterable of elements of type T.
The implicit parameter list accepts the type class Ordering for the type T.
This is what’s used to determine if one of the elements of the Iterable is larger, smaller, or equal to another.
The implementation pulls a pivot element and splits the collection into three sets: elements less than the pivot, elements greater than the pivot, and elements equal to the pivot.
These sets are then sorted and combined to create the final sorted list.
This algorithm works for most collections but has one obvious flaw:
The result is a sorted collection, but the type is Iterable, not List.
The method does work, but the loss of the List type is undesirable.
Let’s see if the sort can be modified to retain the original type of the collection, if possible.
The guts of the algorithm are the same as before, except a generic builder is used to construct the sorted list.
The biggest change is in the signature of the sort method, so let’s deconstruct it.
The ordering members are imported on the first line of the method.
It’s a common habit for folks new to Scala to define generic collection parameters as follows: Col[T] <: Seq[T]
Don’t do this, as this type doesn’t quite mean what you want.
Instead of allowing any subtype of a sequence, it allows only subtypes of a sequence that also have type parameters (which is most collections)
You can run into issues if your collection has no type parameters or more than one type parameter.
Both of these will fail type checking when trying to pass them into a method taking Col[T] <: Seq[T]
For the object Foo, this is because it has no type parameters, but the constraint Col[T] <: Seq[T] requires a single type parameter.
Although there are workarounds, that requirement can be surprising to users of the function.
The workaround is to defer the type-checking algorithm using the implicit system (see section 7.2.3)
To get the compiler to infer the type parameter on the lower bound, we have to defer the type inferencer long enough for it to figure out all the types.
To do that, we don’t enforce the type constraint until we do the implicit lookup using the <:< class.
Although most consider the SeqLike classes to be an implementation detail, they’re important when implementing any sort of generic method against collections.
SeqLike captures the original fully typed collection in its second type parameter.
This allows the type system to carry the most specific type through a generic method so that it can be used in the return value.
The next type parameter in the sort method is the cbf : CanBuildFrom[Coll, T, Coll]
The CanBuildFrom trait, when looked up implicitly, determines how to build new collections of a given type.
The second type parameter represents the type of elements desired in the built collection---that is, the type of elements in the collection the method is going to return.
The final type parameter of CanBuildFrom is the full type of the new collection.
This is the same type as the input collection in the case of sort, because the sort algorithm should return the same type of collection that came in.
The CanBuildFrom class is used to construct the builder b.
This builder is given a sizeHint for the final collection and is used to construct the final sorted collection rather than calling the ++ method directly.
The first line calls the new sort method against an unordered Vector of Ints.
Next, the sort method is called against an ArrayBuffer of Ints.
The new collection method now preserves the most specific type possible.
This implementation of sort is generic but may be suboptimal for different types of collections.
It would be ideal if the algorithm could be adapted such that it was optimized for each collection in the hierarchy.
This is easy to do as a maintainer of the collections library, because the implementations can be placed directly on the classes.
But when developing new algorithms outside the collections library, type classes can come to the rescue.
In Scala 2.9, the type inferencer can still correctly deduce subclasses of LinearSeqLike.
Here’s an example method that will do headtail decomposition on subclasses of LinearSeqLike.
The parameter T is the type of elements in the collection.
The type parameter Coll is the full type of the collection and is recursive, like in the definition of the LinearSeqLike trait.
This alone won’t allow Scala to infer the correct types.
Once this is completed, type inference will work correctly for Coll.
You can use the type class paradigm to encode an algorithm against collections and refine that algorithm when speed improvements are possible.
Let’s start by converting the generic sort algorithm from before into a type class paradigm.
First we’ll define the type class for the sort algorithm.
The Sortable type class is defined against the type parameter A.
The type parameter A is meant to be the full type of a collection.
The sort method takes a value of type A and returns a sorted version of type A.
The generic sort method can now be modified to look as follows:
The generic sort method now takes in the Sortable type class and uses it to sort the input collection.
Now the implicit resolution of default Sortable types needs to be defined.
The GenericSortTrait is defined to contain the implicit look up for the generic QuickSort algorithm.
The quicksort method defines the same type parameters and implicit parameters as the original sort method.
Instead of sorting, it defines a new instance of the Sortable type trait.
Now the GenericSortTrait has to be placed onto the Sortable companion object so that it can be looked up in the default implicit resolution.
The Sortable companion object is defined to extend the GenericSortTrait.
This places the implicit quicksort method in the implicit lookup path when looking for the Sortable[T] type trait.
The appropriate Sortable type trait is found for Vector and the collection is sorted using the quicksort algorithm.
But if we try to call the sort method against something that doesn’t extend from IterableLike, it won’t work.
The compiler complain that it can’t find a Sortable instance for Array[Int]
Scala provides an implicit conversion that will wrap an Array and provide it with standard collection methods.
The trait ArraySortTrait is defined with a single method arraySort.
This method constructs a Sortable type trait using a ClassManifest and an Ordering.
Using raw Arrays in Scala requires a ClassManifest so that the bytecode will use the appropriate method against the primitive arrays.
The algorithm loops through each index in the array and looks for the smallest element in the remainder of the array to swap into that position.
The selection sort algorithm isn’t the most optimal, but it’s a common algorithm and easier to understand than what’s used classically in Java.
This Sortable implementation needs to be added to the appropriate companion object for implicit resolution.
The Sortable companion object is expanded to extend from both the ArraySortTrait, containing the Sortable type class for Array, and the QuickSort trait, containing the Sortable type class for iterable collections.
Now, when calling the sort method with Arrays, the call succeeds.
You can use this technique to support any number of collections and to specialize behavior for collections using techniques shown in section 7.3
Scala provides all the right tools to generically deal with collections.
The complexity of doing so can be high, so it’s a judgment call when and how much abstraction is required for a particular method against collections.
The Scala collections library is one of the most compelling reasons to use Scala.
From the power and versatility of the collection to the ability to preserve specific types on generic methods, Scala collections provide a clean and elegant solution to most problems.
Using the collections API is a matter of understanding what the various type signatures mean and knowing how to flow between collection semantics and evaluation styles.
Although the API is geared for immutability, there’s more than enough support for mutable collections and interfaces.
This chapter provides a great introduction to the key concepts behind the collections API, but knowing the methods defined on the collections and how to string them together is also important.
Because the collections library is always improving, the best source for learning these methods is the scaladoc documentation for the current release.
The next chapter covers Scala actors, which are another important concept in the Scala ecosystem.
They communicate to the external world by sending and receiving messages.
An actor will process received messages sequentially in the order they’re received, but will handle only one message at a time.
This is critical, because it means that actors can maintain state without explicit locks.
Most actors won’t block a thread when waiting for messages, although this can be done if desired.
The default behavior for actors is to share threads among each other when handling messages.
This means a small set of threads could support a large number of actors, given the right behavior.
They accept a limited number of input messages and update their internal state.
All communication is done through messages and each actor stands alone.
Actors aren’t parallelization factories; they process their messages in single-threaded fashion.
They work best when work is conceptually split and each actor can handle a portion of the work.
If the application needs to farm many similar tasks out for processing, this requires a large pool of actors to see any concurrency benefits.
Asynchronous I/O and actors are a natural pairing, as the execution models for these are similar.
Using an actor to perform blocking I/O is asking for trouble.
This can be mitigated, as we’ll discuss in section 9.4
Although many problems can be successfully modeled in actors, some will benefit more.
The architecture of a system designed to use actors will also change fundamentally.
Let’s look at a canonical example of a good system design using actors.
This example uses several tools found in the old Message Passing Interface (MPI) specification used in supercomputing.
This program has a set of documents that live in some kind of search index.
Queries are accepted from users and the index is searched.
Documents are scored and the highest scored documents are returned to the users.
To optimize the query time, a scatter-gather approach is used.
The scatter-gather approach involves two phases of the query: scatter and gather (see figure 9.1)
The first phase, scatter, is when the query is farmed out to a set of subnodes.
Classically, these subnodes are divided topically and store documents about their topic.
These nodes are responsible for finding relevant documents for the query and returning the results, as shown in figure 9.2
The second phase, gather, is when all the topic nodes respond to the main node with their results.
Let’s start by creating a SearchQuery message that can be sent among the actors.
The first is the actual query, and the second is the maximum number of results that should be returned.
We’ll implement one of the topic nodes to handle this message.
The Search node defines the type Scored Document to be a tuple of a double score and a string document.
The index is defined as a HashMap of a query string to scored documents.
The index is implemented such that it pulls in a different set of values for each SearchNode created.
The full implementation of the index is included in the source code for the book.
When it receives a SearchQuery message, it looks for results in its index.
It replies to the sender of the SearchQuery all of these results in a truncated manner so that only maxResults are returned.
These methods differ in that react will defer the execution of the actor until there is a message available.
The receive method will block the current thread until a message is available.
Unless absolutely necessary, receive should be avoided to improve the parallelism in the system.
Now let’s implement the HeadNode actor that’s responsible for scattering queries and gathering results.
It defines a member containing all the SearchNodes that it can scatter to.
It then defines its core behavior in the act method.
When it receives one, it sends it to all the SearchNode children awaiting a future result.
The !! method on actors will send a message and expect a reply at some future time.
The HeadNode can block until the reply is received by calling the apply method on the Future.
This is exactly what it does in the foldLeft over these futures.
The HeadNode is aggregating the next future result with the current query results result to produce the final result list.
This final result list is sent to the original query sender using the reply method.
Although they offer no benefit for the sortBy method, in practice the take method is usually used, and the view and force methods can help improve efficiency by avoiding the creation of intermediate collections.
The system now has a scatter-gather search tree for optimal searching.
The casting of the result type in the HeadNode actor is less than ideal in a statically typed language like Scala.
If a bad index or query string occurs, the whole system will crash.
One of the biggest dangers in the Scala standard actors library is to give actors references to each other.
This can lead to accidentally calling a method defined on another actor instead of sending a message to that actor.
Although that may seem innocuous to some, this behavior can break an actors system, especially if you use locking.
Actors are optimized by minimizing locking to a few minor locations, such as when scheduling and working with a message buffer.
Introducing more locking can easily lead to deadlocks and frustration.
Another disadvantage to passing direct references to actors is transparency, where the location of an actor is tied in to another actor.
The actors can no longer migrate to other locations, either in memory or on the network, severely limiting the system’s ability to handle failure.
Another downside to sending actors directly in the Scala standard library is that actors are untyped.
This means that all the handy type system utilities you could leverage are thrown out the window when using raw actors.
Specifically, the compiler’s ability to find exhausting pattern matches using sealed traits.
This has the benefit of defining every message that an actor can handle and keeping them in a central location for easy lookup.
With a bit of machinery, the compiler can be coerced to warn when an actor doesn’t handle its complete messaging API.
The Scala standard library provides two mechanisms for enforcing type safety and decoupling references from directly using an actor: the InputChannel and OutputChannel traits.
The OutputChannel trait is used to send messages to actors.
This is the interface that should be passed to other actors, and it looks like this:
The OutputChannel trait is templatized by the type of messages that can be sent to it.
It supports sending messages via three methods: !, send, and forward.
The ! method sends a message to an actor and doesn’t expect a reply.
The send method sends a message to an actor and attaches an output channel that the actor can respond to.
Use typed, transparent references forward method is used to send a message to another actor such that the original reply channel is preserved.
The receiver method on OutputChannel returns the raw actor used by the OutputChannel.
Notice the methods that OutputChannel doesn’t have: !! and !?
In the Scala standard library, !! and !? are used to send messages and expect a reply in the current scope.
This is done through the creation of an anonymous actor that can receive the response.
This anonymous actor is used as the replyTo argument for a send call.
The !? method blocks the current thread until a response is received.
The !! method creates a Future object, which stores the result when it occurs.
Any attempt to retrieve the result blocks the current thread until the result is available.
This attaches a function that can be run on the value in the future when it’s available without blocking the current thread.
But when used lightly or with caution, these methods can be helpful.
It’s important to understand the size and scope of the project and the type of problem being solved.
If the problem is too complex to ensure !! and !? behave appropriately, avoid them altogether.
The scatter-gather example requires two changes to promote lightweight typesafe references: removing the direct Actor references in HeadNode and changing the query responses to go through a collection channel.
This change ensures that the HeadNode will only send SearchNodeMessage messages to SearchNodes.
The SearchNodeMessage type is a new sealed trait that will contain all messages that can be sent to SearchNodes.
Rather than directly responding to the sender of the SearchQuery, let’s allow an output channel to be passed along with the SearchQuery that can receive results.
The SearchQuery message now has three parameters: the query, the maximum number of results, and the output channel that will receive the query results.
The new SearchNodeMessage trait is sealed, ensuring that all messages that can be sent to the SearchNode are defined in the same file.
Let’s update the SearchNodes to handle the updated SearchQuery message.
The SearchNode trait is the same as before except for the last line in the react call.
Instead of calling reply with the QueryResponse, the SearchNode sends the response to the requestor parameter of the query.
This new behavior means that the head node can’t just send the same SearchQuery message to the SearchNodes.
Let’s rework the communication of the system, as shown in figure 9.3
This actor is responsible for receiving all results from SearchNodes and aggregating them before sending back to the front end.
One advanced implementation could use prediction to stream results to the front end as they’re returned, attempting to ensure high priority results get sent immediately.
For now, let’s implement the GathererNode such that it aggregates all results first and sends them to the front end.
The maxDocs member is the maximum number of documents to return from a query.
The maxResponses member is the maximum number of nodes that can respond before sending results for a query.
The client member is the OutputChannel where results should be sent.
The GathererNode should be tolerant of errors or timeouts in the search tree.
To do this, it should wait a maximum of one second for each response before returning the query results.
The act method defines the core behavior of this actor.
The combineResults helper method is used to take two sets of query results and aggregate them such that the highest scored results remain.
This method also limits the number of results returned to be the same as the maxDocs member variable.
The bundleResult method is the core behavior of this actor.
The curCount parameter is the number of responses seen so far.
The current parameter is the aggregate of all collected query results from all nodes.
The bundleResult method first checks to see if the number of responses is less than the maximum expected results.
If another query result is received, the method combines the result with the previous set of results and recursively calls itself with bumped values.
If receiving the message times out, the bundleResult method calls itself with the number of responses set to the maximum value.
If the number of responses is at or above the maximum, the current query results are sent to the client.
Finally, the act method is implemented by calling the bundleResult method with an initial count of zero and an empty Seq of results.
The GathererNode stops trying to receive messages after the query results have been sent.
This effectively ends the life of the actor and allows the node to become garbage-collected.
The Scala standard actors library implements its own garbage collection routine that will have to remove references to the GathererNode before the JVM garbage collection can recover memory.
The last piece of implementation required is to adapt the HeadNode to use the GathererNode instead of collecting all the results in futures.
The HeadNode has been changed so that when it receives a SearchQuery, it constructs a new GathererNode.
The gatherer is instantiated using the parameters from the SearchQuery.
The gatherer must also be started so that it can receive messages.
The last piece is to send a new SearchQuery message to all the SearchNodes with the OutputChannel set to the gatherer.
Splitting the scatter and gather computations into different actors can help with throughput in the whole system.
The HeadNode actor only has to deal with incoming messages and do any potential preprocessing of queries before scattering them.
The GathererNode can focus on receiving responses from the search tree.
Limit failures to zones node could even be implemented such that it stopped SearchNodes from performing lookups if enough quality results were received.
Most importantly, if there’s any kind of error gathering the results of one particular query, it won’t adversely affect any other query in the system.
This can be done through the creation of failure zones.
Joe Armstrong, the creator of Erlang, popularized the notion of actors and how to handle failure.
The recommended strategy for working with actors is to let them fail and let another actor, called a supervisor handle that failure.
The supervisor is responsible for bringing the system it manages back into a working state.
Looking at supervisors and actors from a topological point of view, supervisors create zones of failure for the actors they manage.
The actors in a system can be partitioned by the supervisors such that if one section of the system goes down, the supervisor has a chance to prevent the failure from reaching the rest of the system.
Each supervisor actor can itself have a supervisor actor, creating nested zones of failure.
The error handling of supervisors is similar to exception handling.
A supervisor should handle any failure that it knows how to, and bubble up those it doesn’t to outer processes.
If no supervisor can handle the error, then this would bring down the entire system, so bubbling up errors should be done carefully!
Supervisors can be simpler to write than exception handling code.
With exception handling, it’s difficult to know if a try-catch block contained any state-changing code and whether it can be retired.
With supervisors, if an actor is misbehaving, it can restart the portion of the system that’s dependent on that actor.
Each actor can be passed an initial good state and continue processing messages.
Notice the relationship between the supervisor of an actor and the creator of the actor.
If the supervisor needs to recreate an actor upon destruction, the supervisor is also the ideal candidate to start the actor when the system initializes.
This allows all the initialization logic to live in the same location.
Supervisors may also need to act as proxy to the subsystem they manage.
In the event of failure, the supervisor may need to buffer messages to a subsystem until after it has recovered and can begin processing again.
Supervisors are created differently in the various Scala actor libraries.
In the core library, supervisors are created through the link method.
The Akka actors library provides many default supervisor implementations and mechanisms of wiring actors and supervisors together.
One thing that’s common across actor libraries is that supervisors are supported and failure zones are encouraged.
The first failure zone should cover the HeadNode and SearchNode actors.
Upon failure, the supervisor can reload a failing search node and wire it back into the head node.
In the event of failure in this outer zone, the supervisor can restart any failed inner zones and inform the front end of the new actors.
A topological view of this failure handling is shown in figure 9.4
When designing with actors, it’s important to prepare what zones are allowed to fail separately.
The system should be designed such that any one zone does not take down the entire application.
The supervisor for these zones is responsible for restarting the entire tree, or a particular SearchNode, on failure.
In the event of failure, it restarts the underlying search trees or the front end as needed.
We’ll start by defining the supervisor for the search nodes:
The create-SearchTree is responsible for instantiating nodes of the search tree and returning the top node.
This method iterates over the desired size of the tree and creates the SearchNode class from the previous examples.
Remember that each SearchNode uses its assigned ID to load a set of indexed documents and make them available for queries.
In the Scala standard library actors, linking is what creates a supervisor hierarchy.
Linking two actors means that if one fails, both are killed.
It also allows one of them to trap errors from the other.
This is done from the call to trapExit = true in the act method.
The second method is the standard library actor’s act method.
Common linking pitfalls The link method has two restrictions that simplify its use.
It must be called from inside a live actor—that is, from the act method or one of the continuations passed to react.
It should be called on the supervisor with the other actor as the method argument.
Because link alters the behavior of failure handling, it needs to lock both actors it operates against.
Because of this synchronization, it’s possible to deadlock when waiting for locks.
The link method also requires, through runtime asserts, that it’s called against the current live actor.
The actor must be actively running in its scheduled thread.
This means linking should be done internal to the supervisor actor.
This is why all the topological code is pushed down into the supervisor and why it acts as a natural proxy to the actors it manages.
The first line here is the trapExit = true, which allows this actor to catch errors from others.
The next line is a helper function called run, which accepts one parameter, the current head actor, and calls react, which will block waiting for messages.
The first message it handles is the special Exit message.
An Exit message is passed if one of the linked actors fails.
Notice the values that come with an Exit message: deadActor and reason.
The deadActor link allows the supervisor to attempt to pull any partial state from the deadActor if needed, or remove it from any control structures as needed.
Note that the deadActor is already gone and won’t be scheduled anymore at the time of receiving this message.
This may not be ideal in a reallife situation because reconstructing the entire tree could be expensive or the tree might be sprawled over several machines.
This means that the supervisor can block incoming messages when restarting the system.
When the main node crashes, the supervisor receives the Exit message and stops processing messages while it fixes the system.
After restoring things, it will again pull messages from its queue and delegate them down to the search tree.
The supervisor for the scatter-gather search system demonstrates ways to handle the issues of failure in an actors system.
When designing an actors-based system and outlining failure zones, table 9.1 helps make decisions appropriate for that module.
These three decisions are crucial in defining robust concurrent actor systems.
Granularity of failure zones The entire search tree fails and restarts.
Single Search node inner failure zone with Search Tree outer failure zone.
Recovery of failed actor state Actor data is statically pulled from disk.
Limit overload using scheduler zones zone crashes and restarts, it won’t affect external zones.
The Scala actors library makes it easy to lose transparency for actors.
This can be done by passing the reference to a specific actor rather than a proxy or namespace reference.
The second decision can affect the messaging API for actors.
If a subsystem needs to tolerate failure of one of its actors, the other actors need to be updated to communicate with the replacement actor.
For the Scala standard library, using the supervisors as proxies to sub-components is the simplest way to provide transparency.
This means that for fine-grained failure zones, many supervisors must be created, possibly one per actor.
The third decision is one not discussed in the example—that of state recovery.
Most real-life actors maintain some form of state during their lifetimes.
This state may or may not need to be reconstructed for the system to continue functioning.
Although not directly supported in the Scala standard library, one way to ensure state sticks around is to periodically snapshot the actor by dumping its state to a persistent store.
A second method of keeping state would be to pull the last known state from a dead actor and sanitize it for the reconstructed actor.
This method is risky, as the state of a previous actor isn’t in a consistent state and the sanitization process may not be able to recover.
The sanitization process could also be hard to reason through and write.
Another mechanism for handling state is to persist the state after every message an actor receives.
Although not directly supported by the Scala standard library, this could easily be added through a subclass of actor.
Transactors are actors whose message handling functions are executed within a transactional context.
Because actors share threads, an actor that fails to handle its incoming messages could ruin the performance of other actors that share the same threading resources.
The solution to this is to split actors into scheduling zones, similar to splitting them into failure zones.
One type of failure that a supervisor can’t handle well is thread starvation of actors.
If one actor is receiving a lot of messages and spending a lot of CPU time processing them, it can starve other actors.
The actor schedulers also don’t have any notion of priority.
Maybe a high-priority actor in the system must respond as quickly as possible, and could get bogged down by lower priority actors stealing all the resources.
A scheduler is the component responsible for sharing actors among threads.
The scheduler selects the next actor to run and assigns it to a particular thread.
In the Scala actors library, a scheduler implements the IScheduler interface.
A variety of scheduling mechanisms are available for the standard library actors, as shown in table 9.2
This is done through a nifty work-stealing algorithm where every thread has its own scheduler.
Tasks created in a thread are added to its own scheduler.
If a thread runs out of tasks, it steals work from another thread’s scheduler.
The scatter-gather example is a perfect fit for the fork join parallel executor.
Queries are distributed to each SearchNode for executions, and results are aggregated to create the final query results.
The work-stealing pulls and distributes the forked work for a query.
If the system is bogged down, it could degrade to performing similarly to a single-threaded query engine.
Although generally efficient, the ForkJoinScheduler isn’t optimal in situations where task sizes are largely variable.
If the workload starts to grow beyond what the current thread pool can handle, the scheduler will increase the available threads in the pool up until a maximum pool size.
This can help a system handle a large increase in messaging throughput and back off resources during downtime.
There are many implementations of java.util .Executor in the Java standard library as well as common alternatives.
One of these, from my own codebases, was an Executor that would schedule tasks on the Abstract Windows Toolkit (AWT)-rendering thread.
Using this scheduler for an actor guarantees that it handles messages within a GUI context.
This allowed the creation of GUIs where actors could be used to respond to backend events and update UI state.
Each of these schedulers may be appropriate to one or more components in a system.
Some components scheduling may need to be completely isolated from other components.
ForkJoinScheduler Parallelization optimized for tasks that are split up, parallelized, and recovered—that is, things that are forked for processing, then joined together.
This allows actors to use any of the standard Java thread pools and is the recommended way to assign fixed size thread pool.
Scheduling zones are groupings of actors that share the same scheduler.
Just as failure zones isolate failure recovery, so do scheduling zones isolate starvation and contention of subsystems.
Scheduling zones can also optimize the scheduler to the component.
Figure 9.5 shows what a scheduling zone design might be for the scatter-gather example.
Prevent low-latency services from getting clobbered by low-priority processes using scheduling zones to carve out dedicated resources.
The first scheduling zone handles all actors in a search tree.
The ForkJoinScheduler is optimized for the same behavior as the scatter-gather algorithm, so it makes an ideal choice of scheduler for this zone.
The replicated Search tree uses its own ForkJoinScheduler to isolate failures and load between the two trees.
The front end scheduling zone uses a customized scheduler that ties its execution to an asynchronous HTTP server; the handling of messages is done on the same thread as input is taken, and the results are streamed back into the appropriate socket using one of the front-end threads.
This would be ideal if the HTTP server accepting incoming connections used a thread pool of the same size.
The last scheduling zone, not shown, is the scheduling of error recovery.
Out of habit, I tend to place these on a separate scheduling routine so they don’t interfere with any other subcomponent.
Error recovery, when it happens, is the highest priority task for a given subcomponent and shouldn’t steal.
But if more than one subcomponent is sharing the same scheduling zone, then I prefer to keep recovery work separate from core work.
Let’s add scheduling zones to the scatter-gather search tree example.
The only changes required are in the constructor function defined on the supervisor, as shown in the following listing:
The initCoreSize and maxSize arguments are the minimum and maximum number of threads it should store in its thread pool.
The daemon argument specifies whether threads should be constructed as daemons.
This scheduler can shut itself down if the actors within are no longer performing any work.
The last argument is whether or not the scheduler should attempt to enforce fairness in the work-stealing algorithm.
The second additions are the overridden scheduler member of the SearchNode and HeadNode actors.
This override causes the actor to use the new scheduler for all of its behavior.
It can do this only at creation time, so the scheduling zones must be known a-priori.
The actors are now operating within their own fork-join pool, isolated from load in other actors.
One of the huge benefits of using actors is that the topology of your program can change drastically at runtime to handle load or data size.
For example, let’s redesign the scatter-gather search tree so that it can accept new documents on the fly and add.
The tree should be able to grow in the event that a specific node gets to be too large.
To accomplish this, we can treat an actor as a state machine.
Akka is the most performant actors framework available on the JVM.
It’s designed with actor best practices baked into the API.
Writing efficient, robust actors systems is simplest in the Akka framework.
The entire scatter-gather tree is composed of two node types: search (leaves) and head (branches)
A search node holds an index, like the previous topic nodes.
It’s responsible for adding new documents to the index and for returning results to queries.
It’s responsible for delegating queries to all children and setting up a gatherer to aggregate the results.
Although the Scala standard library is elegant, the Akka library makes the robust usage of actors easy.
Akka builds in the notion of transparent actor references, while providing a good set of useful supervisors and schedulers.
Creating failure zones and scheduling zones is much easier in Akka, and the library is standalone.
In general, there’s little reason not to use Akka, especially when attempting to design a distributed topology, as shown in the following listing:
This function contains the message handling behavior for the adaptive search nodes when the node is a leaf.
When the node receives a SearchQuery it executes that query against the local index.
The executeLocalQuery function extracts all the results for a given word.
These are then limited by the desired maximum number of results in the query and sent to the handler.
Note that the handler is of type ActorRef not Actor.
In Akka, there’s no way to gain a direct reference to an actor.
The only way to talk with an actor is to send a message to it using an ActorRef, which is a transparent reference to an actor.
Messages are still sent to actors using the ! operator.
The executeLocalQuery function didn’t change from the Scala actors version to the Akka actors version besides the use of ActorRef:
After updating the index, the document is added to the list of stored documents.
Finally, if the number of documents in this node has gone above the maximum desired per node, the split method is called.
The split method should split this leaf node into several leaf nodes and replace itself with a branch node.
Let’s defer defining the split method until after the parent node is defined.
If the index doesn’t need to be split, the index is updated.
To update the index, the document string is split into words.
These words are grouped together such that the key refers to a single word in a document and the value refers to a sequence of all the same words in the document.
This sequence is later used to calculate the score of a given word in the document.
The current index for a word is extracted into the term list.
The index for the given word is then updated to include the new document and the score for that word in the document.
Let’s first define the branch node functionality before defining the split method:
The ParentNode is also defined with a self type of AdaptiveSearchNode.
Again, the reference to child actors is the ActorRef type.
The method parentNode defines a partial function that handles incoming messages when an actor is a parent.
When the parent receives a SearchQuery it constructs a new gatherer and farms the query down to its children.
In Akka, an actor is constructed using the Actor.actorOf method.
Although the actor is constructed as a gatherer node, the term gatherer is of type ActorRef not GathererNode.
When the ParentNode receives a SearchableDocument it calls getNextChild and sends the document to that child.
The getNextChild method, not shown, selects a child from the children sequence in a round-robin fashion.
This is the simplest attempt to ensure a balanced search tree.
In practice, there would be a lot more effort to ensure the topology of the tree was as efficient as possible (see figure 9.6)
The key behavior of the new adaptive search tree is that it should dynamically change shape.
Any given node should be able to change its state from being a leaf node to a parent node that has children.
Similar to Scala actors, an Akka actor must extend the Actor trait.
The largest difference between Akka and Scala actors is the receive method.
In Akka, receive defines the message handle for all messages, not just the next message received.
Also, receive is called via the Akka library when a message is ready, so receive is not a blocking call.
The receive method is defined to return the leafNode behavior by default.
This means any AdaptiveSearchNode instantiated will start of as a leaf node.
In Akka, to switch the behavior of an actor, there’s a become method that accepts a different message handler.
These nodes are then sent the portion of documents they will be responsible for.
The local index is cleared to allow it to be garbage--collected.
In a production system, this wouldn’t happen until the children acknowledged that they had received the documents and were ready to begin serving traffic.
The behavior of the current actor is switched to the parent behavior in the expression this become parentNode.
Only the root AdaptiveSearchNode needs to be created and the documents sent into the root node.
The tree will dynamically expand into the size required to handle the number of documents.
AKKA’S SCHEDULER AND SUPERVISORS Akka provides an even richer set of actor supervisors and schedulers than the Scala actors library.
These aren’t discussed in the book, but can be found in Akka’s documentation at http:// akka.io/docs/
The Akka 2.0 framework is adding the ability to create actors inside a cluster and allow them to be dynamically moved around to machines as needed.
Actors provide a simpler parallelization model than traditional locking and threading.
A well-behaved actors system can be fault-tolerant and resistant to total system slowdown.
Actors provide an excellent abstraction for designing high-performance servers, where throughput and uptime are of the utmost importance.
For these systems, designing failure zones and failure handling behaviors can help keep a system running even in the event of critical failures.
Splitting actors into scheduling zones can ensure that input overload to any one portion of the system won’t bring the rest of the system down.
Finally, when designing with actors, you should use the Akka library for production systems.
The Akka library differs from the standard library in a few key areas:
Clients of an actor can never obtain a direct reference to that actor.
This drastically simplifies scaling an Akka system to multiple servers because there’s no chance an actor requires the direct reference to another.
If the current message handling routine can’t handle an input message, it’s dropped (or handled by the unknown message handler)
This prevents out-of-memory errors due to message buffers filling up.
All core actors library code is designed to allow user code to handle failures without causing more.
For example, Akka goes to great lengths to avoid causing out-of-memory exceptions within the core library.
This allows user code, your code, to handle failures as needed.
Akka provides most of the basic supervisor behaviors that can be used as building blocks for complex supervision strategies.
Akka provides several means of persisting state “out of the box.”
So, while the Scala actors library is an excellent resource for creating actors applications, the Akka library provides the features and performance needed to make a production application.
This chapter lightly covered a few of the key aspects to actor-related design.
These should be enough to create a fault-tolerant high-performant actors system.
Next let’s look into a topic of great interest: Java interoperability with Scala.
One of the biggest advantages of the Scala language is its ability to seamlessly interact with existing Java libraries and applications.
Although this interaction isn’t completely seamless, Scala offers the tightest integration to Java of any JVM language.
The key to knowing how to integrate Scala and Java lies in the Java Virtual Machine specification and how each language encodes onto that specification.
Scala does its best to translate simple language features directly onto JVM features.
But complicated Scala features are implemented with some compiler tricks, and these tricks are usually the cause of issues when integrating with Java.
For the most part, the Java language translates simply into JVM bytecode; however, it too has language features that use compiler tricks.
These will also cause rough spots in Scala/ Java interaction.
Another benefit of understanding how to interface Scala with Java is that it helps to integrate Scala with every other JVM language.
Because Java is king on the JVM, all alternative JVM languages provide means of using existing Java code.
This means that communications from Scala to another JVM language can be accomplished through Java in the worst case.
Scala is working on language features to integrate directly with dynamic languages, but even with the 2.9.0 release, these features are considered experimental.
This chapter focuses on four big issues in Scala/Java interaction.
The first issue is that Scala treats all types as objects, and Java supports primitives within the language.
This leads to issues that can be solved by creating appropriate interfaces for communication between Java and Scala.
Other mismatches can be alleviated with judicious use of implicit conversions.
The second issue is implicit conversions, which tend to be overutilized.
While extremely useful, they can cause subtle bugs in Scala/Java interaction.
Scala does a lot to support Java serialization seamlessly and succeeds for the most part.
A few advanced Scala features can cause issues with Java serialization.
Scala adheres to a uniform access principlethat is, Scala makes no distinction between methods and fields; they share the same namespace.
Some Java libraries require specific methods or fields to have annotations.
Scala provides some advanced annotation features that enable this to succeed.
Let’s look into the mismatch between Java primitives and Scala objects.
Scala trait can be extended within Java using a bit of trickery.
But this seemingly tight integration runs afoul of three rough patches: primitive boxing, visibility differences, and inexpressible language features.
Primitive boxing is the (semi-)automatic conversion of primitive values on the JVM into objects.
This is done because generic parameters are implemented through typeerasure.
This was one of the means with which Java retained backwards compatibility when it introduced generics.
Scala and Java implement this differently, which we’ll look into in section 10.1.1
Visibility refers to using protected and private modifiers to change the access restrictions on classes and their members.
Inexpressible language features are features within the Scala language that can’t be expressed within the Java language.
Things like curried methods, implicit parameters and higher-kinded types are examples.
It’s best to avoid or hide these features in any code that needs to interface with Scala and Java.
The first difference between Scala and Java is the special treatment of primitives, things created directly on the stack and passed by value, and objects, things created on the heap and passed by reference.
Specifically, code using generic type parameters in Java can’t use primitives.
To get around this, Java defines a set of classes that mimic the types of primitives.
When an object is required, the primitive can be placed into an object.
The object makes a box in which to carry the primitive.
Scala makes no distinction between primitives and objects, and performs boxing behind the scenes on behalf of the developer.
In the Scala language, everything is an object and the compiler does its best to hide the fact that primitives aren’t objects.
In the Java language, the programmer is forced to pay attention to the difference between a primitive and an object containing the same value.
To relieve the overhead of boxing, Java introduced auto-(un)boxing in version 1.5
Autoboxing is an implicit conversion from a primitive type to its boxed type.
This allows you to write a for loop as the following:
In the example, the line int item : foo is a for expression that’s unboxing all integers in the list foo.
Although not seen, this is the same code as the following:
This example is similar except that the int item is explicitly unboxed from the Integer returned from the list.
Although boxing happens automatically in Java, it can be an expensive operation at runtime.
The compiler tries to optimize the usage of scala.Int such.
The language mismatch between Scala and Java that it remains in primitive form throughout the life of a program.
The add method takes two scala.Int values and returns a scala.Int.
The signature for the add method uses the primitive int type.
What happens if we use a generic type with scala.Int? The compiler will generate boxing code as needed:
This method take a generic List class parameterized to have scala.Int elements.
The code creates a sum variable, grabs an iterator to the list, and iterates over the values in the list.
Each of these values is added to the sum variable and the sum is returned.
Let’s take a look at the bytecode in the following listing.
The List class is generic and suffers from the same problem as Java generics.
The implementation of Generic types in Java forces the use of Object at runtime; therefore, primitives can’t be generic type parameters.
Label 20 in the byte code shows that invoking next against the List’s iterator returns the type Object.
Label 25 shows Scala’s version of autoboxing: the BoxesRunTime class.
The important point here is that both Scala and Java use boxing with generic classes.
Scala hides boxing entirely behind scala.Int while Java promotes boxing into the language itself.
This mismatch can cause issues when working with Scala from Java or Java from Scala.
These issues can be solved using one simple rule: Use primitives in methods used from both Scala and Java.
It’s best to use primitives, and arrays, for the simplest interface between Java and Scala.
This simple rule can avoid a few of the issues with Scala/Java interaction.
In Scala, a list of integers has the type java.util.List [scala.Int]
The other is to define an implicit conversion that will shim Java types into Scala types.
The REPL prints the values in the list when describing the return types.
Notice that the correct values are shown and there are no runtime exceptions.
The next retrieves the first value from the cast list.
They subvert the type system in Scala and prevent it from discovering future errors.
The second solution can avoid this pitfall by operating within the type system.
The scalaj-collections library provides primitive-safe implicit conversions between Scala and Java collection types.
This offers the best mechanism to handle primitives in collections, but noncollection types may still require a hand-rolled implicit conversion.
The next big issue is the difference in visibility implementation.
Visibility is enforced both by the Java compiler and by the JVM runtime.
Java embeds visibility restrictions directly into the bytecode that the JVM uses to enforce at runtime.
Scala enforces visibility statically, and does its best to encode visibility constraints for the JVM.
Scala’s visibility design is far more powerful than Java’s and can’t be directly encoded into bytecode for runtime enforcement.
Scala tends to make methods publicly visible and enforces all constraints at compile time, unless the visibility rule in Scala lines up directly with one from Java.
Specifically, in Scala, companion objects are allowed to access protected members of their companion classes.
This means that Scala can’t encode protected members using the JVM’s protected bytecode because that would restrict companion classes from accessing protected members.
The Test class is defined with a single member x.
The Test class is defined with a private field x and a public accessor called x.
This means that in Java an external user of the Test class could access the protected x method.
The main method is defined to construct a new Scala Test instance.
The next line calls the protected x method and prints its value to the console.
Even though the value is protected within Scala, the call succeeds in Java.
The program outputs the value 10 with no runtime visibility exception.
This means that Java clients of Scala classes need to be on their best behavior to prevent modifying or accessing values that they shouldn’t.
Don't call methods with $ in the name from Java.
Scala’s visibility rules are more advanced than Java and cannot be expressed.
When calling into Scala from Java, avoid calling methods with $ in the name, as these are implementation details of Scala’s encoding.
Visibility issues are a subset of a bigger issue with Java/Scala integration—that of inexpressible language features.
Java and Scala both have features that are inexpressible in the other language.
In Scala, everything is an object and there are no static values.
Scala’s objects are implemented in terms of static values on.
Consequently, Java libraries that require static values are hard to interact with from Scala.
Scala has many features unavailable in Java, such as traits, closures, named and default parameters, implicit parameters, and type declarations.
When interacting with Scala, Java can’t use implicit resolution to find missing parameters to methods.
For each of these issues, there’s usually a workaround somewhere, but it’s best to avoid these issues entirely.
You can do this with a simple mechanism: Construct interfaces in Java that define all the types that will be passed between Java and Scala.
SCALA/JAVA INTEGRATION TIP Construct interfaces in Java that define all types that will be passed between Java and Scala.
Place these interfaces into a project that can be shared between the Java portions of code and the Scala portions of code.
By limiting the features used in the integration points, there won’t be any feature mismatch issues.
Because Java is more limited in features and compiles more directly to bytecode, it makes for a great integration language.
Using Java interfaces ensures you avoid the corner case issues of integration, besides those of boxing.
One example where using Java is required is on the Android platform which has an interface called Parcelable.
You can use this interface to allow objects to be passed between processes.
Because this could involve serializing the data, the Parcelable interface requires a static field that the Android platform can use to instantiate a Parcelable.
For example, say that an application needs to pass addresses between processes on the Android platform.
In Java, the Address class would look as shown in the following listing:
The Address class is composed of four members: street, city, state, and zip.
It has a writeToParcel method that’s Android’s way of flattening or serializing the class to send to another process.
The private constructor for Address is used to deserialize the values from the Parcel it was stored in.
The describeContents method returns a bitmask that tells the Android platform the types of data that are contained in the parcel, in case any need special treatment.
The Android system uses this type to create and parse incoming Addresses from other processes.
The solution in this case is to create a split between the pieces that require Java and the pieces that require Scala.
In the case of Address, it’s such a simple class, that writing it completely in Java could be a fine solution.
But if Address were more complex, this splitting would be appropriate.
Let’s pretend that Address uses some advanced Scala type features in some of its member functions.
To get Address to still be Parcelable in Android and to keep the advanced Scala features, it must be split.
The Scala features can stay in an abstract class that the Java statics can extend.
The AbstractAddress class is defined with street, city, state, and zip as constructors and as val members.
The abstract class can also define all the methods required by the Parcelable interface: writeToParcel and describeContents.
Let’s extend the AbstractAddress class in Java to allow for usage in Android:
The Address class is defined with a private constructor that takes in a Parcel and delegates to the constructor defined in Scala.
Then the static CREATOR instance is defined similarly to the Java-only version.
Due to Scala’s tight integration with Java, interfacing with constructors and extending abstract classes can be seamless.
This simple Address Parcelable example highlights what to do when running into APIs developed for Java without Scala in mind.
Another area of concern when integrating with Java is the overuse of implicit conversions to adapt Java libraries into Scala idioms.
One common mechanism of supporting the Scala/Java interaction is to create implicit conversions within Scala that promote Java types into a more Scala-friendly form.
This can help ease the pain of using classes not designed for Scala but comes at a cost.
Implicit conversions carry a few dangers that developers need to be aware of:
This object contains a set of implicit conversions to convert collections from Java to their Scala equivalents and vice versa.
These implicit conversions are immensely handy but also suffer from all the issues associated with this design.
Let’s look into how object identity and equality can become a problem when using JavaConversions.
One of the dangers of using implicits to wrap Scala or Java objects for interoperability is that it can alter object identity.
This breaks equality in any code that might require equality.
Let’s look at a simple example of converting a Java collection into a Scala one:
The values "Hi" and "You" are added to the array list.
The val y is constructed with the type of scala.Iterable.
This invokes an implicit conversion to adapt the Java ArrayList into a Scala Iterable.
Finally, when testing equality of the two collections, the value is false.
When wrapping a Java collection, the wrapped collection isn’t equal to the original.
Implicit views, when interfacing with Java, can cause silent object identity issues and other problems.
For example, the implicit conversion from a Java collection to a Scala collection isn’t as obvious as in the previous example.
The class JavaClass has one method called CreateArray that returns an ArrayList containing the value "HI"
This method takes a scala.Iterable and an AnyRef and checks the equality.
The next line calls the Java class and constructs the new ArrayList.
Finally, the same variable is placed into both sides of the areEqual method.
Because the compiler is running the implicit conversions behind the scenes, the fact that x is being wrapped is less apparent in this code.
Although this example is contrived, it demonstrates how the issue can become hidden behind method calls.
In real-world programming, this issue can be difficult to track down when it occurs, as the method call chains are often more complex.
The second issue facing implicits as a means to ease Java integration is that of chaining implicits.
The implicits that convert from Java to Scala and back again will alter the collection type, but usually not the underlying generic parameter.
This means that if the generic parameter type also needs to be converted for smooth Java/Scala integration, then it’s possible the implicit won’t be triggered.
Let’s look at a common example: boxed types and Java collections.
In Scala, because the compiler doesn’t differentiate between primitives and objects, the type scala.Int can be safely used for generic parameters.
This implicit doesn’t kick in when looking for implicit conversions from Java to Scala.
Let’s naively try to construct an implicit that can convert from a collection type and modify its nested element all in one go.
The naiveWrap method is defined with two type parameters: one for the original type in the Java collection, A, and another for the Scala version of that type, B.
The naiveWrap method takes another implicit conversion from the Java type A to the Scala type B.
The Java list x isn’t able to be converted to an Iterable[Int] directly.
This is the same problem we saw before where the type inferencer doesn’t like inferring the A and B types from the naiveWrap method.
The solution to this problem is one used from 7.2.3: We can defer the type inference of the parameters.
The Converter trait holds the Java collection that needs to be converted.
The asScala method is defined to capture the B type from the naiveWrap method.
This method takes an implicit argument that captures the conversion from A to B.
The Test object is defined with a new implicit wrap method.
The new implicit conversions requires the asScala method to be called directly.
Next a Java ArrayList[java .lang.Integer] is constructed and values are added to it.
Finally, the conversion is attempted using the asScala method, and this time it succeeds.
The downside to this approach is the requirement of the additional method call to ensure the types are inferred correctly.
The explicit asScala method call denotes a transformation to a new object.
This makes it easy to know when a collection is being converted between the Scala and Java libraries.
SCALAJ-COLLECTIONS The scalaj-collections library from Jorge Ortiz provides collection conversions to and from Scala and Java collections.
The library uses the same technique of having an asScala and asJava method implicitly added to collections of the respected types.
The scalaj library offers a more robust solution than what’s available in the standard library.
Although using implicits to wrap Java libraries into Scala libraries can be dangerous, it’s still a helpful technique and is used throughout the standard library.
It’s important to know when only simple implicit conversions won’t be enough and how to solve these issues.
Chaining implicit conversions can solve a lot of the remaining issues.
The important point here is that implicits aren’t magic and can’t automatically convert between Scala and Java types for all situations.
Implicits can and should be used to reduce the overhead of these interaction points.
The next potential issue with Java integration is that of serialization.
Scala’s closures are automatically made serializable and most of the classes are serialization friendly.
When using Scala with Java serialization, it’s recommended you use one of the newer releases.
A corner case is where Scala’s generation of anonymous classes can cause issues with serialization.
We’ll define a set of objects to model characters within a game.
Each person could be in one of two states: alive or dead.
The object PlayerState is used to encapsulate the status enumeration.
Finally, the Player class is constructed with a single member s that holds the player status.
Now, imagine a few of these players are created and stored in some semipermanent fashion using Java serialization.
The game server is running smoothly and everyone’s happy, even those who have dead players.
To simulate this, let’s serialize a single dead player to disk.
The value x is created with a player in the DEAD status.
The value out is constructed as a Java ObjectOutputStream for the file player.out.
The output stream is used to serialize the dead player to disk.
Around this time, there’s a new feature request to allow players to sleep during the game.
The PlayerStatus enumeration is updated to have a new state: sleeping.
The SLEEPING value is added between the ALIVE and DEAD status.
Other than the new value, nothing in the original code has changed.
But when trying to load dead players from disk, there’s an issue:
A new ObjectInputStream is constructed to deserialize the object using Java’s serialization.
What’s happened is the class that used to represent the DEAD value has moved.
The ALIVE, SLEEPING, and DEAD classes are constructed anonymously: they aren’t given named classes.
Scala generates anonymous class names using a simple formula: location in source code + current count of anonymously generated classes for this location.
But when adding the new SLEEPING status, the anonymous class names are changed.
The mistake here was using anonymous classes rather than named classes.
Let’s dig deeper into anonymous classes and their interaction with Java serialization.
Scala will generate anonymous classes to express core language features.
Each of these scenarios has the potential to create a serializable class that can become a refactoring burden.
The X and Y traits are defined to illustrate the class generation.
The test1 method creates an anonymous class for the type refinement.
The test2 method creates an anonymous class from the mixin inheritance.
Notice that anonymous classes are numbered on a per file basis and anonymous functions are numbered based on their class/method scope.
Be wary of Java serialization means that anonymous classes make it easier to break long-term serializability of data, because any anonymous class defined in the file can change the numbering.
For anonymous classes, the simple solution is to ensure that any long-term persisted objects define named objects or classes.
The benefit to this approach is that the generated classfiles are file-order independent.
The result is that the only remaining anonymous class is the closure defined in the test3 method.
The class One and Two are now explicitly named and can be moved around within the file or into other files.
The only remaining issue is the long-term serializability of the anonymous function.
But because of the volatile nature of randomly generated class names, it’s best to avoid persisting closures for any long-running applications.
When no other option is available, you should ensure that closure deserialization issues are properly handled.
When it comes to anonymous functions, it’s best to avoid long-term serialization.
This grants the most amount of flexibility in syntax and usage.
The trait SchedulingService defines the interface for a long-term scheduler.
The single method schedule is used to schedule tasks to be performed at a later time.
The schedule method takes two parameters, a configuration for when to run the task and an anonymous closure to run.
The SchedulingService could leverage the fact that closures are serializable and store the task on the filesystem.
This would let the SchedulingService allow persistent schedules in the face of restarts.
In the face of closure class name instability, this is a bad long-term strategy.
For example, the SchedulingService could use a Job trait instead of a closure.
The Job trait is defined as Serializable and has one abstract method, doWork.
The doWork method will contain the same implementation that used to be in the anonymous closure.
The SchedulingService is updated to take Jobs instead of Function0 [Unit]
Although this doesn’t prevent users from creating anonymous subclasses of Job, it does make it easier for them to explicitly name their Job classes and avoid volatile classnames.
The upside to serialization issues in Scala is that Java serialization is often not used for long-term serialization.
Java’s serialization frequently gets related to remote method invocations and live machine-to-machine messaging or temporary data storage.
This means that in the general case, no special care needs to be taken around anonymous classes.
But in those few situations that are troublesome, there are solutions you can use to avoid refactoring hell.
The next potential wart in Java integration is that of annotations.
Many libraries use annotations for runtime code generation and inspection.
Annotations are pieces of metadata that can be attached to expressions or types.
Annotations can be used to accomplish many different goals, including the following:
Create and enforce additional type system constraints (the continuations plugin defines @cpsParam on types to create additional type-system checks for delimited continuations)
In the JVM ecosystem, many libraries rely on annotations to work properly.
Scala prefers annotations instead of keywords for features like Java serialization.
Understanding annotations in Scala and where they wind up within the bytecode of a class is important for interoperability with Java frameworks.
One of the largest issues facing Scala and Java interoperability is the mismatch of how Scala compiles class members and annotations compared to how Java compiles class members and annotations.
In Java, there’s a separate namespace for class fields and class methods.
Both of these can be created, named, and annotated separately.
In Scala, there’s one namespace for all members of a type.
The compiler takes responsibility for creating fields on a class as needed.
Annotations on a member of a Scala class could compile to multiple methods and fields in the bytecode.
The value member is of type Int and is variable.
In Scala 2.9.0, this class is compiled approximately into the following Java class:
The Simple class has three members: A value field, a value method, and a value_$eq method.
The methods are defined public and the field is defined private.
The annotation is only placed on the field representing the value.
Even though the single member var value compiles into three separate locations in a classfile, the annotation is being placed on only one of them.
JavaBean style getters and setters Some frameworks in Java rely on a Java naming convention for access properties on objects.
This is a convention of the JavaBean specification, where property accessors and setters usually take the names getFoo and setFoo.
Although the JavaBean specification doesn’t require that methods have the string get and set in them, some Java libraries aren’t implemented against the specification, but rather against the naming convention.
The simple class mentioned earlier can be modified to support these libraries, as follows:
This leads to the creation of the following methods: value, value_$eg, getValue, and setValue.
This can be applied to the class itself and the compiler will generate an appropriate BeanInfo class for all vars and vals on the class.
In the best case, this mismatch where one definition can compile to several locations in a classfile can confuse annotation libraries designed to work with Java.
Annotation targets are used to assign where in the resulting class files annotations should be placed.
The different annotations each target a separate area of generated bytecode.
To use one of these annotations, you must apply them against another annotation—that is, the target annotations annotate other annotations with the desired bytecode location.
The class Data is defined with a single member dataId.
The annotation Id also has the annotation annotation .target.getter applied to it.
Scala allows annotations to be placed on expressions, types, members, and classes.
The annotation target classes need to be placed against the annotation type that they wish to change.
This can be simplified by creating a type alias for the annotated type.
The method has the same name as the val or var.
The method has the name of the var with _$eq appended for its name.
The Data class is now modified to use the type alias for its annotation.
This results in the same bytecode as the previous example.
When using a library or framework designed to annotate JavaBeans, it’s helpful to create a wrapper for Scala.
This wrapper should consist of an object, similar to AnnotationHelpers, that has the Java framework’s annotations assigned to the appropriate generated code locations.
This technique is helpful for defining Scala classes that work with the Java Persistence API (JPA)
A second issue needs to be dealt with: some libraries require annotations in locations that Scala doesn’t generate.
As discussed in section 10.1, Scala doesn’t have a way to express static fields on classes.
Although the JVM allows fields associated with class instances at runtime, the Scala language doesn’t support this notion.
You might argue that you can annotate Scala’s objects because they are compiled to static values.
This defines a simple object Foo in the raw namespace.
Scala compiles to bytecode an equivalent to this Java class:
The static block is run when the class is loaded into the JVM.
This instantiates the Foo object and assigns it to the MODULE$ static field.
Scala converts all objects to JVM classes with the same name as the object but with a $ appended to the name.
In this example, note that there’s only one static field.
You also have no way to provide an annotation on the static field.
If a Java library requires static fields or annotations on static fields to work, this library is unusable against Scala classes.
The solution here is the same as before: Use Java for the portion of code that needs to interact with Java.
This is the unfortunate reality of interacting with Java libraries.
A few were designed in such a way as to not be usable from Scala.
This chapter covered the areas of concern and offered solutions to each.
First, is the mismatch between Java’s primitive with boxing and Scala’s unified AnyVal types.
You can simplify this mismatch by preferring primitive types on the Java side.
Because Scala always prefers using the primitive value at runtime, this reduces the total amount of boxing/unboxing overhead within a program.
The second area of concern is when there exists a solution to a problem in both Scala and Java.
The Scala collections API isn’t friendly to use from Java, and the Java collections API lacks many of the functional features found in the Scala version.
To ease integration between Java portions of code and Scala portions, providing implicit conversions on the Scala side can be beneficial.
It’s important to be careful here to ensure you don’t make assumptions about equality.
Using explicit conversion functions can help highlight where object identities are changing.
They can also be used to perform more than one implicit coercion.
The downside is when Java serialization is used for long-term persistence.
Scala allows the easy creation of anonymous classes, classes that could be serialized.
If an object is intended for long-term serialization, the class should be formalized and named.
Otherwise the source code structure may become locked for the lifetime of the serialized object, or worse.
The persistent storage may need to be flushed and migrated.
Finally, when faced with a Java library that won’t work from Scala, it’s best to avoid such a library.
If this isn’t possible, then constructing the portion of code required to interact with the library in Java and exposing a Scala-friendly interface is the only solution.
The next chapter covers functional programming, which is a way of writing programs that may be foreign to those of us coming from an object-oriented or imperative background.
Let’s look into a world where no effects are side effects and operations are deferred as long as possible.
Functional programming is the practice of composing programs using functions.
It’s an area of software design and architecture that has been neglected in mainstream books and classes since the emergence of object-oriented programming.
Functional programming offers a lot to the object-oriented developer and can nicely complement standard object-oriented practices.
Functional programming is a relatively large topic to try to compress into a single chapter.
Instead, this chapter introduces a few key abstractions used in functional programming and demonstrates their usage in two different situations.
The goal is to show one of the many styles of functional programming, rather than turn you into an expert functional programmer.
First, a discussion on some fundamental concepts behind the patterns in functional programming.
Category theory is the mathematical study of collections of concepts and arrows.
For the purposes of computer science, a concept is a type, like String, Int, and so on.
An arrow is a morphism between concepts—something that converts from one concept to another.
Usually in computer science, a morphism is a function defined against two types.
For example, the category of cats includes all the various types of cats in the world as well as the captions needed to convert from a serious cat into a lol cat.
Category theory is the study of categories like these and relationships between them.
The most used category in programming is the categories of types: the classes, traits, aliases and object self types defined in your program.
Category theory shows up in many corners of programming but may not always be recognized.
This section will introduce a library to configure software and introduce the concepts from category theory that are used in the library.
A good way to think of category theory, applied to functional programming, is design patterns.
These concepts can be directly expressed in a functional language like Scala and have library support.
When designing software, if a particular entity fits one of these concepts, a whole slew of operations immediately becomes available as well as the means to reason through usage.
Let’s look at this concept in the context of designing a configuration library.
In section 2.4 we explored the usage of Scala’s Option class as a replacement for nullable values.
In particular, this section showed how we can use Options to create walled gardens—that is, functions can be written as if all types aren’t null.
These functions can be lifted into functions that will propagate empty values.
The lift3 function takes a function defined against raw types and converts it to a function that works with Option types.
Scala’s for expressions are syntactic sugar for the map, flatMap, foreach, and withFilter operations defined on a class.
Each <- of the for expression is converted into a map or flatMap call.
These methods are each associated with a concept in category theory.
The map method is associated with functors, and the flatMap method is associated with monads.
For expressions make an excellent way to define workflows, which we define in section 11.4
Option is a monad because it has both a flatten and flatMap operation that abide by the monadic laws.
For now, let’s first generalize the advanced Option techniques from section 2.4.1
The goal is to use this library, in combination with a variant of the lift3 method, to construct database connections based on the current configuration parameters.
If any of these locations are updated, the program should automatically alter its behavior the next time a database connection is requested.
Let’s define a new trait, Config, that will wrap this logic for us.
Because the filesystem is volatile and configuration isn’t guaranteed to exist, the Config library will also make use of the Option trait to represent configuration values that weren’t found.
The first, map, takes a function that operates on the data stored in the Config object and returns a new Config object.
For example, when reading environment variables of strings, the map method could be used to convert an environment variable into an Integer.
This method takes a function against the current Config object and returns a second Config object.
You can use this to construct new Config objects based on values stored in an initial Config object.
We can use the flatMap operation to read this location and then extract more configuration values from that location.
This method is unsafe, in that it will attempt to read the current configuration environment, wherever configuration is defined to be, and return the resulting configuration values.
As with Option, you shouldn’t use this method until the code calling it knows what to do in the event of failure.
Also, because the get method will read the environment, it can be expensive if performed within a tight loop of the software.
Creating a new Config object is the case of defining the get method, because map and flatMap can be implemented in terms of get.
For now, let’s assume that map and flatMap are implemented appropriately (see the source code for implementations)
The Config object defines a single method called apply, which is the constructor for Config objects.
By-name parameters in Scala are similar to no-argument functions in that they’ll evaluate their associated expressions every time they’re referenced.
This means that defining the get method to reference the data argument will cause the data parameter to be reevaluated each time it’s referenced.
The environment method will read configuration values from the process environment.
The method takes a string of the environment variable to read.
If the environment variable is available, the value is returned inside an Option.
Next, the Scala REPL is started and a Config object pointing to the test_prop property value is created.
When calling get on this test property, the correct value is displayed.
Now let’s look into constructing database connections based on environment variables.
The lift3 method takes a three-argument function and converts it into a threeargument function that works against Option arguments.
Let’s take the simple approach of defining a new lift function to convert three-argument methods into methods that operate on Config objects.
The lift3Config method takes a three-argument function as its own argument.
It returns a new function that takes Config traits of the original parameters.
The implementation uses for expressions to call the underlying flatMap and map operations on the Config object.
The final result is a Config object wrapping the underlying data.
Let’s use this to define a DatabaseConnection that uses environment variables.
This creates a three-argument function that works on Config [Option[String]] types.
Finally, this new function is passed three arguments, one for each environment variable.
It’s almost identical to the lift3 method because both the Config trait and the Option trait are instances of the same abstract concept from category theory.
Let’s try to reverse engineer the raw concepts behind the lift method to see if we can rescue it for both Option and Config.
Functors are transformations from one category to another that can also transform and preserve morphisms.
A morphism is the changing of one value in a category to another in the same category.
In the example of the category of cats, a morphism would be akin to a box that takes a dim cat and converts it into a neon glowing cat.
In the category of types, the most commonly used in computer science, a morphism is a function that converts from one type to another.
The functor would be something that converts cats into dogs.
The functor would be able to convert dim cats into dim dogs and glowing cats into glowing dogs.
The functor could also convert the box so that it can convert dim dogs into glowing dogs.
The circle on the bottom represents the category of all possible types.
For any type T that’s in the category on the bottom, you can place that type in the type constructor F[_] and get a new type F[T] shown on the top category.
For example, for any type T, a Config[T] can be made.
These laws provide a default set of unit tests as well as standard transformations that can be performed on code.
This book doesn’t cover the laws in detail, but we give sufficient grounding in Category theory for you to investigate these laws as needed.
For the transformation to be a functor transformation, it means that all morphisms must be preserved in the transformation.
Functors and monads, and how they relate to categories in the first category, we should have a transformed function that operates on the transformed types.
For example, if I have a function that takes a String and converts it to an Int, I should be able to also take a Config[String] instance and convert it to a Config[Int] instance.
This is what the map method on Option and Config grant.
For any type A, a Functor can construct a type T[A] in the new category.
Given a transformed type T[A] and a morphism in the original category A=>B, a value T[B] can be created.
We have a new function that takes T[A] and returns T[B]
The Functor implementation for Config is defined such that the apply method calls the Config companion object’s apply method.
The map method can delegate to the underlying map method on the Config class.
Finally, let’s create a bit of syntactic sugar so that the map method on the Functor typeclass appears to be on the raw type.
The implicit method functorOps creates a new anonymous class that has a local map method that accepts only a function A => B.
Now, we’ll create the lift method so that it’s generic against the Functor abstraction.
The new lift method uses a Functor to promote elements of the function.
The apply3 method accepts a three-argument function and calls map against each of these methods to chain the method calls.
The issue with this method is that the resulting type is F[F[F[D]]], not F[D]
To resolve this, let’s create a new type trait the extends Functor and adds a flatten method.
This method will be responsible for collapsing the pattern F[F[D]] to F[D], which should allow the above function to work as desired.
Monads are a means of combining a functor application, if that functor is an endofunctor.
An endofunctor is a functor that converts concepts and morphisms in its category back into the same category.
Using the cat example, an endofunctor would be a way of converting cats and genetic cat manipulations into different types of cats and cat genetic manipulations.
Transforming a cat more than once by the same functor could be reduced into single functor application.
Similarly, altering cat genetic manipulations more than once can be reduced into a single alteration.
In computer science, monads are often used to represent computations.
A monad can be used to abstract out the execution behavior of a program.
Some monads can be used to handle concurrency, exceptions, or even side effects.
Using monads in workflows or pipelines is discussed in section 11.4
Let’s look at the programming definition of a monad in the following listing:
The flatten method is used to take a double wrapped type and turn it into a wrapped type.
If a Functor T[_] is applied twice, the monad knows how to combine this to one application.
For example, the List monad can convert a list of lists into a single list with all the underlying elements of the nested lists.
The Monad trait also provides a convenience function flatMap, which chains the flatten and map calls for convenience.
Monads are, among other things, a means of preventing bloat in types and accessors.
We can take a nested list of lists and treat it as a single list, which has a more convenient syntax.
Again, let’s create a convenience implicit to reduce the syntactic noise of using the Monad type trait.
The flatMap method delegates to the Monad trait’s flatMap method.
Now, let’s modify the lift function to make use of the Monad trait.
Monad and functor differences In reality, a monad is the flatten operation for a functor.
If you were to encode the category theory directly into the type system, the flatMap method would require an implicit Functor.
For category theory applied to computer science, in this instance at least, everything is in the category of types.
The type constructor F[_] applied to a type T results in the type F[T], which is in the same category of types.
A monad is a means of taking two such applications and reducing them to a single—that is, F[F[T]] becomes F[T]
Monads are means of combining functor applications on types, hence F[F[T]] being shortened to F[T] through use of a monad.
The new lift method uses a Monad type class instead of a Functor.
This lift method looks similar to the original lift method for Option, except that it can generically lift functions to operate against monads.
The lift method is called using Option as the type parameter.
The result is a new function that accepts three Option[String] values and returns an Option[Connection]
Monads and functors form the basic building blocks of lots of fundamental concepts in programming.
This abstraction can be used as an alternative mechanism of writing the lift function.
Instead of relying on a flatMap operation, a function can curried and values fed into it in an applicative style.
Currying is the conversion of a function of multiple parameters into a chain of functions that accept a single parameter.
A curried function accepts one of its arguments and returns a function that accepts the next argument.
This chain continues until the last function returns a result.
In Scala, any function of multiple parameters can be curried.
Applicative style refers to using curried functions to drive parameters in applicative functors through them.
Applicative functors are functors that also support a method to convert mapped morphisms into morphisms against mapped types.
In English, this means that if we have a list of functions, an applicative functor can create a single function that accepts a list of argument values and returns a new list of results.
Currying is taking a function of several arguments and turning it into a function that takes a single argument and returns a function that takes the next argument that returns a function that takes the next argument and so on, until finally one of the functions returns a value.
In Scala, all functions have a curried method that can be used to convert them from multiargument functions into curried functions.
The first line constructs a function that takes three arguments: an integer, a double, and a string.
This function takes a Double and returns a function that takes a String and returns a String.
A single function of multiple arguments is converted into a chain of functions, each returning another function until all arguments have been satisfied and a return value is made.
Currying is pretty easy to do by hand; let’s try it out.
This line constructs an anonymous function y that takes an Int, called a, and returns the function defined by the rest of the expression.
This same trick defines a nested anonymous function, until eventually the function x defined earlier is called.
Note that this function has the same signature as x.curried.
The trick is that each call to a function captures a portion of the argument list of the original function and returns a new function for the remaining values.
This trick can be used when attempting to promote a function of multiple simple parameters to work with values inside a Functor.
Let’s redefine the lift method to use only a Functor.
The new implementation for the apply3 method in lift uses the map operation on Functor against the curried function.
Let’s break this down to see what’s happening in the types.
The parentheses in the resulting expression have been adjusted to show the true type.
The result is a single function that takes an A and produces a value.
Because the fa parameter is a value of F[A], we can combine the curried function with the fa value using the map method.
The map method on fa is called against the curried function.
The result is a F[_] containing the rest of the function.
In this case the second type parameter is a function B=>C=>D.
The code can’t continue to use the map method defined on Functor because the remaining function is wrapped inside the functor F[_]
To solve this, let’s define a new abstraction, Applicative, as shown in the following listing:
Notice that this is different from a monad, which can flatten F[F[_]]
The lift method can now be completed using applicative functors.
The lift function now requires both a Functor and an Applicative context bound.
As before, the function is curried and applied against the first argument using the functor’s map method.
But the applicative functor’s lift2 method can be used to apply the second argument of the function.
Finally, the lift2 method is used again to apply the third argument of the original function.
The final result is the value of type D wrapped inside the functor F[_]
The result is the same as it was for using functor and monad.
The two reasons to choose this style instead is that there are more things that can implement the lift2 method for applicative functors than can implement the flatten method for monads and that applicative functors can compute in parallel while monadic workflows are sequential.
An alternative syntax to lifting functions into applicative functors is known as applicative style.
This can be used in Scala to simplify the construction of complex function.
Currying and applicative style dependencies, keeping the values inside an applicative functor.
For example, using the Config library defined earlier, you can construct an entire program from functions and applicative applications.
Applicative functors provide a way to take two computations and join them together using a function.
The Traversable example highlights how two collections can be parallelized into pairs.
Applicative functors and parallel processing go together like bread and butter.
Assuming there’s a software system that’s composed of two subsystems: the DataStore and the WorkerPool.
The DataStore class and WorkerPool class are defined with all the methods required for their subcomponent.
The Application class is defined as taking a DataStore instance and a WorkerPool instance.
Now, when constructing the application, the following can be done with applicative style:
The dataStore and workerPool methods are defined as abstraction constructors of DataStore inside a Config object.
The entire system is composed by creating an Applicative instance on the dataStore, combining the workerPool and applying that to an anonymous function (new Application(_,_))
The result is an Application embedded in a Config object.
The Applicative call creates a builder that will use the Config[_] instances to construct something that can accept a function of raw types and return a resulting Config object.
The syntax presented here is a Scala idiom and doesn’t mimic the Haskell directly.
This applicative style, combined with the Config class, can be used to do a form of dependency injection in Scala.
Software can be composed of simple classes that take their dependencies in the constructor and a separate configuration can be used to wire all the pieces together using functions.
This is an ideal blend of object orientation and functional programming in Scala.
For example, if the DataStore trait had an implementation that used a single JDBC connection like the following:
Then the entire application can be configured as shown in the following listing:
This function pulls the value of an environment variable if it exists and is used to pull values for the JDBC connection’s URL, user, and password.
Finally, the applicative builder is used to construct the application from the dataStore and workerPool configuration.
This bit of code represents the configuration of software separate from the definition of its components.
There’s no need to resort to XML or configuration files in Scala.
Let’s look at how the Applicative object build method works.
The build method on Applicative takes two types, F[_] and A.
The F[_] type is required to have an applicative and functor instance available implicitly.
The build method accepts a parameter of type F[A] and returns a new ApplicativeBuilder class.
Let’s look at the ApplicativeBuilder class in the following listing:
Listing 11.4 Configuring an application using the Config class and applicative builder.
The apply method takes a function against raw types A and B and applies the captured member ma against it, creating an F[B]
Like the lift example earlier, this method curries the raw function f and uses the map and lift2 methods to feed arguments to the lifted function inside the functor.
Applicative style is a general concept that can be applied in many situations.
For example, let’s use it to compute all the possible pairings of elements from two collections.
The Applicative builder is used to combine two Traversable lists.
The apply method is given a function that takes two arguments and creates a pairing of the two.
The resulting list is each element of the first list paired with each element of the second list.
Functors and monads help express programs through functions and function transformations.
This applicative style, blended with solid object-oriented techniques, leads to powerful results.
As seen from the config library, applicative style can be used to blend pure functions and those that wall off dangers into things like Option or Config.
Applicative style is usually used at the interface between raw types like String and wrapped types like Option[String]
Another common use case in functional programming is creating reusable workflows.
A monadic workflow is a pipeline of computation that remains embedded inside the monad.
The monad can control the execution and behavior of the computation that’s nested inside it.
Monadic workflows are used to control things like side effects, control flow, and concurrency.
A great example is using monadic workflows for automated resource management.
Monadic workflows can be used to encapsulate a complicated  sequential process.
Monadic workflows are often used with collections to search through a domain model for relevant data.
In the managed resource example, monadic workflows are used to ensure that when a sequential process is complete, resources are cleaned up.
Automated resource management is a technique where a resource, such as a file handle, closes automatically for the programmer when that resource is no longer needed.
Though there are many techniques to perform this function, one of the simplest is to use the loaner pattern.
The loaner pattern is where one block of code owns the resource and delegates its usage to a closure.
The readFile function accepts a File and a handler function.
This stream is loaned to the handler function, ensuring that the stream is closed in the event of an exception.
The example shows how to use the readFile method to read the first byte of the test.txt file.
Notice how the code doesn’t open or close the resource; it’s merely loaned the resource for usage.
This technique is powerful, but it can be built up even further.
It’s possible that a file may need to be read in stages, where each stage performs a portion of the action.
It’s also possible that the file may need to be read repeatedly.
All of this can be handled by creating an automated resource management monad.
Let’s take a cut at defining the class, as shown in the following listing:
The ManagedResource trait has a type parameter representing the resource it manages.
It contains a single method, loan, which external users can utilize to modify the resource.
Now let’s create one of these in the readFile method.
Now the readFile method constructs a ManagedResource with type parameter InputStream.
The loan method on the ManagedResource first constructs the input stream, and then loans it to the function f.
The ManagedResource trait is both a functor and a monad.
Like the Config class, ManagedResource can define the map and flatten operations.
The ManagedResource companion object contains the Functor and Monad implementation so that Scala will find them by default on the implicit context.
The Functor .apply method is implemented by loaning the captured value when the loan method.
The Functor.map method is implemented by calling the loan value of the ma resource and first wrapping this value with the mapping function before calling the passed in function.
Finally, the Monad.flatten operation is performed by calling loan on the outer resource and then calling loan on the inner resource that was returned from the outer resource.
Now that the ManagedResource trait has been made monadic, we can use it to define a workflow against a resource.
A workflow is a euphemism for a collection of functions that perform a large task in an incremental way.
Let’s create a workflow that will read in a file, do some calculations, and write out the calculations.
The first task in reading the file is iterating over all the textual lines in the file.
We can do this by taking the existing readFile method and converting the underlying InputStream into a collection of lines.
The foreach method is defined by calling readLine on the BufferedReader until it’s out of input.
For each line read, as long as it’s not null, the line is fed to the anonymous function f.
Finally, the view method is called on the Traversable to return a lazily evaluated collection of lines.
The LazyTraversable type alias is constructed to simplify referring to a Traversable view of type T where the original collection was also a Traversable.
From now on, we’ll use this alias to simplify the code samples.
Now let’s define the portion of workflow that will read the lines of a file.
The getLines method takes a file and returns a ManagedResource containing a collection of strings.
The method is implemented by a single for expression, workflow.
The workflow first reads the file and pulls the InputStream.
This InputStream is converted into an InputStreamReader, which is then converted into a BufferedReader.
The result is a ManagedResource that loans a collection of line strings, rather than a raw resource.
If a class is a monad or functor, then we can use a for expression to manipulate the types inside the functor without extracting them.
For example, the getLines method could be called early in an application’s lifecycle.
This allows the composition of behavior to be part of the composition of the application.
We should read the input file by line and calculate the lengths of each line.
The resulting calculations will be written to a new file.
The for expression defines a workflow to first obtain a TraversableView of all the lines in a file using the getLines method.
Next, the line length counts are calculated by calling the length method on each line and combining that with the line number.
This method is similar to the readFile method, except that it returns an OutputStream rather than an InputStream.
The next two lines in the workflow adapt the OutputStream into a BufferedWriter.
Finally, the BufferedWriter is issued to write the counts calculations into the output file.
Any kind of file or network manipulation is wrapped into workflows called do-notation, akin to Scala’s do-notation.
Again, this workflow has just composed the behavior of calculating counts but didn’t execute it.
This gives the flexibility of defining portions of program behavior as first-class objects and passing them around or injecting them with a dependency injection framework.
A contingent of functional programmers believes that all side-effecting functions should be hidden inside a monad to give the programmer more control over when things like database access and filesystem access occur.
Though this mind-set can be helpful, it’s also viral in that it contaminates an entire code base with monadic workflows.
Scala comes from the ML family of languages, which don’t mandate the use of a monad for side effects.
Therefore, some code may make heavy use of monads and workflows while others won’t.
Monadic workflows can be powerful and helpful when used in the right situations.
Monads work well when defining a pipeline of tasks that need to be executed but without defining the execution behavior.
These laws—left identity, right identity and association—are covered in most monad-specific material.
In addition, Philip Wadler, the man who enlightened the functional world on monads, has a series of papers that describe common monads and common patterns that are well worth the read.
Monads can also be used to annotate different operations in a pipeline.
In the Config monad, there were several ways to construct a Config instance.
In the case where a Config instance was constructed from a file, the Config monad could use changedetection to avoid reading the file multiple times.
The monad could also construct a dependency graph for calculations and attempt to optimize them at runtime.
Though not many libraries exist that optimize staged monadic behavior in Scala, this remains a valid reason to encode sequences of operations into monadic workflows.
Functional programming has a lot to offer the object-oriented developer.
This can be done through applicative style, such as configuring an application, or through monadic workflows.
Both of these rely heavily on concepts from category theory.
One important thing to notice in this chapter is the prevalence of typeclass pattern with functional style.
The typeclass pattern offers a flexible form of object orientation to the functional world.
When combined with Scala’s traits and inheritance mechanisms, it can be a powerful foundation for building software.
The type classes we presented in this chapter aren’t available in the standard library but are available in the Scalaz extension library (http://mng.bz/WgSG)
The Scalaz library uses more advanced abstractions than those we presented here, but it’s well worth a look.
Scala provides the tools needed to blend the object-oriented and functional programming worlds.
Scala is at its best when these two evenly share a codebase.
The biggest danger to misusing Scala is to ignore its object orientation or its functional programming.
But combining the two is the sweet spot that the language was designed to fulfill.
You’ll have no trouble getting introductions to Scala in books or online, but it’s.
Th ere’s little heavy-handed theory here—just dozens of crisp, practical techniques for coding in Scala.
Written for readers who know Java, Scala, or another OO language.
He is a Scala committer and the maintainer of scala-tools.org.
Scala In Depth contents foreword preface acknowledgments about this book Who should read this book? Roadmap Code downloads and conventions Author online About the author.
