We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
This book introduces the concepts and techniques of functional programming (FP)—we use Scala as the vehicle, but the lessons herein can be applied to programming in any language.
Our goal is to give you the foundations to begin writing substantive functional programs and to comfortably absorb new FP concepts and techniques beyond those covered here.
Throughout the book we rely heavily on programming exercises, carefully chosen and sequenced to guide you to discover FP for yourself.
Expository text is often just enough to lead you to the next exercise.
A word of caution: no matter how long you've been programming, learning FP is challenging.
Inside effects the first chapter, we will explain exactly what this means.
From this single idea and its logical consequences emerges a very different way of building programs, one with its own body of techniques and concepts.
We start by relearning how to write the simplest of programs in a functional way.
From this foundation we will build the tower of techniques necessary for expressing functional programs of greater complexity.
Some of these techniques may feel alien or unnatural at first and the exercises and questions can be difficult, even brain-bending at times.
Keep a beginner's mind, try to suspend judgment, and if.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
When you start to feel more fluent at expressing functional programs, then take a step back and evaluate what you think of the FP approach.
This book does not require any prior experience with Scala, but we won't spend a lot of time and space discussing Scala's syntax and language features.
Instead we'll introduce them as we go, with a minimum of ceremony, mostly using short examples, and mostly as a consequence of covering other material.
These minimal introductions to Scala should be enough to get you started with the exercises.
If you have further questions about the Scala language while working on the exercises, you are expected to do some research and experimentation on your own or follow some of our links to further reading.
The book is organized into four parts, intended to be read sequentially.
Part 1 introduces functional programming, explains what it is, why you should care, and walks through the basic low-level techniques of FP, including how to organize and structure small functional programs, define functional data structures, and handle errors functionally.
These techniques will be used as the building blocks for all subsequent parts.
Part 2 introduces functional design using a number of worked examples of functional libraries.
After Part 2, it may therefore be a good idea to take a break and try getting more practice writing functional programs beyond the shorter exercises we work on throughout the chapters.
Of course, how you read this book is ultimately up to you, and you are free to read ahead if you wish.
We introduce and explain some new idea or technique with an example, then work through a number of exercises, introducing further material via the exercises.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Therefore we  suggeststrongly that you download the exercise source code and do the exercises as you go through each chapter.
We will mark exercises that we think are  or that wehard consider to be  to understanding the material.
The  designation is ourcritical hard effort to give you some idea of what to expect—it is only our guess and you may find some unmarked questions difficult and some questions marked  to behard quite easy.
The  designation is applied to exercises that address conceptscritical that we will be building on and are therefore important to understand fully.
Noncritical exercises are still informative but can be skipped without impeding your ability to follow further material.
Examples are given throughout the book and they are meant to be  rathertried than just read.
Before you begin, you should have the Scala interpreter (REPL) running and ready.
We encourage you to experiment on your own with variations of what you see in the examples.
A good way to understand something is to change it slightly and see how the change affects the outcome.
Sometimes we will show a REPL session to demonstrate the result of running some code.
This will be marked by lines beginning with the  prompt ofscala> the REPL.
Code that follows this prompt is to be typed or pasted into the interpreter, and the line just below will show the interpreter's response, like this:
This lets us give you a complete and concise definition without breaking the flow of the main text with overly formal language, and also makes it easy to refer back to when needed.
There are chapter notes (which includes references to external resources) and.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Throughout the book we provide references to this supplementary material, which you can explore on your own if that interests you.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Functional programming (FP) is based on a simple premise with far-reaching implications: We construct our programs using only.
What does this mean exactly? Performing anyside effects of the following actions directly would involve a side effect:
Reassigning a variable Modifying a data structure in place Setting a field on an object Throwing an exception or halting with an error Printing to the console or reading user input Reading from or writing to a file Drawing on the screen.
Consider what programming would be like without the ability to do these things.
How is it even possible to write useful programs at all? If we can't reassign variables, how do we write simple programs like loops? What about working with data that changes, or handling errors without throwing exceptions? How can we perform I/O, like drawing to the screen or reading from a file?
The answer is that we can still write all of the same programs—programs that can do all of the above and more—without resorting to side effects.
Functional programming is a restriction on  we write programs, but not on  programshow what we can write.
And it turns out that accepting this restriction is tremendously.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Because of their modularity, pure functions are easier to test, to reuse, to parallelize, to generalize, and to reason about.
But reaping these benefits requires that we revisit the act of programming, starting from the simplest of tasks and building upward from there.
In many cases we discover how programs that seem to necessitate side effects have some purely functional analogue.
Nevertheless, FP is a truly radical shift in how programs are organized at every level—from the simplest of loops to high-level program architecture.
The style that emerges is quite different, but it is a beautiful and cohesive approach to programming that we hope you come to appreciate.
In this book, you will learn the concepts and principles of FP as they apply to every level of programming.
We begin in this chapter by explaining what a pure function is, as well as what it isn't.
We also try to give you an idea of just why purity results in greater modularity and code reuse.
A function with input type  and output type  (written in Scala as a single type: A B A ) is a computation which relates every value  of type  to exactly one value => B a A.
Furthermore, if it really is a ,function it will do nothing else.
In other words, a function has no observable effect on the execution of the program other than to compute a result given its inputs; we say that it has no side effects.
We sometimes qualify such functions as  functions to make this morepure explicit.
It takes two integer values and returns an integer value.
Anotheralways return the same integer value example is the  function of a  in Java, Scala, and many otherlength String languages.
For any given string, the same length is always returned and nothing else occurs.
We can formalize this idea of pure functions by using the concept of referential (RT)
This is a property of  in general and not justtransparency expressions.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
For the purposes of our discussion, consider an expression to be any part of a program that can be evaluated to a result, i.e.
The evaluation of this expression results in the same value 5 every time.
This is all it means for an expression to be referentially transparent—in any program, the expression can be replaced by its result without changing the meaning of the program.
And we say that a function is  if its body is RT, assuming RTpure inputs.
Footnote 1mThere are some subtleties to this definition, and we'll be refinining it later in this book.
Referential transparency enables a mode of reasoning about program evaluation called.
When expressions are referentially transparent, wethe substitution model can imagine that computation proceeds very much like we would solve an algebraic equation.
We fully expand every part of an expression, replacing all variables with their referents, and then reduce it to its simplest form.
At each step we replace a term with an equivalent one; we say that computation proceeds by substituting.
In other words, RT enables equals for equals equational reasoning about programs.
This style of reasoning is  natural; you use it all the timeextremely when understanding programs, even in supposedly "non-functional" languages.
Let's look at two examples—one where all expressions are RT and can be reasoned about using the substitution model, and one where some expressions violate RT.
There is nothing complicated here, part of our goal is to illustrate that we are just formalizing something you already likely understand on some level.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
If you wish to "modify" a string, you must create a copy of it.
Suppose we replace all occurrences of the term  with the expressionx referenced by  (its definition), as follows:x.
Now let's look at a function that is  referentially transparent.
The previous stateStringBuilder of the  is destroyed after a call to.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Suppose we substitute the call to  like we did earlier, replacing all occurrences of append y with the expression referenced by :y.
This transformation of the program results in a different outcome.
Conversely, the substitution model is simple to reason about since effects of evaluation are purely local (they affect only the expression being evaluated) and we need not mentally simulate sequences of state updates to understand a block of code.
Even if you haven't used thelocal reasoning name "substitution model", you have certainly used this mode of reasoning when thinking about your code.3
Footnote 3mIn practice, programmers don't spend time mechanically applying substitution to determine if code is pure—it will usually be quite obvious.
We said that applying the discipline of FP buys us greater modularity.
Why is this the case? Though this will become more clear over the course of the book, we can give some initial insight here.
A modular program consists of components that can be understood and reused independently of the whole, such that the meaning of the whole depends only on the meaning of the components and the rules governing their composition; that is, they are.
A pure function is modular and composable because itcomposable separates the logic of the computation itself from "what to do with the result" and.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Input is obtained in exactly one way: via the argument(s) to the function.
By keeping each of these concerns separate, the logic of the computation is more reusable; we may reuse the logic wherever we want without worrying about whether the side effect being done with the result or the side effect being done to request the input is appropriate in all contexts.
We also do not need to mentally track all the state changes that may occur before or after our function's execution to understand what our function will do; we simply look at the function's definition and substitute the arguments into its body.
Let's look at a case where factoring code into pure functions helps with reuse.
This is a simple and contrived example, intended only to be illustrative.
Suppose we are writing a computer game and are required to do the following:
Declares a data type Player with two properties: name, which is a string, and score, an integer.
Takes two Players, compares their scores and declares the winner.
This declares a simple data type  with two properties, , which isPlayer name a character string, and  which is an integer.
The method score declareWinner takes two s, compares their scores and declares the player with the higherPlayer score the winner (unfairly favoring the second player, granted)
The resultprintWinner type of these methods is  indicating that they do not return a meaningfulUnit result but have a side effect instead.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
While this code closely matches the earlier problem statement, it also intertwines the branching logic with that of displaying the result, which makes the reuse of the branching logic difficult.
In this case, the comparison logic is simple enough that we could just inline it, but then we are duplicating logic—what happens when playtesting reveals that our game unfairly favors one player, and we have to change the logic for determining the winner? We would have to change it in two places.
And what if we want to use that same logic to sort a historical collection of past players to display a high score list?
A pure function that takes two players and returns the higher-scoring one.
This version separates the logic of computing the winner from the displaying of the result.
Computing the winner in  is referentially transparent and thewinner impure part—displaying the result—is kept separate in.
We canprintWinner now reuse the logic of  to compute the winner among a list of players:winner.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Reduces the list to just the player with the highest score.
In this example,  is a function on the  data type from thereduceLeft List standard Scala library.
The expression will compare all the players in the list and return the one with the highest score.
We will have a lotwinner reduceLeft more to say about passing functions to functions, but for now just observe that because  is a pure function, we are able to reuse it and combine it withwinner other functions in ways that we didn't necessarily anticipate.
In particular, this usage of  would not have been possible when the side effect of displayingwinner the result was interleaved with the logic for computing the winner.
This was just a simple example, meant to be illustrative, and the sort of factoring we did here is something you've perhaps done many times before.
It's been said that functional programming, at least in small examples, is just normal separation of concerns and "good software engineering"
We will be taking the idea of FP to its logical endpoint in this book, and applying it in situations where is applicability is less obvious.
As we'll learn, any function with side effects can be split into a pure function at the "core" and possibly a pair of functions with side effects; one on the input side, and one on the output side.
This is what we did when we separated the declaration of the winner from our pure function.
This transformation can be repeated to push sidewinner effects to the "outer layers" of the program.
Functional programmers often speak of implementing programs with a pure core and a thin layer on the outside that handles effects.
We will return to this principle again and again throughout the book.
In this chapter, we introduced functional programming and explained exactly what FP is and why you might use it.
In subsequent chapters, we cover some of the fundamentals—how do we write loops in FP? Or implement data structures? How do we deal with errors and exceptions? We need to learn how to do these things and get comfortable with the low-level idioms of FP.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Now that we have committed to using only pure functions, a question naturally emerges: how do we write even the simplest of programs? Most of us are used to thinking of programs as sequences of instructions that are executed in order, where each instruction has some kind of effect.
In this chapter we will learn how to write programs in the Scala language just by combining pure functions.
This chapter is mainly intended for those readers who are new to Scala, to functional programming, or both.
As with learning a foreign language, immersion is a very effective method, so we will start by looking at a small but complete Scala program.
If you have no experience with Scala, you should not expect to understand the code at first glance.
Therefore we will break it down piece by piece to look at what it does.
These are functions that take other functions as arguments, and may themselves return functions as their output.
This can be brain-bending if you have a lot of experience programming in a language  the ability to pass functions around like that.without Remember, it's not crucial that you internalize every single concept in this chapter, or solve every exercise.
In fact, you might find it easier to skip whole sections and spiral back to them when you have more experience onto which to attach these concepts.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We declare an object (also known as a "module") named.
This isMyModule simply to give our code a place to live, and a name for us to refer to it later.
We put our code inside the object, between curly braces.
We will discuss objects, modules, and namespaces in more detail shortly.
The  object has three methods: , , and .MyModule abs formatAbs main Each method is introduced by the  keyword, followed by the name of thedef method which is followed by the arguments in parentheses.
In this case all three methods take only one argument.
If there were more arguments they would be separated by commas.
Following the closing parenthesis of the argument list, an optional type annotation indicates the type of the result (the colon is pronounced "has type")
The body of the method itself comes after an equals ( ) sign.
We will= sometimes refer to the part of a method declaration that goes before the equals sign as the  or , and the code that comes after the equals sign weleft-hand side signature will sometimes refer to as the  or.
The value returned from a method is simply the valuereturn of its right-hand side.
The  method represents aabs pure function that takes an integer and returns its absolute value:1
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The abs method takes a single argument n of type Int, and this is declared with n: Int.
The definition is a single Scala expression that uses the built-in if syntax to negate n if it's less than zero.
The format method is a standard library method defined on String.
Here we are calling it on the msg object, passing in the value of x along with the value of abs applied to x.
This results in a new string with the occurrences of %d in msg replaced with the evaluated results of x and abs(x) respectively.
This method is declared , which means that it cannot be called fromprivate any code outside of the  object.
This function takes an  and returnsMyModule Int a , but note that the return type is not declared.
Scala is usually able toString infer the return types of methods, so they can be omitted, but it's generally considered good style to explicitly declare the return types of methods that you expect others to use.
This method is private to our module, so we can omit the type annotation.
The body of the method contains more than one statement, so we put them inside curly braces.
A pair of braces containing statements is called a .block Statements are separated by new lines or by semicolons.
In this case we are using a new line to separate our statements.
The first statement in the block declares a  named  using the String msg val keyword.
A  is an immutable variable, so inside the body of the val formatAs method the name  will always refer to the same  value.
The Scalamsg String compiler will complain if you try to reassign  to a different value in the samemsg context.
Remember, a method simply returns the value of its right-hand side, which in this case is a block.
And the value of a multi-statement block inside curly braces is simply the same as the value of its last statement.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Interpolated strings can reference Scala values in scope at theString.
An interpolated string has an  (fors 'substitute') just before the first , for example: " s"The absolute.
Finally, our  method is an "outer shell" that calls into our purelymain functional core and performs the effect of printing the answer to the console:
The name  is special because when you run a program, Scala will look formain a method named  with a specific signature.
The  array willString Unit args contain the arguments that were given at the command line that ran the program.
The return type of  indicates that this method does not return a meaningfulUnit value.
There is only one value of type  and it has no inner structure.
Usually a return type of  is a() Unit hint that the method has a side effect.
But since the  method itself is calledmain once by the operating environment and never from anywhere in our program, referential transparency is not violated.
This section discusses the simplest possible way of running your Scala programs, suitable for short examples.
More typically, you'll build and run your Scala code using sbt, the build tool for Scala, and/or an IDE like IntelliJ or Eclipse.
See the book's source code repo on GitHub for more information on getting set up with sbt.
Sbt is very smart about ensuring only the minimum number of files are recompiled when changes are made.
It also has a number of other nice features which we won't discuss here.
But the simplest way we can run this Scala program ( ) is from theMyModule.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We start by putting the code in a file called  or something similar.
These files.class contain compiled code that can be run with the Java virtual machine.
Actually, it's not strictly necessary to compile the code first with.
Ascalac simple program like the one we have written here can just be run using the Scala interpreter by passing it to the  code runner directly:scala.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
It's possible to simply copy and paste the code into the REPL.
It also has a paste mode (accessed with the  command) specifically designed to paste code.:paste It's a good idea to get familiar with the REPL and its features.
In the above, notice that in order to refer to our  method, we had to say main because  was defined in the  object.
An object whose primary purpose is giving its members a namespace is sometimes called a.
A member can be amodule members method declared with the  keyword, or it can be another object declared with def.
Objects can also have other kinds of members that we will ignoreval object for now.
For example, to call the  method on the  object we would say abs MyModule.
The implementations of members within an object can42.toString refer to each other unqualified (without prefixing the object name), but if needed they have access to their enclosing object using a special name: .this.
We can in general omit the dot and2.+(1) parentheses like that when calling a method and applying it to a single argument.
An object's member can be brought into scope by importing it, which allows us to call it unqualified from then on:
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We can bring all of an object's (non-private) members into scope by using the underscore syntax: import MyModule._
The difference between a package and a module is that a package cannot contain  or  members andval def can't be passed around as if it were an object.
For example, we can declare a package at the start of our Scala source file:
However, we cannot say  toMyModule f(mypackage) pass the package to some function , since a package is not af first-class value in Scala.
They can be passed around like any other value, assigned to variables, stored in data structures, and so on.
When writing purely functional programs, it becomes quite natural to want to accept functions as arguments to other functions.
We are going to look at some rather simple examples just to illustrate this idea.
In the chapters to come we'll see how useful this capability really is, and how it permeates our programming style.
But to start, suppose we wanted to adapt our program to print out both the absolute value of a number  the factorial of another number.
First, let's write , which also happens to be our first example offactorial.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Footnote 2mWe can also write this using an ordinary  loop and a mutable variable.
Int is another primitive type in Scala, representing 32-bit integers An inner or local function.
The way we write loops in Scala is with a recursive function, by convention often called  (or sometimes ) and which we'll often define local to anothergo loop function (unlike Java, in Scala, we can define functions inside any block, including within another function definition)
The arguments to  are the state for the loopgo (in this case, the remaining value , and the current accumulated factorial, ).n acc To advance to the next iteration, we simply call  recursively with the new loopgo state (here, ), and to exit from the loop we return a valuego(n-1, n*acc) without a recursive call (here, we return  in the case that )
Footnote 3mThe name 'tail-call optimization' (TCO) is something of a misnomer.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
If we are expecting this to occur for a recursive function we write, we can tell the Scala compiler about this assumption using an  ( ), so it canannotation more information on this give us a compile error if it is not able to optimize the tail calls of the function.
Using that would be cheating; theclosed form solution point here is just to get some practice writing loops using tail-recursive functions.
Now that we have , let's edit our program from before:factorial The two functions,  and , are almostformatAbs formatFactorial.
If we like, we can generalize these to a single function, formatResult , which accepts as an argument  to apply to its argument:the function.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
To declare a function with multiple arguments, we just separate each argument by a comma.
Second, our  function nowformatResult takes another function, which we call  (this is a common naming convention inf FP; see the sidebar below)
A function that takes another function as an argument is called a  (HOF)
Like any other function parameter, wehigher-order function give a type to , the type , which indicates that  expects an f Int => Int f Int and will also return an.
The type of a function expecting an  and a Int Int.
String Int (Int,String) => Int Next, notice that we call the function  using the same syntax as when wef.
Lastly, notice that we can pass aabs(x) factorial(n) reference to  and  to the  function.
In FP, we tend to use one-letter or very short variable names, especially when everything there is to say about a value is implied by its type.
Since functions are usually quite short in FP, many functional programmers feel this makes the code easier to read, since it makes the structure of the code easier to see.
We will introduce other conventions like this throughout the book.
This example isn't terribly exciting, but the same principles apply in larger examples, and we can use first-class functions to factor out duplication whenever we see it.
We'll see many more examples of this throughout this book.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Functions get passed around so often in functional programming that it's convenient to have a lightweight way to declare a function, locally, without having to give it a name.
Also often called , , anonymous functions function literals lambda functions , or just.
We could declare a value of this type like so val f = (x: Int) => x +
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
When we define a function literal, what is actually being defined is an object with a method called.
Scala has a specialapply rule for this method name, so that objects that have an  methodapply can be called as if they were themselves methods.
Because functions are really just ordinary Scala objects, we say that they are.
We will often use "function" to refer to either such afirst-class first-class function or a method, depending on context.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
That is, functions thatmonomorphic operate on only one type of data.
For example, , and  are specificabs factorial to arguments of type , and the higher-order function  is alsoInt formatResult fixed to operate on functions that take arguments of type.
Very often, we wantInt to write code which works for  type it is given.
As an example, here's aany definition of binary search, specialized for searching for a  in an Double.
We index into an array using the same syntax as function application.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We are polymorphic 8 abstracting over the of the array, and the comparison function used for searching it.
There are no interfaces or subtyping here in this example.
One will occasionally see the term  used to refer to this form ofparametric polymorphism polymorphism.
Here, the type variable  is referenced inA three places—the search key is required to have the type , the values of the arrayA are required to have the type  (since it is an ), and the  functionA Array[A] gt must accept two arguments both of type  (since it is an ).A (A,A) => Boolean The fact that the same type variable is referenced in all three places in the type signature enforces that the type must be the same for all three arguments, and the compiler will enforce this fact anywhere we try to call.
If we trybinarySearch to search for a  in an , for instance, we'll get a typeString Array[Int] mismatch error.9
Footnote 9mUnfortunately, Scala's use of subtyping means we sometimes get rather cryptic compile errors, since Scala will try to find a common supertype to use for the  type parameter, and will fall back to using A.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
It is possible to instruct the Scala compiler to produce specialized versions of a function for each of the primitive types, just by adding an annotation to that type parameter:
This can potentially be much more efficient, though the mechanism is rather fragile, since the polymorphic values will get boxed as soon as they are passed to any other polymorphic function or data type which is unspecialized in this way.
As you might have seen when writing , the universe of possibleisSorted implementations is significantly reduced when implementing a polymorphic function.
If a function is polymorphic in some type, , the only operations that canA be performed on that  are those passed into the function as arguments (or that canA be defined in terms of these given operations)
But this is something of a wart inherited from Java.
Let's look at an example of this, a higher-order function for doing what is called.
This function, , takes a value and a function of twopartial application partial1
The name comes from the fact that the function is being applied to some but not all of its required arguments.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The style of reasoning required here is very common in functional programming—we are simply manipulating symbols in a very abstract way, similar to how we would reason when solving an algebraic equation.
Footnote 11mThis is named after the mathematician Haskell Curry, who discovered the principle.
Let's look at a final example, , which feeds the output offunction composition one function in as the input to another function.
Again, the implementation of this function is fully determined by its type signature.
This is such a common thing to want to do that Scala's standard library provides as a method on.
To compose two functions  and , youcompose Function1 f g.
Footnote 12mSolving the  exercise by using this library function is considered cheating.compose.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Interestingly, functions like  do not care whether they are operatingcompose on huge functions backed by millions of lines of code, or a couple of one-line functions.
Polymorphic, higher-order functions often end up being extremely widely applicable, precisely because they say nothing about any particular domain and are simply abstracting over a common pattern that occurs in many contexts.
We'll be writing many more such functions over the course of this book, and this is just a short taste of the style of reasoning and thinking you'll use when writing such functions.
In this chapter we have learned some preliminary functional programming concepts, and enough Scala to get going.
We learned how to define simple functions and programs, including how we can express loops using recursion, then introduced the idea of higher-order functions and got some practice writing polymorphic functions in Scala.
We saw how the implementations of polymorphic functions are often significantly constrained, such that one can often simply 'follow the types' to the correct implementation.
This is something we'll see a lot more of in the chapters ahead.
Although we haven't yet written any large or complex programs, the principles we have discussed here are scalable and apply equally well to programming in the large as they do to programming in the small.
Next up we will look at using pure functions to manipulate data.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We said in the introduction that functional programs do not update variables or modify data structures.
This chapter has a lot of exercises, particularly to help with this last point—writing and generalizing pure functions.
As always, if you need to, consult the hints or the answers, or ask for help online.
Remember, a pure function may only accept some values as input and yield a value as output.
It may not change data in place or perform other side effects.
Doesn't this mean we end up doing a lot of extra copying of the data?
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We will return to this issue after examining the definition of what is perhaps the most ubiquitous of functional data structures, the singly-linked list.
The definition here is identical in spirit to (though simpler than) the  data type defined in Scala's standard library.
This codeList listing makes use of a lot of new syntax and concepts, so don't worry if not everything makes sense at first—we will talk through it in detail.1
Footnote 1mNote—the implementations of  and  here are not tail recursive.
We will be writingsum product tail recursive versions of these functions later in the chapter.
List data type data constructor for List List companion object Pattern matching example Variadic function syntax Creating lists.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Let's look first at the definition of the data type, which begins with the keywords.
In general, we introduce a data type with the sealed trait trait keyword.
A  is an abstract interface that may optionally containtrait implementations of some methods.
Here we are declaring a , called ,trait List with no methods on it.
Technically, an abstract class trait abstract can contain , in the OO sense, which is what separates it from a , which cannotclass constructors trait.
This distinction is not really relevant for our purposes right now.
There are two such implementations or  of  (eachdata constructors List introduced with the keyword ) declared next, to represent each of the twocase possible forms a  can take—it can be , denoted by the data constructor List empty.
Just as functions can be polymorphic, data types can be as well, and by adding the type parameter  after  and then using that [+A] sealed trait List A parameter inside of the  data constructor, we have declared the  dataCons List type to be polymorphic in the type of elements it contains, which means we can use this same definition for a list of  elements (denoted ), Int List[Int]
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We could leaveY List[X] List[Y] out the  in front of the , which would make   in that type+ A List invariant parameter.
These concerns about variance are not very important for the present discussion and are more of an artifact of how Scala encodes data constructors via subtyping, so don't worry if this is not completely clear right now.3
Footnote 3mIt is certainly possible to write code without using variance annotations at all, and function signatures are sometimes simpler (while type inference often gets worse)
Unless otherwise noted, we will be using variance annotations throughout this book, but you should feel free to experiment with both approaches.
Let's look in detail at the functions  and , which we place in the sum product , sometimes called the  to  (see sidebar).object List companion object List.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Another common naming convention is  for the first element of a list (the "head" of the list),  forh t the remaining elements (the "tail"), and  for an entire list.l.
This isn't the most robust test—pattern matching on  will match only the0.0
If the target  the pattern in a case (see below), the result=> matches of that case becomes the result of the entire match expression.
If multiple patterns match the target, Scala chooses the first matching case.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
This is just an  with the same nameobject as the data type (in this case ) where we put various convenienceList functions for creating or working with values of the data type.
Footnote 7mThere is some special support for them in the language that isn't really relevant for our purposes.
Let's look at a few more examples of pattern matching:
What determines if a pattern matches an expression? A pattern may contain , like  or ,  like  and  which match anything,literals 0.0 "hi" variables x xs.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Thestructurally equivalent result expression for a matching case will then have access to these variable assignments in its local scope.
You are strongly encouraged to try experimenting with pattern matching in the REPL to get a sense for how it behaves.
For data types, itA is a common idiom to have a variadic  method in the companionapply object to conveniently construct instances of the data type.
By calling this function  and placing it in the companion object, we canapply invoke it with syntax like  or ,List(1,2,3,4) List("hi","bye") with as many values as we want separated by commas (we sometimes call this the  or just  syntax).list literal literal Variadic functions are just providing a little syntax sugar for creating and passing a  of elements explicitly.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
When data is immutable, how do we write functions that, for example, add or remove elements from a list? The answer is simple.
When we add an element  to1 the front of an existing list, say , we return a new list, in this case xs.
Since lists are immutable, we don't need to actually copy ; weCons(1,xs) xs can just reuse it.
This property of immutable data is called  or just data sharing.
Data sharing of immutable data often lets us implement functions moresharing efficiently; we can always return immutable data structures without having to worry about subsequent code modifying our data.
There's no need to pessimistically make copies to avoid modification or corruption.9
Footnote 9mThis pessimistic copying can become a problem in large programs, when data may be passed through a chain of loosely components, each of which may be forced to make copies of this data.
Using immutable data structures means never having to copy that data just to share it between two components of a system, which promotes keeping these components loosely coupled.
We find that , FP can often achieve greaterin the large efficiency than approaches that rely on side effects, due to much greater sharing of data and computation.
In the same way, to "remove" an element from the front of a list val mylist , we simply return.
We say that functional datamylist structures are , meaning that existing references are never changed bypersistent operations on the data structure.
Let's try implementing a few different functions for "modifying" lists in different ways.
You can place this and other functions we write inside the List companion object.
What are different choices youList could make in your implementation if the  is ? We will return to thisList Nil question in the next chapter.
Again, notice these functions takeList time proportional only to the number of elements being dropped—we do not need to make a copy of the entire .List Footnote 10m  has two argument lists to improve type inference.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The main reasonf for grouping the arguments this way is to assist with type inference.
This is an unfortunate restriction of the ScaladropWhile compiler; other functional languages like Haskell and OCaml provide.
Footnote 11mSee the notes for this chapter for more information and links to further reading.
For instance, here is a function that adds all the elements of one list to the end of another:
Notice that this definition only copies values until the first list is exhausted, so.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
If we were to implement this same function for two arrays, we woulda2 be forced to copy all the elements in both arrays into the result.
Implement a function, , which returns a  consisting of all but the last element of a.
Because of the structure of a singly-linked list, any time we want to replace the of a , even if it is the last  in the list, we must copy all thetail Cons Cons.
Writing purely functional data structures that supportCons different operations efficiently is all about finding clever ways to exploit data sharing, which often means working with more tree-like data structures.
We are not going to cover these data structures here; for now, we are content to use the functional data structures others have written.
Let's look again at the implementations of sum and product.
We've simplified the implementation slightly, so as not to include the "short-circuiting" logicproduct.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Whenever you encounter duplication like this, assum * product we've discussed before, you can generalize it away by pulling subexpressions out into function arguments.
If a subexpression refers to any local variables (the + operation refers to the local variables  and  introduced by the pattern, similarlyx xs for ), turn the subexpression into a function that accepts these variablesproduct as arguments.
Putting this all together for this case, our function will take as arguments the value to return in the case of the empty list, and the function to add an element to the result in the case of a nonempty list:12
Footnote 12mIn the Scala standard library,  is a method on  and its arguments are curriedfoldRight List similarly for better type inference.
Again, placing f in its own argument group after l and z lets type inference determine the input types to f.
One way of describing what  does is that it replaces the constructors of the list, foldRight.
So the value of Nil Cons z f becomes , and foldRight(Cons(a, Nil), z)(f) f(a, z)
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Notice that  must traverse all the way to the end of the listfoldRight (pushing frames onto the call stack as we go) before it can begin collapsing it.
This is a deeper question that we'll return to a few chapters from now.
Convince yourself that this is the case, then write another general list-recursion function,  that is tail-recursive, using the techniques wefoldLeft discussed in the previous chapter.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Its runtime should be linear in the total length of all lists.
There are many more useful functions for working with lists.
After finishing this chapter, we recommend looking through the  to see what other functions there are.
If youscala API documentation find yourself writing an explicit recursive function for doing some sort of list manipulation, check the  API to see if something like the function you needList already exists.
After finishing this section, you're not going to emerge with an automatic sense of when to use each of these functions.
Just get in the habit of looking for possible ways to generalize any explicit recursive functions you write to process lists.
If you do this, you'll (re)discover these functions for yourself and build more of a sense for when you'd use each one.
Reminder: this should be a pure function that returns a new.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Footnote 15mIn the standard library,  and  are methods of .map flatMap List.
There are a number of other useful methods on lists.
You may want to try experimenting with these and other methods in the REPL after reading the API.
These are defined as methods on , rather than asdocumentation List[A] standalone functions as we've done in this chapter.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
One of the problems with  is that while we can often express operationsList and algorithms in terms of very general-purpose functions, the resulting implementation isn't always efficient—we may end up making multiple passes over the same input, or else have to write explicit recursive loops to allow early termination.
You may have some difficulty finding a concise purely functional implementation that is also efficient.
We will return to this implementation in a couple of chapters and hopefully improve on it.
Note: any two values, , and ,x y can be compared for equality in Scala using the expression .x == y.
An ADT is just a data type defined by one or more data constructors, each of which may contain zero or more arguments.
There is actually a deep connection, beyond the scope of this book, between the "addition" and "multiplication" of types to form an ADT and addition and multiplication of numbers.
When you encode a data type as an ADT, the data constructors and associated patterns form part of that type's API, and other code may be written in terms of explicit pattern.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
They work just like the ADTs we've been writing here, but have special syntax:
In this example,  is a pair whose type is ,("Bob", 42) (String,Int) which is syntax sugar for  ( )
Higher arity tuples work similarly—trycase class experimenting with them on the REPL if you're interested.
Algebraic data types can be used to define other data structures.
Pattern matching again provides a convenient way of operating over elements of our ADT.
Note: in Scala, you can use  or  toTree[Int] x.max(y) x max y compute the maximum of two integers  and.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In FP, we approach concerns about encapsulation a bit differently—we don't typically have delicate mutable state which could lead to bugs or violation of invariants if exposed publicly.
Exposing the data constructors of a type is often fine, and the decision to do so is approached much like any other decision about what the public API of a data type should be.17
Footnote 17mIt is also possible in Scala to expose patterns like  and Nil the actual data constructors of the type.Cons independent of.
We do typically use ADTs for cases where the set of cases is closed (known to be fixed)
For  and , changing the set of dataList Tree constructors would significantly change what these data types are.
Can you draw an analogy between this  functionfold and the left and right folds for ?List.
In this chapter we covered a number of important concepts.
We introduced algebraic data types and pattern matching and showed how to implement purely functional data structures, including the singly-linked list.
Also, through the exercises in this chapter, we hope you got more comfortable writing pure functions and generalizing them.
We will continue to develop this skill in the chapters ahead.
Footnote 18mAs you work through more of the exercises, you may want to read appendix Todo discussing different techniques for generalizing functions.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In chapter 1 we said that throwing an exception breaks referential transparency.
The use of  and  means we can no longerthrow catch reason about our program purely locally by substituting terms with their definitions—if we replace substitute  in  with x x + y throw new.
The technique is based on a simple idea: instead of throwing an exception, we return a value indicating an exceptional condition has occurred.
This idea might be familiar to anyone who has used return codes in C to handle exceptions, although in FP it works a bit differently, as we'll see.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Let's consider a more realistic situation where we might use an exception and look at different approaches we could use instead.
Here is an implementation of a function that computes the mean of a list, which is undefined if the list is empty:
Seq is the common interface of various linear sequence-like collections.
A function is typically partial because it makes some assumptions about its inputs that are not implied by the input types.
You may be used to1 throwing exceptions in this case, but we have a few other options.
Let's look at these for our  example:mean Footnote 1mA function may also be partial if it does not terminate for some inputs.
We aren't going to discuss this form of partiality here—a running program cannot recover from or detect this nontermination internally, so there's no question of how best to handle it.
The first possibility is to return some sort of bogus value of type.
WeDouble could simply return  in all cases, and have it result in xs.sum / xs.length.
In other situations we might return  instead of anull value of the needed type.
It allows errors to silently propagate—the caller can forget to check this condition and will not be alerted by the compiler, which might result in subsequent code not working properly.
Often the error won't be detected until much later in the code.
For some output types, we might not even have a sentinel value of that type even if we wanted to! Consider a function like  whichmax finds the maximum value in a sequence according to a custom comparison function: def.
If the input were empty, wemax[A](xs: Seq[A])(greater: (A,A) => Boolean): A cannot invent a value of type.
Nor can  be used here since  is only valid forA null null non-primitive types, and  is completely unconstrained by this signature.A It demands a special policy or calling convention of callers—proper use of the mean function now requires that callers do something other than simply call  and make usemean of the result.
Giving functions special policies like this makes it difficult to pass them to.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The second possibility is to force the caller to supply an argument which tells us what to do in case we don't know how to handle the input:
This makes  into a total function, but it has drawbacks—it requires thatmean immediate callers have direct knowledge of how to handle the undefined case and limits them to returning a.
What if  is called as part of a largerDouble mean computation and we would like to abort that computation if  is undefined? Ormean perhaps we would like to take some completely different branch in the larger computation in this case? Simply passing an  parameter doesn't give usonEmpty this freedom.
We need a way to defer the decision of how to handle undefined cases so that they can be dealt with at the most appropriate level.
The solution is to represent explicitly in the return type that we may not always have a defined value.
We can think of this as deferring to the caller for the error handling strategy.
Option has two cases: it can be defined, in which case it will be a , or itSome can be undefined, in which case it will be.
We can use this for our definitionNone of  like so:mean.
The return type now reflects the possibility that the result is not always defined.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We still always return a result of the declared type (now )Option[Double] from our function, so  is now a.
It takes each value of the inputmean total function type to exactly one value of the output type.
Partial functions abound in programming, and  (and related data types weOption will discuss shortly) is typically how this partiality is dealt with in FP.
Map lookup for a given key returns Option headOption and   return lastOption defined for lists and other iterables Option containing the first or last elements of a sequence if it is nonempty.
These aren't the only examples—we'll see  come up in many differentOption situations.
What makes  convenient is that we can factor out commonOption patterns of error handling via higher order functions, freeing us from writing the usual boilerplate that comes with exception-handling code.
Option can be thought of like a  that can contain at most one element,List and many of the  functions we saw earlier have analogous functions on List.
We are going to do somethingOption slightly different than last chapter.
Last chapter we put all the functions that operated on  in the  companion object.
The  type annotation in default: => B (and the similar annotation in ) indicates the argument willgetOrElse orElse.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Don't worry about this for now—we'll talk much more about it in the next chapter.
Also, the  parameterB>:A on these functions indicates that  must be a  of.
This isn't really important to ourOption[Animal] orElse purposes; it's mostly an artifact of the OO style of placing the functions that operate on a type within the body of the .trait.
But first, as an exercise, implement all of the above functions on.
As you implementOption each function, try to think about what it means and in what situations you'd use it.
It is fine to use pattern matching, though you should be able to implement all the functions besides  and  without resorting to pattern matching.map getOrElse For  and , the type signature should be sufficient to determine themap flatMap implementation.
Although we can explicitly pattern match on an , we will almostOption always use the above higher order functions.
We'll try to give some guidance for when to use each of them, but don't worry if it's not totally clear yet.
The purpose here is mostly to get some basic familiarity so you can recognize these patterns as you start to write more functional code.
The  function can be used to transform the result inside an , if itmap Option exists.
We can think of it as proceeding with a computation on the assumption that an error has not occurred—it is also a way of deferring the error handling to later code:
A dictionary or associative container with String as the key type and Employee as.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Footnote 3mVariance can actually be computed in one pass, but for pedagogical purposes we will compute it using two passes.
The first will compute the mean of the data set, and the second will compute the mean squared difference from this mean.
A common pattern is to transform an  viaOption calls to , , and/or , then use  to do errormap flatMap filter getOrElse handling at the end.
This is often useful when we need to chain together possibly failing computations, trying the second if the first hasn't succeeded.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
It may be easy to jump to the conclusion that once we start using , itOption infects our entire code base.
One can imagine how any callers of methods that take or return  will have to be modified to handle either  or.
ButOption Some None this simply doesn't happen, and the reason why is that we can  ordinarylift functions to become functions that operate on .Option.
For example, the  function lets us operate on values of type map Option[A] using a function of type , returning.
It might be more clear if we make this moreOption[A] => Option[B] explicit in the type signature:
As an example of when you might use , let's look at one more example of amap function that returns :Option.
This example uses the Java standard library's regex package to parse a string into a regular expression pattern.
If there is a syntax error in the pattern (it's not a4 valid regular expression), we catch the exception thrown by the library function and return.
Methods on the  class don't need to know anythingNone Pattern about.
We can simply  them using the  function:Option lift map Footnote 4mScala runs on the Java Virtual Machine and is completely compatible with all existing Java libraries.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Here, the  call will return an , whichpattern(pat) Option[Pattern] will be  if  is not valid.
Notice how we are using the  and None pat matcher.
Because they are inside the , they don'tmatches inside map map need to be aware of the outer containing.
If you don't feel you fully graspOption how this works yet, use the substitution model to execute this on paper step by step, both for the case where  is valid and where it is invalid.pat.
So far we are only lifting functions that take one argument.
But some functions take more than one argument and we would like to be able to lift them too.
The for-comprehension makes this easy, and we can combine as many options as we want:
Internally, Scala will translate the above to ordinary method calls to  and :map flatMap.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Write abothMatch generic function , that combines two  values using a binary function.map2 Option If either  value is , then the return value is too.
If theSome original list contains  even once, the result of the function should be ,None None otherwise the result should be  with a list of all the values.
Footnote 5mThis is a clear instance where it's not possible to define the function in the OO style.
This should not be a method on  (which shouldn't need to know anything about ), and it can't be a methodList Option on .Option.
Sometimes we will want to map over a list using a function that might fail, returning  if applying it to any element of the list returns None.
For example,None parsing a whole list of strings into a list of patterns.
In that case, we can simply sequence the results of the :map.
Unfortunately, this is a little inefficient, since it traverses the list twice.
Wanting to sequence the results of a  this way is a common enough occurrence tomap.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
It is straightforward to do using map and , but try for a more efficient implementation that only looks at thesequence list once.
The big idea in this chapter is that we can represent failures and exceptions with ordinary values, and we can write functions that abstract out common patterns of error handling and recovery.
One thing you may have noticed with  is that it doesn't tell us very much aboutOption what went wrong in the case of an exceptional condition.
For example, we might want a  that gives more information, or if anString exception was raised, we might want to know what that error actually was.
We can craft a data type that encodes whatever information we want about failures.
Sometimes just knowing whether a failure occurred is sufficient in which case we can use ; other times we want more information.
In this section,Option we'll walk through a simple extension to , the  data type, whichOption Either lets us track a  for the failure.
The essential difference is thatOption both cases carry a value.
The  data type represents, in a very general way,Either values that can be one of two things.
We can say that it is a  of twodisjoint union types.
When we use it to indicate success or failure, by convention the Left constructor is reserved for the failure case.6
Footnote 6m  is also often used more generally to encode one of two possibilities, in cases where itEither isn't worth defining a fresh data type.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
There are a few missing functions, though, notably , , and sequence traverse.
Let's look at the  example again, this time returning a  in case ofmean String failure:
Sometimes we might want to include more information about the error, for example a stack trace showing the location of the error in the source code.
In such cases we can simply return the exception in the  side of an :Left Either.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Of course, Left("invalid name") Left("invalid could be an arbitrary expression like  that happens toname") foo(x,y,z)
What would you need to change in order to report  errors? Would you change  or the signature of both map2
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Using algebraic data types such as  and , we can handle errors inOption Either a way that is modular, compositional, and simple to reason about.
In this chapter, we have developed a number of higher-order functions that manipulate errors in ways that we couldn't otherwise if we were just throwing exceptions.
With these new tools in hand, exceptions should be reserved only for truly unrecoverable conditions in our programs.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In chapter 3 we talked about purely functional data structures, using singly-linked lists as an example.
We covered a number of bulk operations on lists — , map.
We noted that each of thesefilter foldLeft foldRight zip operations makes its own pass over the input and constructs a fresh list for the output.
Imagine if you had a deck of cards and you were asked to remove the odd-numbered cards and then remove all the queens.
Ideally, you would make a single pass through the deck, looking for queens and odd-numbered cards at the same time.
This is more efficient than removing the odd cards and then looking for queens in the remainder.
And yet the latter is what Scala is doing in the following code:1
If we manually produce a trace of its evaluation, the steps would look something like this:
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
This view makes it clear how the calls to  and map 2 map.
Wouldn't it be nice if we could somehow fuse sequences of transformations like this into a single pass and avoid creating temporary data structures? We could rewrite the code into a while-loop by hand, but ideally we'd like to have this done automatically while retaining the same high-level compositional style.
We want to use higher-order functions like  and  instead of manually fusingmap filter passes into loops.
Footnote 2mWith program traces like these, it is often more illustrative to not fully trace the evaluation of every subexpression.
For instance, in this case, we've omitted the full expansion of List(1,2,3,4) map.
It turns out that we can accomplish this through the use of  (ornon-strictness more informally, "laziness")
In this chapter, we will explain what exactly this means, and we'll work through the implementation of a lazy list type that fuses sequences of transformations.
Although building a "better" list is the motivation for this chapter, we'll see that non-strictness is a fundamental technique for improving on the efficiency and modularity of functional programs in general.
Before we get to our example of lazy lists, we need to cover some basics.
What is strictness and non-strictness, and how are these concepts expressed in Scala?
A function  is  ifterminate bottom f strict the expression  evaluates to bottom for all  that evaluate tof(x) x bottom.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Strict functions are thestrict norm in most programming languages and most languages don't even provide a way to define non-strict functions.
Unless you tell it otherwise, any function definition in Scala will be strict (and all the functions we have defined so far have been strict)
Foras a concept example, the Boolean functions  and  are non-strict.
The function  takes two  arguments, but only evaluates the second&& Boolean argument if the first is :true.
And  only evaluates its second argument if the first is :|| false.
Another example of non-strictness is the  control construct in Scala:if.
The  language construct could also be thought of as a function acceptingif three parameters: a condition of type , an expression of some type  toBoolean A.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
To be more precise, we would say that the function is strict in its condition parameter, since it will always evaluate theif.
In Scala, we can write non-strict functions by accepting some of our arguments unevaluated, using the following syntax:
The arguments we would like to pass unevaluated have  immediately before=> their type.
In the body of the function, we do not need to do anything to evaluate an argument annotated with.
We also call this=> function with the usual function call syntax:
Footnote 3mThe unevaluated form of an expression is often called a.
An argument that is passed unevaluated to a function will be evaluated once for each place it is referenced in the body of the function.
That is, Scala will not (by default) cache the result of evaluating an argument:
Here,  is referenced twice in the body of , and we have made iti pair.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Adding the  keyword to a  declaration will cause Scala to delaylazy val evaluation of the right hand side until it is first referenced and will also cache the result so that subsequent references of  don't trigger repeated evaluation.
In thisj example, we were going to evaluate  on the next line anyway, so we could havei just written.
Despite receiving its argument unevaluated,  isval j = i pair2 still considered a strict function since it always ends up evaluating its argument.
In other situations, we can use  when we don't know if subsequent codelazy val will evaluate the expression and simply want to cache the result if it is ever demanded.
As a final bit of terminology, a non-strict function that evaluates its arguments each time it references them is said to evaluate those arguments ; if itby name evaluates them only once and then caches their value, it is said to evaluate ,by need or it's said to be.
We'll often refer to unevaluated parameters in Scala as lazy.
Note also that the terms  or  areby-name parameters laziness lazy evaluation sometimes used informally to refer to any sort of non-strict evaluation, not necessarily evaluation by need.
When you encounter the word "lazy" in this book, you can assume that we are using an informal definition.
Let's now return to the problem posed at the beginning of this chapter.
We are going to explore how laziness can be used to improve the efficiency and modularity of functional programs, using , or  as an example.
We'lllazy lists streams see how chains of transformations on streams are fused into a single pass, through the use of laziness.
Footnote 4mThere are some subtle possible variations on this definition of.
We'll touch briefly onStream some of these variations later in this chapter.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Thus the head and tailcons of the stream will not be evaluated until first requested.
See the associated code for this chapter if you're interested in exactly how this syntax is implemented.
Before continuing, let's write a few helper functions to make inspecting streams easier.
You can convert to the regular  type in the standard library.
You can place this and other functionsList that accept a  inside the  trait.Stream Stream.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
You can use  and  together to inspect the streams we'll betake toList creating.
Laziness lets us separate the description of an expression from the evaluation of that expression.
This gives us a powerful ability — we may choose to describe a "larger" expression than we need, then evaluate only a portion of it.
As an example, consider  — we can implement this for  much likefoldRight Stream we did for , but we can implement it lazily:List.
This looks very similar to the  we wrote for , but notice howfoldRight List our combining function, , is non-strict in its second parameter.
If  chooses not tof f evaluate its second parameter, this terminates the traversal early.
We can see this by using  to implement , which checks to see if any value infoldRight exists the  matches a given predicate.Stream.
Since  can terminate the traversal early, we can reuse it tofoldRight implement  rather than writing an explicit recursive function to handleexists early termination.
This is a simple example where separating the concerns of.
This kind of separation of concerns is a central theme in functional programming.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Your implementation should terminate theStream traversal as soon as it encounters a non-matching value.
This willfoldRight takeWhile construct a stream incrementally, and only if the values in the result are demanded by some other expression.
Because the implementations are incremental, chains of transformations will avoid fully instantiating the intermediate data structures.
It's a bit more challenging than the trace we looked at earlier in this chapter.
Apply map to first element Apply filter to first element Apply map to second element Apply filter to second element.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
For this reason,map people sometimes describe streams as "first-class loops" whose logic can be combined using higher-order functions like  and .map filter.
The incremental nature of stream transformations also has important consequences for memory usage.
Of course,13 map filter this is a simple example; in other situations we might be dealing with larger numbers of elements, and the stream elements themselves could be large objects that retain significant amounts of memory.
Being able to reclaim this memory as quickly as possible can cut down on the amount of memory required by your program as a whole.5
Because they are incremental, the functions we've written also work fine for.
Although  is infinite, the functions we've written so far only inspect theones portion of the stream needed to generate the demanded output.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Be careful though, since it's easy to write an expression that never terminates.
Let's see what other functions we can discover for generating streams.
It takes an initial state, and a function for producing both the next state and the next value in the generated stream.
Option is used to indicate when the  should be terminated, if at all.
Notice howunfold Stream closely it mirrors the structure of the  data type.Stream.
While a recursive function consumes datacorecursive and eventually terminates, a corecursive function produces data and .coterminates We say that such a function is , which just means that we can alwaysproductive evaluate more of the result in a finite amount of time (for , we just need tounfold run the function  one more time to generate the next element)
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
If you are curious to learn where they come from and understand some of the deeper connections, follow the references in the chapter notes.
The  function should continue the traversal aszipAll zipAll long as either stream has more elements — it uses  to indicate whetherOption each stream has been exhausted.
Now that we have some practice writing stream functions, let's return to the exercise we covered at the end of chapter 3 — a function, , tohasSubsequence check whether a list contains a given subsequence.
With strict lists and list-processing functions, we were forced to write a rather tricky monolithic loop to implement this function without doing extra work.
It should check if one  is a prefix of another.
For a given , tails unfold Stream returns the  of suffixes of the input sequence, starting with thetails Stream.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
This implementation performs the same number of steps as a more monolithic implementation using nested loops with logic for breaking out of each loop early.
By using laziness, we can compose this function from simpler components and still retain the efficiency of the more specialized (and verbose) implementation.
Can it beStream n n implemented using ? How, or why not? Could it be implemented usingunfold another function we have written?
In this chapter we have introduced non-strictness as a fundamental technique for implementing efficient and modular functional programs.
As we have seen, while non-strictness can be thought of as a technique for recovering some efficiency when writing functional code, it's also a much bigger idea — non-strictness can improve modularity by separating the description of an expression from the "how and when" of its evaluation.
Keeping these concerns separate lets us reuse a description in multiple contexts, evaluating different portions of our expression to obtain different results.
We were not able to do that when description and evaluation were intertwined as they are in strict code.
We saw a number of examples of this principle in action over the course of the chapter and we will see many more in the remainder of the book.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In this chapter, we will be introducing how to write programs that manipulate state in a purely functional way, using the very simple domain of random number.
Although this is an unambitious domain on its own, we'llgeneration be building on what we develop here in chapters to come, and its simplicity makes it a good place to explore some fundamental issues that you will likely encounter as you start writing your own functional APIs.
A word before getting started: don't worry if not everything in this chapter sinks in at first.
The goal is more to give you the basic pattern for how to make stateful APIs purely functional.
We will say a lot more about dealing with state and effects in later parts of the book.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
And here's an excerpt of the API, transcribed to Scala:
Because these state updates are performed as a side effect, these methods are not referentially transparent.
What can we do about this? The key to recovering referential transparency is to make these state updates.
That is, do not update the state as a side effect,explicit but simply return the new state along with the value we are generating.
We will later define other functions in terms of nextInt.
In effect, we separate the  of the next state from the concern of computing.
There is no global mutable memorypropagating being used—we simply return the next state back to the caller.
This leaves the caller of  in complete control of what to do with the new state.
NoticenextInt that we are still  the state, in the sense that users of this API do notencapsulating need to know anything about the implementation of the random number generator itself.
Here is a simple implementation using the same algorithm as.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The details of this implementation aren'tlinear congruential generator really important, but notice that  returns both the generated value and anextInt new  to use for generating the next value.RNG.
This problem of making seemingly stateful APIs pure, and its solution, of having the API  the next state rather than actually mutate anything, is notcompute unique to random number generation.
It comes up quite frequently, and we can always deal with it in this same way.1
Footnote 1mThere is an efficiency loss that comes with computing next states using pure functions, because it means we cannot actually mutate the data in place.
Here, it is not really a problem since the state is just a single.
This loss of efficiency can be mitigated by using efficient purely functional dataLong structures.
It's also possible in some cases to mutate the data in place without breaking referential transparency.
We can mechanicallybar baz s translate this to the purely functional API:
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
For the pure RNG interface above, if you reuse a previous , it will always generate the sameRNG value it generated before.
Notice we return the final state, after generating the two random numbers.
This lets the caller generate more random values using the new state.
You can see the general pattern, and perhaps you can also see how it might get somewhat tedious to use this API directly.
Let's write a few functions to generate random values and see if we notice any patterns we can factor out.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Does this imply that FP is the programming equivalent of trying to write an entire novel without using the letter 'e'? Of course not.
Awkwardness like this is almost always a sign of some missing abstraction waiting to be discovered! When you encounter these situations, we encourage you to plow ahead and look for common patterns you can factor out.
Most likely, this is a problem others have encountered, and you may even rediscover the "standard" solution yourself.
Even if you get stuck, struggling to puzzle out a clean solution yourself will help you to better understand what solutions others have discovered to deal with the same or similar problems.
With practice, experience, and more familiarity with the idioms of FP, expressing a program functionally will become effortless and natural.2
Footnote 2mOf course, good design is still hard, but programming using pure functions becomes easy with experience.
Note: you can use  to obtain the maximum positive1 Int.MaxValue integer value and you can use  to convert an , , to a .x.toDouble Int x Double.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Looking back at our implementations, we notice a common pattern: each of our functions has a type of the form  for some type.
FunctionsRNG => (A, RNG) A of this type describe  that transform  states, and these state actionsstate actions RNG can be built up and combined using general-purpose functions.
To make them convenient to talk about, let's make a type alias for the  stateRNG action data type:
We can now turn methods such as 's  into values of this type:RNG nextInt.
We want to start writing combinators that let us avoid explicitly passing along the  state.
This will become a kind of domain-specific language that does all ofRNG this passing for us.
For example, a simple -transition is the  action,RNG unit which passes the  state through without using it, always returning a constantRNG value rather than a random value.
There is also , for transforming the output of a state action withoutmap modifying the state itself.
Remember,  is just a type alias for a functionRand[A] type , so this is just a kind of function composition.RNG => (A, RNG)
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
What we need is a new combinator intDouble doubleInt , that can combine two RNG actions into one using a binary rather than unarymap2
Use it to reimplement the List ints function you wrote before.
For the latter, you can use the standard library function.
We're starting to see a pattern: We're progressing towards implementations that don't explicitly mention or pass along the  value.
The  and RNG map map2 combinators allowed us to implement, in a rather succinct and elegant way, functions that were otherwise tedious and error-prone to write.
But there are some functions that we can't very well write in terms of  and .map map2
Let's go back to  and see if it can be implemented in terms of positiveInt.
It's possible to get most of the way there, but what do we do in the case that map.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
So we clearly need a more powerful combinator than .map EXERCISE 9: Implement , then use it to reimplement flatMap.
They aresequence general-purpose functions for working with state actions, and don't actually care about the type of the state.
Notice that, for instance,  does not care that it ismap dealing with  state actions and we can give it a more general signature:RNG.
Changing this signature doesn't require modifying the implementation of !map The more general signature was there all along, we just didn't see it.
We should then come up with a more general type than , for handling anyRand type of state:
Here,  is short for "state action" (or even "state transition")
We mightState even want to write it as its own class, wrapping the underlying function like this:
What is important is that we have a single, general-purpose type and using this type we can write general-purpose functions for capturing common patterns of handling and propagating state.
In fact, we could just make  a type alias for :Rand State.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The functions we've written here capture only a few of the most common patterns.
As you write more functional code, you'll likely encounter other patterns and discover other functions to capture them.
In the sections above, we were writing functions that followed a definite pattern.
We would run a state action, assign its result to a , then run another stateval action that used that , assign its result to another , and so on.
In the imperative programming paradigm, a program is a sequence of statements where each statement may modify the program state.
That's exactly what we have been doing, except that our "statements" are really state actions, which are really functions.
As functions, they read the current program state simply by receiving it in their argument, and they write to the program state simply by returning a value.
Imperative programming is about programming with statements that modify some program state, and as we've seen it's entirely reasonable to maintain state without side-effects.
Functional programming has excellent support for writing imperative programs, with the added benefit that such programs can be reasoned about equationally because they are referentially transparent.
We implemented some combinators like , , and ultimately ,map map2 flatMap to handle the propagation of the state from one statement to the next.
But in doing so, we seem to have lost a bit of the imperative mood.
Consider as an example the following (which assumes that we have made a type alias for ):Rand[A] State[RNG, A]
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
But since we have  and map flatMap defined, we can use -comprehension to recover the imperative style:for.
This code is much easier to read (and write), and it looks like what it is—an imperative program that maintains some state.
We get thethe same code next  and assign it to , get the next  after that and assign it to , thenInt x Int y generate a list of length , and finally return the list with all of its elementsx wrapped around the modulus .y.
To facilitate this kind of imperative programming with -comprehensionsfor (or s), we really only need two primitive  combinators—one forflatMap State reading the state and one for writing the state.
If we imagine that we have a combinator  for getting the current state, and a combinator  for setting aget set new state, we could implement a combinator that can modify the state in arbitrary ways:
This method returns a  action that modifies the current state by theState function.
It yields  to indicate that it doesn't have a return value other thanf Unit the state.
The machine has two types of input: You can insert a coin, or you can turn the knob to dispense candy.
It can be in one of two states: locked or unlocked.
It also tracks how many candies are left and how many coins it contains.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Inserting a coin into a locked machine will cause it to unlock if there is any candy left.
Turning the knob on an unlocked machine will cause it to dispense candy and become locked.
Turning the knob on a locked machine or inserting a coin into an unlocked machine does nothing.
A machine that is out of candy ignores all inputs.
In this chapter, we touched on the subject of how to deal with state and state propagation.
We used random number generation as the motivating example, but the overall pattern comes up in many different domains, and this chapter illustrated the basic idea of how to handle state in a purely functional way.
The idea is very simple: we use a pure function that accepts a state as its argument, and it returns the new state alongside its result.
Next time you encounter an imperative API that relies on side effects, see if you can provide a purely functional version of it, and use some of the functions we wrote here to make working with it more convenient.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In this chapter, we are going to build a library for creating and composing parallel and asynchronous computations.
We are going to work iteratively, refining our design and implementation as we gain a better understanding of the domain and the design space.
Before we begin, let's think back to the libraries we wrote in Part 1, for example the functions we wrote for  and.
In each case we defined a dataOption Stream type and wrote a number of useful functions for creating and manipulating values of that type.
But there was something interesting about the functions we wrote.
For instance consider —if you look back, you'll notice we wrote only a few Stream.
WeStream uncons then wrote a large number of  or  withoutderived operations combinators introducing additional primitives, just by combining existing functions.
In Part 1, very little design effort went into creating these nicely compositional libraries.
We created our data types and found, perhaps surprisingly, that it was possible to define a large number of useful operations over these data types, just by combining existing functions.
When you create a library for a new domain, the design process won't always be this easy.
You will need to choose data types and functions that , and this is what makesfacilitate this compositional structure functional design both challenging and interesting.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Generally, it's used to describe a library consisting of one or more data types, along with a collection of (often higher-order) functions for creating, manipulating, and combining values of these types.
In particular, the name is usually applied to libraries with a very compositional structure, where functions are combined in different ways to produce richer and more complex functionality.
Though because this style of organization is so common in FP, we sometimes don't bother to distinguish between an ordinary functional library and a "combinator library"
Our goal in this section is to discover a data type and a set of primitive functions for our domain, and derive some useful combinators.
We hope to show at least a stylized view of this messiness that nonetheless gives some insight into how functional design proceeds in the real world.
Don't worry if you don't follow absolutely every bit of discussion throughout this process.
This chapter is a bit like peering over the shoulder of someone as they think through possible designs.
And because no two people approach this process the same way, the particular path we walk here might not strike you as the most natural one—perhaps it considers issues in what seems like an odd order, skips too fast or goes too slow.
Keep in mind that when you design your own functional libraries, you get to do it at your own pace, take whatever path you want, and whenever questions come up about design choices, you get to think through the consequences in whatever way makes sense for you, which could include running little experiments, creating prototypes, and so on.
With that as disclaimer, why don't we get started? When you begin designing a functional library, you usually have some ideas about what you generally want to be able , and the difficulty in the design process is in refining these ideas andto do finding a data type that enables the functionality you want.
In our case, we'd like to be able to "create parallel computations", but what does that mean exactly? Let's.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
It returns the first element in the collection, or None if the collection is empty.
Footnote 1mAssuming we have  elements and  CPU cores, we could in principle compute  in n n sum O(log n) time.
This is meant to be a simple example (see sidebar) though; in this instance the overhead of parallelization is unlikely to pay for itself.
Complicated examples include all sorts of incidental structure and extraneous detail that can confuse the initial design process.
We are trying to understand the essence of the problem domain, and a good way to do this is to work with very small examples, factor out common concerns across these examples, and gradually add complexity.
In functional design, our goal is to achieve expressiveness not with mountains of special cases, but by building a simple and.
As we think about how what sort of data types and functions could enable parallelizing this computation, we can shift our perspective.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Look at the line , which invokes  on the two halvessum(l) + sum(r) sum recursively.
Just from looking at this single line, we can see that  datawhatever type we choose to represent our parallel computations needs to be able to contain a.
For now, let's just  a container type for our result, invent.
Can we really just do this? Yes, of course! For now, we don't need to worry about what other functions we require, what the internal representation of Par might be, or how these functions are implemented.
We are simply reading off the needed data types and functions by inspecting our simple example.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Already, we can see a problem with both of these types—none of the methods return a meaningful value.
Therefore, if we want to get any information out of a.
This is bad for composability—we cannot manipulate Runnable objects generically, we need to always know something about their internal behavior to get any useful information out of them.
We'd prefer to create as many 'logical' parallel computations as is natural for our problem, and later deal with mapping these logical computations onto actual OS threads.
We now have a choice about the meaning of  and —  couldunit get unit begin evaluating its argument immediately in a separate (logical) thread,  or it2 could simply hold onto its argument until  is called and begin evaluation then.get But notice that in this example, if we want to obtain any degree of parallelism, we require that  begin evaluating its argument immediately.
Footnote 2mWe'll use the term "logical thread" somewhat informally throughout this chapter, to mean a chunk of computation that runs concurrent to the main execution thread of our program.
There need not be a one-to-one correspondence between logical threads and OS threads.
We may have a large number of logical threads mapped onto a smaller number of OS threads via thread pooling, for instance.
Footnote 3mFunction arguments in Scala are strictly evaluated from left to right, so if  delays executionunit until  is called, we will both spawn the parallel computation and wait for it to finish before spawning theget second parallel computation.
But if  begins evaluating its argument immediately, then calling unit get arguably breaks referential transparency.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Given what we have decided so far,  will start evaluating its argumentunit right away.
And the very next thing to happen is that  will wait for thatget evaluation to complete.
So the two sides of the  sign will not run in parallel if we+ simply inline the  and  variables.
Here we can see that  has a verysumL sumR unit definite side-effect, but only.
That is,  simply returns a with regard to get unit.
But as soon asPar[Int] we pass that  to , we explicitly wait for it, exposing the side-effect.
So itPar get seems that we want to avoid calling , or at least delay calling it until the veryget end.
We want to be able to combine asynchronous computations without waiting for them to finish.
We next explored this example a bit to uncover a design choice.
Then, via some experimentation, we discovered an interesting consequence of one option and in the process learned something fundamental about the nature of our problem domain! The overall design process is a series of these little adventures.
You don't need any special license to do this sort of exploration, and you don't need to be an expert in functional programming either.
Let's see if we can avoid the above pitfall of combining  and.
If weunit get don't call , that implies that our  function must return a.
Whatget sum Par[Int] consequences does this change reveal? Again, let's just invent functions with the required signatures:
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In this example,unit accepting the argument lazily doesn't seem to provide any benefit, but perhaps this isn't always the case.
Let's try coming back to this question in a minute.
Take a minute to work through and understand the following (somewhat stylized) program trace:
In this trace, to evaluate , we substitute  into the definition of , assum(x) x sum we've done in previous chapters.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Nothing actually occurs until we  this description, perhaps using aevaluate.
The problem is that if we construct our descriptions strictly,get they are going to be rather heavyweight objects.
Looking back at our trace, our description is going to have to contain the full tree of operations to be performed:
Whatever data structure we use to store this description, it will likely occupy more space than the original list itself! It would be nice if our descriptions were a bit more lightweight.
It seems we should make  lazy and have it begin immediate execution ofmap2 both sides in parallel (this also addresses the problem of giving each side equal "weight"), but something still doesn't feel right about this.
Is it  the case thatalways we want to evaluate the two arguments to  in parallel? Probably not.map2 Consider this simple hypothetical example:
In this case, we happen to know that the two computations we're combining will execute so quickly that there isn't much point in spawning off a separate logical thread to evaluate them.
But our API doesn't give us any way of providing this sort of information.
That is, our current API is very  about whenimplicit computations get forked off the main thread—the programmer does not get to.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
With , we can now make  strict, leaving it up to the programmer tofork map2 wrap arguments if they wish.
A function like  solves the problem offork instantiating our parallel computations too strictly (as an exercise, try revisiting the hypothetical trace from earlier), but more fundamentally it makes the parallelism more explicit and under programmer control.
The first is that we need some way to indicate that the results of the two parallel tasks should be combined.
Separate from this, we have the choice of whether a particular task should be performed asynchronously.
By keeping these concerns separate, we avoid having any sort of global policy for parallelism attached to  and other combinators we write, which would meanmap2 making tough (and ultimately arbitrary) choices about what global policy is best.
Such a policy may in practice be inappropriate in many cases.
Let's now return to the question of whether  should be strict or lazy.
With unit , we can now make  strict without any loss of expressiveness.
The function  is a simple example of a  combinator, as opposedasync derived to a  combinator like.
We were able to define  just in termsprimitive unit async of other operations.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Footnote 4mThis sort of indifference to representation is a hint that the operations are actually more general, and can be abstracted to work for types other than just.
We will explore this topic in detail in part 3.Par.
We still have the question of whether  should begin evaluating itsfork argument immediately, or wait until the computation is  later usingforced something like.
When you are unsure about a meaning to assign to someget function in your API, you can always continue with the design process—at some point later the tradeoffs of different choices of meaning may become clear.
Here, we make use of a helpful trick—we are going to think about what sort of.
This means we lose the ability to control the parallelism strategy used for different parts of our program.
And while there's nothing inherently wrong with having a global resource for executing parallel tasks, we can imagine how it would be useful to have more fine-grained control over what implementations are used where (we might like for each subsystem of a large application to get its own thread pool with different parameters, say)
Notice that coming to these conclusions didn't require knowing exactly how would be implemented, or even what the representation of  will be.
In contrast, if  simply holds onto the computation until later, this requiresfork no access to the mechanism for implementing parallelism.
With this model,  itself does not know how tofork Par actually  the parallelism.
This is a big shift from before, where we were considering  to bePar a "container" of a value that we could "get"
Now it's more of a first-class program that we can.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Because  now just a pure data structure, we will assume that  has somePar run means of implementing the parallelism, whether it spawns new threads, delegates tasks to a thread pool, or uses some other mechanism.
Just by exploring this simple example and thinking through the consequences of different choices, we've sketched out the following API:
The computation will not be spawned until forced by .run run extracts a value from a  by actually performing the computation.Par.
Let's see if we can come up with a representation.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We can block to obtain a value from a  with its  method,Future Future get and it has some extra features for cancellation, only blocking for a certain amount of time, and so on.
Let's try assuming that our  function has an  and seerun ExecutorService if that suggests anything about the representation for :Par.
The simplest possible model for  might just be Par[A] ExecutorService.
Is it really that simple? Let's assume it is for now, and revise our model if we decide it doesn't allow some functionality we'd like.
The way we've worked so far is actually a bit artificial.
In practice, there aren't such clear boundaries between designing your API and choosing a representation, and one does not necessarily precede the other.
Ideas for a representation can inform the API you develop, the API you develop can inform the choice of representation, and it's natural to shift fluidly between these two perspectives, run experiments as questions arise, build prototypes, and so on.
We are going to devote the rest of this section to exploring our API.
Though we got a lot of mileage out of considering a simple example, before we add any new primitive operations let's try to learn more about what is expressible using those we already have.
With our primitives and choices of meaning for them, we have carved out a little universe for ourselves.
We now get to discover what ideas are expressible in this universe.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Now that we have a representation for , we should be able toPar fill these in.
Optional (hard): try to ensure your implementations respect the contract of the  method on  that accepts a timeout.get Future.
You can place these functions and other functions we write inside an object called , like so:Par.
But since  is just a typeStream Option Par alias we can't do this directly.
There is, however, a trick to add infix syntax to  type using.
We won't discuss thatany implicit conversions here since it isn't all that relevant to what we're trying to cover, but if you're interested, check out the code associated with this chapter and also the appendix.
Here's a simple example: using , write a function to convert any function async A => B to one that evaluates its result asynchronously:
What else can we express with our existing combinators? Let's look at a more concrete example.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We could of course  the , sort the resulting list, and re-package it in a run Par with.
And we can pass whatever we want to the other side of , so let'smap2 just pass a no-op:
That is, we can B Par[A] Par[B] any function over a :map Par.
We just combined the operations to make the types line up.
And yet, if you look at the implementations of  and , it shouldmap2 unit be clear this implementation of   something sensible.map means.
This sort of thing can be a hint that  can be further decomposedmap map2 into primitive operations.
And when we consider it,  is actually doing twomap2 things—it is creating a parallel computation that waits for the result of two other.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We couldand then split this into two functions,  and :product map.
This is sort of an interesting little discovery that we can factor things this way.
Is it an improvement? On the one hand,  is "doing one thing only", and product.
But if you look at your implementation of map , you can almost see  hiding inside, except that you are alwaysproduct map2
This is one example where there is a choice ofmap which things we consider primitive and which things are derived.
What else can we implement using our API? Could we  over a list inmap parallel? Unlike , which combines two parallel computations,  (let'smap2 parMap call it) needs to combine  parallel computations.
In some cases we can even implement the operations more efficiently, by assuming something about the underlying representation of the data types we are working with.
But we're interested in exploring what operations are expressible using our existing API, and understanding the relationships between the various operations.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
It's frequently the case that primitive combinators encapsulate some rather tricky logic, and reusing them means we don't have to duplicate this logic.
Let's see how far we can get implementing  in terms of existingparMap combinators:
So we can fork off our  parallelN computations pretty easily, but we need some way of collecting up their results.
Are we stuck? Well, just from inspecting the types, we can see that we need some way of converting our  to the  required by theList[Par[B]] Par[List[B]] return type of .parMap.
Once we have , we can complete our implementation of :sequence parMap.
Notice that we've wrapped our implementation in a call to.
With thisfork implementation,  will return immediately, even for a huge input list.
WhenparMap we later call , it will fork a single asynchronous computation which itselfrun spawns  parallel computations then waits for these computations to finish,N collecting their results up into a list.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Can you think of any other useful functions to write? Experiment with writing a few parallel computations of your own to see which ones can be expressed without additional primitives.
Is there a more general version of the parallel summation function we wrote at the beginning of this chapter? Try using it to find the maximum value of an  inIndexedSeq parallel.
Write a function that takes a list of paragraphs (a ), and returns the totalList[String] number of words across all paragraphs, in parallel.
As the previous section demonstrates, we often get quite far by treating this all as a game of (what seems like) meaningless symbol manipulation! We write down the type signature for an operation we want, then "follow the types" to an implementation.
This isn't cheating; it's a natural butunit different style of reasoning, analogous to the reasoning one does when simplifying an algebraic equation like.
We are treating the API as an ,  or an abstractx algebra 6 set of operations along with a set of  or properties we assume true, and simplylaws doing formal symbol manipulation following the "rules of the game" specified by this algebra.
Footnote 6mWe do mean algebra in the mathematical sense of one or more sets, together with a collection of functions operating on objects of these sets, and a set of.
Axioms are statements assumed true, fromaxioms which we can derive other  that must also be true.
In our case, the sets are values with particulartheorems types, like , , the functions are operations like , , and .Par[A] List[Par[A]] map2 unit sequence.
Up until now, we have been reasoning somewhat informally about our API.
There's nothing wrong with this, but it can be helpful to take a step back and formalize what laws you expect to hold (or would like to hold) for your API.7
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Without realizing it, you have probably mentally built up a model of what properties or laws you expect.
Actually writing these down and making them precise can highlight design choices that wouldn't be otherwise apparent when reasoning informally.
Footnote 7mWe'll have much more to say about this throughout the rest of this book.
In the next chapter, we'll be designing a declarative testing library that lets us define properties we expect functions to satisfy, and automatically generates test cases to check these properties.
And in Part 3 we'll introduce abstract interfaces specified  by sets of laws.only.
Laws often start out this way, as concreteunit(2) examples of  we expect to hold.
In what sense are they equivalent? Thisidentities8 is somewhat of an interesting question.
For now, let's say two  objects arePar equivalent if   argument, the  theyfor any valid ExecutorService Future return results in the same value.
Footnote 8mHere we mean 'identity' in the mathematical sense of a statement that two expressions are identical or equivalent.
We can check that this holds for a particular  with aExecutorService function like:
Just as we can generalize functions, we can generalize laws.
Here we are saying this should hold for  choice of  and.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Our implementation of  cannot, say,unit inspect the value it receives and decide to return a parallel computation with a result of  when the input is —it can only pass along whatever it receives.42 1 Similarly for our —when we submit  objects toExecutorService Callable it for execution, it cannot make any assumptions or change behavior based on the values it receives.
Much like we strive to define functions in terms of simpler functions, each of which  just one thing, we can define laws in terms of simpler laws that each do say just one thing.
We said we wanted this law to hold for  choice of  and.
Something interesting happens if weany x f substitute the identity function for.
Footnote 10mThe identity function has the signature .def id[A](a: A): A = a.
Footnote 11mThis is the same sort of substitution and simplification one might do when solving an algebraic equation.
Substitute identity function for f Simplify Substitute y for unit(x) on both sides.
Fascinating! Our new, simpler law talks only about —apparently themap mention of  was an extraneous detail.
To get some insight into what this newunit law is saying, let's think about what   do.
It cannot, say, throw anmap cannot exception and crash the computation before applying the function to the result (can you see why this violates the law?)
Even more interesting, given , we can perform themap(y)(id) == y substitutions in the other direction to get back our original, more complex law.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Logically, we have the freedom to do so because  cannot possiblymap behave differently for different function types it receives.
Since we get this second law or theorem "for free", simply becauseunit(f(x))
Footnote 13mThe idea of free theorems was introduced by Philip Wadler in a classic paper called Theorems for free!
If it were reified as a data type, we could pattern match and discover opportunities to apply this rule.
You may want to try experimenting with this idea on your own.
As interesting as all this is, these laws don't do much to constrain our implementation.
This seems like it should be obviously true of our implementation, and it is clearly a desirable property, consistent with our expectation of how  shouldfork work.
If this law didn't always hold, we'd have to somehow know when it was safe to call without changing meaning, without any help from the type system.
Surprisingly, this simple property places very strong constraints on our implementation of.
After you've written down a law like this, take off yourfork.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Think through any possible corner cases, try to come up with counterexamples, and even construct an informal proof that the law holds—at least enough to convince a skeptical fellow programmer.
We are expecting that  for fork(x) == x all choices of , and any choice of.
We have a pretty goodx ExecutorService sense of what  could be—it's some expression making use of , , and x fork unit.
What about map2 ? What are some possible implementations of it? There's aExecutorService.
Why is it important in FP? In functional programming, it is easy, and expected, that we will factor out common functionality into generic, reusable, components that can be.
Side effects hurt compositionality, but more generally,composed any hidden or out-of-band assumptions or behavior that prevent us from treating our components (be they functions or anything else) as black.
Giving our APIs an algebra, with laws that are meaningful and aid reasoning, make the API more usable for clients, but also mean we can treat the objects of our API as black boxes.
As we'll see in Part 3, this is crucial for our ability to factor out common patterns across the different libraries we've written.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Try running the following example using your current implementation: Footnote 15mIn the next chapter we'll be writing a combinator library for testing that can help discover problems like these automatically.
Footnote 16mThere's actually another minor problem with this implementation—we are just calling  onget the inner  returned from.
This means we are not properly respecting any timeouts that have beenFuture fa placed on the outer .Future.
Notice that we are submitting the Callable first, and , we are submitting another  to the within that callable Callable.
The outer  gets submitted andCallable picked up by the sole thread.
Within that thread, before it will complete, we submit and block waiting for the result of another.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Even this is a good exercise—it forces you to document invariants or assumptions that were previously implicit.
We are going to try to fix our implementation, since being able to run our parallel computations on fixed size thread pools seems like a useful capability.
The problem with the above implementation of  is that we are invoking  fork submit.
Thisinside blocking leads to deadlock when there aren't any remaining threads to run the task we are submitting.
So it seems we have a simple rule we can follow to avoid deadlock:
A  should never submit and then block on the result of a Callable .Callable.
You may want to take a minute to prove to yourself that our parallel tasks cannot deadlock, even with a fixed-size thread pool, so long as this rule is followed.
The only problem is that we aren't actually forking a separate logical thread to evaluate.
This is still a useful combinator, though, since it lets usfork delay instantiation of a parallel computation until it is actually needed.
There's absolutely nothing wrong with doing this, so long as these local violations of referential transparency aren't.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The details of this are quite finicky to get right.
The nice thing is that we are confining this detail to one small block of code, rather than forcing users to have to think about these issues throughout their use of the API.
Taking a step back from these details, the purpose here is not necessarily to figure out the best, nonblocking implementation of , but more to show thatfork laws are important.
They give us another angle to consider when thinking about the design of a library.
If we hadn't tried writing some of these laws out, we may not have discovered this behavior of  until much later.fork.
In general, there are multiple approaches you can consider when choosing laws for your API.
You can think about your conceptual model, and reason from there to postulate laws that should hold.
You can also  laws you think are conjure up useful.
Footnote 17mThis last way of generating laws is probably the weakest, since it can be a little too easy to just have the laws reflect the implementation, even if the implementation is buggy or requires all sorts of unusual side conditions that make composition difficult.
After you've written down your API and have at least a prototype implementation, try using it for progressively more complex or realistic scenarios.
Often you'll find that these scenarios require only some combination of existing primitive or derived combinators, and this is a chance to factor out common usage patterns into other combinators; occasionally you'll find situations where your existing primitives are insufficient.
We say in this case that the API is not expressive enough.
Can this be implemented in terms of existing combinators or is a new primitive required?
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Notice what happens when we try to define this using only.
If we try map , we obtain the type map(a)(a => if (a) ifTrue else ifFalse)
If we had an  we could force the outer Par[Par[A]] ExecutorStrategy to obtain a , but we'd like to keep our  values agnostic to the Par Par[A] Par.
This is a case where our existing primitives are insufficient.
It's a good idea to try to explore some related examples around the particular one that cannot be expressed, to see if a common pattern emerges.
If it's useful to be able to choose between  parallel computations based ontwo the results of a first, it should be useful to choose between  computations:N.
Let's say that  runs , then uses that to select a parallel computationchoiceN a from.
Here, instead of a list of computations, we have a Map of them:18
Footnote 18m  ( ) is a purely functional data structure.Map API link.
If you want, stop reading here and see if you can come up with a combinator.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
If you look at your implementation of , you can see you aren't really using much of the API of.
Whenever you generalize functions like this, take a look at the result when you're finished.
Although the function you've written may have been motivated by some very specific use case, the signature and implementation may have a more general meaning.
In this case,  is perhaps no longer the most appropriatechooser name for this operation, which is actually quite general—it is a parallel computation that, when run, will run an initial computation whose result is used to determine a second computation.
Nothing says that this second computation needs to even exist before the first computation's result is available.
This function,generated which comes up quite often in combinator libraries, is usually called  or bind.
Is  really the most primitive possible function? Let's play around withflatMap it a bit more.
Recall when we first tried to implement , we ended up with achoice.
From there we took a step back, tried some related examples, andPar[Par[A]] eventually discovered.
But suppose instead we simply  anotherflatMap conjured combinator, let's call it , for converting  to :join Par[Par[A]] Par[A]
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We'll call it  since conceptually, it is a parallel computation that whenjoin run, will run the inner computation, wait for it to finish (much like Thread.join ), then return its result.
We have an example that demands a function with the given signature, and so we just bring it into existence.
Optional: can it be implemented in a wayjoin that avoids deadlock, even when run on bounded thread pools as in ? Canfork you see how to implement  using ? And can you implement flatMap join join using ?flatMap.
We are going to stop here, but you are encouraged to try exploring this algebra further.
Try more complicated examples, discover new combinators, and see what you find! Here are some questions to consider:
Can you implement a function with the same signature as , but using  and ?map2 bind unit How is its meaning different than that of ?map2 Can you think of laws relating  to the other primitives of the algebra?join Are there parallel computations that cannot be expressed using this algebra? Can you think of any computations that cannot even be expressed by adding new primitives to the algebra? In this chapter, we've chosen a "pull" model for our parallel computations.
That is, when we run a computation, we get back a , and we can block to obtain the result of thisFuture.
Are there alternative models for  that don't require us to ever block on a Future Par ?Future.
In this chapter, we worked through the design of a library for defining parallel and asynchronous computations.
Although this domain is interesting, the goal of this chapter was to give you a window into the process of functional design, to give you a sense of the sorts of issues you're likely to encounter and to give you ideas for how you can handle them.
If you didn't follow absolutely every part of this, or if certain conclusions felt like logical leaps, don't worry.
No two people take the same path when designing a library, and as you get more practice with functional design, you'll start to develop your own tricks and techniques for exploring a problem and possible designs.
In the next chapter, we are going to look at a completely different domain, and take yet another meandering journey toward discovering an API for that domain.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In the last chapter, we worked through the design of a functional library for expressing parallel computations.
There, we introduced the idea of an API forming an —that is, a collection of data types, functions over these data types, andalgebra importantly,  or  that express relationships between these functions.laws properties We also hinted at the idea that it might be possible to somehow check these laws automatically.
This chapter will work up to the design and implementation of a simple but powerful  library.
What does this mean? The general idea ofproperty-based testing such a library is to decouple the specification of program behavior from the creation of test cases.
The programmer focuses on specifying the behavior and giving high-level constraints on the test cases; the framework then handles generating (often ) test cases satisfying the constraints and checking thatrandom programs behave as specified for each case.
As an example, in , a property-based testing library for Scala, aScalaCheck property looks something like:
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Check that reversing a list twice gives back the original list Check that the first element becomes the last element after reversal A property which is obviously false.
Here,  is not a , but a , which isintList List[Int] Gen[List[Int]] something that knows how to generate test data of type.
We can combine and compose generators in different ways, reuse them, and so on.
The function  creates a  by combining a  with someforAll property Gen[A] predicate  to check for each value generated.
Like ,A => Boolean Gen properties can also have a rich API.
Here in this simple example we have used && to combine two properties.
The resulting property will hold only if neither property can be  by any of the generated test cases.
The output indicates that ScalaCheck has generated 100 test cases (of type ) and that the predicates were satisfied for each.
Don't try writing your properties down as executable ScalaCheckInt code, an informal description is fine.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Reversing a list and summing it should give the same result as summing the original, non-reversed list.
What should the sum be if all elements of the list are the same value?
Can you think of other properties? EXERCISE 2: What are properties that specify the function that finds the.
We'll talk more about these features later, but just to give an idea of what is possible:
In the event of discovering a failing test, the test runner triesTest case minimization smaller sizes until finding the  size test case that also fails, which is moreminimal illuminating for debugging purposes.
For instance, if a property fails for a list of size 10, the test runner checks smaller lists and reports the smallest failure.
Exhaustive test case generation: We call the set of possible values that could be produced by some  the.
If the property holds for all values in a domain, we have an actual proof, rather than just the absence of evidence to the contrary.
Footnote 2mThis is the same usage of 'domain' as —generators describethe domain of a function possible inputs to functions we would like to test.
Note that we will also still sometimes use 'domain' in the more colloquial sense, to refer to a subject or area of interest, e.g.
And while there's nothing wrong with it, we are going to be deriving our own library in this chapter, starting from scratch.
This is partially for pedagogical purposes, but there's another reason: we want to encourage the view that no existing library (even one designed by supposed experts) is authoritative.
Don't treat existing libraries as a cookbook to be followed.
Most libraries contain a whole lot of  design choices, manyarbitrary made unintentionally.
Look back to the previous chapter—notice how on several occasions, we did some informal reasoning to rule out entire classes of possible designs.
This sort of thing is an inevitable part of the design process (it is impossible to fully explore every conceivable path), but it means it's easy to miss out on workable designs.
When you start from scratch, you get to revisit all the fundamental assumptions that went into designing the library, take a different path, and discover things about.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
As a result, you might arrive at a design that's much better for your purposes.
But even if you decide you like the existing library's solution, spending an hour or two of playing with designs and writing down some type signatures is a great way to learn more about a domain, understand the design tradeoffs, and improve your ability to think through design problems.
In this section, we will embark on another messy, iterative process of discovering data types and a set of primitive functions and combinators for doing property-based testing.
As before, this is a chance to peer over the shoulder of someone working through possible designs.
The particular path we take and the library we arrive at isn't necessarily the same as what you would discover.
If property-based testing is unfamiliar to you, even better; this is a chance to explore a new domain and its design space, and make your own discoveries about it.
If at any point, you're feeling inspired or have ideas of your own about how to design a library like this, don't wait for an exercise to prompt you—  andput the book down go off to implement and play with your ideas.
You can always come back to this chapter if you want ideas or get stuck on how to proceed.
What data types should we use for our testing library? What primitives should we define, and what should their meanings be? What laws should our functions satisfy? As before, we can look at a simple example and "read off" the needed data types and functions, and see what we find.
For inspiration, let's look at the ScalaCheck example we showed earlier:
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We can learn many things by looking at this signature.
Notice what we are not specifying—the size of the list to generate.
For this to be implementable, this implies our generator must either assume or be told this size.
So it seems that generators must be told the size of test cases to generate.
We could certainly imagine an API where this were explicit:
This would certainly be a useful combinator, but  having to explicitlynot specify sizes is powerful as well—it means the test runner has the freedom to choose test case sizes, which opens up the possibility of doing the test case minimization we mentioned earlier.
If the sizes are always fixed and specified by the programmer, the test runner won't have this flexibility.
Keep this concern in mind as we get further along in our design.
But again, it doesn't seemList[Int] => Boolean like  should care about the types of the generator and the predicate, asforAll long as they match up.
Here, we've simply invented a new type,  (short for "property", followingProp the ScalaCheck naming), for the result of binding a  to a predicate.
We mightGen not know the internal representation of  or what other functions it supportsProp but based on this example, we can see that it has an  operator, so let's introduce&& that:
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Now that we have a few fragments of an API, let's discuss what we want our types and functions to.
InforAll check ScalaCheck, this  method has a side effect of printing to console.
This ischeck probably fine to expose as a convenience function, but it's not a basis for composition.
In order to combine  values using combinators like , we need Prop && check (or whatever function "runs" properties) to return some meaningful value.
What type should that value be? Well, let's consider what sort of information we'd like to get out of checking our properties.
At a minimum, we need to know whether the property succeeded or failed.
In this representation,  is nothing more than a non-strict , andProp Boolean any of the usual  functions ('and', 'or', 'not', 'xor', etc) can be defined for Boolean.
If a property fails, we'd likeProp Boolean to perhaps know how many tests succeeded first, and what the arguments were that resulted in the failure.
And if it succeeded, we might like to know how many tests it ran.
Let's try to return an  to indicate this success or failure:Either.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Type aliases like this can help readability of an API.
There's a problem, what type do we return in the failure case? We don't know anything about the type of the test cases being generated internal to the .Prop Should we add a type parameter to , make it a ? Then Prop Prop[A] check could return.
Before going too far down this path, let's askEither[A,Int] ourselves, do we really care though about the  of the value that caused thetype property to fail? Not exactly.
We would only care about the type if we were going to do further computation with this value.
Most likely we are just going to end up printing this value to the screen, for inspection by the person running these tests.
This suggests we can get away with the following type:
In the case of failure,  returns a , where  is some check Left(s) s String representation of the value that caused the property to fail.
As a general rule, whenever you return a value of some type  from a function, think about whatA callers of your function are likely to do with that value.
Will any of them care that the value is of type , or will they always convert your  to some other uniformA A representation (like  in this case)? If you have a good understanding of allString the ways callers will use your function, and they all involve converting your value to some other type like , there's often no loss in expressiveness to simplyString return that  directly.
That takes care of the return value of , at least for now, but what aboutcheck the arguments to ? Right now, the  method takes no arguments.
Ischeck check this sufficient? We can think about what information  will have access to justProp.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Without knowing more about the representation of , it's hard to say whetherGen there's enough information here to be able to generate values of type  (which isA what we need to implement )
So for now let's turn our attention to , tocheck Gen get a better idea of what it means and what its dependencies might be.
Let's take a step back to reflect on what we've learned so far.
By inspecting a simple example, we learned that our library deals with at least two fundamental types, , and , and we've loosely assignedGen Prop meanings to these types.
In looking at , we made what seems like an importantGen distinction between generators whose "size" is chosen explicitly by the programmer (as in ), and generators where the testing framework islistOfN allowed to pick sizes (as in )
We noted this as something to keep in mindlistOf for later.
Somewhat arbitrarily, we then chose to look at  first, and determined weProp couldn't commit to a concrete representation for  without first knowing theProp representation of.
We've made a note of this, and plan on returning to Gen Prop shortly.
Have we made a mistake by starting with ? Not at all.
It doesn't matter much where we begin our inquiry—the domain will inexorably guide us to make all the design choices that are required.
As you do more functional design, you'll develop a better intuition for where a good place is to start.
We determined earlier that a  was something that knowsGen[A] how to generate values of type.
What are some ways it could do that? Well, itA could  generate these values.
Look back at the example from chapterrandomly six—there, we gave an interface for a purely functional random number generator and showed how to make it convenient to combine computations that made use of it.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
But in addition to randomly generating values, it might be nice when possible to exhaustively enumerate a sequence of values, and be notified somehow that all possible values have been generated.
If we can get through all possible values without finding a failing case, this is an actual  that our property holds overproof its domain.
Clearly exhaustive generation won't always be feasible, but in some cases it might be.
If we want to support both modes of test case generation (random and exhaustive), we need to extend.
The first element of the pair is the generator of random values, the second is an exhaustive list of values.
Note that with this representation, the test runner will likely have to choose between the two modes based on the number of test cases it is running.
We'll get to writing this logic a bit later, after we nail down exactly how to represent our "dual-mode" generators.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
There's a few problems with the current encoding of these dual-mode generators.
So let's use a  in place of .Stream List We are going to use the  type we developed last chapter and also promoteStream our type alias to a data type.5
They have an unfortunate "off-by-one" error—they strictly evaluate their first element.
Generic code like what we are writing in this chapter can't assume it is desireable to evaluate any part of the stream until it is explicitly requested.
Try implementing , , , and .Gen unit boolean choose listOfN.
What should we do about infinite domains, like a  generator in some range:Double.
To randomly sample from these domains is straightforward, but what should we.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We could change its meaning to "the domain is infinite, use random sampling to generate test cases", but then we lose the ability to determine that we have exhaustively enumerated our domain, or that the domain is simply empty.
How can we distinguish these cases? One simple way to do this is with :Option.
We'll adopt the convention that a  in  signals to the testNone exhaustive runner should switch to random sampling, because the domain is infinite or otherwise not worth fully enumerating.
Note that this is a prettyexhaustive Some typical usage of.
Although we introduced  as a way of doingOption Option error handling,  gets used a lot whenever we need a simple way ofOption encoding one of two possible cases.
Footnote 6mWe could also choose to put the  on the outside:
You mayOption Option[Stream[A]] want to explore this representation on your own.
You will find that it doesn't work out so well as it requires that we be able to decide  that the domain is not worth enumerating.
We will see examples later of generatorsup front where it isn't possible to make this determination.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
When using a type like this with no particular meaning attached to it, it can be a good practice to define type aliases and functions which help codify and document the meaning you have assigned to the type:
This is pretty low cost—it doesn't require us to reimplement or replicate the API of  or.
But it helps with documentation of theOption Stream API and makes it easier to implement a refactoring later which promotes  to its own data type.Domain.
As we discussed last chapter, we are interested in understanding what operations are , and what operations are , and in finding a small yetprimitive derived expressive set of primitives.
A good way to explore what is expressible with a given set of primitives is to pick some concrete examples you'd like to express, and see if you can assemble the functionality you want.
As you do so, look for patterns, try factoring out these patterns into combinators, and refine your set of primitives.
We encourage you to stop reading here and simply  with the primitives andplay combinators we've written so far.
If you want some concrete examples to inspire you, here are some ideas:
If we can generate a single  in some range, do we need a new primitive to generate an Int pair in some range?(Int,Int)
Can we produce a  from a ? What about a  from a Gen[Option[A]] Gen[A] Gen[A] ?Gen[Option[A]]
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In fact, if you rely exclusively on concrete, obviously "useful" or "important" examples to design your API, you'll often miss out on aspects of the design space and generate designs with ad hoc, overly specific features.
We don't want to overfit our API to the particular examples we happen to think of.
Weright now want to reduce the problem to its , and sometimes the best wayessence to do this is.
Don't try to solve important problems or produceplay useful functionality.
Just experiment with different representations, primitives, and operations, let questions naturally arise, and explore whatever piques your curiosity.
Here, we are going to take a bit of a shortcut.
Notice that  is composed of aGen few other types, , , and.
This can often be a hint that theStream State Option API of  is going to have many of the same operations as these types.
Let's seeGen if there's some familiar operations from , , and  that weStream State Option can also define for .Gen.
Your implementation should be almost trivially defined in terms of the  and  functions on , , and.
After you've implemented Gen map , you may want to revisit your implementation of  for  and definechoose Double it in terms of  and .uniform map Footnote 7mYou've probably noticed by now that many data types support , , and.
That is, by composing our data types in certain well-defined ways, we can obtain the implementations of , , andmap map2 so on for free, without having to write any additional code!
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
But  and  are not expressive enough to encode somemap map2 generators.
Suppose we'd like a  where both integers are odd,Gen[(Int,Int)] or both are even.
In bothList[Double] these cases there is a dependency—we generate a value, then use that value to determine what generator to use next.
For this we need , another9 flatMap function we've seen before.
Footnote 9mTechnically, this first case can be implemented by generating the two integers separately, and using  to make them both odd or both even.
But a more natural way is to choose an even or oddmap2 generator based on the first value generated.
You can make  and this version of  in the flatMap listOfN.
What are some possible ways you could combine the two exhaustive streams? Can.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Now that we know more about our representation of generators, let's return to our definition of.
Our  representation has revealed information about theProp Gen requirements for.
First, we notice that our properties can succeed in one ofProp two ways—they can be  correct, by exhaustive enumeration, or they canproven succeed when no counterexamples are found via random generation.
It only includes two cases, one for success, and one for failure.
We can create a new data type, , to represent the two ways a test can succeed:Status.
A test can succeed by being , if the domain has been fully enumeratedproven and no counterexamples found, or it can be merely , if the test runnerunfalsified had to resort to random sampling.
But  is stillEither Prop missing some information—we have not specified how many test cases to examine before we consider the property to be passed.
We could certainly hardcode something, but it would be better to propagate this dependency out:
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We can see that  does not have enough information to return a .forAll Prop Besides the number of test cases to run,  must have all the informationProp needed for generated test cases to return a , but if it needs to generateStatus random test cases, it is going to need an.
There are certain parameters that go into generating test cases, and if we think of other parameters besides the number of test cases and the source of randomness, we can just add these as extra arguments to .Prop.
We have a choice about whether to use exhaustive or random test case generation.
For our implementation, we'll spend a third of our test cases examining elements from the exhaustive.
Footnote 10mHere is a question to explore—might there be a way to track the expected size of the exhaustive stream, such that the decision to use random data could be made up front? For some primitives, it is certainly possible, but is it possible for all our primitives?
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Notice we are catching exceptions and reporting them as test failures, rather than bringing down the test runner (which would lose information about what argument triggered the failure)
That is, we would ideally like our framework to find the  or simplest failing test case, to bettersmallest illustrate the problem and facilitate debugging.
Let's see if we can tweak our representations to support this.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Shrinking: After we have found a failing test case, we can run a separate procedure to minimize the test case, by successively decreasing its "size" until it no longer fails.
This is called , and it usually requires us to write separate code for each data type toshrinking implement this minimization process.
Sized generation: Rather than shrinking test cases after the fact, we simply generate our test cases in order of increasing size and complexity.
So, we start small and increase size until finding a failure.
This idea can be extended in various ways, to allow the test runner to make larger jumps in the space of possible sizes while still making it possible to find the smallest failing test.
There's nothingshrinking wrong with this approach (it is also used by the Haskell library  thatQuickCheck ScalaCheck is based on) but we are going to see what we can do with sized generation.
It's a bit simpler and in some ways more modular—our generators only need to be knowledgeable about how to generate a test case of a given size, they don't need to be aware of the 'schedule' used to search the space of test cases and the test runner therefore has the freedom to choose this schedule.
Rather than modifying our  dataGen type, for which we've already written a number of useful combinators, we are going to introduce sized generation as a separate layer in our library.
That is, we are going to introduce a type, , for representing sized generators:SGen.
YouGen SGen can add this as a method to .Gen.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The SGen Prop Prop.forAll version of  looks like this:SGen forAll.
Can you see how this function is not possible to implement?  is expectingSGen to be told a size, but  does not receive any size information.
Much like weProp did with the source of randomness and number of test cases, we simply need to propagate this dependency to.
But rather than just propagating thisProp dependency  to the caller of , we are going to have  accept a as is Prop Prop.
This puts  in charge of invoking the underlying generatorsmaximum Prop with various sizes, up to and including the maximum specified size, which means it can also search for the smallest failing test case.
Footnote 12mThis rather simplistic implementation gives an equal number of test cases to each size being generated, and increases the size by  starting from.
This implementation highlights a couple minor problems with our representation of.
For one, there are actually now  ways that a propertyProp three can succeed.
It can be , if the domain of the generator has been fullyproven.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
It can be , if the domain of the generator hasexhausted been fully examined, but only up through the maximum size.
Notice that we lack a way to distinguish between  and Status Proven.
Why is that? Can you see how to fix it?Exhausted The problem is that  is a totally opaque function from  to , andSGen Size Gen.
We can add this distinction by making  into a data type with two cases:SGen.
We have converged on what seems like a reasonable API.
We could keep tinkering with it, but at this point let's try  the library to construct tests and see if weusing notice any deficiencies, either in what it can express or its general usability.
Usability is somewhat subjective, but we generally like to provide convenient.
We aren't necessarily aiming here to make the library more expressive, we simply want to make it more pleasant to use.
Let's revisit an example that we mentioned at the start of this.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The maximum of a list should be greater than or equal toList API docs link.
We can introduce a helper function in  for actually  our Prop running Prop values and printing their result to the console in a useful format:
We are using default arguments here to make it more convenient to call in the case that the defaults are fine.
The Scala standard library implementation of  crashes whenmax given the empty list (rather than returning an ).Option.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Recall that in the previous chapter we looked at laws we expected to hold for our parallel computations.
Can we express these laws with our library? The first "law" we looked at was actually a particular test case:
We certainly can express this, but the result is somewhat ugly.14
We've expressed the test, but it's verbose, cluttered, and the "idea" of the test is obscured by details that aren't really relevant here.
Notice that this isn't a question of the API being expressive enough—yes we can express what we want, but a combination of missing helper functions and poor syntax obscures the intent.
Our first observation is that  is a bit too generalforAll for this test case.
We aren't varying the input to this test, we just have a hardcoded example.
Hardcoded examples should be just as convenient to write as in a traditional unit testing library.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We provide the unit generator, which, of course, only generates a single value.
The value will be ignored in this case, simply used to drive the evaluation of the given.
Notice that this combinator isBoolean general-purpose, having nothing to do with —we can go ahead and move itPar into the  companion object.
Can we do something about the  and  noise?p(ES).get p2(ES).get There's something rather unsatisfying about it.
For one, we're forcing this code to be aware of the internal implementation details of , simply to compare two Par.
One improvement is to move the equality comparison intoPar , and into a helper function, which means we only have to run a single  atPar Par.
So, we are  equality to operate in , which is a bit nicer than having tolifting Par run each side separately.
But while we're at it, why don't we move the  of running.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
S.map2(g)((_,_)) is a rather noisy way of combining two generators to produce a pair of their outputs.
We can even introduce  as a pattern , which lets us** using custom extractors write:
This syntax works nicely when tupling up multiple generators—when pattern matching, we don't have to nest parentheses like would be required when using the tuple pattern directly.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Footnote 16mWe cannot use the standard Java/Scala  method, or the  method in Scala (whichequals == delegates to the  method), since that method returns a  directly, and we need to return a equals Boolean.
These might seem like minor changes, but this sort of factoring and cleanup can greatly improve the usability of your library, and the helper functions we've written make the properties easier to read and more pleasant to write.
You may want to add versions of  and  for sized generators as well.forAllPar checkPar.
Let's look at some other properties from the previous chapter.
This property implicitly states that the equality holds  choices of , for all types.
We are forced to pick particularfor all y values for :y.
We can certainly range over more choices of , but what we have here isy probably good enough.
The implementation of  cannot care about the values ofmap our parallel computation, so there isn't much point in constructing the same test for.
What   be affected by is the  of theDouble String map can structure parallel computation.
If we wanted greater assurance that our property held, we.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Here, we are only supplying Par expressions with one level of nesting.
So far, our library seems quite expressive, but there's one area where it's lacking: we don't currently have a good way to test higher-order functions.
While we have lots of ways of generating , using our generators, we don't really havedata a good way of generating .functions.
For instance, let's consider the  function defined for  and takeWhile List.
Recall that this function returns the longest prefix of its input whoseStream.
Can you think of a good property expressing the relationship between.
This works, but is there a way we could let the testing framework handle generating functions to use with ?  Let's consider our options.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Still, there are many situations where being able to generate functions for testing is useful.
We are simply generating constant functions that ignore their input.
In the case of , where we need atakeWhile function that returns a , this will be a function that always returns Boolean true or always returns —clearly not very interesting for testing the behavior offalse our function.
See what you can discover about this problem and if there is a nice general solution that you can incorporate into the library we've developed so far.
This exercise is optional, but you may find it interesting to work on.
Try writing properties to specify the behavior of some of the other functions we wrote for and , for instance , , , and .List Stream take drop filter unfold.
Try writing a sized generator for producing the  data type we defined in chapter 3,Tree then use this to specify the behavior of the  function we defined for.
Can youfold Tree think of ways to improve the API to make this easier? Try writing properties to specify the behavior of the  function we defined for sequence.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Isn't it interesting that many of the functions we've implemented here, for our Gen type, look quite similar to other functions we've defined on , , ,Par List Stream and ? As an example, for  we defined:Option Par.
And in this chapter we defined  for  (as a method on ):map Gen Gen[A]
And we've defined very similar-looking functions for other data types.
We have to wonder, is it merely that our functions share similar-looking signatures, or do they satisfy the same  as well? Let's look at a law we introduced for  in thelaws Par previous chapter:
Fascinating! Not only do these functions share similar-looking signatures, they also in some sense have analogous meanings in their respective domains.
For each law, see if an analogous law holds for .State Gen It appears there are deeper forces at work! We are uncovering some rather.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In this chapter, we worked through another extended exercise in functional library design, using the domain of property-based testing as inspiration.
Once again, we reiterate that our goal was not necessarily to learn about property-based testing per.
We hope thesese chapters are giving you ideas about how to approach functional library design in your own way and preparing you for the sorts of issues and questions you'll encounter.
Developing an understanding of the overall process is much more important than following absolutely every small design decision we made as we explored the space of this particular domain.
In the next chapter, we'll look at another domain, , with its own set ofparsing challenges and design questions.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In this chapter, we will work through the design of a combinator library for creating , using  parsing as a motivating use case.
This isalgebraic design just a natural evolution of what we've already been doing to different degrees in past chapters—designing our interface and associated laws first and letting this guide our choice of data type representations.
Here, we take this idea to its logical limit to see what it buys us.
At a few key points during this chapter, we will be giving more open-ended exercises, intended to mimic the scenarios you might encounter when designing and implementing your own libraries from scratch.
You'll get the most out of this chapter if you use these opportunities to put the book down and spend some time investigating possible approaches.
When you design your own libraries, you won't be handed a nicely chosen sequence of type signatures to fill in with implementations.
You will have to make the decisions about what types and combinators should even exist and a goal in part 2 of this book has been to prepare you for doing this on your own.
As always, in this chapter, if you get stuck on one of the exercises or want some more ideas, you can keep reading or consult the answers.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
TheseANTLR libraries  code for a parser based on a specification of thegenerate grammar.
This approach works fine and can be quite efficient, but comes with all the usual problems of code generation—they produce as as their output a monolithic chunk of code that is difficult to debug.
It's also quite difficult to reuse fragments of logic since we cannot introduce new combinators or helper functions to abstract over common patterns in our parsers.
In a parser combinator library, parsers are ordinary values that can be created and manipulated in a first-class way within the language.
Reusing parsing logic is trivial (we simply introduce a new combinator), and we don't have to delegate to any sort of separate external tool.
Recall that we defined an algebra to mean a collection of functions operating over some data type(s),  specifying relationships between thesealong with a set of laws functions.
In past chapters, we moved rather fluidly between inventing functions in our algebra, refining the set of functions, and tweaking our data type representations.
Laws were somewhat of an afterthought—we worked out the laws only after we had a representation and an API fleshed out.
There's absolutely nothing wrong with this style of design , but here we are going to take a different1 approach.
We will  with the algebra (including its laws) and decide on astart representation later.
This approachalgebraic design can be used for any design problem but works particularly well for parsing, because it's easy to imagine what combinators are required to be able to parse different inputs.
Overall, you might find this approach very natural, or you might2 find it extremely disconcerting to do so much work without committing to any concrete representations.
Ours will be designed for3 expressiveness (we'd like to be able to parse arbitrary grammars), speed, and good error reporting.
This last point is important—if there are parse errors, we want to be able to indicate exactly where the error is and accurately indicate its cause.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Error reporting is often somewhat of a second-class citizen in parsing libraries, but we are going to make sure we give careful thought to it.
Footnote 3mIncluding a parser combinator library in Scala's standard library.
As in the previous chapter, we are deriving our own library from first principles partially for pedagogical purposes, and to further encourage the idea that no library is authoritative.
Scala's parser combinators don't really satisfy our goals of providing speed and good error reporting (see the chapter notes for some additional discussion)
For simplicity and for speed, our library will create parsers that operate on strings as input.
We need to pick some parsing tasks to help us4 discover a good algebra for our parsers.
What should we look at first? Something practical like parsing an email address, JSON, or HTML? No! These can come later.
For now we are content to focus on a pure, simple domain of parsing various combinations of repeated letters and jibberish words like  and "abracadabra"
As silly as this sounds, we've seen before how simple examples like this"abba" help us ignore extraneous details and focus on the essence of the problem.
We can make the parsing library more generic, at some cost.
So let's start with the simplest of parsers, one that recognizes the single character input.
As we've done in past chapters, we can just  a"a" invent combinator for the task, :char.
What have we done here? We have conjured up a type, , which isParser parameterized on a single parameter indicating the  of the.
Thatresult type Parser is, running a parser should not simply yield a yes/no response to the input—if it succeeds, we want to get back a  of some useful type, and if it fails we expectresult.
The  parser will succeed only if theinformation about the failure char('a') input is the string  and we have chosen (somewhat arbitrarily) to have it return"a" that same character  as its result.'a'
This talk of "running a parser" makes it clear our algebra needs to be extended somehow to support that.
Wait a minute, what is ? It's another type we've just conjuredParseError.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We are in the process of specifying an ParseError Parser.
What's with the funny  type argument? It's not too important forParser[_] right now, but that is Scala's syntax for a type parameter that is itself a type constructor.
Just like making  a type argument lets the 5 ParseError Parsers interface work for multiple representations of , making ParseError.
Footnote 5mWe will say much more about this in the next chapter.
Footnote 6mWe will say much more about this in the next chapter.
What if we want to recognize either the string   the string "abra" or ? We could certainly add a very specialized combinator for it:"cadabra"
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
But choosing between two parsers seems like something that would be more generally useful, regardless of their result type, so let's go ahead and make this polymorphic:
We have also made  an implicit conversion and added another implicit string.
So given a , we can then Parser[String] val P: Parsers.
Other binary operators or methods can be added to the body of.
WeParserOps are going to follow the discipline of keeping the primary definition directly in.
See the codeParsers ParserOps for this chapter for more examples.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We can now recognize various strings, but we don't have any way of talking about repetition.
For instance, how would we recognize three repetitions of our.
We made  parametric in the choice of , since it doesn't seem like itlistOfN A should care whether we have a , a , or someParser[String] Parser[Char] other type of parser.
At this point, we have just been collecting up required combinators, but we haven't tried to refine our algebra into a minimal set of primitives, and we haven't talked about laws at all.
We are going to start doing this next, but rather than give away the "answer", we are going to ask you to examine a few more simple use cases yourself and try to design a minimal algebra with associated laws.
This should be a challenging exercise, but enjoy struggling with it and see what you can come up with.
Here are additional parsing tasks to consider, along with some guiding questions:
A  that recognizes zero or more  characters, and whose result value isParser[Int] 'a' the number of  characters it has seen.
If we are trying to parse a sequence of zero or more  and are only interested in the"a" number of characters seen, it seems inefficient to have to build up, say, a List[Char] only to throw it away and extract the length.
Could something be done about this? Are the various forms of repetition primitive in your algebra, or could they be defined in terms of something simpler? We introduced a type  earlier, but so far we haven't chosen any functions forParseError.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
This seems like a limitation given that we'd like meaningful error messages from our parsers.
Can you do something about it? Does  mean the same thing as ? This is a choice you get to make.
You don't necessarily need the laws to be complete, just write down some laws that you expect should hold for any.
Spend some time coming up with combinators and possible laws based on this guidance.
When you feel stuck or at a good stopping point, then continue reading to the next section, which walks through a possible design.
As long as they support the required laws and functions, you do not even need to make your representations public.
This, as we'll see later, makes it easy for primitive combinators to use cheap tricks internally that might otherwise break referential transparency.
There is a powerful idea here, namely, that a type is given meaning based on its relationship to other types (which are specified by the set of functions and their laws), rather than its internal representation.
This9 is a viewpoint often associated with category theory, a branch of mathematics we've mentioned before.
See the chapter notes for more on this connection if you're interested.
Footnote 9mThis sort of viewpoint might also be associated with object-oriented design, although OO has not traditionally placed much emphasis on algebraic laws.
Furthermore, a big reason for encapsulation in OO is that objects often have some mutable state and making this public would allow client code to violate invariants, a concern that is not as relevant in FP.
We are going to walk through discovering a set of combinators for the parsing tasks mentioned above.
If you worked through this design task yourself, you will likely have taken a different path and may have ended up with a different set of combinators, which is absolutely fine.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
First, let's consider the parser that recognizes zero or more repetitions of the character , and returns the number of characters it has seen.
We can start by'a' adding a primitive combinator for it, let's call it :many.
This isn't quite right, though—we need a  that counts theParser[Int] number of elements.
Better to introduce another combinator that should be familiar by now, :map.
We have a strong expectation for the behavior of —it should merelymap transform the result value if the  was successful.
No additional inputParser characters should be examined by a , a failing parser cannot become amap successful one via a  and vice versa, and in general, we expect  to be map map.
Let's go ahead andstructure preserving Par Gen formalize this by stipulating the now-familiar law:
How should we document this law that we've just added? We could put it in a documentation comment, but in the preceding chapter we developed a way to make our laws.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
This will come in handy later when we go to test that our implementation of behaves as we expect.
In the interest of keeping this chapter shorter, we won't be giving  implementations of all the laws, but that doesn't mean you shouldn't try writingProp them out yourself!
Incidentally, now that we have , we can actually implement  in termsmap char of :string.
And similarly, there's another combinator, , that can be defined insucceed terms of  and :string map.
This parser always succeeds with the value , regardless of the input stringa (since  will always succeed, even if the input is empty)
Does thisstring("") combinator seem familiar to you? We can specify its behavior with a law:
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The combination of  and  certainly lets us express the parsing task ofmany map counting the number of  characters, but it seems inefficient to be constructing'a' a  only to discard its values and extract its length.
It would be nice ifList[Char] we could run a  purely to see what portion of the input string it examines.Parser Let's conjure up a combinator for that purpose:
We call it  since it returns the portion of the input string examined byslice the  parser  i f  successful.
With , our parser can now be written as slice.
The  function here is nowslice ParserOps _.length referencing the  method on , rather than the  method on length String length.
What if we want to recognize  or more one 'a'
It feels like  should not have to be primitive, but should be definedmany1 somehow in terms of.
Really,  is just   .many many1(p) p followed by many(p) So it seems we need some way of running one parser, followed by another, assuming the first is successful.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
With , we can now implement the parser for zero or more  followedmany1 'a' by one or more  as:'b'
Now that we have , is  really primitive? Let's think about what map2 many will do.
It tries running ,  another , and another, and so onmany(p) p followed by p.
Can you spot what it is? We are calling  recursively in the second argument to , which is  inmany map2 strict.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Consider a simplified program trace of the evaluation of  for some parser.
We are only showing the expansion ofmany(p) p the left side of the  here:or.
Because a call to  always evaluates its second argument, our map2 many function will never terminate! That's no good.
Let's go ahead and make product and  non-strict in their second argument:map2
Try this here and make the necessary changes to your existing combinators.
What do you think of that approach in this instance?
Conceptually, many product feels like it should have been non-strict in its second argument anyway, since if the first  fails, the second will not even be consulted.Parser.
Now that we're considering whether combinators should be non-strict, let's revisit :or.
We will assume that  is left-biased, meaning it tries running  on the inputor p1 then tries  only if  fails.
You may wish to think about the consequences of having a version of.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Let's take a step back and look at the primitives we have so far:
Using these primitives, we can express repetition and nonempty repetition ( ,  and ) as well as combinators like  and.
It can't"4aaaa" be expressed with  because our choice of the second parser product depends on the result of the first (the second parser depends on its context)
We want to run the first parser, then do a  using the number extracted from the first parser'slistOfN result.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Can you see how this signature implies an ability to sequence parsers? EXERCISE 8: Using  and any other combinators, write theflatMap.
To parse the digits, you can make use of a new primitive, , which promotes a regular expression to a regex.
So it appears we have a new primitive, , which enablesflatMap.
This is not the firstmap map2 time  has made an appearance, but up until now we have not really triedflatMap to pin down any laws for it.
We'll be working through that in this chapter in a later section.
So far we have not discussed error reporting at all.
We've been focused exclusively on discovering a set of primitives that let us express parsers for different grammars.
But besides just being able to parse a grammar, we want to be able to determine how the parser should respond when given unexpected input.
Even without knowing what an implementation of  will look like, weParsers can reason very abstractly about  by a set ofwhat information is being specified combinators.
None of the combinators we have introduced so far say anything about  should be reported in the event of failure or what otherwhat error message information a  should contain.
Our existing combinators onlyParseError specify what the grammar is and what to do with the result if successful.
If we were to declare ourselves done and move to implementation at this point, we would have to make some arbitrary decisions about error reporting and error messages that are unlikely to be universally appropriate.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
For each combinator, try to come up with laws specifying what itsParser behavior should be.
What if we wanted to choose a different errorExpected "cadabra" message like ?"Expected The Magic Word (TM)"
Given , if  fails on the input, do we  want to run , ora or b a always b are the cases where we might not want to? If there are cases, can you think of additional combinators that would allow the programmer to specify when.
How do you want to handle reporting the  of errors?location.
Once you are satisfied with your design, you can continue reading.
The next section works through a possible design in detail.
By starting with the algebra first, we are forced to think differently—we must think of functions in terms of.
Thewhat information they specify signatures specify what information is given to the implementation, and the implementation is free to use this information however it wants as long as it respects the specified laws.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Now that you've spent some time coming up with some good error reporting combinators, we are now going to work through one possible design.
Again, you may have arrived at a different design, which is absolutely fine.
This is just another opportunity to see a worked design process.
We are going to progressively introduce our error reporting combinators.
None of the primitives so far let us assign an error message to a parser.
The intended meaning of  is that if  fails, its  willlabel p ParseError "somehow incorporate"
What does this mean exactly? Well, we could justmsg assume  and that the returned type ParseError = String ParseError will  the label.
The problem with this is that we'd like our parse error to alsoequal tell us  the problem occurred.
We've picked a concrete representation for  here that includes theLocation full input, an offset into this input, and the line and column numbers (which can be computed, lazily, derived from the full input and offset)
We can now say more precisely what we expect from —in the event of failure with , label Left(e)
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
What about the ? We would like for this to be filled in by the Location implementation with the location where the error occurred.
Is the  combinator sufficient for all our error reporting needs? Not quite.label Let's look at an example:
What sort of  would we like to get back from ParseError run(p)("abra ? (Note the capital  in ) The immediate cause is thatcAdabra") A cAdabra.
That error will have a location,'A' 'a' and it might be nice to report it somehow.
But reporting  that low-level erroronly would not be very informative, especially if this were part of a large grammar and the parser were being run on a larger input.
We have some more context that would be useful to know—the immediate error occurred while in the  labeled Parser.
Ideally, we"second magic word" could be told that while parsing , we encountered an"second magic word" error due to an unexpected capital , which pinpoints the error and gives us the'A' context needed to understand it.
And we can imagine that perhaps the top-level parser (  in this case) might be able to provide an even higher-level description ofp what the parser was doing when it failed ( , say),"parsing magic spell" which could also be informative.
So, it seems wrong to assume that one level of error reporting will always be sufficient.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Unlike ,  does not throw away the label(s) attached to —itlabel scope p merely adds additional information in the event that  fails.
First, we modify the functions that pull information out of a.
This is a stack of error messages indicating what the  was doing whenParser it failed.
This is a somewhat unusual data structure—we have , the current stack,stack but also a list of other failures ( ) that occurred previously in aotherFailures chain of  combinators.
We can write helper functions later to make constructing and manipulating.
For now, our concern is just making sure it contains all the potentially relevant information for error reporting, and it seems like.
Let's go ahead and pick this as ourParseError concrete representation.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Footnote 13mWe could also represent  as a trie in which shared prefixes of the error stack areParseError not duplicated, at a cost of having more expensive inserts.
It is easier to recover this sharing information during formatting of errors, which happens only once.
Now we are giving the  implementation all the information it needsParsers to construct nice, hierarchical errors if it chooses.
As a user of , we willParsers judiciously sprinkle our grammar with  and  calls which the label scope.
Note that itParsers would be perfectly reasonable for implementations of  to not use the fullParsers power of  and retain only basic information about the cause andParseError location of errors.14
Footnote 14mWe may want to explicitly acknowledge this by relaxing the laws specified for Parsers implementations, or making certain laws optional.
There is one last concern regarding error reporting that we need to address.
As we just discussed, when we have an error that occurs inside an  combinator, weor need some way of determining which error(s) to report.
We don't want to  haveonly a global convention for this, we sometimes want to allow the programmer to control this choice.
The -labeled parser will report an"jibberish" error due to expecting the first word to be , and the "abba" "magic spell"
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In other instances, we may want to allow theor parser to consider the next branch of the .or.
So, it appears we need a primitive for letting the programmer indicate when to commit to a particular parsing branch.
This is useful for more than just providing good error messages—it alsofailure improves efficiency by letting the implementation avoid lots of possible parsing branches.
One common solution to this problem is to have all parsers  ifcommit by default they examine at least one character to produce a result.
Where  is a parser that always fails (we could introduce this as a primitivefail combinator if we like)
That is, even if  fails midway through examining thep input,  reverts the commit to that parse and allows  to be run.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Suppose this parser is run on —after parsing the first "abra cadabra!" , we don't know whether to expect another  (the first branch) or "abra" "abra"
Note that we still haven't written an implementation of our algebra! But this exercise has been more about making sure our combinators provide a way for users of our library to convey the right information to the implementation.
It is up to the implementation to figure out how to use this information in a way that satisfies the laws we've stipulated.
Let's write that JSON parser now, shall we? We don't have an implementation of our algebra yet, but that doesn't actually matter! Our JSON parser doesn't need to know the internal details of how parsers are represented.
It will be constructing a parser purely using the set of primitives we've defined and any derived combinators.
Recall that we have built up the following set of primitives:
Again, we will be asking  to drive the process of writing this parser.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
If you aren't already familiar with the JSON format, you may want to read the and the.
We are going to write a rather dumb parser that simply parses a syntax tree from the document without doing any further processing.
Let's introduce a data type for this: Footnote 17mSee the chapter notes for discussion of alternate approaches.
To allow this, we can place theParsers implementation in a regular function that takes a  value as its argument:Parsers.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Footnote 18mUnfortunately, we have to mention the covariance of the  type constructor again, ratherParser than inferring the variance from the definition of .Parsers.
You don't need to worry (yet) about the representation of.
AsParser you go, you will undoubtedly discover additional combinators and idioms, notice and factor out common patterns, and so on.
Use the skills and knowledge you've been developing throughout this book, and have fun! If you get stuck, you can always consult the answers.
Any general purpose combinators you discover can be added to the  Parsers trait directly.
You will probably want to introduce combinators that make it easier to parse the tokens of the JSON format (like string literals and numbers)
For this you could use the regex primitive we introduced earlier.
You could also add a few primitives like , , letter digit.
Consult the hints if you'd like a bit more guidance.
Aren't you curious to try testing it? Now it's finally time toParser[JSON] come up with a representation for  and implement the  interfaceParser Parsers using this representation.
This is a very open-ended design task, but the algebra we have designed places very strong constraints on possible representations.
You should be able to come up with a simple, purely functional representation of.
Footnote 19mNote that if you try running your JSON parser once you have an implementation of ,Parsers you may get a stack overflow error.
See the end of the next section for a discussion of this.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Replace  with whatever data type you use for representing yourMyParser parsers.
When you have something you are satisfied with, get stuck, or want some more ideas, keep reading.
In the next section, we will work through an implementation of our Parsers interface.
Here, we are going to spend some time refining the laws for just two of our combinators,  and.
We've introduced these combinatorsflatMap succeed several times before (though  was previously called ), for differentsucceed unit data types.
Earlier, as an exercise, we asked the question: is  associative? That is, do weor expect that  is equal to ? As designers of the(a or b) or c a or (b or c) API, we get to choose whatever laws we wish—let's choose to make the answer.
Associativity is a nice property to expect of operations in an algebra.
If  isyes or associative, it means we do not need to have knowledge of a parser's context (what parser it is grouped with) to understand its behavior.
The associativity of  makes us wonder: is there a kind of associativity lawor for ? Yes, there is.
The formulation is usually not given directly in termsflatMap of.
The problem with  is the "shape" of the two sides isflatMap flatMap different:
On the left side we have a , but on the right side we have aParser[A] function.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
However, there is a combinator, let's call it , that is equivalent inseq expressiveness to , which has a function on  sides:flatMap both.
Footnote 20mRecall that the type  has just a single value, written.
The key insight here was that  could be converted to Unit => Parser[A] and vice versa.Parser[A]
Now that we have an operation with the same shape on both sides, we can ask whether it should be associative as well.
Choose a data type, such as  or .Option List Define functions analogous to  and  for this type and show that theseq succeed implementations satisfy the same laws.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Can you think of any laws to specify the relationship between  and ?seq kor.
The  branch has to build up a , which are a littleelse ParseError inconvenient to construct right now, so we've introduced a helper function on.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We have a representation for  that at least supports Parser.
Footnote 21mRecall that  contains the full input string and an offset into this string.Location.
We introduced a new type here, , rather than just using.
InResult Either the event of success, we return a value of type  as well as the number ofA characters of input consumed, which can be used by the caller to update the.
This type is starting to get at the essence of what a Location 22 Parser is—it is a kind of state action that can fail, similar to what we built in chapter 6.23 It receives an input state, and if successful returns a value as well enough information to control how the state should be updated.
Footnote 22mNote that returning an  would give  the ability to set the ,(A,Location) Parser input which is granting it too much power.
This understanding gives us a way of framing how to build a fleshed out representation supporting all the fancy combinators and laws we have stipulated.
We simply consider what each primitive requires that we track in our state type (just a  may not be sufficient), and work through the details of howLocation each combinator transforms this state.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Notice that  is a bit less efficient thanParser slice it could be, since it must still construct a value only to discard it.
In the event of failure, we want to push a newscope message onto the  stack.
It returns an updated copy of the objectcopy case class with one or more arguments replaced, using the usual default argument mechanism in Scala.
If no new value is specified for a field, it will have the same value as in the original object.
The function  is defined on —it just applies a function tomapError Result the failing case:
Because we push onto the stack after the inner parser has returned, the bottom of the stack will have more detailed messages that occurred later in parsing.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We willParseError label make a design decision that  trims the error stack, cutting off more detailedlabel messages from inner scopes, using only the most recent location from the bottom of the stack:
Recall what we specified for theor attempt expected behavior of : it should run the first parser, and if it fails or in an.
We said thatuncommitted state consuming at least one character should result in a committed parse, and that.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We can support the behavior we want by adding one more piece of information to the  case of —a  value indicating whether theFailure Result Boolean parser failed in a committed state:
Itattempt uses a helper function, , which we can define on :uncommit Result.
And the implementation of  simply checks the  flag beforeor isCommitted running the second parser—if the first parser fails in a committed state, then we skip running the second parser.
What about ? The implementation is simple, we just advance theflatMap location before calling the second parser.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
There is one subtlety—if the first parser consumesadvanceBy Location any characters, we ensure that the second parser is committed, using a helper function,  on :addCommit ParseError.
Footnote 25mYou will find, unfortunately, that it stack overflows for large inputs (for instance, )
So long as any combinators that do repetition are defined in terms of  (which they all can be), this solves the problem.
There are a lot of choices to make, but a key insight is that we typically want to combine or group labels attached to the same location when presenting the error as a  for display.String.
Can you think of a way of modifying the List[Char] representation to make slicing more efficient?Parser.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In this chapter, we introduced an approach to writing combinator libraries called algebraic design, and used it to design a parser combinator library and implement a JSON parser.
Along the way, we discovered a number of very similar combinators to previous chapters, which again were related by familiar laws.
In part 3, we will finally understand the nature of the connection between these libraries and learn how to abstract over their common structure.
We hope you have come away from these chapters with a basic sense for how functional design can proceed, and more importantly, we hope these chapters have motivated you to try your hand at.
Before starting on part 3, we encourage you to venture beyond this book and try writing some more functional code and designing some of your own libraries.
Have fun, enjoy struggling with design problems that come up, and see what you discover.
Next we will begin to explore the universe of patterns and abstractions which the chapters so far have only hinted at.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
By the end of Part 2, we were getting comfortable with considering data types in terms of their —that is, the operations they support and the laws thatalgebras govern those operations.
Hopefully you will have noticed that the algebras of very different data types tend to follow certain patterns that they have in common.
In this chapter, we are going to begin identifying these patterns and taking advantage of them.
We will consider algebras in the abstract, by writing code that doesn't just operate on one data type or another but on  data types that share a commonall algebra.
The first such abstraction that we will introduce is the.
We choose tomonoid1 start with monoids because they are very simple and because they are ubiquitous.
Monoids come up all the time in everyday programming, whether we're aware of them or not.
Whenever you are working with a list or concatenating strings or accumulating the result of a loop, you are almost certainly using a monoid.
The prefix "mon-" means "one", and in category theory a monoid is a category with one object.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The laws of"monoid" associativity and identity are collectively called the.
Some type A A binary associative operation that takes two values of type  and combines them intoA one.
A value of type  that is an identity for that operation.A.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Footnote 2mThe Greek prefix "endo-" means "within", in the sense that an endofunction's codomain is within its domain.
Use your property to test the monoids we have written.
As a programmer, it's natural to think of thehaving instance of type  as being.
The monoid is actually both things—the type together with the instance.
When we say that a method accepts a value of type , we don't say that it takes a monoid, but that itMonoid[A] takes  that the type  is a monoid.evidence A.
Just what  a monoid, then? It is simply an implementation of an interfaceis governed by some laws.
Stated tersely, a monoid is a type together with an.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
If you look at the signatures for and  on , you might notice something about thefoldLeft foldRight List.
The components of a monoid fit these argument types like a glove.
So if we had a list of s, we could simply pass the  and  of the String op zero.
Notice that it doesn't matter if we choose  or  whenfoldLeft foldRight folding with a monoid —we should get the same result.
This is precisely because3 the laws of associativity and identity hold.
A left fold associates operations to the left while a right fold associates to the right, with the identity element on the left and right respectively: Footnote 3mAs of this writing, with Scala at version 2.9.2, the implementation of  in the standardfoldRight library is not tail-recursive, so for large lists it matters operationally which we choose since they have different memory usage characteristics.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
But what if our list has an element type that doesn't have a  instance?Monoid Well, we can always  over the list to turn it into a type that does.map.
But you can also write  and foldLeft foldRight foldLeft foldRight using ! Try it.foldMap.
The fact that a monoid's operation is associative means that we have a great deal of flexibility in how we fold a data structure like a list.
We have already seen that operations can be associated to the left or right to reduce a list sequentially with.
But we could instead split the data into chunks, foldfoldLeft foldRight them , and then combine the chunks with the monoid.
Folding to thein parallel right, the combination of chunks , , , and  would look like this:a b c d.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
This parallelism might give us some efficiency gains, because the two inner op s could be run simultaneously in separate threads.
As a nontrivial use case, let's say that we wanted to count the number of words in a.
Parsing sequentially like that, the parser state could be as simple as tracking whether the last character seen was a whitespace.
But imagine doing this not for just a short string, but an enormous text file.
It would be nice if we could work with chunks of the file in parallel.
The strategy would be to split the file into manageable chunks, process several chunks in parallel, and then combine the results.
In that case, the parser state needs to be slightly more complicated, and we need to be able to combine intermediate results regardless of whether the section we're looking at is at the beginning, end, or middle of the file.
In other words, we want that combination to be associative.
To keep things simple and concrete, let's consider a short string and pretend it's a large file:
If we split this string roughly in half, we might split it in the middle of a word.
Clearly, just countingdolor the words as an  is not sufficient.
We need to find a data structure that canInt handle partial results like the half words  and , and can track the completedo lor words seen so far, like , , and .ipsum sit amet.
The partial result of the word count could be represented by an algebraic data.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
A  is the simplest case, where we have not seen any complete words yet.Stub But a  keeps the number of complete words we have seen so far, in .Part words The value  holds any partial word we have seen to the left of those words,lStub and  holds the ones on the right.rStub.
For example, counting over the string  would result in "lorem ipsum do" since there is one complete word.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
If you take the length of two strings and add them up, it's the same as taking the length of the concatenation of those two strings:
Here,  is a function from  to  that length String Int preserves the.
Such a function is called a .monoid structure monoid homomorphism4
A monoid homomorphism  between monoids  and  obeys thef M N following general law for all values  and :x y.
Footnote 4mThis word comes from Greek, "homo" meaning "same" and "morphe" meaning "shape"
The same law holds for the homomorphism from  to  in theString WC current example.
This property can be very useful when designing your own libraries.
If two types that your library uses are monoids, and there exist functions between them, it's a good idea to think about whether those functions are expected to preserve the monoid structure and to check the monoid homomorphism law with automated tests.
There is a higher-order function that can take any function of type A =>
Sometimes there will be a homomorphism in both directions between two monoids.
Such a relationship is called a  ("iso-"monoid isomorphism meaning "equal") and we say that the two monoids are isomorphic.
For example, if we want to concatenate a list of strings, doing so with a left or right fold can be less efficient than we would like.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
At every step of the fold, we are allocating the full intermediate  onlyString to discard it and allocate a larger string in the next step:
A more efficient strategy would be to combine the list by halves.
That is, to first construct  and , then add those together.
You will need to come up with a creative IndexedSeq[Int]
In chapter 3, we implemented the data structures  and , both of whichList Tree could be folded.
In chapter 5, we wrote , a lazy structure that also can beStream folded much like a  can, and now we have just written a fold for List.
For example, if we have a structure full of integers and want to calculate their sum, we can use foldRight:
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Looking at just this code snippet, we don't really know the type of.
Itints could be a , a , or a , or anything at all with a Vector Stream List foldRight method.
Here we are abstracting over a type constructor , much like we did with the F type in the previous chapter.
Footnote 5mJust like values and functions have types, types and type constructors have.
Scala uses kindskinds to track how many type arguments a type constructor takes, whether it is co- or contravariant in those arguments, and what their kinds are.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The  abstraction by itself is not all that compelling, and with theMonoid generalized  it's only slightly more interesting.
The real power offoldMap monoids comes from the fact that they .compose.
This means, for example, that if types  and  are monoids, then the tuple type A B is also a monoid (called their ).(A, B) product.
Some data structures also have interesting monoids as long as their value types are monoids.
For instance, there is a monoid for merging key-value s, as long asMap the value type is a monoid.
Using these simple combinators, we can assemble more complex monoids fairly easily:
This allows us to combine nested expressions using the monoid, with no.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
A frequency map contains one entry per word, with that word as the key, and the number of times that word appears as the value under that key.
The fact that multiple monoids can be composed into one means that we can perform multiple calculations simultaneously when folding a data structure.
For example, we can take the length and sum of a list at the same time in order to calculate the mean:
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The ability to compose monoids really comes into its own when you have complicated nested structures.
But it can be somewhat tedious to assemble them by hand in each case.
Fortunately, Scala has a facility that can make monoids a lot more pleasant to work with by making instances implicit:
Such implicit instances can then be used to implicitly construct more complicated instances:
If we make all of our monoid instances implicit, a simple function definition with an implicit argument can automatically assemble arbitrarily complex monoids for us.
All we have to do is specify the type of the monoid we want.
As long as that type has an implicit  instance, it will be returned to us.Monoid.
A common solution is to put values in a simple wrapper type and make that type the monoid.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
For example if we have a list foldMap Foldable of integers and we want to sum them:ints.
In this chapter we introduced the concept of a monoid, a simple and common type of abstract algebra.
When you start looking for it, you will find ample opportunity to exploit the monoidal structure of your own libraries.
The associative property lets you fold any  data type and gives you the flexibility of doing so inFoldable parallel.
Monoids are also compositional, and therefore they let you write folds in a declarative and reusable way.
Monoid has been our first totally abstract trait, defined only in terms of its abstract operations and the laws that govern them.
This gave us the ability to write useful functions that know nothing about their arguments except that their type happens to be a monoid.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In each case, we proceeded by writing a small set of primitives and then a number of derived combinators defined purely in terms of existing combinators.
You probably noticed some similarities between implementations of different derived combinators across the combinator libraries we wrote.
For instance, we implemented a  functionmap for each data type, to lift a function taking one argument.
If a data type  implements  with a signature like above, we say that  is a F map F.
Which is to say that we can provide an implementation of the followingfunctor.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Much like we did with  in the previous chapter, we introduce a trait Foldable that is parameterized on a type constructor.
What can we do with this abstraction? There happens to be a small but useful library of functions that we can write using just.
It's all well and good to introduce a new combinator like this in the abstract, but we should think about what it  for concrete data types like , ,means Gen Option etc.
For example, if we  a , we get two lists of thedistribute List[(A, B)] same length, one with all the s and the other with all the s.
So we just wrote a generic unzip that works not just for lists, but for any functor!
Whenever we create an abstraction like this, we should consider not only what abstract methods it should have, but which  we expect should hold for thelaws implementations.
If you remember, back in chapter 7 (on parallelism) we found a general law for :map.
Later in Part 2, we found that this law is not specific to.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Not only did we implement  for many data types, we also implemented a map to lift a function taking two arguments.
Makes a generator of a random C that runs random generators ga and gb, combining their results with the function f.
Makes a parser that produces C by combining the results of parsers pa and pb with the function f.
Combines two Options with the function f, if both have a value.
These functions have more in common than just the name.
In spite of operating on data types that seemingly have nothing to do with one another, the implementations are exactly identical! And again the only thing that differs in the type signatures is the particular data type being operated on.
This confirms what we have been suspecting all along—that these are particular instances of some more general pattern.
We should be able to exploit that fact to avoid repeating ourselves.
For example, we should be able to write  once and for all in such amap2 way that it can be reused for all of these data types.
We've made the code duplication particularly obvious here by choosing uniform names for our functions, taking the arguments in the same order, and so on.
It may be more difficult to spot in your everyday work.
But the more combinator libraries you write, the better you will get at identifying patterns that you can factor out into a common abstraction.
In this chapter, we will look at an abstraction that unites , , , , and some other data typesParser Gen Par Option we have already looked at: They are all  We will explain in a momentmonads.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Is that a minimal set? Well, the data types in question all had a flatMap.
We will unify under aunit flatMap single concept all data types that have these combinators defined.
Since Monad provides a default implementation of map, it can extend Functor.
All monads are functors, but not all functors are monads.
But "monad" is already a perfectly good name in common use.
The name comes from category theory, a branch of mathematics that has inspired a lot of functional programming concepts.
The name "monad" is intentionally similar to "monoid", and the two concepts are related in a deep way.
To tie this back to a concrete data type, we can implement the  instanceMonad for :Gen.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We only need to implement  and  and we get  and  atunit flatMap map map2 no additional cost.
We have implemented them once and for all, for any data type for which it is possible to supply an instance of ! But we're just gettingMonad started.
There are many more combinators that we can implement once and for all in this manner.
Try to implement a  monad, see what issues you runMonad State into, and think about possible solutions.
Now that we have our primitive combinators for monads, we can look back at previous chapters and see if there were some other combinators that we implemented for each of our monadic data types.
Many of of them can be implemented once for all monads, so let's do that now.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We can implement this combinator for all monads  by addingM it to our  trait.
We should also give it a more generic name such as Monad.
For example, how does it behave in the  monad? What about ?M List Option Describe in your own words the general meaning of .replicateM.
There was also a combinator  for our  data type to take twoproduct Gen generators and turn them into a generator of pairs, and we did the same thing for.
In both cases, we implemented  in terms of .Par product map2 So we can definitely write it generically for any monad.
We don't have to restrict ourselves to combinators that we have seen already.
Now that we're thinking in theplay around abstract about monads, we can take a higher perspective on things.
We know that  and  let us combine pairs, or when we otherwisefactor map2 have  of two things.
But what about when we have  of two thingsboth either (sometimes called their )? What happens then? We would havecoproduct something like this:
The combinators we have seen here are only a small sample of the full library that  lets us implement once and for all.Monad.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In chapters past, we found that the  function, for all of the data types thatmap support it, obeys the functor law.
But what about ? Certainly we would expect the functor laws to alsoMonad hold for , but what else would we expect? What laws should govern Monad.
For example, if we wanted to combine three monadic values into one, which two would we expect to be combined first? Would it matter? To answer this question, let's for a moment take a step down from the abstract level and look at a simple concrete example of using the  monad.Gen.
Say we are testing a product order system and we need to mock up some orders.
We might have an  case class and a generator for that class.Order.
Above, we are generating the  inline, but there might be places where weItem want to generate an  separately.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
And that should do exactly the same thing, right? It seems safe to assume that.
How can we be sure? It's not exactly the same code.
Once you expand them out, those two implementations are clearly not identical.
And yet when we look at the -comprehension, it seems perfectly reasonable tofor assume that the two implementations do exactly the same thing.
In fact, it would be surprising and weird if they didn't.
It's because we are assuming that flatMap obeys an :associative law.
It's not so easy to see that the law we have discovered is an  law.associative Remember the associative law for monoids? That was very clear:
But our associative law for monads doesn't look anything like that! Fortunately, there's a way we can make the law clearer if we consider not the monadic values of types like , but monadic  of types like.
Footnote 1mThis name comes from category theory and is after the Swiss mathematician Heinrich Kleisli.
We can now state the associative law for monads in a much more symmetric.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
It seems that weflatMap compose have found another minimal set of monad combinators:  and .compose unit.
The other monad law is now pretty easy to see.
Just like  was an zero identity for  in a monoid, there is an identity element for  in aelement append compose.
Indeed, that is exactly what  is, and that is why we chose its name theunit way we did:
This function has exactly the right type to be passed to.
The effectcompose should be that anything composed with  is that same thing.
This usually takesunit the form of two laws,  and :left identity right identity.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The data types for which we've given monad instances don't seem to have very much to do with each other.
Yes,  factors out code duplicationMonad among them, but what  a monad exactly? What does "monad" ?is mean.
You may be used to thinking of interfaces as providing a relatively complete API for an abstract data type, merely abstracting over the specific representation.
After all, a singly-linked list and an array-based list may be implemented differently behind the scenes, but they will share a common interface in terms of which a lot of useful and concrete application code can be written.
The  combinators are often just a small fragment ofMonad the full API for a given data type that happens to be a monad.
So  doesn'tMonad generalize one type or another; rather, many vastly different data types can satisfy the  interface.Monad.
We have discovered that there are at least three minimal sets of primitive combinators, and instances of  will have to provideMonad Monad.
And we know that there are two monad laws to be satisfied, associativity and identity, that can be formulated in various ways.
So we can state quite plainly what a monad :is.
A monad is an implementation of one of the minimal sets of monadic combinators, satisfying the laws of associativity and identity.
It doesn't say very much about what it implies—what a monad.
In order to really  what's going on with monads (or with anything for that matter),understand we need to think about them in terms of things we already understand.
We need to connect this new knowledge into a wider context.
To say "this data type is a monad" is to say something very specific about how.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
But what exactly? To begin to answer the question of what monads , let's look at another couple of monads and compare their behavior.mean.
To really distill monads to their essentials, we should look at the simplest interesting specimen, the identity monad, given by the following type:
Applying Id Id to  is an identity since the wrapped type and the unwrapped type are totallyA isomorphic (we can go from one to the other and back again without any loss of information)
But what is the meaning of the identity ? Let's try using it inmonad the REPL:
When we write the exact same thing with a -comprehension, it might befor clearer:
So what is the  of  for the identity monad? It's simply variableaction flatMap substitution.
The variables  and  get bound to  and ,a b "Hello, " "monad!" respectively, and then they get substituted into the expression.
In fact, wea + b could have written the same thing without the  wrapper, just using Scala's ownId variables:
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
So now we have at least aId partial answer to the question of what monads mean.
We could say that monads provide a scheme for binding variables.
Stated more dramatically, a monad is a kind of programming language that supports variable substitution.
Let's see if we can get the rest of the answer.
Recall that we implementedState some combinators for , including  and :State map flatMap.
It looks like  definitely fits the profile for being a monad.
But its typeState constructor takes two type arguments and  requires a type constructor ofMonad one argument, so we can't just say.
But if we choose someMonad[State] particular  then we have something like , which is the kind ofS State[S, _] thing expected by.
So  doesn't just have one monad instance but aMonad State whole family of them, one for each choice of.
We would like to be able toS partially apply  to where the  type argument is fixed to be some concreteState S type.
This is very much like how you would partially apply a function, except at the type level.
For example, we can create an  type constructor, which is anIntState alias for  with its first type argument fixed to be :State Int.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
And  is exactly the kind of thing that we can build a  for:IntState Monad.
Of course, it would be really repetitive if we had to manually write a separate instance for each specific state type.
But instead we can use something similar to lambda syntax at the type level.
For example, we could have declared  directly inline like this:IntState.
This syntax can be a little jarring when you first see it.
But all we are doing is declaring an anonymous type within parentheses.
This anonymous type has, as one of its members, the type alias , which looks just like before.
We can use this trick to partially apply the  type constructor andState declare a  trait.
An instance of  is then a monadStateMonad StateMonad[S] instance for the given state type .S.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Again, just by giving implementations of  and , we getunit flatMap implementations of all the other monadic combinators for free.
What is the meaning of  in the  monad?replicateM State How does  behave? What about ?map2 sequence.
Let's now look at the difference between the  monad and the  monad.Id State Remember that the primitive operations on  (besides the monadicState operations  and ) are that we can read the current state with unit flatMap.
Remember that we also discovered that these combinators constitute a minimal set of primitive operations for.
What does this tell us about the meaning of the  ? Let's study aState monad simple example:
This function numbers all the elements in a list using a  action.
It keepsState a state that is an  which is incremented at each step.
We are then reversing the result since we constructed it in reverse order .2
Footnote 2mThis is asymptotically faster than appending to the list in the loop.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We aregetState setState for obviously getting variable binding just like in the  monad—we are binding theId value of each successive state action ( , , and then ) togetState acc setState variables.
But there is more going on, literally  At each line in thebetween the lines.
What does the difference between the action of  and the action of  tellId State us about monads in general? We can see that a chain of s (or anflatMap equivalent -comprehension) is like an imperative program with statements thatfor assign to variables, and the monad specifies what occurs at statement boundaries.
For example, with , nothing at all occurs except unwrapping and re-wrapping inId the  constructor.
With , the most current state gets passed from oneId State statement to the next.
With the  monad, a statement may return  andOption None terminate the program.
With the  monad a statement may return manyList results, which causes statements that follow it to potentially run multiple times, once for each result.
What are its primitive operations? What is the action of ? What meaning does it give toflatMap monadic functions like , , and ? What meaningsequence join replicateM does it give to the monad laws?
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In this chapter, we took a pattern that we had seen repeated throughout the book and we unified it under a single concept: monad.
This allowed us to write a number of combinators once and for all, for many different data types that at first glance don't seem to have anything in common.
We discussed laws that they all satisfy, the monad laws, from various perspectives, and we tried to gain some insight into what it all means.
An abstract topic like this cannot be fully understood all at once.
It requires an iterative approach where you keep revisiting the topic from new perspectives.
When you discover new monads, new applications of them, or see them appear in a new context, you will inevitably gain new insight.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In the previous chapter on monads we saw how a lot of the functions we have been writing for different combinator libraries can be expressed in terms of a single interface, .Monad.
Monads provide a very powerful interface, as evidenced by the fact that we can use  to essentially write imperative programs in a purely functional way.flatMap But sometimes this is more power than we need, and such power comes at a price.
As we will see in this chapter, the price is that we lose some compositionality.
We can reclaim it by instead using , which are simpler and moreapplicative functors general than monads.
By now you have seen  and  many times for differentsequence traverse monads, implemented them in terms of each other, and generalized them to any monad :M.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
What you may not have noticed is that almost all of the useful combinators we wrote in the previous chapter can be implemented just in terms of  and map2 unit.
What's more, for all the data types we have discussed,  can be written map2
No it's not, because there are monadic combinators such as  and join flatMap that cannot be implemented with just  and.
Since so many combinators can be written in terms of just these two functions, there is a name for data types that can implement  and.
There is an additional combinator, , that we haven't yet discussed.
The apply and  combinators can be implemented in terms of each other, so amap2 apply.
Here it is in terms of  and :map2 apply unit apply unit.
You should already have a good sense of what  means.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Let's drop down to  and look at a concreteOption implementation:
But one of themOption contains a  (unless it is  of course)
The action of  is to applyfunction None apply the function inside one argument to the value inside the other.
In the case of the application idiom from the previous chapter, the implementation is literally justIdentity.
In the example above, the idiom is  so the methodOption additionally encodes the rule for , which handles the case where either theNone function or the value are absent.
Both are a kind of functionapply map application in a context, but there's a very specific difference.
For example,apply if the second argument to  is  in the case of  then there is noapply None Option function at all.
But in the case of , the function must exist independently of themap context.
This is easy to see if we rearrange the type signature of  a little andmap compare it to :apply.
The only difference is the  around the function argument type.
The F apply function is strictly more powerful, since it can have that added  effect.
It makesF sense that we can implement  in terms of , but not the other waymap apply around.
We have also seen that we can implement  as well in terms of .map2 apply We can extrapolate that pattern and implement , , etc.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
If we Curry this function, we get a slightly different type.
It's the same function, but one that can be gradually applied to arguments one at a time.
Pass that to  (which is just  in the case of ) and we have:unit Some Option.
We can now use idiomatic function application three times to apply this to three values.
We just Curry the the function we want to lift, pass the result to , and then  as many times as there are arguments.
Another interesting fact is that we have just discovered a new minimal definition of ! We already know that a monad can be given byMonad implementations of , , and.
Well, since we can implement  inmap unit join map terms of , we now know that a  can be given by , , andapply Monad unit join either  or.
This gives us further insight into what a monad is! Amap2 apply monad is just an applicative functor with an additional combinator, .join.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Since weall monads are applicative functors have already implemented  instances for a great many data types, we don'tMonad have to write  instances for them.
We can simply change Applicative Monad so that it extends  and overrides :Applicative apply.
But the converse is not true—not all applicative functors are monads.
Using together with either  or , we cannot implement  or unit map2 apply flatMap.
Let's look at the signatures of  and  side by side, rearrangingflatMap apply the signature of  slightly from what we're used to in order to make thingsflatMap clearer:
The difference is in the type of the function argument.
So the only way to pass the  from the secondF A argument to the function of type  in the first argument is to somehowA => B consider both  contexts.
For example, if  is  we pattern-match on bothF F Option arguments to determine if they are  or :Some None.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Likewise with  the function argument  is only invoked if both of the map2 f arguments are.
The important thing to note is that whether theOption Some.
But in , the second argument is a function that produces an , and itsflatMap F structure  on the value of the  from the first argument.
So in the depends A Option case, we would match on the first argument, and if it's  we apply the functionSome.
And whether  result is  or f Option[B] that Some None actually depends on the value inside the first :Some.
One way to put this difference into words is to say that applicative operations while monadic operations may alter a structure.
But if you flatMap over a list with three elements, you may get many more since each function application can introduce a whole  of new values.List.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
To further illustrate the difference between applicative functors and monads, getting a better understanding of both in the process, we should look at an example of an applicative functor that is not a monad.
In chapter 4, we looked at the  data type and considered the question ofEither how such a data type would have to be modified to allow us to report multiple errors.
To make this concrete, think of validating a web form submission.
You would want to give an error to the user if some of the information was entered incorrectly, and you would want to tell the user about all such errors at the same time.
It would be terribly inefficient if we only reported the first error.
The user would have to repeatedly submit the form and fix one error at a time.
This is the situation with  if we use it monadically.
First, let's actuallyEither write the monad for the partially applied  type.Either.
Now consider what happens in a sequence of s like this, where eachflatMap of the functions , , and validateEmail validPhone validatePostcode has type  for a given type :Either[String, T] T.
If  fails with an error, then  and validName validBirthdate won't even run.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We could run the validation functions in any order, and there's no reason we couldn't run them all first and then combine any errors we get.
ButList Vector the  monad we have is not sufficient for this.
Notice that in the case of  there isFailure Failure always at least one error, stored in.
This data will likely be collected from the user as strings, and we must make sure that the data meets a certain specification, or give a list of errors to the user indicating how to fix the problem.
The specification might say that  cannotname be empty, that  must be in the form , and that birthdate "yyyy-MM-dd"
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
And to validate an entire web form, we can simply lift the WebForm constructor with :apply.
If any or all of the functions produce , the whole Failure validWebForm method will return all of those failures combined.
What sort of laws should we expect applicative functors to obey? Well, we should definitely expect them to obey the functor law:
What does this mean for applicative functors? Let's remind ourselves of our implementation of :map.
This definition of  simply shows that an applicative functor  in amap is a functor specific way.
And this definition, together with the functor law, imply the first applicative law.
Putting the identity functionunit through  and then  results in the identity function itself.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The second thing that the laws demand of applicative functors is that they.
This is stated as the :preserve function composition composition law.
Both sides of the  sign are applying  to  and then applying  toF[A] == g x f the result of that.
In other words they are applying the composition of  and  to .f g x All that this law is saying is that these two ways of lifting function composition should be equivalent.
In fact, we can write this more tersely using :map2
We're lifting the higher-order function  which, given the arguments , , and  will_(_(_)) f g x return.
And the composition law essentially says that even though theref(g(x)) might be different ways to implement  for an applicative functor, they shouldmap3 all be equivalent.
The two remaining applicative laws establish the relationship between unit and.
Theapply homomorphism interchange homomorphism law states that passing a function and a value through ,unit followed by idiomatic application, is the same as passing the result of regular application through :unit.
We know that passing a function to  and then  is the same asunit apply simply calling , so we can restate the homomorphism law:map.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The  completes the picture by saying that  should haveinterchange law unit the same effect whether applied to the first or the second argument of :apply.
The interchange law can be stated a different way, showing a relationship between  and  with regard to lifted function application:map2 map.
Just like the monad laws, these are simple sanity checks that the applicative functor works in the way that we would expect.
They ensure that , , , and  behave in aapply unit map map2 consistent manner.
It is not possible,compose Monad but it is instructive to attempt it and understand why this is the case.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Are there data types otherList Foldable than  that are ? Of course!List traversable.
But traversable data types are too numerous for us to write specialized and  methods for each of them.
This is as general as a typecommutes F M M[F[A]] signature gets.
What  is saying is that  can be swapped with an Traverse[F] F M inside of it, as long as that  is an applicative functor.
We have seen lots ofM examples of this in past chapters, but now we are seeing the general principle.
The  method, given an  full of s, will turn each  into an traverse F A A M[B] and then use the applicative functor to combine the s into an  full of s.M[B] F B.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
At this point you might be asking yourself what the difference is between a traversal and a fold.
Both of them take some data structure and apply a function to the data within in order to produce a result.
For example, when traversing a  with an -valued function, weList Option would expect the result to always either be  or to contain a list of the sameNone length as the input list.
This sounds like it might be a law! But we can choose a simpler applicative functor than  for our law.
We already know that  is a monad so it's also an applicative functor:Id.
Then our law, where  is an  for some , can be writtenxs F[A] Traverse[F] like this:
If we replace  with  here, then this is just the functor identitytraverse map law! This means that in the context of the  applicative functor,  and Id map.
This implies that  can extend traverse Traverse and we can write a default implementation of  in terms of Functor map.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
A valid instance of  must then override at least either  orTraverse sequence .traverse.
But what is the relationship between  and ? The answerTraverse Foldable involves a connection between  and .Applicative Monoid.
Suppose that our  were a type constructor  that takes any type to M ConstInt , so that  throws away its type argument  and just gives us Int ConstInt[A] A :Int.
Then in the type signature for , if we instantiate  to be traverse M ConstInt , it becomes:
Indeed, if  is something likefoldMap Foldable F then what we need to implement this signature is a way of combining the List.
Indeed, given a constant functor like above, we can turn any  into an Monoid :Applicative.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
This is ConstInt generalized to any A, not just Int.
For this reason, applicative functors are sometimes called "monoidal functors"
The operations of a monoid map directly onto the operations of an applicative.
This means that  can extend  and we can give a defaultTraverse Foldable.
Note that  now extends both   !Traverse Foldable and Functor Importantly,  itself cannot extend.
Even though it's possibleFoldable Functor to write  in terms of a fold for most foldable data structures like , it is notmap List possible .in general.
Can you think of a  thatFoldable Functor Foldable is not a functor?
So what is  really for? We have already seen practical applications ofTraverse particular instances, such as turning a list of parsers into a parser that produces a list.
But in what kinds of cases do we want the ? What sort ofgeneralization generalized library does  allow us to write?Traverse.
Using a State State action to  a collection, we can implement complex traversals that keeptraverse some kind of internal state.
There's an unfortunate amount of type annotation necessary in order to partially apply  in the proper way, but traversing with  is common enoughState State that we can just have a special method for it and write those type annotations once and for all:
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
To demonstrate this, here is a  traversal that labels every element withState its position.
By the same token, we can keep a state of type , to turn anyList[A] traversable functor into a List:
Add the current element and set the new list as the new state.
It begins with the empty list  as the initial state, and at every element in theNil traversal it adds it to the front of the accumulated list.
This will of course construct the list in the reverse order of the traversal, so we end by reversing the list we get from running the completed state action.
Note that we  because in thisyield () instance we don't want to return any value other than the state.
AndtoList zipWithIndex in fact most traversals with  will follow this exact pattern: We get theState current state, compute the next state, set it, and yield some value.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
It should obey the following law, for all  and  of the appropriate types:x y.
It is in the nature of a traversal that it must preserve the shape of its argument.
This is well demonstrated when we try to combine two structures into one.
Given  can we combine a value of some type  andTraverse[F] F[A] another of some type  into an ? We could try using  to writeF[B] F[C] mapAccum a generic version of :zip.
Notice that this version of  is not able to handle arguments of differentzip "shapes"
For example if  is  then it can't handle lists of different lengths.
InF List this implementation, the list  must be at least as long as.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In the case of  for example, the result of  will have the shape of the List zipR argument, and it will be padded with  on the left if  is longer than .fb None fb fa.
In the chapter on streams and laziness we talked about how multiple passes over a structure can be fused into one.
In the chapter on monoids, we looked at how we can use monoid products to carry out multiple computations over a foldable structure in a single pass.
Using products of applicative functors, we can likewise fuse multiple traversals of a traversable structure.
This function will, given two functions  and , traverse  a singlef g fa time, collecting the results of both functions at once.
Not only can we use composed applicative functors to fuse traversals, traversable functors themselves compose.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
As we saw earlier in this chapter,  instances always compose, but  instances do not.Applicative Monad If you tried before to implement general monad composition, then you would have found that in order to implement  for nested monads  and , you wouldjoin M N have to write something of a type like.
Then we can join the adjacent  layers as well as the adjacent M[M[N[N[A]]]] M layers using their respective  instances.N Monad.
Expressivity and power often comes at the price of compositionality and modularity.
The issue of composing monads is often addressed with a custom-written version of each monad that is specifically constructed for composition.
And the general strategyOption of taking advantage of  works only with traversable functors.
ToTraverse compose with  for example (which cannot be traversed), a specialized State.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
There is no generic compositionStateT strategy that works for every monad.
See the chapter notes for more information about monad transformers.
Applicative functors are a very useful abstraction that is highly modular and compositional.
The functions  and  allow us to lift functions and values,unit map while  and  give us the power to lift functions of higher arities.
Together,  and  let us construct complexApplicative Traverse nested and parallel traversals out of simple elements that need only be written once.
This is in stark contrast to the situation with monads, where each monad's composition with others becomes a special case.
It's a wonder that monads have historically received so much attention and applicative functors have received so little.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In this chapter, we will introduce the I/O monad (usually written 'the  monad'),IO which extends what we've learned so far to handle , like writing to aexternal effects file, reading from a database, etc, in a purely functional way.
It provides the most straightforward way of embedding imperative programming into FP, while preserving referential transparency and keeping pure code separate from what we'll call  code.
We will be making an important distinction here in this chaptereffectful between  and .effects side effects It illustrates a key technique for dealing with external effects—using pure functions to compute a  of an imperative computation, which is then executed by adescription separate.
This is a powerful technique we'll be using throughout part 4; part of our goal is to equip you with the skills needed to begin crafting your own descriptions for interesting effectful programs you are faced with.
We are going to work our way up to the  monad by first considering a veryIO simple example, one we discussed in chapter 1 of this book:
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In chapter 1, we factored out the logic for computing the winner into a function separate from displaying the winner:
We commented in chapter 1 that it was  possible to factor an impurealways function into a pure 'core' and two functions with side effects, (possibly) one that produces the pure function's input and (possibly) one that accepts the pure function's output.
In this case, we factored the pure function  out of winner.
Interestingly, we can continue this sort of factoring—the printWinner function is also doing two things—it is computing a message, and then printing that message to the console.
We could factor out a pure function here as well, which might be beneficial if later on we decide we want to display the winner message in some sort of UI or write it to a file instead:
These might seem like silly examples, but the same principles apply in larger, more complex programs and we hope you can see how this sort of factoring is quite natural.
We aren't changing what our program does, just the internal details of how it is factored into smaller functions.
We can evenfunction with side effects is a pure function waiting to get out formalize this a bit.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We will extend this to handle 'input' side effects shortly.
For now, though, consider applying this strategy repeatedly to a program.
Each time we apply it, we make more functions pure and push side effects to the outer layers of the program.
We sometimes call these impure functions the 'imperative shell' around the pure core of the program.
Eventually, we reach functions that seem to  sidenecessitate effects like the built-in , which has type.
It turns out that even functions like  can be factored into a pure core, byprintln introducing a new data type we'll call :IO.
Our  function is now pure—it returns an  value, whichprintWinner IO simply describes an action that is to take place without actually  it.
Weexecuting say that  has or produces an  or is , but it is only theprintWinner effect effectful interpreter of  (its  function) that actually has  effects.IO run side.
Other than technically satisfying the requirements of FP, has the  typeIO actually bought us anything? This is a subjective question, but as with any other data type, we can access the merits of  by considering what sort of algebra itIO provides—is it something interesting, from which we can define a large number of useful operations and assemble useful programs, with nice laws that give us the ability to reason about what these larger programs will do? Not really.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The only thing we can say about  as it stands right now is that it forms a IO (  is the identity, and  is the associative operation)
All it seems to have given us is the ability to delay when a side effect gets 'paid for'
Now we will let you in on a secret—you, as the programmer, get to invent whatever API you wish to represent your computations, including those that interact with the universe external to your program.
This process of crafting pleasing, useful, and composable descriptions of what you want your programs to do is at its core.
You are crafting a little language and anlanguage design associated  that will allow you to express various programs.
If you don'tinterpreter like something about this language you have created, change it! You should approach this task just like any other combinator library design task, and by now you've had plenty of experience designing and writing such libraries.
As we have seen many times before, sometimes, when building up your little language, you will encounter a program that cannot be expressed.
So far, our IO type can represent only 'output' effects.
There is no way to express IO computations that must, at various points, wait for input from some external source.
Suppose we would like to write a program that prompts the user for a temperature in degrees fahrenheit, then converts this value to celsius and echoes it to the user.
Footnote 2mWe are not doing any sort of error handling here.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Unfortunately, we run into problems if we want to make  into aconverter pure function that returns an :IO.
In Scala,  is a  with a side effect of capturing a line of inputreadLine def from the console.
We could wrap a call to this in an , butString IO we have nowhere to put the result! We don't yet have a way of representing this sort of effect with our current  type.
The problem is we cannot express IO IO computations that  of some meaningful type—our interpreter of yield a value IO just produces  as its output.
Should we give up on our  type and resort toUnit IO using side effects? Of course not! We extend our  type to allow , by addingIO input a type parameter to :IO.
Notice we've added IO and  functions so  can be used in for-comprehensions.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Our  definition no longer has side effects—it is a RT converter description of a computation that will have side effects when interpreted via.
And because it forms a , we can use all theconverter.run Monad combinators we've written previously.
Here's a larger example, an interactive program which prompts the user for input in a loop, then computes the factorial of the input.
This code uses a few  functions we haven't seen yet, , ,Monad when foreachM.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The details of this code aren't too important; the point here is just to demonstrate how we can embed an imperative programming language into the purely functional subset of Scala.
All the usual imperative programming tools are here—we can write loops, perform I/O, and so on.
If you squint, it looks a bit like normal imperative code.
Imperative factorial using a mutable IO reference Allocation a mutable reference Modify reference in a loop Dereference to obtain the value inside a reference See sidebar.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
You mayMonad want to think about what these combinators mean for types other than.
Notice that not all these combinators make sense for every monadicIO type.
For instance, what does  mean for ? For forever Option.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The  monad is a kind of least common denominator for expressing programsIO with external effects.
When programming  the  monad, wewithin IO have many of the same problems and difficulties as we would in ordinary imperative programming, which has motivated functional programmers to search for more composable ways of describing effectful programs.
We can store them in lists, pass them to functions,values create them dynamically, etc.
Any common pattern we notice can be wrapped up in a function and reused.
This is one reason why it is sometimes argued that functional languages provide , as compared tomore powerful tools for imperative programming languages like C or Java.
Reifying  computations as values means we can craft a more interesting interpreterIO than the simple -based "interpreter" baked into the  type itself.
Later on in thisrun IO chapter, we will build a more refined  type and sketch out an interpreter that usesIO nonblocking I/O in its implementation.
Interestingly, client code like our converter example remains identical—we do not expose callbacks to the programmer at all! They are entirely an implementation detail of our  interpreter.IO.
An  is a completely opaqueIO IO[A] description of a computation that yields a single.
Most of the generalA purpose combinators for  are functions that can be defined for anyIO monad—  itself is not contributing anything new, which means weIO only have the monad laws to reason with.
What this means in practice is that we generally need to know something more about an  than just its type to compose it withIO[A] other computations (contrast this with, say, )
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In this section, we are going to explore what an   and introduce aIO[A] means more nuanced  type that clarifies what is actually going on in a functionalIO program that performs I/O.
A primary purpose here is to get you thinking, and to make it clear how it is possible to craft more interesting interpreters of  than theIO simple one we baked into the  function.
Don't worry too much about followingrun absolutely everything in this section.
With that said, let's see what we can learn about the meaning of.
You mayIO have noticed that our  type is roughly equivalent to our  type, introduced inIO Id chapter 13 as the simplest possible type that gives rise to a monad.
Aside from the fact that  is non-strict, and that retrieving the value is doneIO using  instead of , the types are identical! Our "IO" type is just arun value non-strict value.
We are relying on the fact thatIO Scala allows unrestricted side effects at any point in our programs.
But let's think about what happens when we evaluate the  function of an.
Duringrun IO evaluation of , the pure part of our program will occasionally make requests ofrun the outside world (like when it invokes ), wait for a result, and thenreadLine pass this result to some further pure computation (which may subsequently make some further requests of the outside world)
Our current  type is completelyIO inexplicit about where these interactions are occurring.
But we can model these interactions more explicitly if we choose:
This type separates the pure and effectful parts of an  computation.
An  can be a pure value, or it can beMonad IO[A] a request of the external computation.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We can think of  much like an expression of type , but it is an expression that isExternal[I] I "external" that must be evaluated by whatever program is running this  action.IO The  function defines what to do when the result of the request becomesreceive available, it is sometimes called the.
We'll see a bit later how this cancontinuation be exploited to write an interpreter for  that uses nonblocking I/O internally.IO.
The simplest possible representation of  would be simply aExternal[I] nonstrict value:6
Footnote 7mOf course, Scala will not technically prevent us from invoking a function with side effects at any point in our program.
This discussion assumes we are following the discipline of not allowing side effects unless this information is tracked in the type.
With this representation, we canExternal F define an  type that grants access to exactly the effects we want:F.
Now an  is an  computation that can only read from andIO[Console,A] IO write to the console.
We can introduce other types for different I/O capabilities—a8 file system , granting read/write access (or even just read access) to the fileF.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Notice, interestingly, that nothing about  impliesConsole that any side effects must actually occur! That is a property of the  of interpreter F values now required to actually run an :IO[F,A] Footnote 8m.
A completely valid  could ignore  requests andRun[Console] PrintLine always return the string  in response to  requests:"Hello world!" ReadLine.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
With this definition, the interpreter is no longer tail recursive, and it is up to the to ensure that the chains of  calls built up duringMonad[F] flatMap.
Footnote 10mNote on this signature: when passing a  or  to a function, we typicallyMonad[F] Monad[G] give the variable the same name as its type parameter (  or  in this case)
These last two exercises demonstrate something rather amazing—there is nothing about our  type nor our  or  which require side effects of anyIO Run[F] F.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
From the perspective of our  programs, we cannot even  anIO distinguish interpreter that uses 'real' side effects from one like  that doesn't.
With our  type, the behavior of this external interpreter is modeled only in aIO very opaque way—we say that each evaluation we request from the interpreter results in a new interpreter state and beyond that make no assumptions about how the sequence of requests made will affect future behavior.
Footnote 11mOur definition for an arbitrary  is telling us the same thing, since a Monad[F] Monad[F] provides us with a strategy for sequencing programs, but makes no additional assumptions about the nature of this sequencing.
In this section we will develop an  type and associated interpreter that usesIO nonblocking I/O internally.
Footnote 12mSee chapter notes for more discussion and links to further reading on this.
In this section, we willIO develop an  type and a  function that uses.
By this weIO run constant stack space mean that the stack usage of  is not dependent on the input in any way.
Torun start, let's look at a few examples of  programs that can be problematic.
We'llIO make use of a helper function, , which constructs an IO.apply.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Here are some perfectly reasonable  programs that can present problems:IO.
For those that fail, can you explain why? It can be a little difficult to tell what's going on just by looking at the stack trace.
You may want to instead trace on paper the evaluation of these expressions until a pattern becomes clear.
To fix the problem, we are going to build what is called a  versiontrampolined of.
Footnote 13mWe are not going to work through derivation of this type, but if you are interested, follow the references in the chapter notes for more information.
What is ? First, note that we can extract an  from a Trampoline A using a tail-recursive function, :Trampoline[A] run.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
A valid implementation willmap flatMap Trampoline never evaluate  or call  in the implementation of  or .force() run flatMap map.
Can you see how it works?  lets us return control to the  function, andMore run represents a call to  as an ordinary value.
The key to making it work is that we reify  calls as data,flatMap but associate these calls to the right, which lets us implement the  function as arun tail-recursive function.
There is some overhead to using it, but its advantage is that we gain predictable stack usage.14
Footnote 14mThere are some interesting optimizations to this scheme—it isn't necessary to return to the central loop after every function call, only periodically to avoid stack overflows.
We can now add the same trampolining behavior directly to our  type:IO.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We have a  constructor, as before, but we now have two additional More Bind constructors.
OnceMonad IO again, a correct implementation of  will not invoke  or .flatMap run force() Test your implementation using the examples we gave above.
Can you see why it works? Again, you may want to try tracing their execution until the pattern becomes clear, or even construct a proof that stack usage is guaranteed to be bounded.
We can use this to expose infix syntax for all the various  combinators—we just add the following function toMonad the  companion object:IO.
The  type used for  frequently includes operations that can take a long time toF IO complete and which do not occupy the CPU, like accepting a network connection from a server socket, reading a chunk of bytes from an input stream, writing a large number of bytes to a file, and so on.
Our current   function will wait idly forIO run these operations to complete:
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Thenonblocking details of these libraries vary, but to give the general idea, a nonblocking  ofsource bytes might have an interface like this:
We give requestBytes a callback function indicating what to do when the resultrequestBytes.
Using this sort of library directly is quite painful, for obvious reasons.15
Footnote 15mEven this API is rather nicer than what is offered directly by the  package in Java (nio API link ), which supports nonblocking I/O.
One of the nice things about making I/O computations into values is that we can build more interesting interpreters for these values than the default 'interpreter' which performs I/O actions directly.
Here we will sketch out an interpreter that uses nonblocking I/O internally.
The ugly details of this interpreter are completely hidden from code that uses the  type, and code that uses  can be written in aIO IO much more natural style without explicit callbacks.
Recall that earlier, we implemented a function with the following signature:
The version we use here can be backed by a nonblocking I/O primitive.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Footnote 16mThis definition can be extended to handle timeouts, cancellation, and deregistering callbacks.
We won't be discussing these extensions here, but you may be interested to explore this on your own.
Future looks almost identical to , except we have replaced the IO Request and  cases with  and  constructors.
We will only be using it here as an implementation detail of the run function for  (this could be enforced using access modifiers).IO.
Also implement , for anIO runAsync asynchronous evaluator for , and , the synchronous evaluator:Future run.
With this in place, we can asynchronously evaluate any  to a IO[Future,A]
But what do we do if we have an , say, and weFuture[A] IO[Console,A]
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
With this in place, we can now write the following to asynchronously evaluate an :IO[Console,A]
That's it! In general, for any  type, we just require a way to translate that  to aF F and we can then evaluate  programs asynchronously.
Footnote 17mRecall that  doesn't do anything special with its argument—the argument willFuture.unit still be evaluated in the main thread using ordinary function calls.
But because we are returning a , we have more flexibility.
Here, weFuture delegate  interpretation to a shared thread pool, to avoid blocking theReadLine main computation thread while waiting for input.
Notice that nothing requires us to implement nonblocking calls everywhere in our interpreter—  uses thePrintLine ordinary blocking  call directly (with the assumption that submitting thisprintln as a task to a thread pool is not worth the overhead)
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In the  companion object, we define an  function, whichFuture apply evaluates its argument in a thread pool.
It returns a  which notifies anyLater listeners once the result is available.
The implementation is somewhat involved; see the answer code for this chapter if you are interested.
What is remarkable is that regardless of the implementation, we get to retain the same straightforward style in code that uses , rather than being forced toIO program with callbacks directly.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Despite the fancy implementation of our  interpreter and the advantage ofIO having first-class  values, the  type fundamentally present us the same levelIO IO of abstraction as ordinary imperative programming.
This means that writing efficient, streaming I/O will generally involve monolithic loops.
This works, although it requires loading the contents of fahrenheit.txt entirely into memory to work on it, which could be problematic if the file is very large.
We would prefer to perform this task using roughly constant memory—read a line or a fixed size buffer full of lines from , convert tofarenheit.txt celsius, dump to , and repeat.
To achieve this efficiency we couldcelsius.txt expose a lower-level file API that gives access to I/O handles:
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The only problem with this is we now need to write a monolithic loop:
There's nothing inherently wrong with writing a monolithic loop like this, but it's not composable.
Suppose we decide later that we'd like to compute a 5-element moving average of the temperatures.
Modifying our  function to do thisloop would be somewhat painful.
Compare that to the equivalent change we'd make to our -based code, where we could define a  function and justList movingAvg stick it before or after our conversion to celsius:
Also, if we have a  instead of just aFoldable group monoid it can be implemented more efficiently.
The point to all this is that programming with a composable abstraction like is much nicer than programming directly with the primitive I/O operations.List.
Lists are not really special—they are just one instance of a composable API that is pleasant to use.
We should not have to give up all the nice compositionality we've.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Ifwhatever we like the metaphor of lists or streams, we can find a way to encode a list-like API for expressing I/O computations.
If we invent or discover some other composable abstraction, we can often find some way of using that.
This chapter introduced the simplest model for how external effects and I/O can be handled in a purely functional way.
We began with a discussion of effect-factoring and demonstrated how effects can be moved to the outer layers of a program.
From there, we defined two  data types, one with a simple interpreter built into theIO type, and another which made interactions with the external universe more explicit.
This second representation allowed us to implement a more interesting interpreter of  values that used nonblocking I/O internally.IO.
The  monad is not the final word in writing effectful programs.
It isIO important because it represents a kind of lowest common denominator.
We don't normally want to program with  directly, and in chapter 15 we will discuss howIO to build nicer, more composable abstractions.
Before getting to that, in our next chapter, we will apply what we've learned so far to fill in the other missing piece of the puzzle:
At various placeslocal effects throughout this book, we've made use of local mutation rather casually, with the assumption that these effects were not.
Next chapter we explore whatobservable this means in more detail, show more example usages of local effects, and show how  can be enforced by the type system.effect scoping.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In the first chapter of this book we introduced the concept of referential transparency, setting the premise for purely functional programming.
We declared that pure functions cannot mutate data in place or interact with the external world.
In the previous chapter on I/O, we learned that this is not exactly true.
We can write purely functional and compositional programs that describe interactions with the outside world.
These programs are unaware that they can be interpreted with an evaluator that has side-effects.
In this chapter, we will develop a more mature concept of referential transparency.
We will consider the idea that effects can occur  inside anlocally expression, and that we can guarantee that no other part of the larger program can observe these effects occurring.
We will also introduce the idea that expressions can be referentially transparent some programs and not others.with regard to.
Up until this point, you may have had the impression that in purely functional programming we're not allowed to use mutable state.
But if we look carefully, there is nothing about the definitions of referential transparency and purity that disallows mutation of  state.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
By that definition, the following function is pure, even though it uses a while loop, an updatable , and a mutable array:var.
The  function sorts a list by turning it into a mutable array, sortingquicksort the array in place using the well-known Quicksort algorithm, and then turning the array back into a list.
It's not possible for any caller to know that the individual subexpressions inside the body of  are not referentially transparent orquicksort that the local methods , , and  are not pure, because at noswap partition qs point does any code outside the  function hold a reference to thequicksort mutable array.
Since all of the mutation is locally scoped, the overall function is.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Some algorithms, like Quicksort, need to mutate data in place in order to work correctly or efficiently.
Fortunately for us, we can always safely mutate data that is created locally.
Any function can use side-effecting components internally and still present a pure external interface to its callers.
But working with mutable data can force us to take a monolithic approach.
For example, the constituent parts of  above would have directquicksort side-effects if used on their own, which makes it impossible to compose them in the same way that we would compose pure functions.
Of course, we could just make them work in , but that's really not appropriate for local mutable state.
We want to be able to distinguish between effects that areIO safe to run (like locally mutable state) and external effects like I/O.
The most natural approach is to make a little language for talking about mutable state.
Writing and reading a state is something we can already do with the.
What we'll pass instead is a kind of token marked with the type.
A function called with the token then has the authority to mutate data that isS tagged with the same type .S.
This new data type will employ Scala's type system to gain two static guarantees.
That is, we want code that violates these invariants to :not complile.
A mutable object can never be observed outside of the scope in which it was created.
If we hold a reference to a mutable object, then nothing can observe us mutating it.
We will call this new local effects monad , which could stand for "StateST Thread", "State Transition", "State Token", or "State Tag"
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Cache the value in case run is called more than once.
The reason the  method is protected is that an  represents the ability to run S state, and we don't want the mutation to escape.
We will start byST answering the question of how we specify the initial state.
As always, don't feel compelled to understand every detail of the implementation of.
What matters is the idea that we can use the type system toST constrain the scope of mutable state.
Our first example of an application for the  monad is a little language for talkingST about mutable references.
This takes the form of a combinator library with some primitive combinators.
The language for talking about mutable memory cells should have these primitive commands:
Allocate a new mutable cell Write to a mutable cell Read from a mutable cell.
The data structure we'll use for mutable references is just a wrapper around a protected :var.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The methods on  to read and write the cell are pure since they just returnSTRef actions.
Notice that the type  is  the type of the cell that's being mutated,ST S not.
Nevertheless, in order to call S apply and actually run one of these  actions, you do need to have a value of type .ST S That value therefore serves as a kind of token—an authorization to mutate or access the cell, but it serves no other purpose.
The question of how to give an initial state is answered by the  methodapply on the  companion object.
The  is constructed with an initial valueSTRef STRef for the cell, of type.
But what is returned is not a naked , but an  actionA STRef ST that constructs the  when run.
That is, when given the token of type .STRef S.
It's a little awkward rightST now because we have to choose a type  arbitrarily.
This little program allocates two mutable  cells, swaps their contents, addsInt.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
By now you will have figured out the plot with the  monad.
The plan is to use ST to build up a computation that, when run, allocates some local mutable state,ST.
The whole computation is referentially transparent because all the mutable state is private and locally scoped.
For example,guarantee an  contains a mutable  and we want Scala's type system to guaranteeSTRef var that we can never extract an  out of an  action because that would violateSTRef ST the invariant that the mutable reference is local to the  action, violatingST referential transparency in the process.
So how do we safely run  actions? First we must differentiate betweenST actions that are safe to run and ones that aren't.
ST[S, STRef[S, Int]] (not safe to run) ST[S, Int] (completely safe to run)
The former is an  action that contains a mutable reference.
A value of type  is quite literally just an , evenST[S,Int] Int though computing the  may involve some local mutable state.
Fortunately forInt us, there's an exploitable difference between these two types.
We want to disallow running an action of type ST[S, STRef[S,A]] because that would expose the.
And in general we want to disallowSTRef running any  where  involves the type.
On the other hand, it's easy toST[S,T] T S see that it should always be safe to run an  action that doesn't expose a mutableST object.
If we have such a pure action of a type like , it should be safeST[S,Int] to pass it an  to get the  out of it.
In order to represent this, we will introduce a new trait that represents ST actions that are safe to run.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
This is very similar to the idea behind the  trait from the previousTrans chapter.
A value of type  is a function that takes a   andRunnableST[A] type S produces a  of type .value ST[S,A]
In the section above we arbitrarily chose  as our  type.
Let's insteadNothing S wrap it in  making it polymorphic in.
Then we do not have toRunnableST S choose the type  at all.
We are now ready to write the  function that will call  on anyrunST apply polymorphic  by arbitrarily choosing a type for.
Since  isrunST ST run protected on the  trait, it's accessible from the companion object but nowhereST else:
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The expression  uses mutable state internally but it does not haverunST(p) any side-effects.
As far as any other expression is concerned, it's just a pair of integers like any other.
It will always return the same pair of integers and it will do nothing else.
Most importantly, we  run acannot program that tries to return a mutable reference.
In this example, we arbitrarily choose  just to illustrate the point.
TheNothing point is that the type  is bound in the  method, so when we say S apply new.
And this is guaranteed by Scala's type system! As a corollary, the fact that you cannot get an  out of an  action guaranteesSTRef ST that if you have an  then you are inside of the  action that created it, soSTRef ST it's always safe to mutate the reference.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The wildcard type is an artifact of Scala's interoperability with Java's type system.
Fortunately, when you have an , using itSTRef[_,Int] will cause a type error:
This type error is caused by the fact that the wildcard type in ref represents some concrete type that only  knows about.
In this caseref it's the  type that was bound in the  method of the S apply RunnableST where it was created.
Scala is unable to prove that this is the same type as.
Therefore, even though it's possible to abuse the wildcard type toR get the naked  out, this is still safe since we can't use it to mutateSTRef or access the state.
Mutable references on their own are not all that useful.
In this section we will define an algebra for manipulating mutable arrays in the  monad and then write an in-place QuickSort algorithmST compositionally.
We will need primitive combinators to allocate, read, and write mutable arrays:
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Write a value at the give index of the array Read the value at the given index of the array Turn the array into an immutable list Construct an array of the given size filled with the value v.
A thing to note is that Scala cannot create arrays for every type.
It requires that there exist a  for the type in implicit scope.
Scala's standard libraryManifest provides manifests for most types that you would in practice want to put in an array.
The  function simply gets that manifest out of implicit scope.implicitly.
Just like with s, we always return s packaged in an  actionSTRef STArray ST with a corresponding  type, and any manipulation of the array (even reading it), isS an  action tagged with the same type.
Using these primitives, we can write more complex functions on arrays.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Not everything can be done efficiently using these existing combinators.
For example, the Scala library already has an efficient way of turning a list into an array.
For example, the quicksort ST function that swaps two elements of the array:swap.
With those components written, quicksort can now be assembled out of them in the  monad:ST.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
As you can see, the  monad allows us to write pure functions thatST nevertheless mutate the data they receive.
Scala's type system ensures that we don't combine things in an unsafe way.
Come up with a minimal set of primitive combinators for creating and manipulating hash maps.
In the preceding section, we talked about effects that are not observable because they are entirely local to some scope.
There are no programs that can observe the mutation of data to which it doesn't hold a reference.
But there are other effects that may be non-observable, depending on who is looking.
As a simple example let's take a kind of side-effect that occurs all the time in ordinary Scala programs, even ones that we would usually consider purely functional.
We could be forgiven if weFoo("hello") assumed that it was a completely referentially transparent expression.
But each time it appears, it produces a   in a certain sense.
But testing for  (a notion inherited from the Javareference equality language) with , we get.
The two appearances of  areeq false Foo("hello") not references to the "same object" if we look under the hood.
Notice that if we evaluate  and store the result as , thenFoo("hello") x substitute  to get the expression , it has a different result.x x eq x.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Therefore, by our original definition of referential transparency, every data.
The effect is that a new and unique object isconstructor in Scala has a side-effect.
For most programs this makes no difference because most programs do not.
It is only the  method that allows our programs toeq observe this side-effect occurring.
We could therefore say that it's not a side-effect at all in the context of the vast majority of programs.
Our definition of referential transparency doesn't take this into account.
It seems like we need the definition to be more general:
This definition is only slightly modified to reflect the fact that not all programs can observe the same effects.
We say that an effect of  is  by  ife non-observable p it doesn't affect the referential transparency of  with regard to .e p.
We should also note that this definition makes some assumptions.
What is meant by "evaluating"? And what is the standard by which we determine whether the results of two evaluations are the same?
In Scala, there is a kind of standard answer to these questions.
Since Scala is a strictlyreduction to some normal form evaluated language, we can force the evaluation of an expression  to normal forme in Scala by assigning it to a :val.
And referential transparency of  with regard to a program  means that we cane p rewrite  replacing every appearance of  with .p e v.
But what do we mean by "changing the result"? We mean that the two results, before and after rewriting, are in some sense equivalent.
And what it means for two expressions to be equal is a little more nuanced than it might at first appear.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
In Scala we usually refer to extensional when talking about whether two functions are equivalent.
We could just as well take the position of requiring  equality whichx intensional.
But whatever context we choose, the point is that we must choose  context.some.
Above, we talked about how the  method is able to  the side-effect ofeq observe object creation.
Let's look more closely at this idea of observable behavior.
It requires that we delimit what we consider observable and what we don't.
Take for example this method that has a definite side-effect:
It may compute the same result, but we can say that the observable behavior of the program has changed.
We need to decide up front whether changes in standard output are something we care to observe—whether it's part of the changes in behavior that  in ourmatter context.
In this case it's exceedingly unlikely that any other part of the program will be able to observe that  side-effect occurring inside .println timesTwo.
IttimesTwo hidden dependency requires access to the standard output stream.
But as we have seen above, most programs that we would consider purely functional also require access to some of the underlying machinery of Scala's environment, like being able to construct objects in memory and discard them.
At the end of the day, we have to decide for ourselves which effects are important enough to track.
We could use the  monadIO to track  calls, but maybe we don't want to bother.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
But if the program's correct behavior depends in some way on the what it prints to the console (like if it's a UNIX utility), then we definitely want to track it.
This brings us to an essential point: Keeping track of effects is a  wechoice make as programmers.
It's a value judgement, and there are trade-offs associated with how we choose.
But as with the context of referential transparency, in Scala there is a kind of standard choice.
For example it would be completely valid and possible to track memory allocations in the type system if that really mattered to us.
But in Scala we have the benefit of automatic memory management so the cost of explicit tracking is usually higher than the benefit.
The policy we should adopt is to track those effects that program correctness.
If a program is fundamentally about reading and writing files, then filedepends on.
I/O should be tracked in the type system to the extent feasible.
If a program relies on object reference equality, it would be nice to know that statically as well.
Static type information lets us know what kinds of effects are involved, and thereby lets us make educated decisions about whether they matter to us in a given context.
The  type in this chapter and the  monad in the previous chapter shouldST IO have given you a taste for what it's like to track effects in the type system.
You're limited only by your imagination and the expressiveness of Scala's types.
In this chapter, we discussed two different implications of referential transparency.
We saw that we can get away with mutating data that never escapes a local.
At first blush it may seem that mutating state can't be compatible with pure functions.
But as we have seen, we can write components that have a pure interface and mutate local state behind the scenes, using Scala's type system to guarantee purity.
We also discussed that what counts as a side-effect is actually a choice made by the programmer or language designer.
When we talk about functions being pure, we should have already chosen a context that establishes what it means for two things to be equal, what it means to execute a program, and which effects we care to take into account when observing the program's behavior.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We said in the introduction to part 4 that functional programming is a complete paradigm.
Any program that can be imagined can be expressed functionally, including those that interact with the external world.
But it would be disappointing if the  type were our only way of constructing such programs.
While programming  the  monad, we have towithin IO reason about our programs much like we would in ordinary imperative programming.
Not only can functional programs embed arbitrary imperative programs; in this chapter we show how to recover the high-level, compositional style developed in parts 1-3 of this book, even for programs that interact with the outside world.
The design space in this area is enormous, and our goal here is more to convey ideas and give a sense of what is possible.1
Footnote 1mAs always, there is more discussion and links to further reading in the chapter notes.
Rather than simply giving the 'answer' up front, we will build up a library for streaming, composable I/O incrementally.
We are going to start by considering a very simple, concrete usage scenario, which we'll use to highlight some of the problems with imperative I/O embedded in the  monad:IO.
Check whether the number of lines in a file is greater than 40,000
This is a rather simplistic task, intended to be illustrative and help us get at the.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Footnote 2mFor simplicity, in this chapter we are not going to parameterize our  type on the  languageIO F used.
Obtain a stateful Iterator from the Source Has side effect of advancing to next element.
Although this code is rather low-level, there are a number of  things aboutgood it.
If we didn't buffer our input, we could keep as little as a single line of the file in memory at a time.
It also terminates early, as soon as the answer is known, rather than reading the entire file and then returning an answer.
For one, we have to remember to close the file when we're done.
This might seem obvious, but if we forget to do this or (more commonly) if we close the file outside of a  block and anfinally exception occurs first, the file will remain open.
A3 resource leak file handle is an example of a scarce —the operating system can only haveresource a limited number of files open at any given time.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We want to write programs that are —that is, they should closeresource safe file handles as soon as they are finished with them (whether because of normal termination or an exception), and they should not attempt to read from a closed file.
Likewise for other resources like network connections, database connections, and so on.
Using  directly can be problematic because it means our programsIO are , and we get no help from theentirely responsible for ensuring resource safety compiler in making sure of this.
It would be nice if our library would ensure resource safety by construction.
The exact implementation of this combinator depends on the representation of , but the implementation should ensure that theIO resource is released, either just after the  action finishesusing successfully or immediately if an exception occurs.
As an exercise, you may wish to implement  for our existing  type.bracket IO.
But even aside from the problems with resource safety, there is something rather low-level and unsatisfying about this code.
Opening and closinghow files and catching exceptions is a separate concern from the fundamental algorithm being expressed, but this code intertwines these concerns.
This isn't just ugly, it's not , and our code will be difficult to extend later.
Check whether the number of  lines in the file exceeds 40,000nonempty.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Find a line index before 40,000 where the first letter of consecutive lines spells out ."abracadabra"
For this first case, we could imagine passing a  intoString => Boolean our  function.
But for the second case, we would need to modify ourlinesGt40k loop to keep track of some further state, and besides being uglier, the resulting code will likely be tricky to get right.
In general, writing efficient code in the IO monad generally means writing monolithic loops, and monolithic loops are not composable.
Let's compare this to the case where we have a  for theStream[String] lines being analyzed.
Much nicer! With a , we get to assemble our program from preexistingStream combinators,  and.
If we want to filter these lines, wezipWithIndex exists can do so easily:
A natural question to ask is, could we just write the above programs if reading from an actual file? Not quite.
WeStream[String] could cheat by writing a function, , which returns an lines.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We are cheating because the  insidelazy I/O Stream[String] the  is not actually a pure value.
As elements of the stream are forced, it willIO execute side effects of reading from the file, and only if we examine the entire stream and reach its end will we close the file.
Although it is appealing that lazy I/O lets us recover the compositional style to some extent, it is problematic for several reasons:
The resource (in this case, a file) will be released only if we traverse to the end of the stream.
Nothing stops us from traversing that same  again, after the file has been closed,Stream resulting in either excessive memory usage (if the  is one that caches or Stream memoizes its values once forced) or an error if the  is unmemoized and this causes a readStream from a closed file handle.
Also, having two threads traverse an unmemoized  atStream the same time can result in unpredictable behavior.
In more realistic scenarios, we won't necessarily have full knowledge of what is happening with the  we created.
This is bad for the compositional style typically used in FP, where most of our code won't know anything about a value other than its type.
Lazy I/O is problematic, but it would be nice to recover the high-level style we are accustomed to from our usage of  and.
In the next section, we'llStream List introduce the notion of  or , which is our firststream transducers stream processors step toward achieving this.
A stream transducer specifies a transformation from one stream to another.
We are using the term  more generally here, to refer to a sequence, possibly lazilystream generated or supplied by an external source (for instance, a stream of lines from a file, a stream of HTTP requests, a stream of mouse click positions, etc)
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Footnote 5mWe have chosen to omit variance annotations in this chapter for simplicity, but it is possible to write this as .Process[-I,+O]
A  can be used to transform a stream containing  values to aProcess[I,O] I stream of  values (this is sometimes called a stream transducer)
Process is not a typical function , whichProcess[I,O] Stream[I] => Stream[O]
Instead, we have a state machine which must be interpreted by a.
There are three possible statesdriver a  can be in, each of which signals something to the driver:Process.
Footnote 6mWe could choose to have  produce just a single value.
Halt indicates to the driver that no more elements should be read from the input stream or emitted to the output.
Let's look at a sample driver that will actually interpret these requests.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Thus, given  and ,  produces a p: Process[I,O] in: Stream[I] p(in)
What is interesting is that  is agnostic to how it is fedStream[O] Process.
We have written a driver that feeds a  from a , but we canProcess Stream also write drivers that perform.
We'll get to writing such a driver a bit later, butIO first, we are going to explore the large number of operations expressible with.
The implementation simply calls  on any values produced by the map Process.
Given two processes,  and , append x y x ++
For the implementation, we simply replace the  of  with Halt x.
This uses a helper function, , which behaves just like the emitAll Emit constructor but combines adjacent emit states into a single.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Consistent use of  lets us assume that an  will always beemitAll Emit followed by an  or a , which avoids stack overflow errors in certain Await Halt.
The  function just emits a singleProcess Monad unit value, then halts:
To write the  instance, we have to partially apply the  parameter of Monad I , which we've seen before:Process.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We use the same trick introduced in chapter 13 of placing a toMonadic implicit conversion in the companion object to give us infix syntax for the Monad combinators:
The  instance is exactly the same 'idea' as the  for.
WhatMonad Monad List makes  more interesting than just  is it can accept.
And it canProcess List input transform that input through mapping, filtering, folding, grouping, and so on.
It turns out that  can express almost any stream transformation, all whileProcess remaining agnostic to how exactly it is obtaining its input or what should happen with its output.
The way we will build up complex stream transformations is by  composing values.
Given two  values,  and , we can feed the output Process Process f g f.
We'll call this operation  (pronounced 'pipe' or 'compose')g |> and implement it as a function on.
As soon as values are emitted by , they areg fuses f g f transformed by .g Footnote 8mThis operation might remind you of function composition, which feeds the (single) output of a function in as the (single) input to another function.
We won'tProcess categories be discussing that much here, but see the chapter notes.
Wef: I => O Process[I,O] repeatedly , then  the value received, transformed by .Await Emit f.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
This pattern is quite common—we often have some  whose steps weProcess wish to repeat forever.
We define a recursive internalProcess function (often called  or ) whose parameters are the  used for thego loop state transformation.
In the case of , the only piece of state is the current repeat.
We thenProcess call this internal function with some initial state.
Let's look at another one, , which outputs a running total of the values seen sosum far:
Again, we use the same pattern of an inner function which tracks the current state (in this case, the total so far)
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Let's get write some more  combinators to get accustomed to thisProcess style of programming.
Try to work through implementations of at least some of these exercises until you get the hang of it.
It should emit the number of elements seencount so far, for instance,  should yield count(Stream("a", "b", "c"))
It should emit a running average of the valuesmean seen so far.
Just as we have seen many times before throughout this book, when we notice common patterns when defining a series of functions, we can factor these patterns out into generic combinators.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Each has a single piece of state, and a state transition function that updates this state in response to input and produces a single output.
Define this combinator and implement mean sum count in terms of it.mean.
It emits a running countzipWithIndex of values emitted along with each value.
Note that because  fuses, there is no|> penalty to implementing the 'trimming' of this last form with a separate combinator.
We can now express the core stream transducer for our line-counting problem as.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We'll look first at a simplisticProcess approach in which sources are a completely separate type from ; later,Process we will consider a generalized  type which can represent sources as wellProcess as single-input stream transducers.
As the definitions of  and  demonstrate, we can implement variousfilter map operations with helper functions that simply attach the appropriate  ontoProcess the output of the.
We only need to provide an  for Source List interpreter that actually performs the  actions and feeds them to the transducer,Source IO.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Helper function: evaluates a, and runs cleanup if an exception occurs.
We tryOr(acc ++ h) since h may be a non-strict Seq like Stream which forces some computations that can fail.
Notice we are guaranteed to run the  action, whether we terminaterelease normally or if an exception occurs during processing.
This is important since we9 will often construct  values backed by some resource like a file handle weSource want to ensure gets closed.
Here is an example of a primitive , createdSource from the lines of a file: Footnote 9mOne might reasonably ask—if we are eliminating usage of exceptions by using  and Either.
For one, not all functions in our programs areOption defined for all inputs and we typically still use exceptions to signal unrecoverable errors.
We may also be using some third-party API which may throw exceptions or errors.
We deal with resource safety in just two places, the collect function we wrote earlier, and the definition of —the knowledge of how tolines allocate and release a resource is encapsulated in a single type, , and Source.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
This is in contrast to ordinary imperative I/O (in the  monad orIO otherwise) where any code that reads from files must repeat the same (error-prone) patterns to ensure resource safety.
Although we can get quite far with  and , and the simpleProcess Source way we have combined them here is resource safe, these data types are too simple to express a number of interesting and important use cases.
We'd like to write a program that reads this and produces :celsius.txt.
Our program should work in a streaming fashion, emitting to the output file as lines are read from the input file, while staying resource safe.
With the library we have so far, we can certainly produce a  containing theSource[Double] temperatures we need to output to :celsius.txt.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Unfortunately,  lacks the ability to actually write these lines to theSource output file.
In general, one way we can handle these expressiveness problems is by adding extra cases to.
Here, we could try solving our immediate problemSource by first introducing a new type, , analogous to :Sink Source.
How might we integrate this into our  API? Let's imagine a newSource combinator, :observe.
Implementing this combinator will likely require an additional Source constructor and updates to our  implementation (taking care to ensurecollect resource safety in our usage of the )
Assuming we do this, our completeSink scenario now looks something like:
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
This uses the helper function , which ignores the output of a ,run Source evaluating it only for its effects.
Ultimately, this approach of adding special cases to  starts gettingSource rather ugly.
Let's take a step back and consider some additional scenarios for which our existing API is insufficient.
Add corresponding temperatures together, convert the result to celsius, apply a 5-element moving average, and output to.
Concatenate these files into a single logical stream, convert this stream to celsius, and output the joined stream to .celsius.txt Multi-sink output: As above, but rather than producing a single output file, produce an output file for each input file in.
Execute this query, generating a stream of rows, which are further processed using other stream transformations before being assembled into an HTTP response.
Here, the effect is no longer just a sink—we need to get back a result and continue processing.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Footnote 10mStill, you may be interested to explore this approach.
It is a challenging design exercise—the problem is coming up with a nice, small set of primitive combinators that lets us express all the programs we wish to write.
We don't want to have 50 special cases which, in addition to being ugly, makes writing the collect function extremely complicated.
If you decide to experiment with this approach, think about what combinators are needed to express each of these scenarios and any others you can think of.
Can the combinator be expressed using existing primitives in a resource safe way? If not, you can try adding another primitive case for it, refining your primitives as we did throughout part 2, and updating your  function to handle additional casescollect in a resource-safe way.
Our existing  type implicitly assumes an  or Process environment context containing a single stream of values.
In order to make  extensible, we areProcess going to parameterize on the protocol used for issuing requests of the driver.
This works in much the same way as the  type we covered in chapter 13:IO.
Unlike , a  represents a  of  values ('O' forIO Process[F,O] stream O 'output'), produced by (possibly) making external requests using the protocol .F Otherwise, the  parameter serves the same role here as the  type constructor weF F used for .IO.
This type is more general than the previous  (which we'll refer toProcess from now on as a 'single-input ' or a ), and we can representProcess Process1 single-input  as a special instance of this generalized  type.Process Process We'll see how this works in a later section.
First, let's note that a number of operations are defined for  Process.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We use the same smart constructors as before,  and , withemitAll emit similar definitions:
We will also introduce the helper function, , which just curries the await constructor for better type inference:Await.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Let's see what else we can express with this new  type.
The Process F parameter gives us a lot of flexibility.
Before, we were forced to introduce a separate type to represent sources.
Footnote 11mThere are some issues with making this representation resource-safe that we'll discuss shortly.
Whereas before,  was a completely separate type from , nowSource Process it is merely a particular instance of it! To see how  is indeed aProcess[IO,O] source of  values, consider what the  constructor looks like when weO Await substitute  for :IO F.
Thus, any requests of the 'external' world can be satisfied, just by running the action.
If this action returns an  successfully, we invoke the  functionIO A recv.
If the action throws a special exception (perhaps called ) itEnd indicates normal termination and we switch to the  state.
And if thefallback action throws any other exception, we switch to the  state.
Below iscleanup simple interpreter of  which collects up all the values emitted:Source.
Footnote 12mThere are some design decisions here—we are using an exception, , for control flow, but weEnd could choose to indicate normal termination with , say with Option type Step[A] =
Programs can certainly choose to throw and catch exceptions internally if they wish.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Importantly, we are guaranteed to run either  or  beforefallback cleanup halting the , regardless of whether exceptions occur.
We'll see later howProcess this allows us to define a  backed by some resource like a file handle thatProcess we want to close in a prompt, deterministic fashion.
Notice how in the  case, we run  and block waiting for its resultAwait req before continuing the interpretation.
It turns out that we don't require  inIO particular, any  will do, so long as we have a  and as long as F Monad[F] F supports catching (and throwing) exceptions.
Rather than invoking  on our  values, we can simply  into the run IO flatMap to obtain the result.
We define a function on  to produce an req Process[F,O]
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Luckily, the  type we developed in chapter 13 isIO already suitable for this, and as an added bonus, it supports the use of asynchronous I/O primitives as well.
To make the discussion concrete, suppose we have lines: representing the lines of some large file.
When should we close the file handle? At the very end of our program? No, ideally we would close the file once we know we are done reading from.
We are certainlylines done if we reach the last line of the file—at that point there are no more values to produce and it is certainly safe to close the file.
So this gives us our first simple rule to follow: a resource should close itself immediately after emitting its final value.
How do we do this? We do this by placing the file-closing action in the argument of any , and the  function(s) above willfallback Await collect.
But thisEnd is not sufficient—we also want to ensure that the file-closing action is run in the event of an uncaught exception.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
As an example, let's use this policy to create a  backed byProcess[IO,O] the lines of a file.
We define it terms of the more general combinator, ,resource the  analogue of the  function we introduced earlier for :Process bracket IO.
Emit the value and repeat the step action Release resource when exhausted Also release in event of error.
However, we cannot  make sure that  keeps its only lines and  parameters up to date whenever it produces an fallback cleanup Await.
Thus, we have our second simple rule to follow: any process, , which pullsd.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
This sounds rather error prone, but luckily, we get to deal with this concern in just a single place, the  combinator.
We'll show how that works shortly in the|> next section, when we show how to encode single-input processes using our general  type.Process.
We now have nice, resource-safe sources, but we don't yet have any way to apply transformations to them.
Fortunately, our  type can also represent theProcess single-input processes we introduced earlier in this chapter.
Lets look at how this works—the encodingI is a bit unusual in Scala, but there's nothing fundamentally new here:
It is a bit strange to define the type  inside of.
Notice that  takes one parameters, , but we have just one instance, ,f X Get which fixes  to be the  in the outer.
Therefore, the type X I One[I] One[I]#f13 can only ever be a request for a value of type ! Moreover, we get  that I evidence X is equal to  in the form of the  which comes equipped with a pair ofI Eq[X,I] functions to convert between the two types.
We'll see how the  value gets used14 Eq a bit later during pattern matching.
But now that we have all this, we can define.
Footnote 14mWe are prevented from instantiating an , say, because there is only oneEq[Int,String] public constructor, , which takes just a single type parameter and uses the identity function forEq.refl[A] both  and .to from.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
To see what's going on, it helps to substitute the definition of  into aIs[I]#f call to :Await.
Therefore,  must accept an  as its argument, whichGet: f[I] recv I means that  can only be used to request  values.
This is important toAwait I understand—if this explanation didn't make sense, try working through these definitions on paper, substituting the type definitions.
Our  alias supports all the same operations as our old single-input Process1
Using these, our definitions of, for instance,  and  look almostlift filter identical to before, except they return a :Process1
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
The implementation looks very similar to before, but we make sure to run the  of the left process before thefinalizer right process halts.
Recall that we are using the  argument of finalizer Await to finalize resources—see the implementation of the  combinator fromresource earlier.
We use a helper function, —it runs the  of a  butkill cleanup Process ignores any of its remaining output:
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
With , we can add convenience functions on  for attaching various|> Process transformations to the output.
We can add similar convenience functions for , , and so on.take takeWhile See the chapter code for more examples.
One of the scenarios we mentioned earlier was zipping or merging of input streams:
We can address these sorts of scenarios with  as well.
Once again, we simply craft an appropriateProcess choice of :F Footnote 15mThe name 'Tee' comes from the letter 'T', which approximates a diagram merging two inputs (the top of the 'T') into a single output.
This looks quite similar to our  type from earlier, except that we now haveIs two possible values,  and , and we get an L R Either[Eq[X,I], Eq[X,I2]] for pattern matching.
With , we can now define a type alias, , which acceptsT Tee.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Once again, we define a few convenience functions for building these particular types of :Process.
Zipping is a special case of —we readTee Tee from the left, then the right (or vice versa), then emit the pair.
See the chapter notes and chapter code for some additional discussion of this.
This transducer will halt as soon as either input is exhausted, just like the zip funtion on.
Let's define a  which continues as long as eitherList zipWithAll input has elements.
We accept a value to 'pad' each side with when its elements run out:
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
This uses a few helper functions—  and  ignore one of the inputspassR passL to a  and echo the other branch.Tee.
There are a lot of other  combinators we could write.
Nothing requires thatTee we read values from each input in lockstep.
We could read from one input until some condition is met, then switch to the other; we could read five values from the left, then ten values from the right, read a value from the left then use it to determine how many values to read from the right, and so on.
We will typically want to feed a  by connecting it to two processes.
We canTee define a function on  that combines  processes using a.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
This uses two helper functions,  and , which serve the samefeedL feedR purpose as before—to feed the  in a loop as long as it expects values fromTee either side.
The one subtlety in this definition is we make sure to run cleanup for both inputs before halting.
What is nice about this overall approach is that we have exactly four places in the library where we must do anything to ensure resource safety: , ,  and the  interpreter.
All the other clienttee |> resource collect code that uses these and other combinators is guaranteed to be resource safe.
A  provides a sequenceSink[F[_], O] of functions to call with the input type.
The function returns  (later,O F[Unit] we'll see how to get back values from sinks)
Let's look at a file  that writesSink strings to a file:
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
And notice what  included—there is no exception handlingisn't code here—the combinators we are using guarantee that the  will beFileWriter closed if exceptions occur or when whatever is feeding the  signals it is done.Sink.
We can use  to implement a combinator , which pipes the output of a tee to to a :Process Sink.
When run via , this will open the input file and the output file andcollect incrementally transform the input stream, ignoring commented lines.
The implementation isto Unit identical! The operation had a more general type than we gave it before.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Channel is useful when a pure pipeline must execute some I/O action as one of its stages.
A typical example might be an application that needs to execute database queries.
It would be nice if our database queries could return a.
This wouldSource[Row] Row allow the program to process the result set of a query using all the fancy stream transducers we've built up so far.
Here's a very simple query executor, which uses  as theMap[String,Any] (untyped) row representation:
This implementation is is directly closing the connection when finished.
A real application may obtain the connections from some sort of connection pool and release the connection back to the pool when finished.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
Realistic programs may need to allocate resources dynamically, while transforming some input stream.
Concatenate these files into a single logical stream, convert this stream to celsius, and output the joined stream to .celsius.txt Multi-sink output: As above, but rather than producing a single output file, produce an output file for each input file in.
Can these capabilities be incorporated into our definition of , in aProcess way that preserves resource safety? Yes, they can! We actually already have the power to do these things, using the  combinator that we have alreadyflatMap defined for an arbitrary  type.Process.
For instance,  plus our existing combinators let us write this firstflatMap scenario as:
Trim the stream to at most a single element; see chapter code We can give eval infix syntax using implicits; see chapter code for details.
This code is completely resource-safe—all file handles will be closed automatically by the runner as soon as they are finished, even in the presence of exceptions.
Any exceptions encountered will be thrown to the  functioncollect when invoked.
We can write to multiple files just by switching the order of the calls to :flatMap.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
And of course, we can attach transformations, mapping, filtering and so on at any point in the process:
There are additional examples using this library in the chapter code.
The ideas presented in this chapter are extremely widely applicable.
A surprising number of programs can be cast in terms of stream processing—once you are aware of the abstraction, you begin seeing it everywhere.
File I/O: We've already demonstrated how to use stream processing for file I/O.
Although we have focused on line-by-line reading and writing for the examples here, we can also use the library for processing binary files.
Message processing, state machines, and actors: Large systems are often organized as a system of loosely-coupled components that communicate via message passing.
These systems are often expressed in terms of , which communicate via explicit messageactors sends and receives.
We can express components in these architectures as stream processors, which lets us describe extremely complex state machines and behaviors while retaining a high-level, compositional API.
Servers, web applications: A web application can be thought of as converting a stream of HTTP requests to a stream HTTP responses.
Big data, distributed systems: Stream processing libraries can be  and distributed.
The key insight here is that parallelized Process values being composed need not all live on the same machine.
If you're curious to learn more about these applications (and others), see the chapter notes for additional discussion and links to further reading.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We began this book with the introduction of a simple premise: that we assemble our programs using only pure functions.
From this sole premise and its consequences we were led to develop a new approach to programming, one with its own ideas, techniques, and abstractions.
In this final chapter, we constructed a library for stream processing and incremental I/O, demonstrating that we can retain the compositional style developed throughout this book even for programs that interact with the outside world.
While good design is always hard, over time, expressing code functionally becomes effortless.
By this point, you have all the tools needed to start functional.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
We welcome reader comments about anything in the manuscript — other than typos and other simple mistakes.
These will be cleaned up during production of the book by copyeditors and proofreaders.
