Programming Scala, the image of a Malayan tapir, and related trade dress are trademarks of O’Reilly Media, Inc.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in this book, and O’Reilly Media, Inc.
While every precaution has been taken in the preparation of this book, the publisher and authors assume no responsibility for errors or omissions, or for damages resulting from the use of the information contained herein.
To my mother, who gave me an appreciation for good writing and the accompanying intellectual.
Programming Scala introduces an exciting new language that offers all the benefits of a modern object model, functional programming, and an advanced type system.
Packed with code examples, this comprehensive book teaches you how to be productive with Scala quickly, and explains what makes this language ideal for today’s scalable, distributed, component-based applications that support concurrency and distribution.
You’ll also learn how Scala takes advantage of the advanced Java Virtual Machine as a platform for programming languages.
Learn more at http://programmingscala.com or at the book’s catalog page.
Welcome to Programming Scala Programming languages become popular for many reasons.
Sometimes, programmers on a given platform prefer a particular language, or one is institutionalized by a vendor.
Sometimes, popularity derived from technical merit gives way to fashion and fanaticism.
C++, Java, and Ruby have been the objects of fanatical devotion among programmers.
Sometimes, a language becomes popular because it fits the needs of its era.
Java was initially seen as a perfect fit for browser-based, rich client applications.
Smalltalk captured the essence of object-oriented programming (OOP) as that model of programming entered the mainstream.
Today, concurrency, heterogeneity, always-on services, and ever-shrinking development schedules are driving interest in functional programming (FP)
It appears that the dominance of object-oriented programming may be over.
We gravitated to Scala from other languages because Scala embodies many of the optimal qualities we want in a general-purpose programming language for the kinds of applications we build today: reliable, high-performance, highly concurrent Internet and enterprise applications.
Scala is a multi-paradigm language, supporting both object-oriented and functional programming approaches.
Scala is scalable, suitable for everything from short scripts up to large-scale, component-based applications.
Scala is sophisticated, incorporating state-of-the-art ideas from the halls of computer science departments worldwide.
Its creator, Martin Odersky, participated in the development of Java for years and understands the needs of professional developers.
Both of us were seduced by Scala, by its concise, elegant, and expressive syntax and by the breadth of tools it put at our disposal.
In this book, we strive to demonstrate why all these qualities make Scala a compelling and indispensable programming language.
If you are an experienced developer who wants a fast, thorough introduction to Scala, this book is for you.
You may be evaluating Scala as a replacement for or complement to your current languages.
Maybe you have already decided to use Scala, and you need to learn its features and how to use it well.
Either way, we hope to illuminate this powerful language for you in an accessible way.
We assume that you are well versed in object-oriented programming, but we don’t assume that you have prior exposure to functional programming.
We assume that you are experienced in one or more other programming languages.
We draw parallels to features in Java, C#, Ruby, and other languages.
If you know any of these languages, we’ll point out similar features in Scala, as well as many features that are new.
Whether you come from an object-oriented or functional programming background, you will see how Scala elegantly combines both paradigms, demonstrating their complementary nature.
Based on many examples, you will understand how and when to apply OOP and FP techniques to many different design problems.
In the end, we hope that you too will be seduced by Scala.
Even if Scala does not end up becoming your day-to-day language, we hope you will gain insights that you can apply regardless of which language you are using.
Conventions Used in This Book The following typographical conventions are used in this book: Italic.
Indicates new terms, URLs, email addresses, file names, and file extensions.
Constant width Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.
Constant width bold Shows commands or other text that should be typed literally by the user.
Constant width italic Shows text that should be replaced with user-supplied values or by values determined by context.
Using Code Examples This book is here to help you get your job done.
In general, you may use the code in this book in your programs and documentation.
You do not need to contact us for permission unless you’re reproducing a significant portion of the code.
For example, writing a program that uses several chunks of code from this book does not require permission.
Selling or distributing a CD-ROM of examples from O’Reilly books does require permission.
Answering a question by citing this book and quoting example code does not require permission.
Incorporating a significant amount of example code from this book into your product’s documentation does require permission.
An attribution usually includes the title, author, publisher, and ISBN.
Getting the Code Examples You can download the code examples from http://examples.oreilly.com/ 9780596155964/
See the README.txt file in the distribution for instructions on building and using the examples.
Some of the example files can be run as scripts using the scala command.
We have adopted a file naming convention to indicate each of these cases, although as you learn Scala it should become obvious from the contents of the files, in most cases: *-script.scala.
Files that end in -script.scala can be run on a command line using scala, e.g., scala foo-script.scala.
We use this naming convention, along with one or more embedded comments about the errors, so it will be clear that they are invalid.
Also, these files are skipped by the build process for the examples.
In the distribution, they are used either by other compiled or script files, such as tests, not all of which are listed in this book.
With a subscription, you can read any page and watch any video from our library online.
Access new titles before they are available for print, and get exclusive access to manuscripts in development and post feedback for the authors.
Copy and paste code samples, organize your favorites, download chapters, bookmark key sections, create notes, print out pages, and benefit from tons of other time-saving features.
To have full digital access to this book and others on similar topics from O’Reilly and other publishers, sign up for free at http://my.safaribooksonline.com.
How to Contact Us Please address comments and questions concerning this book to the publisher:
We have a web page for this book, where we list errata, examples, and any additional information.
Why Scala? Today’s enterprise and Internet applications must balance a number of concerns.
Beyond simply providing business logic, applications must support secure access, persistence of data, transactional behavior, and other advanced features.
Applications must be highly available and scalable, requiring designs that support concurrency and distribution.
Applications are networked and provide interfaces for both people and other applications to use.
To meet these challenges, many developers are looking for new languages and tools.
Venerable standbys like Java, C#, and C++ are no longer optimal for developing the next generation of applications.
Java was immediately hailed as an ideal language for writing browser-based applets, where a secure, portable, and developer-friendly application language was needed.
The reigning language of the day, C++, was not suitable for this domain.
It is one of the most popular languages in use for the development of web and enterprise applications.
In 1995, Java provided a syntax similar enough to C++ to entice C++ developers, while avoiding many of that language’s deficiencies and “sharp edges.” Java adopted the most useful ideas for the development problems of its era, such as object-oriented programming (OOP), while discarding more troublesome techniques, such as manual memory management.
These design choices struck an excellent balance that minimized complexity and maximized developer productivity, while trading-off performance compared to natively compiled.
While Java has evolved since its birth, many people believe it has grown too complex without adequately addressing some newer development challenges.
Developers want languages that are more succinct and flexible to improve their productivity.
This is one reason why so-called scripting languages like Ruby and Python have become more popular recently.
The never-ending need to scale is driving architectures toward pervasive concurrency.
However, Java’s concurrency model, which is based on synchronized access to shared, mutable state, results in complex and error-prone programs.
While the Java language is showing its age, the Java Virtual Machine (JVM) on which it runs continues to shine.
The optimizations performed by today’s JVM are extraordinary, allowing byte code to outperform natively compiled code in many cases.
Today, many developers believe that using the JVM with new languages is the path forward.
Sun is embracing this trend by employing many of the lead developers of JRuby and Jython, which are JVM ports of Ruby and Python, respectively.
The appeal of Scala for the Java developer is that it gives you a newer, more modern language, while leveraging the JVM’s amazing performance and the wealth of Java libraries that have been developed for over a decade.
Programmer… Dynamically typed languages like Ruby, Python, Groovy, JavaScript, and Smalltalk offer very high productivity due to their flexibility, powerful metaprogramming, and elegance.
Statically Typed Versus Dynamically Typed Languages One of the fundamental language design choices is static versus dynamic typing.
The word “typing” is used in many contexts in software.
The following is a “plausible” definition that is useful for our purposes.
A type system is a tractable syntactic method for preserving the absence of certain program behaviors by classifying phrases according to the kinds of values they compute.
Pierce, Types and Programming Languages (MIT Press, 2002) Note the emphasis on how a type system allows reasoning about what a system excludes from happening.
That’s generally easier than trying to determine the set of all allowed possibilities.
A type system is used to catch various errors, like unsupported operations on particular data structures, attempting to combine data in an undefined way (e.g., trying to add an integer to a string), breaking abstractions, etc.
Informally, in static typing, a variable is bound to a particular type for its lifetime.
Its type can’t be changed and it can only reference type-compatible instances.
That is, if a variable refers to a value of type A, you can’t assign a value of a different type B to it, unless B is a subtype of A, for some reasonable definition of “subtype.”
In dynamic typing, the type is bound to the value, not the variable.
So, a variable might refer to a value of type A, then be reassigned later to a value of an unrelated type X.
The term dynamically typed is used because the type of a variable is evaluated when it is used during runtime, while in a statically typed language the type is evaluated at parse time.
This may seem like a small distinction, but it has a pervasive impact on the philosophy, design, and implementation of a language.
We’ll explore some of these implications as we go through the book.
Scala and Java are statically typed languages, whereas Ruby, Python, Groovy, JavaScript, and Smalltalk are dynamically typed languages.
For simplicity, we will often use the terms static language and dynamic language as shorthands for statically typed language and dynamically typed language, respectively.
In strong typing, every variable (for static typing) or value (for dynamic typing) must have an unambiguous type.
While most languages allow some mixture of strong versus weak typing, Scala, Java, and Ruby are predominantly strongly typed languages.
Some languages, like C and Perl, are more weakly typed.
Despite their productivity advantages, dynamic languages may not be the best choices for all applications, particularly for very large code bases and high-performance applications.
There is a longstanding, spirited debate in the programming community about the relative merits of dynamic versus static typing.
We won’t go through all the arguments here, but we will offer a few thoughts for consideration.
Optimizing the performance of a dynamic language is more challenging than for a static language.
In a static language, optimizers can exploit the type information to make decisions.
In a dynamic language, fewer such clues are available for the optimizer, making optimization choices harder.
While recent advancements in optimizations for dynamic languages are promising, they lag behind the state of the art for static languages.
So, if you require very high performance, static languages are probably a safer choice.
Integrated development environment (IDE) features like autocompletion (sometimes called code sense) are easier to implement for static languages, again because of the extra type information available.
When using a static language, you have to think about appropriate type choices more often, which forces you to weigh design choices more carefully.
While this may slow down daily design decisions, thinking through the types in the application can result in a more coherent design over time.
Another small benefit of static languages is the extra checking the compiler performs.
We think this advantage is often oversold, as type mismatch errors are a small fraction of the runtime errors you typically see.
The compiler can’t find logic errors, which are far more significant.
Only a comprehensive, automated test suite can find logic errors.
For dynamically typed languages, the tests must cover possible type errors, too.
If you are coming from a dynamically typed language, you may find that your test suites are a little smaller as a result, but not that much smaller.
Many developers who find static languages too verbose often blame static typing for the verbosity when the real problem is a lack of type inference.
In type inference, the compiler infers the types of values based on the context.
Type inference reduces verbosity significantly, making the code feel more like code written in a dynamic language.
We have worked with both static and dynamic languages, at various times.
We find both kinds of languages compelling for different reasons.
We believe the modern software developer must master a range of languages and tools.
Sometimes, a dynamic language will be the right tool for the job.
At other times, a static language like Scala is just what you need.
Introducing Scala Scala is a language that addresses the major needs of the modern developer.
It is a statically typed, mixed-paradigm, JVM language with a succinct, elegant, and flexible syntax, a sophisticated type system, and idioms that promote scalability from small, interpreted scripts to large, sophisticated applications.
That’s a mouthful, so let’s look at each of those ideas in more detail: Statically typed.
As we described in the previous section, a statically typed language binds the type to a variable for the lifetime of that variable.
In contrast, dynamically typed languages bind the type to the actual value referenced by a variable, meaning that the type of a variable can change along with the value it references.
Of the set of newer JVM languages, Scala is one of the few that is statically typed, and it is the best known among them.
Scala improves upon Java’s support for OOP with the addition of traits, a clean way of implementing classes using mixin composition.
If you’re a Java programmer, think of traits as unifying interfaces with their implementations.
Also, Scala does not support “static” or class-level members of types, since they are not associated with an actual instance.
Instead, Scala supports a singleton object construct to support those cases where exactly one instance of a type is needed.
Interest in FP is increasing because of the ways it simplifies certain design problems, especially concurrency.
Instead, programs written in pure functional languages communicate by passing messages between concurrent, autonomous processes.
Scala supports this model with its Actors library, but it allows for both mutable and immutable variables.
Functions are “first-class” citizens in FP, meaning they can be assigned to variables, passed to other functions, etc., just like other values.
This feature promotes composition of advanced behavior using primitive operations.
Because Scala adheres to the dictum that everything is an object, functions are themselves objects in Scala.
Scala also offers closures, a feature that dynamic languages like Python and Ruby have adopted from the functional programming world, and one sadly absent from recent versions of Java.
Closures are functions that reference variables from the scope enclosing the function definition.
That is, the variables aren’t passed in as arguments or defined as local variables within the function.
A closure “closes around” these references, so the function invocation can safely refer to the variables even when the variables have gone out of scope! Closures are such a powerful abstraction that object systems and fundamental control structures are often implemented using them.
A JVM and .NET language While Scala is primarily known as a JVM language, meaning that Scala generates JVM byte code, a .NET version of Scala that generates Common Language Runtime (CLR) byte code is also under development.
When we refer to the underlying “runtime,” we will usually discuss the JVM, but most of what we will say applies equally to both runtimes.
When we discuss JVM-specific details, they generalize to the .NET version, except where noted.
The Scala compiler uses clever techniques to map Scala extensions to valid byte code idioms.
From Scala, you can easily invoke byte code that originated as Java source (for the JVM) or C# source (for .NET)
Conversely, you can invoke Scala code from Java, C#, etc.
Running on the JVM and CLR allows the Scala developer to leverage available libraries and to interoperate with other languages hosted on those runtimes.
A succinct, elegant, and flexible syntax Java syntax can be verbose.
Scala uses a number of techniques to minimize unnecessary syntax, making Scala code as succinct as code in most dynamically typed languages.
Type inference minimizes the need for explicit type information in many contexts.
Combined with some syntactic sugar, this feature permits the user to define methods that look and behave like operators.
As a result, libraries outside the core of the language can feel “native” to users.
A sophisticated type system Scala extends the type system of Java with more flexible generics and a number of more advanced typing constructs.
The type system can be intimidating at first, but most of the time you won’t need to worry about the advanced constructs.
Type inference helps by automatically inferring type signatures, so that the user doesn’t have to provide trivial type information manually.
When you need them, though, the advanced type features provide you with greater flexibility for solving design problems in a type-safe way.
Together, they allow applications to be constructed from reusable “components” in a type-safe and succinct manner.
As we will see, many common design patterns and architectural techniques like dependency injection are easy to implement in Scala without the boilerplate code or lengthy XML configuration files that can make Java development tedious.
If you trust Java’s and C#’s performance, you can trust Scala’s performance.
Of course, some particular constructs in the language and some parts of the library may perform significantly better or worse than alternative options in other languages.
As always, you should profile your code and optimize it when necessary.
In fact, a design philosophy of Scala is that OOP and FP are more synergistic than opposed.
The Seductions of Scala Today, our industry is fortunate to have a wide variety of language options.
The power, flexibility, and elegance of dynamically typed languages have made them very popular again.
Yet the wealth of Java and .NET libraries and the performance of the JVM and CLR meet many practical needs for enterprise and Internet projects.
Scala is compelling because it feels like a dynamically typed scripting language, due to its succinct syntax and type inference.
Yet Scala gives you all the benefits of static typing, a modern object model, functional programming, and an advanced type system.
These tools let you build scalable, modular applications that can reuse legacy Java and .NET APIs and leverage the performance of the JVM and CLR.
Compared to languages like Java and Ruby, Scala is a more difficult language to master because it requires competency with OOP, FP, and static typing to use it most effectively.
It is tempting to prefer the relative simplicity of dynamically typed languages.
In a dynamically typed language, it is often necessary to use metaprogramming features to implement advanced designs.
While metaprogramming is powerful, using it well takes experience and the resulting code tends to be hard to understand, maintain, and debug.
In Scala, many of the same design goals can be achieved in a type-safe manner by exploiting its type system and mixin composition through traits.
We feel that the extra effort required day to day to use Scala will promote more careful reflection about your designs.
Over time, this discipline will yield more coherent, modular, and maintainable applications.
Fortunately, you don’t need all of the sophistication of Scala all of the time.
Much of your code will have the simplicity and clarity of code written in your favorite dynamically typed language.
An alternative strategy is to combine several, simpler languages, e.g., Java for objectoriented code and Erlang for functional, concurrent code.
Such a decomposition can work, but only if your system decomposes cleanly into such discrete parts and your team can manage a heterogeneous environment.
Scala is attractive for situations in which a single, all-in-one language is preferred.
That said, Scala code can happily coexist with other languages, especially on the JVM or .NET.
Installing Scala To get up and running as quickly as possible, this section describes how to install the command-line tools for Scala, which are all you need to work with the examples in the book.
Version 2.8 introduces many new features, which we will highlight throughout the book.
We will work with the JVM version of Scala in this book.
If you need to install Java, go to http://www.java.com/en/download/manual.jsp and follow the instructions to install Java on your machine.
Download the installer for your environment and follow the instructions on the downloads page.
Go to the download directory in a terminal window, and install Scala with the java command.
On Mac OS X, the easiest route to a working Scala installation is via MacPorts.
Follow the installation instructions at http://www.macports .org/, then sudo port install scala.
Throughout this book, we will use the symbol scala-home to refer to the “root” directory of your Scala installation.
On Unix-like systems, expand the compressed file into a location of your choosing.
Afterward, add the scala-home/ bin subdirectory in the new directory to your PATH.
To test your installation, run the following command on the command line:
Of course, the version number you see will be different if you installed a different release.
From now on, when we show command output that contains the version number, we’ll show it as version 2.8.0.final.
Congratulations, you have installed Scala! If you get an error message along the lines of scala: command not found, make sure your environment’s PATH is set properly to include the correct bin directory.
Note that Scala uses many JDK classes as its own, for example, the String class.
You can also find downloads for the API documentation and the sources for Scala itself on the same downloads page.
For More Information As you explore Scala, you will find other useful resources that are available on http:// scala-lang.org.
You will find links for development support tools and libraries, tutorials, the language specification [ScalaSpec2009], and academic papers that describe features of the language.
The documentation for the Scala tools and APIs are especially useful.
This documentation was generated using the scaladoc tool, analogous to Java’s javadoc tool.
See “The scaladoc Command-Line Tool” on page 352 for more information.
You can also download a compressed file of the API documentation for local browsing using the appropriate link on the downloads page, or you can install it with the sbaz package tool, as follows:
The installed documentation also includes details on the scala tool chain (including sbaz) and code examples.
A Taste of Scala It’s time to whet your appetite with some real Scala code.
In the following examples, we’ll describe just enough of the details so you understand what’s going on.
The goal is to give you a sense of what programming in Scala is like.
We’ll explore the details of the features in subsequent chapters.
For our first example, you could run it one of two ways: interactively, or as a “script.” Let’s start with the interactive mode.
Start the scala interpreter by typing scala and the return key on your command line.
The last line is the prompt that is waiting for your input.
The interactive mode of the scala command is very convenient for experimentation (see “The scala Command-Line Tool” on page 345 for more details)
Type in the following two lines of code: val book = "Programming Scala" println(book)
The first line uses the val keyword to declare a read-only variable named book.
Note that the output returned from the interpreter shows you the type and value of book.
The second line prints the value of book, which is “Programming Scala”
Experimenting with the scala command in the interactive mode (REPL) is a great way to learn the details of Scala.
Many of the examples in this book can be executed in the interpreter like this.
However, it’s often more convenient to use the second option we mentioned, writing Scala scripts in a text editor or IDE and executing them with the same scala command.
We’ll do that for most of the remaining examples in this chapter.
By the way, that’s a comment on the first line (with the name of the source file for the code example)
Scala follows the same comment conventions as Java, C#, C++, etc.
To run this script, go to a command window, change to the same directory, and run the following command:
The file is interpreted, meaning it is compiled and executed in one step.
Interpreting Versus Compiling and Running Scala Code To summarize, if you type scala on the command line without a file argument, the interpreter runs in interactive mode.
You type in definitions and statements that are evaluated on the fly.
Finally, you can compile Scala files separately and execute the class file, as long as it has a main method, just as you would normally do with the java command.
There are some subtleties you’ll need to understand about the limitations of using the interpreter modes versus separate compilation and execution steps.
Whenever we refer to executing a script, we mean running a Scala source file with the scala command.
When explicit type information for variables is written in the code, these type annotations follow the colon after the item name (i.e., Pascal-like syntax)
Why doesn’t Scala follow Java conventions? Recall that type information is often inferred in Scala (unlike Java), meaning we don’t always show type annotations explicitly.
Compared to Java’s type item convention, the item: type convention is easier for the compiler to analyze unambiguously when you omit the colon and the type annotation and just write item.
In this case, the return type is Seq[String], where Seq (“sequence”) is a particular kind of collection.
It is a parameterized type (like a generic type in Java), parameterized here with String.
Note that Scala uses square brackets ([...]) for parameterized types, whereas Java uses angle brackets (<...>)
Scala allows angle brackets to be used in method names, e.g., naming a “less than” method < is common.
So, to avoid ambiguities, Scala uses square brackets instead for parameterized types.
It takes an argument list with a single String argument named s.
The body of the function literal is after the “arrow,” =>
The result of this call is returned by the function literal.
In Scala, the last expression in a function is the return value, although you can have return statements elsewhere, too.
The return keyword is optional here and is rarely used, except when returning out of the middle of a block (e.g., in an if statement)
The value of the last expression is the default return value of a function.
So, map passes each String in strings to the function literal and builds up a new collection with the results returned by the function literal.
To exercise the code, we create a new Upper instance and assign it to a variable named up.
As in Java, C#, and similar languages, the syntax new Upper creates a new instance.
The up variable is declared as a read-only “value” using the val keyword.
This code does exactly the same thing, but with a third fewer characters.
On the first line, Upper is now declared as an object, which is a singleton.
We are declaring a class, but the Scala runtime will only ever create one instance of Upper.
Scala uses objects for situations where other languages would use “class-level” members, like statics in Java.
We don’t really need more than one instance here, so a singleton is fine.
Why doesn’t Scala support statics? Since everything is an object in Scala, the object construct keeps this policy consistent.
Java’s static methods and fields are not tied to an actual instance.
We don’t declare any variables that might cause thread-safety issues.
The implementation of upper on the second line is also simpler.
Scala can usually infer the return type of the method (but not the types of the method arguments), so we drop the explicit declaration.
Also, because there is only one expression in the method body, we drop the braces and put the entire method definition on one line.
The equals sign before the method body tells the compiler, as well as the human reader, where the method body begins.
We have also exploited a shorthand for the function literal.
Because map takes one argument, a function, we can use the “placeholder” indicator _ instead of a named parameter.
That is, the _ acts like an anonymous variable, to which each string will be assigned before toUpperCase is called.
Note that the String type is inferred for us, too.
As we will see, Scala uses _ as a “wildcard” in several contexts.
On the last line, using an object rather than a class simplifies the code.
Instead of creating an instance with new Upper, we can just call the upper method on the Upper object directly (note how this looks like the syntax you would use when calling static methods in a Java class)
See “The Predef Object” on page 145 for details on the types and methods that are automatically imported or defined.
Because Upper is an object, this main method works exactly like a static main method in a Java class.
In Scala, main must be a method in an object.
In Java, main must be a static method in a class.
The command-line arguments for the application are passed to main in an array of strings, e.g., args: Array[String]
The first line inside the main method uses the same shorthand notation for map that we just examined:
We use a _ placeholder shortcut again in another function literal that we pass to foreach.
In this case, each string in the collection is passed as an argument to printf:
To be clear, these two uses of _ are completely independent of each other.
Method chaining and function-literal shorthands, as in this example, can take some getting used to, but once you are comfortable with them, they yield very readable code with minimal use of temporary variables.
The last line in main adds a final line feed to the output.
This time, you must first compile the code to a JVM .class file using scalac:
You should now have a file named Upper.class, just as if you had just compiled a Java class.
You may have noticed that the compiler did not complain when the file was named upper3.scala and the object was named Upper.
Unlike Java, the file name doesn’t have to match the name of the type with public scope.
In fact, unlike Java, you can have as many public types in a single file as you want.
Furthermore, the directory location of a file doesn’t have to match the package declaration.
However, you can certainly follow the Java conventions, if you want to.
Now, you can execute this command for any list of strings.
Therefore, we have met the requirement that a programming language book must start with a “hello world” program.
A Taste of Concurrency There are many reasons to be seduced by Scala.
One reason is the Actors API included in the Scala library, which is based on the robust Actors concurrency model built into Erlang (see [Haller2007])
In the Actor model of concurrency ([Agha1987]), independent software entities called Actors share no state information with each other.
By eliminating the need to synchronize access to shared, mutable state, it is far easier to write robust, concurrent applications.
In this example, instances in a geometric Shape hierarchy are sent to an Actor for drawing on a display.
Imagine a scenario where a rendering “farm” generates scenes in an animation.
As the rendering of a scene is completed, the shape “primitives” that are part of the scene are sent to an Actor for a display subsystem.
The Shape class hierarchy is defined in a shapes package.
You can declare the package using Java syntax, but Scala also supports a syntax similar to C#’s “namespace” syntax, where the entire declaration is scoped using curly braces, as used here.
The Java-style package declaration syntax is far more commonly used, however, being both compact and readable.
The Point class represents a two-dimensional point on a plane.
In Scala, the whole class body is the constructor, so you list the arguments for the primary constructor after the class name and before the class body.
Because we put the val keyword before each parameter declaration, they are automatically converted to read-only fields with the.
Of course, in a real application, you would not implement drawing in “domain model” classes like this, since the implementations would depend on details like the operating system platform, graphics API, etc.
Now that we have defined our shapes types, let’s return to Actors.
We define an Actor that receives “messages” that are shapes to draw:
The Actor is declared to be part of the shapes package.
The first import statement imports all the types in the scala.actors package.
Because * is a valid character for a function name, it can’t be used as the import wildcard.
The first case does a type comparison with the message.
There is no explicit variable for the message instance in the code; it is inferred.
If the message is of type Shape, the first case matches.
The message instance is cast to a Shape and assigned to the variable s, and then the draw method is called on it.
If the message is not a Shape, the second case is tried.
If the message is the string "exit", the Actor prints a message and terminates execution.
Actors should usually have a way to exit gracefully! The last case clause handles any other message instance, thereby functioning as the default case.
The Actor reports an error and then drops the message.
Any is the parent of all types in the Scala type hierarchy, like Object is the root type in Java and other languages.
Hence, this case clause will match any message of any type.
Pattern matching is eager; we have to put this case clause at the end, so it doesn’t consume the messages we are expecting! Recall that we declared draw as an abstract method in Shape and we implemented draw in the concrete subclasses.
Hence, the code in the first case statement invokes a polymorphic operation.
Pattern Matching Versus Polymorphism Pattern matching plays a central role in functional programming just as polymorphism plays a central role in object-oriented programming.
Functional pattern matching is much more important and sophisticated than the corresponding switch/case statements found in most imperative languages, like Java.
In our example here, we can begin to see that joining functional-style pattern matching with object-oriented polymorphic dispatching is a powerful combination that is a benefit of mixed paradigm languages like Scala.
By default, it runs in its own thread (there are alternatives we will discuss in Chapter 9), waiting for messages.
Five messages are sent to the Actor, using the syntax actor ! message.
The third message does the same thing for a Triangle.
The fourth message sends a Double that is approximately equal to Pi.
This is an unknown message for the Actor, so it just prints an error message.
The final message sends an “exit” string, which causes the Actor to terminate.
To try out the Actor example, start by compiling the first two files.
You can get the sources from the O’Reilly download site (see “Getting the Code Examples” on page xix for details), or you can create them yourself.
While the source file names and locations don’t have to match the file contents, you will notice that the generated class files are written to a shapes directory and there is one class file for each class we defined.
The class file names and locations must conform to the JVM requirements.
Now you can run the script to see the Actor in action:
Recap and What’s Next We made the case for Scala and got you started with two sample Scala programs, one of which gave you a taste of Scala’s Actors library for concurrency.
In This Chapter We ended the previous chapter with a few “teaser” examples of Scala code.
This chapter discusses uses of Scala that promote succinct, flexible code.
We’ll discuss organization of files and packages, importing other types, variable declarations, miscellaneous syntax conventions, and a few other concepts.
We’ll emphasize how the concise syntax of Scala helps you work better and faster.
Separate compile and run steps aren’t required for simple programs that have few dependencies on libraries outside of what Scala provides.
You compile and run such programs in one shot with the scala command.
If you’ve downloaded the example code for this book, many of the smaller examples can be run using the scala command, e.g., scala filename.scala.
See the README.txt files in each chapter’s code examples for more details.
See also “Command-Line Tools” on page 343 for more information about using the scala command.
Semicolons You may have already noticed that there were very few semicolons in the code examples in the previous chapter.
You can use semicolons to separate statements and expressions, as in Java, C, PHP, and similar languages.
In most cases, though, Scala behaves like many scripting languages in treating the end of the line as the end of a statement or an expression.
When a statement or expression is too long for one line, Scala can usually infer when you are continuing on to the next line, as shown in this example:
When you want to put multiple statements or expressions on the same line, you can use semicolons to separate them.
We used this technique in the ShapeDrawingActor example in “A Taste of Concurrency” on page 16:
Variable Declarations Scala allows you to decide whether a variable is immutable (read-only) or not (readwrite) when you declare it.
An immutable “variable” is declared with the keyword val (think value object):
To be more precise, the array reference cannot be changed to point to a different Array, but the array itself can be modified, as shown in the following scala session:
An immutable val must be initialized—that is, defined—when it is declared.
Scala also requires you to initialize a var when it is declared.
You can assign a new value to a var as often as you want.
In this case, the object that stockPrice refers to can’t be changed, because Doubles in Scala are immutable.
There are a few exceptions to the rule that you must initialize vals and vars when they are declared.
When used as constructor parameters, the mutable or immutable variables specified will be initialized when an object is instantiated.
Both keywords can be used to declare “abstract” (uninitialized) variables in abstract types.
Also, derived types can override vals declared inside parent types.
As we will see, this promotes better object-oriented design and is consistent with the principles of “pure” functional programming.
It may take some getting used to, but you’ll find a newfound confidence in your code when it is written in an immutable style.
The var and val keywords only specify whether the reference can be changed to refer to a different object (var) or not (val)
They don’t specify whether or not the object they reference is mutable.
Method Declarations In Chapter 1 we saw several examples of how to define methods, which are functions that are members of a class.
Method definitions start with the def keyword, followed by optional argument lists, a colon character (:) and the return type of the method, an equals sign (=), and finally the method body.
Method Default and Named Arguments (Scala Version 2.8) Many languages let you define default values for some or all of the arguments to a method.
Consider the following script with a StringUtil object that lets you join a list of strings with a user-specified separator:
The second one uses a single space as the “default” separator.
It would be nice if we could eliminate the second joiner method and declare that the separator argument in the first joiner has a default value.
In fact, in Scala version 2.8, you can now do this:
Scala version 2.8 offers another enhancement for method argument lists, named arguments.
We could actually write the last line of the previous example in several ways.
Why is this useful? First, if you choose good names for the method arguments, then your calls to those methods document each argument with a name.
In the first line, it may not be obvious what the second " " argument is for.
In the second case, we supply the name separator, which suggests the purpose of the argument.
The second benefit is that you can specify the parameters in any order when you specify them by name.
Combined with default values, you can write code like the following:
The script creates instances with zero or more named parameters.
The examples we have shown use constant values as the defaults.
Most languages with default argument values only allow constants or other values that can be determined at parse time.
However, in Scala, any expression can be used as the default, as long as it can compile where used.
For example, an expression could not refer to an instance field that will be computed inside the class or object body, but it could invoke a method on a singleton object.
Finally, another constraint on named parameters is that once you provide a name for a parameter in a method invocation, the rest of the parameters appearing after it must also be named.
Here is an implementation of a factorial calculator, where we use a conventional technique of calling a second, nested method to do the work:
Like a local variable declaration in many languages, a nested method is only visible inside the enclosing method.
If you try to call fact outside of factorial, you will get a compiler error.
Did you notice that we use i as a parameter name twice, first in the factorial method and again in the nested fact method? As in many languages, the use of i as a parameter name for fact “shadows” the outer use of i as a parameter name for factorial.
This is fine, because we don’t need the outer value of i inside fact.
We only use it the first time we call fact, at the end of factorial.
What if we need to use a variable that is defined outside a nested function? Consider this contrived example:
Note that the nested count method uses the n value that is passed as a parameter to countTo.
There is no need to pass n as an argument to count.
Because count is nested inside countTo, n is visible to it.
The declaration of a field (member variable) can be prefixed with keywords indicating the visibility, just as in languages like Java and C#
Similarly the declaration of a nonnested method can be prefixed with the same keywords.
Inferring Type Information Statically typed languages can be very verbose.
Scala supports type inference (see, for example, [TypeInference] and [Pierce2002])
The language’s compiler can discern quite a bit of type information from the context, without explicit type annotations.
Here’s the same declaration rewritten in Scala, with inferred type information:
Recall from Chapter 1 that Scala uses square brackets ([...]) for generic type parameters.
We specify Map[Integer, String] on the lefthand side of the equals sign.
On the righthand side, we instantiate the actual type we want, a HashMap, but we don’t have to repeat the type parameters.
For completeness, suppose we don’t actually care if the instance is of type Map (the Java interface type)
It can be of type HashMap for all we care:
This declaration requires no type annotations on the lefthand side because all of the type information needed is on the righthand side.
In most cases, the return type of the method can be inferred, so the : and return type can be omitted.
Code written in these languages require type annotations less often than in Scala, because Scala’s type inference algorithm has to support object-oriented typing as well as functional typing.
So, Scala requires more type annotations than languages like Haskell.
Here is a summary of the rules for when explicit type annotations are required in Scala.
When Explicit Type Annotations Are Required In practical terms, you have to provide explicit type annotations for the following situations:
A variable declaration, unless you assign a value to the variable (e.g., val name = "Programming Scala")
When you explicitly call return in a method (even at the end) b.
When a method is overloaded and one of the methods calls another; the.
The Any type is the root of the Scala type hierarchy (see “The Scala Type Hierarchy” on page 155 for more details)
If a block of code returns a value of type Any unexpectedly, chances are good that the type inferencer couldn’t figure out what type to return, so it chose the most generic type possible.
Let’s look at examples where explicit declarations of method return types are required.
In the following script, the upCase method has a conditional return statement for zerolength strings:
Actually, for this particular script, an alternative fix is to remove the return keyword from the line.
It is not needed for the code to work properly, but it illustrates our point.
Let’s remove the : Int return type on the nested fact method:
When one such method calls another, we have to add a return type to the one doing the calling, as in this example:
ERROR: Won't compile: needs a String return type on the second "joiner"
The two joiner methods concatenate a List of strings together.
The first method also takes an argument for the separator string.
The second method calls the first with a “default” separator of a single space.
If you run this script, you get the following error:
Since the second joiner method calls the first, it requires an explicit String return type.
The final scenario can be subtle, when a more general return type is inferred than what you expected.
You usually see this error when you assign a value returned from a function to a variable with a more specific type.
For example, you were expecting a String, but the function inferred an Any for the returned object.
Let’s see a contrived example that reflects a bug where this scenario can occur:
Running this script returns the following error: ...11: error: type mismatch; found   : List[Any] required: List[String]
We intended for makeList to return a List[String], but when strings.length equals zero, we returned List(0), incorrectly “assuming” that this expression is the correct way to create an empty list.
Since the else expression returns a List[String], the result of strings.toList, the inferred return type for the method is the closest common supertype of List[Int] and List[String], which is List[Any]
Note that the compilation error doesn’t occur in the function definition.
We only see it when we attempt to assign the value returned from makeList to a List[String] variable.
Alternatively, when there isn’t a bug, it may be that the compiler just needs the “help” of an explicit return type declaration.
Investigate the method that appears to return the unexpected type.
In our experience, you often find that you modified that method (or another one in the call path) in such a way that the compiler now infers a more general return type than necessary.
Another way to prevent these problems is to always declare return types for methods, especially when defining methods for a public API.
Let’s revisit our StringUtil example and see why explicit declarations are a good idea (adapted from [Smith2009a])
Here is our StringUtil “API” again with a new method, toCollection:
The toCollection method splits a string on spaces and returns an Array containing the substrings.
The return type is inferred, which is a potential problem, as we will see.
The method is somewhat contrived, but it will illustrate our point.
Here is a client of StringUtil that uses this method:
If you compile these files with scala, you can run the client as follows: $ scala -cp ...
Everything is fine at this point, but now imagine that the code base has grown.
StringUtil and its clients are now built separately and bundled into different JARs.
Imagine also that the maintainers of StringUtil decide to return a List instead of the default:
The only difference is the final call to toList that converts the computed Array to a List.
Then you run the same client, without recompiling it first:
However, had an explicit return type of Seq been declared, which is a parent for both Array and List, then the implementation change would not have forced a recompilation of the client.
When developing APIs that are built separately from their clients, declare method return types explicitly and use the most general return type you can.
This is especially important when APIs declare abstract methods (see, e.g., Chapter 4)
There is another scenario to watch for when using declarations of collections like val map = Map(), as in the following example:
Why did the second command print () instead of 4? Look carefully at what the scala interpreter said the first command returned: double (Int)Unit.
We defined a method named double that takes an Int argument and returns Unit.
The method doesn’t return an Int as we would expect.
The cause of this unexpected behavior is a missing equals sign in the method definition.
Now, the output says we have defined double to return an Int and the second command does what we expect it to do.
Scala regards a method with the equals sign before the body as a function definition and a function always returns a value in functional programming.
On the other hand, when Scala sees a method body without the leading equals sign, it assumes the programmer intended the method to be a “procedure” definition, meant for performing side effects only with the return value Unit.
In practice, it is more likely that the programmer simply forgot to insert the equals sign!
When the return type of a method is inferred and you don’t use an equals sign before the opening parenthesis for the method body, Scala infers a Unit return type, even when the last expression in the method is a value of another type.
Literals Often, a new object is initialized with a literal value, such as val book = "Programming Scala"
Let’s discuss the kinds of literal values supported by Scala.
We’ll cover literal syntax for functions (used as values, not member methods), tuples, and certain types like Lists and Maps as we come to them.
Integer Literals Integer literals can be expressed in decimal, hexadecimal, or octal.
For Long literals, it is necessary to append the L or l character at the end of the literal.
The valid values for an integer literal are bounded by the type of the variable to which the value will be assigned.
Ranges of allowed values for integer literals (boundaries are inclusive)
A compile-time error occurs if an integer literal number is specified that is outside these ranges, as in the following examples:
If there are no digits before the period, i.e., the number is less than 1.0, then there must be one or more digits after the period.
For Float literals, append the F or f character at the end of the literal.
You can optionally append a D or d for a Double.
The format of the exponential part is e or E, followed by an optional + or -, followed by one or more digits.
To avoid parsing ambiguities, you must have at least one space after a floating-point literal, if it is followed by a token that starts with a letter.
The type of the variable to which they are assigned will be inferred to be Boolean:
Character Literals A character literal is either a printable Unicode character or an escape sequence, written between single quotes.
It is a compile-time error if a backslash character in a character or string literal does not start a valid escape sequence.
String Literals A string literal is a sequence of characters enclosed in double quotes or triples of double quotes, i.e., """..."""
For string literals in double quotes, the allowed characters are the same as the character literals.
The string literals bounded by triples of double quotes are also called multi-line string literals.
These strings can cover several lines; the line feeds will be part of the string.
They can include any characters, including one or two double quotes together, but not three together.
Symbol Literals Scala supports symbols, which are interned strings, meaning that two symbols with the same “name” (i.e., the same character sequence) will actually refer to the same object in memory.
Symbols are used less often in Scala than in some other languages, like Ruby, Smalltalk, and Lisp.
A symbol literal is a single quote ('), followed by a letter, followed by zero or more digits and letters.
Note that an expression like '1 is invalid, because the compiler thinks it is an incomplete character literal.
If you want to create a symbol that contains whitespace, use e.g., scala.Symbol(" Programming Scala ")
Tuples How many times have you wanted to return two or more values from a method? In many languages, like Java, you only have a few options, none of which is very appealing.
You could pass in parameters to the method that will be modified for all or some of the “return” values, which is ugly.
Or you could declare some small “structural” class that holds the two or more values, then return an instance of that class.
The types of the xi elements are unrelated to each other; you can mix and match types.
These literal “groupings” are instantiated as scala.TupleN instances, where N is the number of items in the tuple.
Tuple instances are immutable, first-class values, so you can assign them to variables, pass them as values, and return them from methods.
The tupleator method simply returns a “3-tuple” with the input arguments.
The first statement that uses this method assigns the returned tuple to a single variable t.
The first print statement calls Tuple3.toString, which wraps parentheses around the item list.
The following three statements print each item in t separately.
The last two lines show that we can use a tuple expression on the lefthand side of the assignment.
It’s worth noting that there’s more than one way to define a tuple.
We’ve been using the more common parenthesized syntax, but you can also use the arrow operator between two values, as well as special factory methods on the tuple-related classes:
However, three useful classes to understand now are the Option class and its two subclasses, Some and None.
Most languages have a special keyword or object that’s assigned to reference variables when there’s nothing else for them to refer to.
In Java, null is a keyword, not an object, and thus it’s illegal to call any methods on it.
But this is a confusing choice on the language designer’s part.
Why return a keyword when the programmer expects an object? To be more consistent with the goal of making everything an object, as well as to conform with functional programming conventions, Scala encourages you to use the Option type for variables and function return values when they may or may not refer to a value.
When there is no value, use None, an object that is a subclass of Option.
When there is a value, use Some, which wraps the value.
None is declared as an object, not a class, because we really only need one instance of it.
In that sense, it’s like the null keyword, but it is a real object with methods.
You can see Option, Some, and None in action in the following example, where we create a map of state capitals in the United States:
For now, we want to focus on the two groups of println statements, where we show what happens when you retrieve the values from the map.
If you run this script with the scala command, you’ll get the following output:
The first group of println statements invoke toString implicitly on the instances returned by get.
We are calling toString on Some or None instances because the values returned by Map.get are automatically wrapped in a Some, when there is a value in the map for the specified key.
Note that the Scala library doesn’t store the Some in the map; it wraps the value in a Some upon retrieval.
Conversely, when we ask for a map entry that doesn’t exist, the None object is returned, rather than null.
The second group of println statements goes a step further.
After calling Map.get, they call get or getOrElse on each Option instance to retrieve the value it contains.
Option.get requires that the Option is not empty—that is, the Option instance must actually be a Some.
In this case, get returns the value wrapped by the Some, as demonstrated in the println where we print the capital of Alabama.
We also show the alternative method, getOrElse, in the last two println statements.
This method returns either the value in the Option, if it is a Some instance, or it returns the second argument we passed to getOrElse, if it is a None instance.
In other words, the second argument to getOrElse functions as the default return value.
So, getOrElse is the more defensive of the two methods.
Note that because the Map.get method returns an Option, it automatically documents the fact that there may not be an item matching the specified key.
Most languages would return null (or the equivalent) when there is no “real” value to return.
Using Option makes the behavior more explicit in the method signature, so it’s more self-documenting.
Also, thanks to Scala’s static typing, you can’t make the mistake of attempting to call a method on a value that might actually be null.
While this mistake is easy to do in Java, it won’t compile in Scala because you must first extract the value from the Option.
So, the use of Option strongly encourages more resilient programming.
Because Scala runs on the JVM and .NET and because it must interoperate with other libraries, Scala has to support null.
So, how would you write a method that returns an Option? Here is a possible implementation of get that could be used by a concrete subclass of Map (Map.get itself is abstract)
It returns true if the map contains a value for the specified key.
The getValue method is intended to be an internal method that retrieves the value from the underlying storage, whatever it is.
Note how the value returned by getValue is wrapped in a Some[B], where the type B is inferred.
However, if the call to contains(key) returns false, then the object None is returned.
You can use this same idiom when your methods return an Option.
Its pervasive use in Scala code makes it an important concept to grasp.
Organizing Code in Files and Namespaces Scala adopts the package concept that Java uses for namespaces, but Scala offers a more flexible syntax.
Just as file names don’t have to match the type names, the package structure does not have to match the directory structure.
So, you can define packages in files independent of their “physical” location.
The next example shows a contrived example that defines packages using the nested package syntax in Scala, which is similar to the namespace syntax in C# and the use of modules as namespaces in Ruby:
A total of three classes are defined between the two packages.
It is not necessary to use a separate package clause for each package.
Following the conventions of Java, the root package for Scala’s library classes is named scala.
Scala does not allow package declarations in scripts that are executed directly with the scala interpreter.
The reason has to do with the way the interpreter converts statements in scripts to valid Scala code before compiling to byte code.
See “The scala Command-Line Tool” on page 345 for more details.
Importing Types and Their Members To use declarations in packages, you have to import them, just as you do in Java and similarly for other languages.
The following example illustrates several ways to import Java types:
You can import all types in a package, using the underscore ( _ ) as a wildcard, as shown on the first line.
You can also import individual Scala or Java types, as shown on the second line.
As shown on the third line, you can import all the static methods and fields in Java types.
If java.io.File were actually a Scala object, as discussed previously, then this line would import the fields and methods from the object.
Finally, you can selectively import just the types you care about.
First, we can put import statements almost anywhere we want, not just at the top of the file, as required by Java.
This feature allows us to scope the imports more narrowly.
For example, we can’t reference the imported BigInteger definitions outside the scope of the method.
Another advantage of this feature is that it puts an import statement closer to where the imported items are actually used.
The second feature shown is the ability to rename imported items.
This effectively makes it invisible and unavailable to the importing scope.
This is a useful technique when you want to import everything except a few particular items.
Aliasing is useful if you want to give the item a more convenient name or you want to avoid ambiguities with other items in scope that have the same name.
Imports are Relative There’s one other important thing to know about imports: they are relative.
Note that the last import statement nested in the scala.actor package scope is relative to that scope.
It’s fairly rare that you’ll have problems with relative imports, but the problem with this convention is that they sometimes cause surprises, especially if you are accustomed to languages like Java, where imports are absolute.
If you get a mystifying compiler error that a package wasn’t found, check that the statement is properly relative to the last import statement or add the _root_
Also, you might see an IDE or other tool insert an import _root_...
Abstract Types And Parameterized Types We mentioned in “A Taste of Scala” on page 10 that Scala supports parameterized types, which are very similar to generics in Java.
We could use the two terms interchangeably, but it’s more common to use “parameterized types” in the Scala community and “generics” in the Java community.
The most obvious difference is in the syntax, where Scala uses square brackets ([...]), while Java uses angle brackets (<...>)
For example, a list of strings would be declared as follows:
If you look at the declaration of scala.List in the Scaladocs, you’ll see that the declaration is written as ...
The + in front of the A means that List[B] is a subtype of List[A] for any B that is a subtype of A.
If there is a - in front of a type parameter, then the relationship goes the other way; Foo[B] would be a supertype of Foo[A], if the declaration is Foo[-A]
Scala supports another type abstraction mechanism called abstract types, used in many functional programming languages, such as Haskell.
Abstract types were also considered for inclusion in Java when generics were adopted.
For a very detailed comparison of these two mechanisms, see [Bruce1998]
Abstract types can be applied to many of the same design problems for which parameterized types are used.
However, while the two mechanisms overlap, they are not redundant.
Running this script with scala produces the following output: Hello Scala! import java.io._
The BulkReader abstract class declares three abstract members: a type named In, a val field source, and a read method.
As in Java, instances in Scala can only be created from concrete classes, which must have definitions for all members.
The derived classes, StringBulkReader and FileBulkReader, provide concrete definitions for these abstract members.
For now, note that the type field works very much like a type parameter in a parameterized type.
In fact, we could rewrite this example as follows, where we show only what would be different:
Just as for parameterized types, if we define the In type to be String, then the source field must also be defined as a String.
Note that the StringBulkReader’s read method simply returns the source field, while the FileBulkReader’s read method reads the contents of the file.
As demonstrated by [Bruce1998], parameterized types tend to be best for collections, which is how they are most often used in Java code, whereas abstract types are most useful for type “families” and other type scenarios.
For example, we’ll see how to constrain the possible concrete types that can be used.
Unlike Java, the keyword is usually not required for abstract members.
Applied to a member to prohibit overriding it in a derived class or trait.
Marks a method parameter as optional, as long as a typecompatible substitute object is in the scope where the method is called.
Used in function literals to separate the argument list from the function body.
Used in parameterized and abstract type declarations to constrain the allowed types.
Recap and What’s Next We covered several ways that Scala’s syntax is concise, flexible, and productive.
In the next chapter, we will round out some Scala essentials before we dive into Scala’s support for object-oriented programming and functional programming.
Before we dive into Scala’s support for object-oriented and functional programming, let’s finish our discussion of the essential features you’ll use in most of your programs.
Operator? Operator? An important fundamental concept in Scala is that all operators are actually methods.
These remaining characters are called operator characters, and they include characters such as /, <, etc.
As in most languages, you can’t reuse reserved words for identifiers.
Recall that some of them are combinations of operator and punctuation characters.
For example, a single underscore ( _ ) is a reserved word!
However, like Java, Scala reserves the dollar sign for internal use, so you shouldn’t use it in your own identifiers.
After an underscore, you can have either letters and digits or a sequence of operator characters.
It tells the compiler to treat all the characters up to the next whitespace as part of the identifier.
Similarly, if you have operator characters after the underscore, you can’t mix them with letters and digits.
Pattern matching identifiers In pattern matching expressions, tokens that begin with a lowercase letter are parsed as variable identifiers, while tokens that begin with an uppercase letter are parsed as constant identifiers.
This restriction prevents some ambiguities because of the very succinct variable syntax that is used, e.g., no val keyword is present.
Syntactic Sugar Once you know that all operators are methods, it’s easier to reason about unfamiliar Scala code.
You don’t have to worry about special cases when you see new operators.
This flexible method naming gives you the power to write libraries that feel like a natural extension of Scala itself.
You could write a new math library with numeric types that accept all the usual mathematical operators, like addition and subtraction.
You could write a new concurrent messaging layer that behaves just like Actors.
The possibilities are constrained only by Scala’s method naming limitations.
When designing your own libraries and APIs in Scala, keep in mind that obscure punctuational operators are hard for programmers to remember.
Overuse of these can contribute a “line noise” quality of unreadability to your code.
Stick to conventions and err on the side of spelling method names out when a shortcut doesn’t come readily to mind.
Methods Without Parentheses and Dots To facilitate a variety of readable programming styles, Scala is flexible about the use of parentheses in methods.
If a method takes no parameters, you can define it without parentheses.
If you add empty parentheses, then callers may optionally add parentheses.
The convention in the Scala community is to omit parentheses when calling a method that has no side effects.
So, asking for the size of a sequence is fine without parentheses, but defining a method that transforms the elements in the sequence should be written with parentheses.
This convention signals a potentially tricky method for users of your code.
It’s also possible to omit the dot (period) when calling a parameterless method or one that takes only one argument.
When does this syntactical flexibility become useful? When chaining method calls together into expressive, self-explanatory “sentences” of code:
Scala’s liberal approach to parentheses and dots on methods provides one building block for writing Domain-Specific Languages.
We’ll learn more about them after a brief discussion of operator precedence.
An exception is = when used for assignment, when it has the lowest precedence.
Since * and / have the same precedence, the two lines in the following scala session behave the same:
In a sequence of left-associative method invocations, they simply bind in left-to-right order.
In a sequence of right-associative method invocations, they bind from right to left.
In this case, list is added to the List(e, f), then a is prepended to create the final list.
It’s usually better to add parentheses to remove any potential uncertainty.
Any method whose name ends with a : binds to the right, not the left.
Finally, note that when you use the scala command, either interactively or with scripts, it may appear that you can define “global” variables and methods outside of types.
This is actually an illusion; the interpreter wraps all definitions in an anonymous type before generating JVM or .NET CLR byte code.
Domain-Specific Languages Domain-Specific Languages, or DSLs, provide a convenient syntactical means for expressing goals in a given problem domain.
For example, SQL provides just enough of a programming language to handle the problems of working with databases, making it a Domain-Specific Language.
While some DSLs like SQL are self-contained, it’s become popular to implement DSLs as subsets of full-fledged programming languages.
This allows programmers to leverage the entirety of the host language for edge cases that the DSL does not cover, and saves the work of writing lexers, parsers, and the other building blocks of a language.
Consider this example of a style of test writing called Behavior-Driven Development (see [BDD]) using the Specs library (see “Specs” on page 363):
Notice how much this code reads like English: “This should test that in the following scenario,” “This value must equal that value,” and so forth.
This example uses the superb Specs library, which effectively provides a DSL for the Behavior-Driven Development testing and engineering methodology.
By making maximum use of Scala’s liberal syntax and rich methods, Specs test suites are readable even by non-developers.
This is just a taste of the power of DSLs in Scala.
We’ll see other examples later and learn how to write our own as we get more advanced (see Chapter 11)
Scala if Statements Even the most familiar language features are supercharged in Scala.
As in most every language, Scala’s if evaluates a conditional expression, then proceeds to a block if the result is true, or branches to an alternate block if the result is false.
What’s different in Scala is that if and almost all other statements are actually expressions themselves.
So, we can assign the result of an if expression, as shown here:
Note that if statements are expressions, meaning they have values.
In this example, the value configFilePath is the result of an if expression that handles the case of a configuration file not existing internally, then returns the absolute path to that file.
This value can now be reused throughout an application, and the if expression won’t be reevaluated when the value is used.
Because if statements are expressions in Scala, there is no need for the special-case ternary conditional expressions that exist in C-derived languages.
You won’t see x ? doThis() : doThat() in Scala.
Scala provides a mechanism that’s just as powerful and more readable.
What if we omit the else clause in the previous example? Typing the code in the scala interpreter will tell us what happens:
The type inference picks a type that works for all outcomes of the if expression.
Unit is the only possibility, since no value is one possible outcome.
Scala for Comprehensions Another familiar control structure that’s particularly feature-rich in Scala is the for loop, referred to in the Scala community as a for comprehension or for expression.
This corner of the language deserves at least one fancy name, because it can do some great party tricks.
It expresses the idea that we are traversing a set of some kind, “comprehending” what we find, and computing something new from it.
A Dog-Simple Example Let’s start with a basic for expression:
As you might guess, this code says, “For every element in the list dogBreeds, create a temporary variable called breed with the value of that element, then print it.” Think of the <- operator as an arrow directing elements of a collection, one by one, to the scoped variable by which we’ll refer to them inside the for expression.
Filtering What if we want to get more granular? Scala’s for expressions allow for filters that let us specify which elements of a collection we want to work with.
So to find all terriers in our list of dog breeds, we could modify the previous example to the following:
You’ve now found all the terriers that don’t hail from Yorkshire, and hopefully learned just how useful filters can be in the process.
Yielding What if, rather than printing your filtered collection, you needed to hand it off to another part of your program? The yield keyword is your ticket to generating new collections with for expressions.
In the following example, note that we’re wrapping up the for expression in curly braces, as we would when defining any block:
Most of the time, you’ll prefer using curly braces when you have more than one filter, assignment, etc.
Every time through the for expression, the filtered result is yielded as a value named breed.
These results accumulate with every run, and the resulting collection is assigned to the value filteredBreeds (as we did with if statements earlier)
In this case, filteredBreeds is of type List[String], since it is a subset of the dogBreeds list, which is also of type List[String]
Expanded Scope One final useful feature of Scala’s for comprehensions is the ability to define variables inside the first part of your for expressions that can be used in the latter part.
Note that without declaring upcasedBreed as a val, you can reuse it within the body of your for expression.
This approach is ideal for transforming elements in a collection as you loop through them.
Finally, in “Options and for Comprehensions” on page 308, we’ll see how using Options with for comprehensions can greatly reduce code size by eliminating unnecessary “null” and “missing” checks.
Scala while Loops Familiar in many languages, the while loop executes a block of code as long as a condition is true.
For example, the following code prints out a complaint once a day until the next Friday the 13th has arrived:
Table 3-1 later in this chapter shows the conditional operators that work in while loops.
Scala do-while Loops Like the while loop, a do-while loop executes some code while a conditional expression is true.
The only difference that a do-while checks to see if the condition is true after running the block.
As it turns out, there’s a more elegant way to loop through collections in Scala, as we’ll see in the next section.
Generator Expressions Remember the arrow operator (<-) from the discussion about for loops? We can put it to work here, too.
This clean one-liner is possible because of Scala’s RichInt class.
An implicit conversion is invoked by the compiler to convert the 1, an Int, into a RichInt.
RichInt defines a to method that takes another integer and returns an instance of Range.Inclusive.
This subclass of the class Range inherits a number of methods for working with sequences and iterable data structures, including those necessary to use it in a for loop.
This should paint a clearer picture of how Scala’s internal libraries compose to form easy-to-use language constructs.
When working with loops in most languages, you can break out of a loop or continue the iterations.
Scala doesn’t have either of these statements, but when writing idiomatic Scala code, they’re not necessary.
Use conditional expressions to test if a loop should continue, or make use of recursion.
Better yet, filter your collections ahead of time to eliminate complex conditions within your loops.
However, because of demand for it, Scala version 2.8 includes support for break, implemented as a library method, rather than a built-in break keyword.
Conditional Operators Scala borrows most of the conditional operators from Java and its predecessors.
You’ll find the ones listed in Table 3-1 in if statements, while loops, and everywhere else conditions apply.
The righthand side is only evaluated if the lefthand side is true.
The righthand side is only evaluated if the lefthand side is false.
The value on the left is greater than or equal to the value on the right.
The value on the left is less than or equal to the value on the right.
They stop evaluating expressions as soon as the answer is known.
For example, we’ll see that == has a different meaning in Scala versus Java.
Otherwise, these operators should all be familiar, so let’s move on to something new and exciting.
Pattern Matching An idea borrowed from functional languages, pattern matching is a powerful yet concise way to make a programmatic choice between multiple conditions.
Pattern matching is the familiar case statement from your favorite C-like language, but on steroids.
A Simple Match To begin with, let’s simulate flipping a coin by matching the value of a boolean:
It looks just like a C-style case statement, right? The only difference is the last case with the underscore ( _ ) wildcard.
It matches anything not defined in the cases above it, so it serves the same purpose as the default keyword in Java and C# switch statements.
So, if you try to put a case _ clause before any other case clauses, the compiler will throw an “unreachable code” error on the next clause, because nothing will get past the default clause!
What if we want to work with matches as variables?
Variables in Matches In the following example, we assign the wildcard case to a variable called other Number, then print it in the subsequent expression.
If we generate a 7, we’ll extol that number’s virtues.
Otherwise, we’ll curse fate for making us suffer an unlucky number:
Matching on Type These simple examples don’t even begin to scratch the surface of Scala’s pattern matching features.
Here we pull each element out of a List of Any type of element, in this case containing a String, a Double, an Int, and a Char.
For the first three of those types, we let the user know specifically which type we got and what the value was.
When we get something else (the Char), we just let the user know the value.
We could add further elements to the list of other types and they’d be caught by the other wildcard case.
Matching on Sequences Since working in Scala often means working with sequences, wouldn’t it be handy to be able to match against the length and contents of lists and arrays? The following example does just that, testing two lists to see if they contain four elements, the second of which is the integer 3:
In the second case we’ve used a special wildcard pattern to match a List of any size, even zero elements, and any element values.
You can use this pattern at the end of any sequence match to remove length as a condition.
Recall that we mentioned the “cons” method for List, ::
The expression a :: list prepends a to a list.
You can also use this operator to extract the head and tail of a list:
It may look strange to start the method definition like the following:
Hopefully hiding the details with the ellipsis makes the meaning a little clearer.
The processList method is actually one statement that crosses several lines.
It first matches on head :: tail, where head will be assigned the first element in the list and tail will be assigned the rest of the list.
That is, we’re extracting the head and tail from the list using ::
When this case matches, it prints the head and calls process List recursively to process the tail.
It prints an end of line and terminates the recursion.
Matching on Tuples (and Guards) Alternately, if we just wanted to test that we have a tuple of two items, we could do a tuple match:
In the second case in this example, we’ve extracted the values inside the tuple to scoped variables, then reused these variables in the resulting expression.
In the first case we’ve added a new concept: guards.
The guard is evaluated when matching, but only extracting any variables in the preceding part of the case.
In this example, the only difference between the two patterns is the guard expression, but that’s enough for the compiler to differentiate them.
Recall that the cases in a pattern match are evaluated in order.
For example, if your first case is broader than your second case, the second case will never be reached.
You may include a “default” case at the end of a pattern match, either using the underscore wildcard character or a meaningfully named variable.
When using a variable, it should have no explicit type or it should be declared as Any, so it can match anything.
On the other hand, try to design your code to avoid a catch-all clause by ensuring it only receives specific items that are expected.
Matching on Case Classes Let’s try a deep match, examining the contents of objects in our pattern match:
Poor Charlie gets the cold shoulder, as we can see in the output: Hi Alice! Hi Bob! Who are you, 32 year-old person named Charlie?
For now, it will suffice to say that a case class allows for very terse construction of simple objects with some predefined methods.
Our pattern match then looks for Alice and Bob by inspecting the values passed to the constructor of the Person case class.
Charlie falls through to the catch-all case; even though he has the same age value as Bob, we’re matching on the name property as well.
This type of pattern match becomes extremely useful when working with Actors, as we’ll see later on.
Case classes are frequently sent to Actors as messages, and deep pattern matching on an object’s contents is a convenient way to “parse” those messages.
Matching on Regular Expressions Regular expressions are convenient for extracting data from strings that have an informal structure, but are not “structured data” (that is, in a format like XML or JSON, for example)
Commonly referred to as regexes, regular expressions are a feature of nearly all modern programming languages.
They provide a terse syntax for specifying complex matches, one that is typically translated into a state machine behind the scenes for optimum performance.
Regexes in Scala should contain no surprises if you’ve used them in other programming languages.
We start with two regular expressions, one for records of books and another for records of magazines.
Calling .r on a string turns it into a regular expression; we use raw (triplequoted) strings here to avoid having to double-escape backslashes.
Notice that each of our regexes defines two capture groups, connoted by parentheses.
Each group captures the value of a single field in the record, such as a book’s title or author.
Every match sets a field to the captured result; every miss is set to null.
We can then use those values on the righthand side of the case clause, as we have in the previous example.
The variable names title and author within the extractor are arbitrary; matches from capture groups are simply assigned from left to right, and you can call them whatever you’d like.
What we won’t cover in this section is the details of writing regular expressions.
Scala’s Regex class uses the underlying platform’s regular expression APIs (that is, Java’s or .NET’s)
Consult references on those APIs for the hairy details, as they may be subtly different from the regex support in your language of choice.
Binding Nested Variables in Case Clauses Sometimes you want to bind a variable to an object enclosed in a match, where you are also specifying match criteria on the nested object.
Suppose we modify a previous example so we’re matching on the key-value pairs from a map.
We’ll store our same Person objects as the values and use an employee ID as the key.
We’ll also add another attribute to Person, a role field that points to an instance from a type hierarchy:
The case objects are just singleton objects like we’ve seen before, but with the special case behavior.
We’re matching on particular kinds of Person objects inside the enclosing tuple.
We also want to assign the Person to a variable p, so we can use it for printing:
If we weren’t using matching criteria in Person itself, we could just write p: Person.
For example, the previous match clause could be written this way:
It is analogous to using “capture groups” in a regular expression to pull out substrings we want, instead of splitting the string in several successive steps to extract the substrings we want.
Using try, catch, and finally Clauses Through its use of functional constructs and strong typing, Scala encourages a coding style that lessens the need for exceptions and exception handling.
But where Scala interacts with Java, exceptions are still prevalent.
Even Java’s checked exceptions are treated as unchecked by Scala.
However, there is a @throws annotation that is useful for Java interoperability.
Thankfully, Scala treats exception handling as just another pattern match, allowing us to make smart choices when presented with a multiplicity of potential exceptions.
We also define unknown as a catch-all case, just to be safe.
If we weren’t hardcoding this program to fail, the finally block would be reached and the user would be informed that everything worked out just fine.
You can use an underscore (Scala’s standard wildcard character) as a placeholder to catch any type of exception (really, to match any case in a pattern matching expression)
However, you won’t be able to refer to the exception in the subsequent expression.
Name the exception variable if you need it; for example, if you need to print the exception as we do in the catch-all case of the previous example.
Pattern matching aside, Scala’s treatment of exception handling should be familiar to those fluent in Java, Ruby, Python, and most other mainstream languages.
Concluding Remarks on Pattern Matching Pattern matching is a powerful and elegant way of extracting information from objects, when used appropriately.
Recall from Chapter 1 that we highlighted the synergy between pattern matching and polymorphism.
Most of the time, you want to avoid the problems of “switch” statements that know a class hierarchy, because they have to be modified every time the hierarchy is changed.
In our drawing Actor example, we used pattern matching to separate different “categories” of messages, but we used polymorphism to draw the shapes sent to it.
We could change the Shape hierarchy and the Actor code would not require changes.
Pattern matching is also useful for the design problem where you need to get at data inside an object, but only in special circumstances.
One of the unintended consequences of the JavaBeans (see [JavaBeansSpec]) specification was that it encouraged people to expose fields in their objects through getters and setters.
Access to “state information” should be encapsulated and exposed only in ways that make logical sense for the type, as viewed from the abstraction it exposes.
Instead, consider using pattern matching for those “rare” times when you need to extract information in a controlled way.
As we will see in “Unapply” on page 129, the pattern matching examples we have shown use unapply methods defined to extract.
These methods let you extract that information while hiding the implementation details.
In fact, the information returned by unapply might be a transformation of the actual information in the type.
Finally, when designing pattern matching statements, be wary of relying on a default case clause.
Under what circumstances would “none of the above” be the correct answer? It may indicate that the design should be refined so you know more precisely all the possible matches that might occur.
Enumerations Remember our examples involving various breeds of dog? In thinking about the types in these programs, we might want a top-level Breed type that keeps track of a number of breeds.
Such a type is called an enumerated type, and the values it contains are called enumerations.
While enumerations are a built-in part of many programming languages, Scala takes a different route and implements them as a class in its standard library.
This means there is no special syntax for enumerations in Scala, as in Java and C#
Instead, you just define an object that extends the Enumeration class.
Hence, at the byte code level, there is no connection between Scala enumerations and the enum constructs in Java and C#
We can see that our Breed enumerated type contains several variables of type Value, as in the following example:
Each declaration is actually calling a method named Value that takes a string argument.
We use this method to assign a long-form breed name to each enumeration value, which is what the Value.toString method returned in the output.
Note that there is no namespace collision between the type and method that both have the name Value.
One of them takes no arguments, another takes an Int ID value, and another takes both an Int and String.
These Value methods return a Value object, and they add the value to the enumeration’s collection of values.
In fact, Scala’s Enumeration class supports the usual methods for working with collections, so we can easily iterate through the breeds with a for loop and filter them by name.
The output above also demonstrated that every Value in an enumeration is automatically assigned a numeric identifier, unless you call one of the Value methods where you specify your own ID value explicitly.
You’ll often want to give your enumeration values human-readable names, as we did here.
Here’s another enumeration example adapted from the Scaladoc entry for Enumeration:
When a name isn’t assigned using one of the Value methods that takes a String argument, Value.toString prints the name of the type that is synthesized by the compiler, along with the ID value that was generated automatically.
Also, the import made the type alias, type Weekday = Value, in scope, which we used as the type for the argument for the isWorkingDay method.
If you don’t define a type alias like this, then you would declare the method as def isWorkingDay(d: Week Day.Value)
Since Scala enumerations are just regular objects, you could use any object with vals to indicate different “enumeration values.” However, extending Enumeration has several advantages.
It automatically manages the values as a collection that you can iterate over, etc., as in our examples.
It also automatically assigns unique integer IDs to each value.
Case classes (see “Case Classes” on page 136) are often used instead of enumerations in Scala because the “use case” for them often involves pattern matching.
Recap and What’s Next We’ve covered a lot of ground in this chapter.
We learned how flexible Scala’s syntax can be, and how it facilitates the creation of Domain-Specific Languages.
Then we explored Scala’s enhancements to looping constructs and conditional expressions.
We experimented with different uses for pattern matching, a powerful improvement on the familiar case-switch statement.
You should now be prepared to read a fair bit of Scala code, but there’s plenty more about the language to put in your tool belt.
In the next four chapters, we’ll explore Scala’s approach to object-oriented programming, starting with traits.
Introducing Traits Before we dive into object-oriented programming, there’s one more essential feature of Scala that you should get acquainted with: traits.
Understanding the value of this feature requires a little backstory.
In Java, a class can implement an arbitrary number of interfaces.
This model is very useful for declaring that a class exposes multiple abstractions.
For many interfaces, much of the functionality can be implemented with boilerplate code that will be valid for all classes that use the interface.
Java provides no built-in mechanism for defining and using such reusable code.
Instead, Java programmers must use ad hoc conventions to reuse implementation code for a given interface.
In the worst case, the developer just copies and pastes the same code into every class that needs it.
Often, the implementation of an interface has members that are unrelated (“orthogonal”) to the rest of the instance’s members.
The term mixin is often used for such focused and potentially reusable parts of an instance that could be independently maintained.
Have a look at the following code for a button in a graphical user interface, which uses callbacks for “clicks”:
The primary constructor takes a label argument and a list of callbacks that are invoked when the button’s click method is invoked.
For now, we want to focus on one particular problem.
This goes against the Single Responsibility Principle (see [Martin2003]), a means to the design goal of separation of concerns.
We would like to separate the buttonspecific logic from the callback logic, such that each logical component becomes simpler, more modular, and more reusable.
The callback logic is a good example of a mixin.
This separation is difficult to do in Java, even if we define an interface for the callback behavior.
We still have to embed the implementation code in the class somehow, compromising modularity.
The only other alternative is to use a specialized tool like aspect-oriented programming (AOP; see [AOSD]), as implemented by AspectJ (see [AspectJ]), an extension of Java.
It seeks to modularize these concerns, yet enable the fine-grained “mixing” of their behaviors with other concerns, including the core domain logic of the application, either at build or runtime.
Traits As Mixins Scala provides a complete mixin solution, called traits.
In our example, we can define the callback abstraction in a trait, as in a Java interface, but we can also implement the abstraction in the trait (or a derived trait)
We can declare classes that “mix in” the trait, much the way you can declare classes that implement an interface in Java.
However, in Scala we can even mix in traits at the same time we create instances.
That is, we don’t have to declare a class first that mixes in all the traits we want.
So, Scala traits preserve separation of concerns while giving us the ability to compose behavior on demand.
If you come from a Java background, you can think of traits as interfaces with optional implementations.
Or, if you prefer, you can think of traits as a “constrained” form of multiple inheritance.
Other languages provide constructs that are similar to traits, such as modules in Ruby, for example.
Let’s use a trait to separate the callback handling from the button logic.
So, let’s create a trait that implements this pattern, and then use it to handle callback behavior.
To simplify things, we’ll start with a single callback that counts the number of button clicks.
The logic for managing callbacks (i.e., the clickedCallbacks list) is omitted, as are the two auxiliary constructors.
The click method now only cares about the visual appearance of a “physical” button being clicked.
Button has only one concern, handling the “essence” of being a button.
Here is a trait that implements the logic of the Observer Pattern:
In this case, the structural type is defined by a method with a particular signature.
Any type that has a method with this signature can be used as an observer.
The main thing to notice for now is how this structural type minimizes the coupling between the Subject trait and any potential users of the trait.
Subject is still coupled by the name of the method in Observer through the structural type, i.e., to a method named receiveUpdate.
There are several ways we can reduce this remaining coupling.
We make it a var, rather than a val, because List is immutable, so we must create a new list when an observer is added using the addObserver method.
For now, notice that addObserver uses the list cons “operator” method (::) to prepend an observer to the list of observers.
The scala compiler is smart enough to turn the following statement:
Note that we wrote observer :: observers, with the existing observers list on the righthand side.
Recall that any method that ends with : binds to the right.
So, the previous statement is equivalent to the following statement:
The notifyObservers method iterates through the observers, using the foreach method and calls receiveUpdate on each one.
We use the placeholder _ to shorten the following expression:
This expression is actually the body of an “anonymous function,” called a function literal in Scala.
This is similar to a lambda and like constructs used in many other languages.
In Java, the foreach method would probably take an interface, and you would pass an instance of a class that implements the interface (e.g., the way Comparable is typically used)
In Scala, the List[A].foreach method expects an argument of type (A) => Unit, which is a function taking an instance of type A—where A represents the type of the elements of the list (Observer, in this case)—and returning Unit (like void in Java)
We chose to use a var with immutable Lists for the observers in this example.
We could have used a val with a mutable type, like ListBuffer.
That choice would make a little more sense for a real application, but we wanted to avoid the distraction of explaining new library classes.
Once again, we learned a lot of Scala from a small example.
Here is ObservableButton, which subclasses Button and mixes in Subject:
We start by importing everything in the observer package, using the _ wildcard.
Actually, we have only defined the Subject trait in the package.
The new class uses the with keyword to add the Subject trait to the class.
Using the super keyword (see “Overriding Abstract and Concrete Methods” on page 112), it first invokes the “superclass” method, Button.click, and then it notifies the observers.
Since the new click method overrides Button’s concrete implementation, the override keyword is required.
The with keyword is analogous to Java’s implements keyword for interfaces.
You can specify as many traits as you want, each with its own with keyword.
A class can extend a trait, and a trait can extend a class.
In fact, our Widget class earlier could have been declared to be a trait.
If you declare a class that uses one or more traits and it doesn’t extend another class, you must use the extends keyword for the first trait listed.
The error should really say, “with found, but extends expected.” To demonstrate this code, let’s start with a class for observing button clicks that simply counts the number of clicks:
Finally, let’s write a test that exercises all these classes.
We will use the Specs library (discussed in “Specs” on page 363) to write a Behavior-Driven Development ([BDD]) “specification” that exercises the combined Button and Subject types:
If you downloaded the code examples from the O’Reilly site, you can follow the directions in its README files for building and running the examples in this chapter.
The output of the specs “target” of the build should include the following text:
Notice that the strings A Button Observer should and observe button clicks correspond to strings in the example.
The output of a Specs run provides a nice summary of the requirements for the items being tested, assuming good choices were made for the strings.
The button is clicked three times, using the for loop.
If you are accustomed to using an XUnit-style TDD tool, like JUnit (see [JUnit]) or ScalaTest (see [ScalaTestTool] and “ScalaTest” on page 361), then the last line is equivalent to the following JUnit assertion:
Suppose we need only one ObservableButton instance? We actually don’t have to declare a class that subclasses Button with Subject.
We can incorporate the trait when we create the instance.
The next example shows a revised Specs file that instantiates a Button with Subject mixed in as part of the declaration:
The revised declaration of observableButton actually creates an anonymous class in which we override the click method, as before.
The main difference with creating anonymous classes in Java is that we can incorporate traits in this process.
Java does not let you implement a new interface while instantiating a class.
Finally, note that the inheritance hierarchy for an instance can be complex if it mixes in traits that extend other traits, etc.
Stackable Traits There are a couple of refinements we can do to improve the reusability of our work and to make it easier to use more than one trait at a time, i.e., to “stack” them.
First, let’s introduce a new trait, Clickable, an abstraction for any widget that responds to clicks:
We’re starting with a new package, ui2, to make it easier to keep older and newer versions of the examples distinct in the downloadable code.
The Clickable trait looks just like a Java interface; it is completely abstract.
If Clickable were a class, we would have to add the abstract keyword in front of the class keyword.
This code is like Java code that implements a Clickable interface.
When we previously defined ObservableButton (in “Traits As Mixins” on page 76), we overrode Button.click to notify the observers.
When we refactor the code this way, we realize that we don’t really care about observing buttons; we care about observing clicks.
Here is a trait that focuses solely on observing Clickable:
It calls super.click(), but what is super in this case? At this point, it could only appear to be Clickable, which declares but does not define the click method, or it could be Subject, which doesn’t have a click method.
In fact, super will be bound when this trait is mixed into an instance that defines a concrete click method, such as Button.
Except for declaring abstract classes, the abstract keyword is only required on a method in a trait when the method has a body, but it calls the super method that doesn’t have a concrete implementation in parents of the trait.
We instantiate a Button with the ObservableClicks trait mixed in, but now there is no override of click required.
Hence, this client of Button doesn’t have to worry about properly overriding click.
The JavaBeans specification (see [JavaBeansSpec]) has the idea of “vetoable” events, where listeners for changes to a JavaBean can veto the change.
Let’s implement something similar with a trait that vetoes more than a set number of clicks:
Isn’t the field declared to be a val? There is no constructor defined to initialize it to another value.
This trait also declares a count variable to keep track of the number of clicks seen.
It only calls the super.click() method if the count is less than or equal to the maxAllowed count.
Here is a Specs object that demonstrates ObservableClicks and VetoableClicks working together.
Note that a separate with keyword is required for each trait, as opposed to using one keyword and separating the names with commas, as Java does for implements clauses:
The observableButton is declared as follows: new Button("Okay") with ObservableClicks with VetoableClicks.
We can infer that the click override in VetoableClicks is called before the click override in ObservableClicks.
Loosely speaking, since our anonymous class doesn’t define click itself, the method lookup proceeds right to left, as declared.
In the meantime, what happens if we use the traits in the reverse order?
ObservableClicks now has precedence over VetoableClicks, so the count of clicks is incremented, even when some clicks are subsequently vetoed! So, the order of declaration matters, which is important to remember for preventing unexpected behavior when traits impact each other.
Perhaps another lesson to note is that splitting objects into too many fine-grained traits can obscure the order of execution in your code! Breaking up your application into small, focused traits is a powerful way to create reusable, scalable abstractions and “components.” Complex behaviors can be built up through declarative composition of traits.
Constructing Traits Traits don’t support auxiliary constructors, nor do they accept an argument list for the primary constructor, the body of a trait.
However, they can’t pass arguments to the parent class constructor (even literal values), so traits can only extend classes that have a no-argument primary or auxiliary constructor.
However, like classes, the body of a trait is executed every time an instance is created that uses the trait, as demonstrated by the following script:
Notice the order of invocation of the class and trait constructors.
So, while you can’t pass construction parameters to traits, you can initialize fields with default values or leave them abstract.
If a concrete field in a trait does not have a suitable default value, there is no “fail-safe” way to initialize the value.
All the alternative approaches require some ad hoc steps by users of the trait, which is error-prone because they might do it wrong or forget to do it all.
Perhaps the field should be left abstract, so that classes or other traits that use this trait are forced to define the value appropriately.
Another solution is to move that field to a separate class, where the construction process can guarantee that the correct initialization data is supplied by the user.
It might be that the whole trait should actually be a class instead, so you can define a constructor for it that initializes the field.
Class or Trait? When considering whether a “concept” should be a trait or a class, keep in mind that traits as mixins make the most sense for “adjunct” behavior.
If you find that a particular trait is used most often as a parent of other classes, so that the child classes behave as the parent trait, then consider defining the trait as a class instead, to make this logical relationship more clear.
We said behaves as, rather than is a, because the former is the more precise definition of inheritance, based on the Liskov Substitution Principle—see [Martin2003], for example.
Avoid concrete fields in traits that can’t be initialized to suitable default values.
Use abstract fields instead, or convert the trait to a class with a constructor.
Of course, stateless traits don’t have any issues with initialization.
It’s a general principle of good object-oriented design that an instance should always be in a known valid state, starting from the moment the construction process finishes.
Recap and What’s Next In this chapter, we learned how to use traits to encapsulate and share cross-cutting concerns between classes.
We covered when and how to use traits, how to “stack” multiple traits, and the rules for initializing values within traits.
In the next chapter, we explore how the fundamentals of object-oriented programming work in Scala.
Even if you’re an old hand at object-oriented programming, you’ll want to read the next several chapters to understand the particulars of Scala’s approach to OOP.
If you’re coming from the Java world, you’ll notice some notable improvements over the limitations of Java’s object model.
We’ll use the term instance to refer to a class instance generically, meaning either an object or an instance of a class, to avoid the potential for confusion between these two concepts.
We will see later that additional keywords can also be used, like final to prevent creation of derived classes and abstract to indicate that the class can’t be instantiated, usually because it contains or inherits member declarations without providing concrete definitions for them.
An instance can refer to itself using the this keyword, just as in Java and similar languages.
Following Scala’s convention, we use the term method for a function that is tied to an instance.
Some other object-oriented languages use the term “member function.” Method definitions start with the def keyword.
Two or more methods can have the same name as long as their full signatures are unique.
The signature includes the type name, the list of parameters with types, and the method’s return value.
There is an exception to this rule due to type erasure, which is a feature of the JVM only, but is used by Scala on both the JVM and .NET platforms, to minimize incompatibilities.
Suppose two methods are identical except that one takes a parameter of type List[String] while the other takes a parameter of type List[Int], as follows:
You’ll get a compilation error on the second method because the two methods will have an identical signature after type erasure.
The scala interpreter will let you type in both methods.
However, if you try to load the previous example using the :load file command, you’ll get the same error scalac raises.
Also by convention, we use the term field for a variable that is tied to an instance.
The term attribute is often used in other languages (like Ruby)
Note that the state of an instance is the union of all the values currently represented by the instance’s fields.
As we discussed in “Variable Declarations” on page 24, read-only (“value”) fields are declared using the val keyword, and read-write fields are declared using the var keyword.
We use the term member to refer to a field, method, or type in a generic way.
Note that field and method members (but not type members) share the same namespace, unlike Java.
Finally, new instances of reference types are created from a class using the new keyword, as in languages like Java and C#
Note that you can drop the parentheses when using a default constructor (i.e., one that takes no arguments)
In fact, there are no public constructors for these types, so an expression like val i = new Int(1) won’t compile.
A child (or derived) class can have one and only one parent (or base) class.
The sole exception is the root of the Scala class hierarchy, Any, which has no parent.
Here are snippets of one of the first we saw, in “Abstract Types And Parameterized Types” on page 47:
As in Java, the keyword extends indicates the parent class, in this case BulkReader.
In Scala, extends is also used when a class inherits a trait as its parent (even when it mixes in other traits using the with keyword)
Also, extends is used when one trait is the child of another trait or class.
If you don’t extend a parent class, the default parent is AnyRef, a direct child class of Any.
Constructors in Scala Scala distinguishes between a primary constructor and zero or more auxiliary constructors.
In Scala, the primary constructor is the entire body of the class.
It has a label and a list of callback functions that are invoked if the button is clicked.
The click method iterates through the list of callbacks and invokes each one.
The primary constructor, which is the body of the entire class, has a parameter list that takes a label string and a list of callback functions.
Because each parameter is declared as a val, the compiler generates a private field corresponding to each parameter (a different internal name is used), along with a public reader method that has the same name as the parameter.
If a parameter has the var keyword, a public writer method is also generated with the parameter’s name as a prefix, followed by _=
For example, if label were declared as a var, the writer method would be named label_= and it would take a single argument of type String.
There are times when you don’t want the accessor methods to be generated automatically.
In other words, you want the field to be private.
Add the private keyword before the val or var keyword, and the accessor methods won’t be generated.
For you Java programmers, Scala doesn’t follow the JavaBeans [JavaBeansSpec] convention that field reader and writer methods begin with get and set, respectively, followed by the field name with the first character capitalized.
When an instance of the class is created, each field corresponding to a parameter in the parameter list will be initialized with the parameter automatically.
No constructor logic is required to initialize these fields, in contrast to most other object-oriented languages.
It uses the convenient require function that is imported automatically into the current scope (as we’ll discuss in “The Predef Object” on page 145)
If the list is null, require will throw an exception.
Scala even makes it difficult to pass null as the second parameter to the constructor; it won’t type check when you compile it.
However, you can assign null to a value, as shown.
The first auxiliary constructor accepts a label and a single callback.
It calls the primary constructor, passing the label and a new List to wrap the single callback.
It calls the primary constructor with Nil (which represents an empty List object)
The constructor then prints a warning message that there are no callbacks, since lists are immutable and there is no way to replace the callback list val with a new one.
To avoid infinite recursion, Scala requires each auxiliary constructor to invoke another constructor defined before it (see [ScalaSpec2009])
The constructor invoked may be either another auxiliary constructor or the primary constructor, and it must be the first statement in the auxiliary constructor’s body.
Additional processing can occur after this call, such as the warning message printed in our example.
Because all auxiliary constructors eventually invoke the primary constructor, logic checks and other initializations done in the body will be performed consistently for all instances created.
There are a few advantages of Scala’s constraints on constructors: Elimination of duplication.
Because auxiliary constructors invoke the primary constructor, potential duplication of construction logic is largely eliminated.
Code size reduction As shown in the examples, when one or more of the primary constructor parameters is declared as a val or a var, Scala automatically generates a field, the appropriate accessor methods (unless they are declared private), and the initialization logic for when instances are created.
There is also at least one disadvantage of Scala’s constraints on constructors: Less flexibility.
Sometimes it’s just not convenient to have one constructor body that all constructors are forced to use.
In such cases, it may simply be that the class has too many responsibilities and it should be refactored into smaller classes.
Calling Parent Class Constructors The primary constructor in a derived class must invoke one of the parent class constructors, either the primary constructor or an auxiliary constructor.
The on parameter is declared as a var, so it is mutable.
Note that they must invoke a preceding constructor in RadioButton WithCallbacks, as before.
Declaring all these constructors in each class could get tedious after a while, but we explored techniques in Chapter 4 that can eliminate repetition.
While super is used to invoke overridden methods, as in Java, it cannot be used to invoke a super class constructor.
Nested Classes Scala lets you nest class declarations, like many object-oriented languages.
Suppose we want all Widgets to have a map of properties.
These properties could be size, color, whether or not the widget is visible, etc.
We might use a simple map to hold the properties, but let’s assume that we also want to control access to the properties, and to perform other operations when they change.
Here is one way we might expand our original Widget example from “Traits As Mixins” on page 76 to add this feature:
We added a Properties class that has a private, mutable reference to an immutable HashMap.
We also added three public methods that retrieve the size (i.e., the number of properties defined), retrieve a single element in the map, and update the map with a new element, respectively.
We might need to do additional work in the update method, and we’ve indicated as much with comments.
You can see from the previous example that Scala allows classes to be declared inside one another, or “nested.” A nested class make sense when you have enough related functionality to lump together in a class, but the functionality is only ever going to be used by its “outer” class.
So far, we’ve covered how to declare classes, how to instantiate them, and some of the basics of inheritance.
In the next section, we’ll discuss visibility rules within classes and objects.
For convenience, we’ll use the word “type” in this section to refer to classes and traits generically, as opposed to referring to member type declarations.
We’ll include those when we use the term “member” generically, unless otherwise indicated.
Most object-oriented languages have constructs to constrain the visibility (or scope) of type and type-member declarations.
These constructs support the object-oriented form of encapsulation, where only the essential public abstraction of a class or trait is exposed and implementation information is hidden from view.
You’ll want to use public visibility for anything that users of your classes and objects should see and use.
Keep in mind that the set of publicly visible members form the abstraction exposed by the type, along with the type’s name itself.
The conventional wisdom in object-oriented design is that fields should be private or protected.
If access is required, it should happen through methods, but not everything should be accessible by default.
The virtue of the Uniform Access Principle (see “When Accessor Methods and Fields Are Indistinguishable: The Uniform Access Principle” on page 123) is that we can give the user the semantics of public field access via either a method or direct access to a field, whichever is appropriate for the task.
The art of good object-oriented design includes defining minimal, clear, and cohesive public abstractions.
There are two kinds of “users” of a type: derived types, and code that works with instances of the type.
Derived types usually need more access to the members of their parent types than users of instances do.
Scala’s visibility rules are similar to Java’s, but tend to be both more consistently applied and more flexible.
For example, in Java, if an inner class has a private member, the enclosing class can see it.
In Scala, the enclosing class can’t see a private member, but Scala provides another way to declare it visible to the enclosing class.
As in Java and C#, the keywords that modify visibility, such as private and protected, appear at the beginning of declarations.
You’ll find them before the class or trait keywords for types, before the val or var for fields, and before the def for methods.
Protected types are visible only within the same package and subpackages.
To keep things simple, we’ll use fields for member examples.
Unfortunately, you can’t apply any of the visibility modifiers to packages.
Therefore, a package is always public, even when it contains no publicly visible types.
Public Visibility Any declaration without a visibility keyword is “public,” meaning it is visible everywhere.
This is in contrast to Java, which defaults to public visibility only within the enclosing package (i.e., “package private”)
Other object-oriented languages, like Ruby, also default to public visibility:
Protected Visibility Protected visibility is for the benefit of implementers of derived types, who need a little more access to the details of their parent types.
Any member declared with the protected keyword is visible only to the defining type, including other instances of the same type and any derived types.
When applied to a type, protected limits visibility to the enclosing package.
Java, in contrast, makes protected members visible throughout the enclosing package.
Scala handles this case with scoped private and protected access:
When you compile this file with scalac, you get the following output.
The file names before the N: line numbers have been removed from the output to better fit the space.
The // ERROR comments in the listing mark the lines that fail to parse.
Finally, because ProtectedClass4 is declared protected, it is not visible in the scopeB package.
Private Visibility Private visibility completely hides implementation details, even from the implementers of derived classes.
Any member declared with the private keyword is visible only to the defining type, including other instances of the same type.
When applied to a type, private limits visibility to the enclosing package:
They are completely invisible to the subclass, as indicated by the error messages.
Nor can it access a private field in a Nested class.
Note, however, that the equalFields method can access private members of the other instance.
Curiously, our previous example was able to declare a public class that subclassed a protected class without a similar error.
Finally, just as for protected type declarations, the private types can’t be subclassed outside the same package.
Scoped Private and Protected Visibility Scala allows you to fine-tune the scope of visibility with the scoped private and protected visibility declarations.
Note that using private or protected in a scoped declaration is interchangeable, as they behave identically, except under inheritance when applied to members.
While either choice behaves the same in most scenarios, it is more common to see private[X] rather than protected[X] used in code.
In the core libraries included with Scala, the ratio is roughly five to one.
Let’s begin by considering the only differences in behavior between scoped private and scoped protected—how they behave under inheritance when members have these scopes:
In contrast, for a derived class outside the same package, it has no access to any of the scoped private members of Class1
However, all the scoped protected members are visible in both derived classes.
We’ll use scoped private declarations for the rest of our examples and discussion, since use of scoped private is a little more common in the Scala library than scoped protected, when the previous inheritance scenarios aren’t a factor.
First, let’s start with the most restrictive visibility, private[this], as it affects type members:
Since they are part of the expression that started on line 5, the compiler stopped after the first error.
The private[this] members are only visible to the same instance.
An instance of the same class can’t see private[this] members of another instance, so the equalFields method won’t parse.
Otherwise, the visibility of class members is the same as private without a scope specifier.
When declaring a type with private[this], use of this effectively binds to the enclosing package, as shown here:
In the same package, attempting to declare a public or protected subclass fails.
Similarly, an attempt to declare a class in unrelated scopeB using PrivateClass1 also fails.
Hence, when applied to types, private[this] is equivalent to Java’s package private visibility.
Next, let’s examine type-level visibility, private[T], where T is a type:
When members of T are declared private[T] the behavior is equivalent to private.
It is not equivalent to private[this], which is more restrictive.
Finally, let’s look at the effect of package-level scoping of type members:
The only errors are when we attempt to access members scoped to scopeA from the unrelated package scopeB and when we attempt to access a member from a nested package scopeA2 that is scoped to that package.
When a type or member is declared private[P], where P is the enclosing package, then it is equivalent to Java’s package private visibility.
Final Thoughts on Visibility Scala visibility declarations are very flexible, and they behave consistently.
They provide fine-grained control over visibility at all possible scopes, from the instance level (private[this]) up to package-level visibility (private[P], for a package P)
For example, they make it easier to create “components” with types exposed outside of the component’s top-level package, while hiding implementation types and type members within the “component’s” packages.
Finally, we have observed a potential “gotcha” with hidden members of traits.
Be careful when choosing the names of members of traits.
If two traits have a member of the same name and the traits are used in the same instance, a name collision will occur even if both members are private.
Recap and What’s Next We introduced the basics of Scala’s object model, including constructors, inheritance, nesting of classes, and rules for visibility.
In the next chapter we’ll explore Scala’s more advanced OOP features, including overriding, companion objects, case classes, and rules for equality between objects.
We’ve got the basics of OOP in Scala under our belt, but there’s plenty more to learn.
Overriding Members of Classes and Traits Classes and traits can declare abstract members: fields, methods, and types.
These members must be defined by a derived class or trait before an instance can be created.
Most object-oriented languages support abstract methods, and some also support abstract fields and types.
When overriding a concrete member, Scala requires the override keyword.
It is optional when a subtype defines (“overrides”) an abstract member.
Conversely, don’t use override unless you are actually overriding a member.
It catches misspelled members that were intended to be overrides.
It catches a potentially subtle bug that can occur if a new member is added to a.
That is, the derived-class member was never intended to override a base-class member.
Because the derived class member won’t have the override keyword, the compiler will throw an error when the new base-class member is introduced.
Having to add the keyword reminds you to consider what members should or should not be overridden.
It helps catch errors of the first type (misspellings), but it can’t help with errors of the second type, since using the annotation is optional.
Attempting to Override final Declarations However, if a declaration includes the final keyword, then overriding the declaration is prohibited.
In the following example, the fixedMethod is declared final in the parent class.
Attempting to compile the example will result in a compilation error:
This constraint applies to classes and traits as well as members.
In this example, the class Fixed is declared final, so an attempt to derive a new type from it will also fail to compile:
Some of the types in the Scala library are final, including JDK classes like String and all the “value” types derived from AnyVal (see “The Scala Type Hierarchy” on page 155)
For declarations that aren’t final, let’s examine the rules and behaviors for overriding, starting with methods.
Overriding Abstract and Concrete Methods Let’s extend our familiar Widget base class with an abstract method draw, to support “rendering” the widget to a display, web page, etc.
We’ll also override a concrete method familiar to any Java programmer, toString(), using an ad hoc format.
The state of a Widget is one thing; how it is rendered on different platforms, thick clients, web pages, mobile devices, etc., is a separate issue.
So, drawing is a very good candidate for a trait, especially if you want your GUI abstractions to be portable.
However, to keep things simple, we will handle drawing in the Widget hierarchy itself.
The draw method is abstract because it has no body; that is, the method isn’t followed by an equals sign (=), nor any text after it.
Therefore, Widget has to be declared abstract (it was optional before)
Each concrete subclass of Widget will have to implement draw or rely on a parent class that implements it.
We don’t need to return anything from draw, so its return value is Unit.
Since AnyRef defines toString, the override keyword is required for Widget.toString.
Here is the revised Button class, with draw and toString methods:
Button also overrides toString, so the override keyword is required.
The super keyword is analogous to this, but it binds to the parent type, which is the aggregation of the parent class and any mixed-in traits.
The search for super.toString will find the “closest” parent type toString, as determined by the linearization process (see “Linearization of an Object’s Hierarchy” on page 159)
In this case, since Clickable doesn’t define toString, Widget.toString will be called.
Overriding a concrete method should be done rarely, because it is errorprone.
Should you invoke the parent method? If so, when? Do you call it before doing anything else, or afterward? While the writer of the parent method might document the overriding constraints for the method, it’s difficult to ensure that the writer of a derived class will honor those constraints.
A much more robust approach is the Template Method Pattern (see [GOF1995])
Overriding Abstract and Concrete Fields Most object-oriented languages allow you to override mutable fields (var)
Fewer OO languages allow you to define abstract fields or override concrete immutable fields (val)
For example, it’s common for a base class constructor to initialize a mutable field and for a derived class constructor to change its value.
We’ll discuss overriding fields in traits and classes separately, as traits have some particular issues.
We would like the ability to override the value in a class that mixes in this trait.
Unfortunately, in Scala version 2.7.X, it is not possible to override a val defined in a trait.
However it is possible to override a val defined in a parent class.
Version 2.8 of Scala does support overriding a val in a trait.
Because the override behavior for a val in a trait is changing, you should avoid relying on the ability to override it, if you are currently using Scala version 2.7.X.
Unfortunately, the version 2.7 compiler accepts code that attempts to override a traitdefined val, but the override does not actually happen, as illustrated by this example:
However, if you run this script with scala version 2.8.0, you get this output:
Attempts to override a trait-defined val will be accepted by the compiler, but have no effect in Scala version 2.7.X.
There are three workarounds you can use with Scala version 2.7
The first is to use some advanced options for scala and scalac.
The -Xfuture option will enable the override behavior supported in version 2.8
The -Xcheckinit option will analyze your code and report whether the behavior change will break it.
The option -Xexperimental, which enables many experimental changes, will also warn you that the val override behavior is different.
The second workaround is to make the val abstract in the trait.
This forces an instance using the trait to assign a value.
Declaring a val in a trait abstract is a perfectly useful design approach for both versions of Scala.
In fact, this will be the best design choice, when there is no appropriate default value to assign to the val in the trait:
So, an abstract val works fine, unless the field is used in the trait body in a way that will fail until the field is properly initialized.
Unfortunately, the proper initialization won’t occur until after the trait’s body has executed.
While it appears that we are creating an instance of the trait with new AbstractT2 ..., we are actually using an anonymous class that implicitly extends the trait.
As you might expect, the inverse is calculated too early.
Note that a divide by zero exception isn’t thrown; the compiler recognizes the value is infinite, but it hasn’t actually “tried” the division yet!
As an exercise, try selectively removing (or commenting out) the different println statements, one at a time.
Then try putting it back, but after the val value = 10 line.
What this experiment really shows is that side effects (i.e., from the println statements) can be unexpected and subtle, especially during initialization.
Scala provides two solutions to this problem: lazy values, which we discuss in “Lazy Vals” on page 190, and pre-initialized fields, which is demonstrated in the following refinement to the previous example:
We instantiate an anonymous inner class, initializing the value field in the block, before the with AbstractT2 clause.
This guarantees that value is initialized before the body of AbstractT2 is executed, as shown when you run the script:
Also, if you selectively remove any of the println statements, you get the same expected and now predictable results.
Now let’s consider the second workaround we described earlier, changing the declaration to var.
This solution is more suitable if a good default value exists and you don’t want to require instances that use the trait to always set the value.
In this case, change the val to a var, either a public var or a private var hidden behind reader and writer methods.
Either way, we can simply reassign the value in a derived trait or class.
Returning to our VetoableClicks example, here is the modified VetoableClicks trait that uses a public var for maxAllowed:
Since the body of the trait is executed before the body of the class using it, reassigning the field value happens after the initial assignment in the trait’s body.
However, as we saw before, that reassignment could happen too late if the field is used in the trait’s body in some calculation that will become invalid by a reassignment later! You can avoid this problem if you make the field private and define a public writer method that redoes any dependent calculations.
Another disadvantage of using a var declaration is that maxAllowed was not intended to be writable.
As we will see in Chapter 8, read-only values have important benefits.
We would prefer for maxAllowed to be read-only, at least after the construction process completes.
We can see that the simple act of changing the val to a var causes potential problems for the maintainer of VetoableClicks.
The maintainer must carefully consider whether or not the value will change and if a change will invalidate the state of the instance.
This issue is especially pernicious in multithreaded systems (see “The Problems of Shared, Synchronized State” on page 193)
Avoid var fields when possible (in classes as well as traits)
Overriding Abstract and Concrete Fields in Classes In contrast to traits, overriding a val declared in a class works as expected.
Here is an example with both a val override and a var reassignment in a derived class:
The override keyword is required for the concrete val field name, but not for the var field count.
This is because we are changing the initialization of a constant (val), which is a “special” operation.
If you run this script, the output is the following:
Both fields are overridden in the derived class, as expected.
Here is the same example modified so that both the val and the var are abstract in the base class:
It’s important to emphasize that name and count are abstract fields, not concrete fields with default values.
A similar-looking declaration of name in a Java class, String name;, would declare a concrete field with the default value (null in this case)
Java doesn’t support abstract fields or types (as we’ll discuss next), only methods.
Like parameterized types, they provide an abstraction mechanism at the type level.
The example shows how to declare an abstract type and how to define a concrete value in derived classes.
The concrete derived class StringBulkReader provides a concrete value using type In = String.
Unlike fields and methods, it is not possible to override a concrete type definition.
However, the abstract declaration can constrain the allowed concrete type values.
Finally, you probably noticed that this example also demonstrates defining an abstract field, using a constructor parameter, and an abstract method.
The definition of the Observer type is a structural type with a method named receiveUpdate.
Observers must have this “structure.” Let’s generalize the implementation now, using an abstract type:
Now, AbstractSubject declares type Observer as abstract (implicitly, because there is no definition)
Since the original structural type is gone, we don’t know exactly how to notify an observer.
So, we also added an abstract method notify, which a concrete class or trait will define as appropriate.
All notify has to do is call the observer function, passing the subject as the sole argument.
Here is a specification that exercises these two variations, observing button clicks as before:
In this case, we don’t need another “observer” instance at all.
We just maintain a count variable and pass a function literal to addObserver to increment the count (and ignore the button)
It requires no special instances, no traits defining abstractions, etc.
AbstractSubject is more reusable than the original definition of Subject, because it imposes fewer constraints on potential observers.
AbstractSubject illustrates that an abstraction with fewer concrete details is usually more reusable.
In fact, the following two definitions are functionally equivalent, from the perspective of the user:
This equivalence is an example of the Uniform Access Principle.
Clients read and write field values as if they are publicly accessible, even though in some cases they are actually calling methods.
The reader method in the second version does not have parentheses.
Recall that consistency in the use of parentheses is required if a method definition omits parentheses.
This is only possible if the method takes no arguments.
For the Uniform Access Principle to work, we want to define field reader methods without parentheses.
Contrast that with Ruby, where method parentheses are always optional as long as the parse is unambiguous.
As a bit of syntactic sugar, the compiler allows invocations of methods with this format to be written in either of the following ways:
We named the private variable cnt in the alternative definition.
Scala keeps field and method names in the same namespace, which means we can’t name the field count if a method is named count.
Many languages, like Java, don’t have this restriction because they keep field and method names in separate namespaces.
However, these languages can’t support the Uniform Access Principle as a result, unless they build in ad hoc support in their grammars or compilers.
Since member object definitions behave similar to fields from the caller’s perspective, they are also in the same namespace as methods and fields.
There is one other benefit of this namespace “unification.” If a parent class declares a parameterless method, then a subclass can override that method with a val.
If the parent’s method is concrete, then the override keyword is required:
Why is this feature useful? It allows derived classes and traits to use a simple field access when that is sufficient, or a method call when more processing is required, such as lazy initialization.
The same argument holds for the Uniform Access Principle, in general.
Overriding a def with a val in a subclass can also be handy when interoperating with Java code.
Turn a getter into a val by placing it in the constructor.
You’ll see this in action in the following example, in which our Scala class Person implements a hypothetical PersonInterface from some legacy Java code:
If you only have a few accessors in the Java code you’re integrating with, this technique makes quick work of them.
What about overriding a parameterless method with a var, or overriding a val or var with a method? These are not permitted because they can’t match the behaviors of the things they are overriding.
If you attempt to use a var to override a parameterless method, you get an error that the writer method, override name_=, is not overriding anything.
This would also be inconsistent with a philosophical goal of functional programming, that a method that takes no parameters should always return the same result.
Because a var is changeable, the noparameter “method” defined in the parent type would no longer return the same result consistently.
If you could override a val with a method, there would be no way for Scala to guarantee that the method would always return the same value, consistent with val semantics.
That issue doesn’t exist with a var, of course, but you would have to override the var with two methods, a reader and a writer.
Companion Objects Recall that fields and methods defined in objects serve the role that class “static” fields and methods serve in languages like Java.
When object-based fields and methods are closely associated with a particular class, they are normally defined in a companion object.
First, recall that if a class (or a type referring to a class) and an object are declared in the same file, in the same package, and with the same name, they are called a companion class (or companion type) and a companion object, respectively.
There is no namespace collision when the name is reused in this way, because Scala stores the class name in the type namespace, while it stores the object name in the term namespace (see [ScalaSpec2009])
The two most interesting methods frequently defined in a companion object are apply and unapply.
Apply Scala provides some syntactic sugar in the form of the apply method.
When an instance of a class is followed by parentheses with a list of zero or more parameters, the compiler invokes the apply method for that instance.
This is true for an object with a defined apply method (such as a companion object), as well as an instance of a class that defines an apply method.
In the case of an object, apply is conventionally used as a factory method, returning a new instance.
So, you can create a new Pair as follows: val p = Pair(1, "one")
It looks like we are somehow creating a Pair instance without a new.
If there are several alternative constructors for a class and it also has a companion object, consider defining fewer constructors on the class and defining several overloaded apply methods on the companion object to handle the variations.
However, apply is not limited to instantiating the companion class.
It could instead return an instance of a subclass of the companion class.
Here is an example where we define a companion object Widget that uses regular expressions to parse a string representing a Widget subclass.
When a match occurs, the subclass is instantiated and the new instance is returned:
Widget.apply receives a string “specification” that defines which class to instantiate.
The string might come from a configuration file with widgets to create at startup, for example.
The string format is the same format used by toString()
The match expression applies each regular expression to the string.
If successful, it extracts the substring in the first capture group in the regular expression and assigns it to the variable label.
Finally, a new Button with this label is created, wrapped in a Some.
Finally, if apply can’t match the string, it returns None.
The first match statement implicitly invokes Widget.apply with the string "(button: label=click me, (Widget))"
If a button wrapped in a Some is not returned with the label "click me", this test will fail.
Next, a similar test for a TextField widget is done.
The final test uses an invalid string and confirms that None is returned.
A better implementation would use a factory design pattern from [GOF1995]
Nevertheless, the example illustrates how an apply method can be used as a real factory.
There is no requirement for apply in an object to be used as a factory.
Neither is there any restriction on the argument list or what apply returns.
However, because it is so common to use apply in an object as a factory, use caution when using apply for other purposes, as it could confuse users.
However, there are good counterexamples, such as the use of apply in Domain-Specific Languages (see Chapter 11)
The factory convention is less commonly used for apply defined in classes.
For example, in the Scala standard library, Array.apply(i: int) returns the element at index i in the array.
Many of the other collections use apply in a similar way.
Finally, as a reminder, although apply is handled specially by the compiler, it is otherwise no different from any other method.
You can overload it, you can invoke it directly, etc.
Unapply The name unapply suggests that it does the “opposite” operation of apply.
Indeed, it is used to extract the constituent parts of an instance.
Hence, unapply is often defined in companion objects and is used to extract the field values from instances of the corresponding companion types.
Here is an expanded button.scala with a Button object that defines an unapply extractor method:
Button.unapply takes a single Button argument and returns a Some wrapping the label value.
We’ll see how to handle more than one field in a moment.
The first three examples (in clauses) confirm that Button.unapply is only called for actual Button instances or instances of derived classes, like RadioButton.
Since unapply takes a Button argument (in this case), the Scala runtime type checks the instance being matched.
It then looks for a companion object with an unapply method and invokes that method, passing the instance.
The default case clause case _ is invoked for the instances that don’t type check as compatible.
The remaining examples (in clauses) confirm that the correct values for the label are extracted.
The Scala runtime automatically extracts the item in the Some.
What about extracting multiple fields? For a fixed set of known fields, a Some wrapping a Tuple is returned, as shown in this updated version of RadioButton:
Apply and UnapplySeq for Collections What if you want to build a collection from a variable argument list passed to apply? What if you want to extract the first few elements from a collection and you don’t care about the rest of it? In this case, you define apply and unapplySeq (“unapply sequence”) methods.
The [A] type parameterization on these methods allows the List object, which is not parameterized, to construct a new List[A]
See “Understanding Parameterized Types” on page 249 for more details.
Most of the time, the type parameter will be inferred based on the context.
The parameter list xs: A* is a variable argument list.
Callers of apply can pass as many A instances as they want, including none.
Internally, variable argument lists are stored in an Array[A], which inherits the toList method from Iterable that we used here.
Accepting variable arguments to a function can be convenient for users, and converting the arguments to a List is often ideal for internal management.
Symbols are more commonly used in Ruby, for example, where the same symbol would be written as :four.
The unapplySeq method is trivial; it returns the input list wrapped in a Some.
However, this is sufficient for pattern matching, as shown in this example:
The List(x, y, _*) syntax means we will only match on a list with at least two elements, and the first two elements will be assigned to x and y.
Companion Objects and Java Static Methods There is one more thing to know about companion objects.
Whenever you define a main method to use as the entry point for an application, Scala requires you to put it in an object.
However, at the time of this writing, main methods cannot be defined in a.
Because of implementation details in the generated code, the JVM won’t find the main method.
For now, you must define any main method in a singleton object (i.e., a “non-companion” object; see [ScalaTips])
Consider the following example of a simple Person class and companion object that attempts to define main:
This code compiles fine, but if you attempt to invoke Person.main, using scala -cp ...
The separate singleton object PersonTest defined in this example has to be used.
You get an assertion error from the second call to assert, which is intentional:
In fact, this is a general issue with methods defined in companion objects that need to be visible to Java code as static methods.
You have to put these methods in singleton objects instead.
Consider the following Java class that attempts to create a user with Person.apply:
Do not define main or any other method in a companion object that needs to be visible to Java code as a static method.
If you have no other choice but to call a method in a companion object from Java, you can explicitly create an instance of the object with new, since the object is a “regular” Java class in the byte code, and call the method on the instance.
Case Classes In “Matching on Case Classes” on page 67, we briefly introduced you to case classes.
Case classes have several useful features, but also some drawbacks.
Let’s rewrite the Shape example we used in “A Taste of Concurrency” on page 16 to use case classes.
Adding the case keyword causes the compiler to add a number of useful features automatically.
The keyword suggests an association with case expressions in pattern matching.
Indeed, they are particularly well suited for that application, as we will see.
First, the compiler automatically converts the constructor arguments into immutable fields (vals)
Second, the compiler automatically implements equals, hashCode, and toString methods to the class, which use the fields specified as constructor arguments.
In fact, the generated toString methods produce the same outputs as the ones we implemented ourselves.
Also, the body of Point is gone because there are no methods that we need to define! The following script uses these methods that are now in the shapes:
As we’ll see in “Equality of Objects” on page 142, the == method actually invokes the equals method.
Even outside of case expressions, automatic generation of these three methods is very convenient for simple, “structural” classes, i.e., classes that contain relatively simple fields and behaviors.
Third, when the case keyword is used, the compiler automatically creates a companion object with an apply factory method that takes the same arguments as the primary constructor.
The previous example used the appropriate apply methods to create the Points, the different Shapes, and also the List itself.
That’s why we don’t need new; we’re actually calling apply(x,y) in the Point companion object, for example.
You can have secondary constructors in case classes, but there will be no overloaded apply method generated that has the same argument list.
You’ll have to use new to create instances with those constructors.
The companion object also gets an unapply extractor method, which extracts all the fields of an instance in an elegant fashion.
The following script demonstrates the extractors in pattern matching case statements:
Syntactic Sugar for Binary Operations By the way, remember in “Matching on Sequences” on page 65 when we discussed matching on lists? We wrote this case expression:
It turns out that the following expressions are identical: case head :: tail => ...
We are using the companion object for the case class named ::, which is used for nonempty lists.
When used in case expressions, the compiler supports this special infix operator notation for invocations of unapply.
It works not only for unapply methods with two arguments, but also with one or more arguments.
For an unapply that takes one argument, you would have to insert an empty set of parentheses to avoid a parsing ambiguity:
From the point of view of clarity, this syntax is elegant for some cases when there are two arguments.
For lists, head :: tail matches the expressions for building up lists, so there is a beautiful symmetry when the extraction process uses the same syntax.
However, the merits of this syntax are less clear for other examples, especially when there are N != 2 arguments.
This method is useful when you want to make a new instance of a case class that is identical to another instance with a few fields changed.
The second circle is created by copying the first and specifying a new radius.
The generated implementation of Circle.copy looks roughly like the following:
So, default values are provided for all the arguments to the method (only two in this case)
When using the copy method, the user specifies by name only the fields that are changing.
The values for the rest of the fields are used without having to reference them explicitly.
Case Class Inheritance Did you notice that the new Shapes code in “Case Classes” on page 136 did not put the case keyword on the abstract Shape class? This is allowed by the compiler, but there are reasons for not having one case class inherit another.
Suppose we want to add a string field to all shapes representing an id that the user wants to set.
Now the derived shapes need to pass the id to the Shape constructor.
So, the complete set of required modifications are as follows:
Note that we also have to add the val keywords.
Under inheritance, the equals methods don’t obey all the standard rules for robust object equality.
If you run this script, you get the following output: FancyCircle == Circle? false Circle == FancyCircle? true.
So, Circle.equals evaluates to true when given a FancyCircle with the same values for the Circle fields.
While you might argue that, as far as Circle is concerned, they really are equal, most people would argue that this is a risky, “relaxed” interpretation of equality.
It’s true that a future version of Scala could generate equals methods for case classes that do exact type-equality checking.
So, the conveniences provided by case classes sometimes lead to problems.
It is best to avoid inheritance of one case class by another.
Note that it’s fine for a case class to inherit from a non-case class or trait.
It’s also fine for a non-case class or trait to inherit from a case class.
Because of these issues, it is possible that case class inheritance will be deprecated and removed in future versions of Scala.
Equality of Objects Implementing a reliable equality test for instances is difficult to do correctly.
Effective Java ([Bloch2008]) and the Scaladoc page for AnyRef.equals describe the requirements for a good equality test.
Consult these references when you need to implement your own equals and hashCode methods.
Recall that these methods are created automatically for case classes.
Here we focus on the different equality methods available in Scala and their meanings.
There are some slight inconsistencies between the Scala specification (see [ScalaSpec2009]) and the Scaladoc pages for the equality-related methods for Any and AnyRef, but the general behavior is clear.
Some of the equality methods have the same names as equality methods in other languages, but the semantics are sometimes different!
The equals Method The equals method tests for value equality.
They do not need to refer to the same instance.
Hence, equals behaves like the equals method in Java and the eql? method in Ruby.
The == and != Methods While == is an operator in many languages, it is a method in Scala, defined as final in Any.
Since == and != are declared final in Any, you can’t override them, but you don’t need to, since they delegate to equals.
In Java, C++, and C#, the == operator tests for reference, not value equality.
Whatever language you’re used to, make sure to remember that in Scala, == is testing for value equality.
The ne and eq Methods The eq method tests for reference equality.
Hence, eq behaves like the == operator in Java, C++, and C#, but not == in Ruby.
Array Equality and the sameElements Method Comparing the contents of two Arrays doesn’t have an obvious result in Scala:
That’s a surprise! Thankfully, there’s a simple solution in the form of the sameElements method:
Remember to use sameElements when you want to test if two Arrays contain the same elements.
While this may seem like an inconsistency, encouraging an explicit test of the equality of two mutable data structures is a conservative approach on the part of the language designers.
In the long run, it should save you from unexpected results in your conditionals.
Recap and What’s Next We explored the fine points of overriding members in derived classes.
We learned about object equality, case classes, and companion classes and objects.
In the next chapter, we’ll learn about the Scala type hierarchy—in particular, the Predef object that includes many useful definitions.
We’ll also learn about Scala’s alternative to Java’s static class members and the linearization rules for method lookup.
The Predef Object For your convenience, whenever you compile code, the Scala compiler automatically imports the definitions in the java.lang package (javac does this, too)
The compiler also imports the definitions in the analogous Scala package, scala.
Hence, common Java or .NET types can be used without explicitly importing them or fully qualifying them with the java.lang.
Similarly, a number of common, Scala-specific types are made available without qualification, such as List.
Where there are Java and Scala type names that overlap, like String, the Scala version is imported last, so it “wins.” The compiler also automatically imports the Predef object, which defines or imports several useful types, objects, and functions.
You can learn a lot of Scala by viewing the source for Predef.
It is available by clicking the “source” link in the Predef Scaladoc page, or you can download the full source code for Scala at http://www.scala-lang .org/
Table 7-1 shows a partial list of the items imported or defined by Predef on the Java platform.
Predef declares the types and exceptions listed in the table using the type keyword.
But didn’t we just say that definitions in java.lang are imported automatically, like String? The reason there is a type definition is to enable support for a uniform string type across all runtime environments.
There are two type parameters, A and B, one for each item in the pair.
Recall from “Abstract Types And Parameterized Types” on page 47 that we explained the meaning of the + in front of each type parameter.
In “Understanding Parameterized Types” on page 249, we’ll discuss + and other type qualifiers in more detail.
Hence, we can create Pair instances as in this example:
The types A and B, shown in the definition of Pair, are inferred.
Map and Set appear in both the types and values lists.
Hence, Map and Set in Predef are values, not object definitions, because they refer to objects defined elsewhere, whereas Pair and Triple are defined in Predef itself.
The types Map and Set are assigned the corresponding immutable classes.
The definition of the ArrowAssoc class and the Map and Set values in Predef make the convenient Map initialization syntax possible.
Recall that there can be no type parameters on the Map companion object because there can be only one instance.
The compiler knows that String does not define a -> method, so it looks for an implicit conversion in scope to a type that defines such a method, such as ArrowAssoc.
It is applied to each item to the left of an arrow ->, e.g., the "Alabama" string.
These strings are wrapped in ArrowAssoc instances, upon which the -> method is then invoked.
When it is invoked, it is passed the string on the righthand side of the ->
The method returns a tuple with the value, ("Alabama", "Montgomery"), for example.
In this way, each key -> value is converted into a tuple and the resulting comma-separated list of tuples is passed to the Map.apply factory method.
The description may sound complicated at first, but the beauty of Scala is that this map initialization syntax is not an ad hoc language feature, such as a special-purpose operator -> defined in the language grammar.
Instead, this syntax is defined with normal definitions of types and methods, combined with a few general-purpose parsing conventions, such as support for implicits.
You can use the same techniques to write your own convenient “operators” for mini Domain-Specific Languages (see Chapter 11)
This “bare” println method is defined in Predef, then imported automatically by the compiler.
Similarly, all the other I/O methods defined by Predef, e.g., readLine and format, call the corresponding Console methods.
Finally, the assert, assume, and require methods are each overloaded with various argument list options.
For the full list of features defined by Predef, see the corresponding Scaladoc entry in [ScalaAPI2008]
Four Ways to Create a Two-Item Tuple We now know four ways to create a two-item tuple (twople?):
Classes and Objects: Where Are the Statics? Many object-oriented languages allow classes to have class-level constants, fields, and methods, called “static” members in Java, C#, and C++
These constants, fields, and methods are not associated with any instances of the class.
An example of a class-level field is a shared logging instance used by all instances of a class for logging messages.
An example of a class-level constant is the default logging “threshold” level.
An example of a class-level method is a “finder” method that locates all instances of the class in some repository that match some user-specified criteria.
Another example is a factory method, as used in one of the factory-related design patterns (see [GOF1995])
To remain consistent with the goal that “everything is an object” in Scala, class-level fields and methods are not supported.
Instead, Scala supports declarations of classes that are singletons, using the object keyword instead of the class keyword.
The objects provide an object-oriented approach to “static” data and methods.
Just as for classes and traits, the body of the object is the constructor, but since the system instantiates the object, there is no way for the user to specify a parameter list for the constructor, so they aren’t supported.
Any data defined in the object has to be initialized with default values.
For the same reasons, auxiliary constructors can’t be used and are not supported.
We’ve already seen some examples of objects, such as the specs objects used previously for tests, and the Pair type and its companion object, which we explored in “The Predef Object” on page 145:
Note that this is the same syntax that is commonly used in languages with static fields and methods.
When an object named MyObject is compiled to a class file, the class file name will be MyObject$.class.
In Java and C#, the convention for defining constants is to use final static fields.
C# also has a constant keyword for simple fields, like ints and strings.
In Scala, the convention is to use val fields in objects.
Finally, recall from “Nested Classes” on page 95 that class definitions can be nested within other class definitions.
You can define nested objects, traits, and classes inside other objects, traits, and classes.
Package Objects Scala version 2.8 introduces a new scoping construct called package objects.
They are used to define types, variables, and methods that are visible at the level of the corresponding package.
To understand their usefulness, let’s see an example from Scala version 2.8 itself.
The collection library is being reorganized to refine the package structure and to use it more consistently (among other changes)
They wanted to move types to new packages, but avoid breaking backward compatibility.
The package object construct provided a solution, along with other benefits.
It’s not required, but it’s a useful convention for package objects.
Here is the full package object definition (at the time of this writing; it could change before the 2.8.0 final version is released):
Note that pairs of declarations like type List[+] = ...
Because the contents of the scala package are automatically imported by the compiler, you can still reference all the definitions in this object in any scope without an explicit import statement for fully qualified names.
Other than the way the members in package objects are scoped, they behave just like other object declarations.
While this example contains only vals and types, you can also define methods, and you can subclass another class or trait and mix in other traits.
Another benefit of package objects is that it provides a more succinct implementation of what was an awkward idiom before.
Without package objects, you would have to put definitions in an ad hoc object inside the desired package, then import from the object.
For example, here is how List would have to be handled without a package object:
Finally, another benefit of package objects is the way they provide a clear separation between the abstractions exposed by a package and the implementations that should be hidden inside it.
In a larger application, a package object could be used to expose all the public types, values, and operations (methods) for a “component,” while everything else in the package and nested packages could be treated as internal implementation details.
Sealed Class Hierarchies Recall from “Case Classes” on page 136 that we demonstrated pattern matching with our Shapes hierarchy, which use case classes.
Otherwise, if someone defines a new subtype of Shape and passes it to this match statement, a runtime scala.MatchError will be thrown, because the new shape won’t match the shapes covered in the match statement.
However, it’s not always possible to define reasonable behavior for the default case.
There is an alternative solution if you know that the case class hierarchy is unlikely to change and you can define the whole hierarchy in one file.
In this situation, you can add the sealed keyword to the declaration of the common base class.
So, if you cover all those classes in the case expressions (either explicitly or through shared parent classes), then you can safely eliminate the default case expression.
No default case is necessary, since we cover all the possibilities.
Conversely, if you omit one of the classes and you don’t provide a default case or a case for a shared parent class, the compiler warns you that the “match is not exhaustive.” For example, if you comment out the case for Put, you get this warning:
You also get a MatchError exception if a Put instance is passed to the match.
Every time you add or remove a class from the hierarchy, you have to modify the file, since the entire hierarchy has to be declared in the same file.
It’s much less “costly” if you can extend the system by adding new derived types in separate source files.
This is why we picked the HTTP method hierarchy for the example.
Avoid sealed case class hierarchies if the hierarchy changes frequently (for an appropriate definition of “frequently”)
Finally, you may have noticed some duplication in the example.
Why didn’t we put that field in the parent HttpMethod class? Because we decided to use case classes for the concrete classes, we’ll run into the same problem with case class inheritance that we discussed in “Case Class Inheritance” on page 140, where we added a shared id field in the Shape hierarchy.
We need the body argument for each HTTP method’s constructor, yet it will be made a field of each method type automatically.
So, we would have to use the override val technique we demonstrated previously.
We could remove the case keywords and implement the methods and companion objects that we need.
However, in this case, the duplication is minimal and tolerable.
So, we can declare that method abstract in HttpMethod, then use it as we see fit.
The loop at the end of the script calls bodyLength.
Case classes and sealed class hierarchies have very useful properties, but they aren’t suitable for all situations.
The Scala Type Hierarchy We have mentioned a number of types in Scala’s type hierarchy already.
Let’s look at the general structure of the hierarchy, as illustrated in Figure 7-1
When the underlying “runtime” is discussed, the points made apply equally to the JVM and the .NET CLR, except where noted.
Defines a few final methods like ==, !=, isInstanceOf[T] (for type checking), and asInstanceOf[T] (for type casting), as well as default versions of equals, hashCode, and toString, which are designed to be overridden by subclasses.
AnyVal Any The parent of all value types, which correspond to the primitive types on the runtime platform, plus Unit.
All the AnyVal instances are immutable value instances, and all the AnyVal types are abstract final.
Rather, new instances are created with literal values (e.g., 3.14 for a Double) or by calling methods on instances that return new values.
Unit Serves the same role as void in most imperative languages.
All other types, the reference types, are children of AnyRef.
Table 7-4 lists some of the more commonly used reference types.
The Either can be pattern matched for its Left or Right subtypes.
Name Parent Description exception-handling idiom, it is conventional to use Left for the exception.
AnyRef Trait representing a function that takes N arguments, each of which can have its own type, and returns a value of type R.
Iterable[+T] AnyRef Trait with methods for operating on collections of instances.
Users implement the abstract elements method to return an Iterable instance.
List[+T] Seq[T] sealed abstract class for ordered collections with functionalstyle list semantics.
It is the most widely used collection in Scala, so it is defined in the scala package, rather than one of the collection packages.
It has two subclasses, case object Nil, which extends List[Noth ing] and represents an empty list, and case final class :: [T], which represents a non-empty list, characterized by a head element and a tail list, which would be Nil for a one-element list.
Nothing All other types Nothing is the subtype of all other types.
It is used primarily for defining other types in a type-safe way, such as the special List subtype Nil.
Null All reference types Null has one instance, null, corresponding to the runtime’s concept of null.
It is a sealed abstract type and the only allowed instances are an instance of its derived case class Some[T], wrapping an instance of T, or its derived case object None, which extends Option[Nothing]
Predef AnyRef An object that defines and imports many commonly used types and methods.
ScalaObject AnyRef Mixin trait added to all Scala reference type instances.
These other collections come in two varieties: mutable and immutable.
Only an immutable version of List is provided; for a mutable list, use a ListBuffer, which can return a List via the toList method.
For Scala version 2.8, the collections implementations reuse code from.
Users of the collections would normally not use any types defined in this package.
Consistent with its emphasis on functional programming (see Chapter 8), Scala encourages you to use the immutable collections, since List is automatically imported and Predef defines types Map and Set that refer to the immutable versions of these collections.
Predef defines a number of implicit conversion methods for the value types (excluding Unit)
There are implicit conversions between the “numeric” types—Byte, Short, Int, Long, and Float—to the other types that are “wider” than the original.
It catches a Throwable and returns it as the Left value or returns the normal result as the Right value.
It pattern matches on the result and prints an appropriate message.
Linearization of an Object’s Hierarchy Because of single inheritance, the inheritance hierarchy would be linear, if we ignored mixed-in traits.
When traits are considered, each of which may be derived from other traits and classes, the inheritance hierarchy forms a directed, acyclic graph (see [ScalaSpec2009])
The term linearization refers to the algorithm used to “flatten” this graph for the purposes of resolving method lookup priorities, constructor invocation order, binding of super, etc.
Informally, we saw in “Stackable Traits” on page 82 that when an instance has more than one trait, they bind right to left, as declared.
This list of strings built up by the m methods reflects the linearization of the inheritance hierarchy, with a few missing pieces we’ll discuss shortly.
We’ll also see why C1 is at the end of the list.
First, let’s see what the invocation sequence of the constructors looks like:
We had to reverse the list on the last line, because the way it was constructed put the elements in the reverse order.
For proper construction to occur, the parent types need to be constructed before the derived types, since a derived type often uses fields and methods in the parent types during its construction process.
The output of the first linearization script is actually missing three types at the end.
The full linearization for reference types actually ends with ScalaObject, AnyRef, and Any.
Scala inserts the ScalaObject trait as the last mixin, just before AnyRef and Any that are the penultimate and ultimate parent classes of any reference type.
The “value types,” subclasses of AnyVal, are all declared abstract final.
Since we can’t subclass them, their linearizations are simple and straightforward.
The linearization defines the order in which method lookup occurs.
The one in C2 is called first, since the instance is of that type.
From which of the traits did we traverse to C1? Actually, it is breadthfirst, with “delayed” evaluation, as we will see.
Let’s modify our first example and see how we got to C1:
Now we pass the name of the caller of super.m as a parameter, then C1 prints out who called it.
Put the actual type of the instance as the first element.
Working from left to right, remove any type if it appears again to the right of the current position.
Let’s work through the algorithm using a slightly more involved example:
What the algorithm does is push any shared types to the right until they come after all the types that derive from them.
Try modifying the last script with different hierarchies and see if you can reproduce the results using the algorithm.
Overly complex type hierarchies can result in method lookup “surprises.” If you have to work through this algorithm to figure out what’s going on, try to simplify your code.
Recap and What’s Next We have finished our survey of Scala’s object model.
If you come from an objectoriented language background, you now know enough about Scala to replace your existing object-oriented language with object-oriented Scala.
Scala supports functional programming, which offers powerful mechanisms for addressing a number of design problems, such as concurrency.
We’ll see that functional programming appears to contradict object-oriented programming, at least on the surface.
That said, a guiding principle behind Scala is that these two paradigms complement each other more than they conflict.
Combined, they give you more options for building robust, scalable software.
Scala lets you choose the techniques that work best for your needs.
Every decade or two, a major computing idea goes mainstream.
These ideas may have lurked in the background of academic computer science research, or possibly in some lesser-known field of industry.
The transition to mainstream acceptance comes in response to a perceived problem for which the idea is well suited.
Long the topic of computer science research and even older than object-oriented programming, functional programming offers effective techniques for concurrent programming, which is growing in importance.
Because functional programming is less widely understood than object-oriented programming, we won’t assume that you have prior experience with it.
As you’ll see, functional programming is not only a very effective way to approach concurrent programming, which we’ll explore in depth in Chapter 9, but functional programming can also improve your objects.
Of course, we can’t provide an exhaustive introduction to functional programming.
To learn more about it, [O’Sullivan2009] has a more detailed introduction in the context of the Haskell language.
What Is Functional Programming? Don’t all programming languages have functions of some sort? Whether they are called methods, procedures, or GOTOs, programmers are always dealing in functions.
Functional programming is based on the behavior of functions in the mathematical sense, with all the implications that starting point implies.
Functions in Mathematics In mathematics, functions have no side effects.
No matter how much work sin(x) does, all the results are returned and assigned to y.
No global state of any kind is modified internally by sin(x)
Hence, we say that such a function is free of side effects, or pure.
This property simplifies enormously the challenge of analyzing, testing, and debugging a function.
You can do these things without having to know anything about the context in which the function is invoked, except for any other functions it might call.
However, you can analyze them in the same way, working bottom up to verify the whole “stack.” This obliviousness to the surrounding context is known as Referential Transparency.
You can call such a function anywhere and be confident that it will always behave the same way.
If no global state is modified, concurrent invocation of the function is straightforward and reliable.
In functional programming, you can compose functions from other functions.
An implication of composability is that functions can be treated as values.
In the functional paradigm, functions become a primitive type, a building block that’s just as essential to the work of programming as integers or strings.
When a function takes other functions as arguments or returns a function, it is called a higher-order function.
In mathematics, two examples of higher-order functions from calculus are derivation and integration.
Variables that Aren’t The word “variable” takes on a new meaning in functional programming.
If you come from a procedural or object-oriented programming background, you are accustomed to variables that are mutable.
In the expression y = sin(x), once you pick x, then y is fixed.
To be more precise, it is the values that are immutable.
Functional programming languages prevent you from assigning a new value to a variable that already has a value.
If you can’t change a variable, then you can’t have loop counters, for example.
We’re accustomed to objects that change their state when we call methods on them.
Almost all the difficulty of multithreaded programming lies in synchronizing access to shared, mutable state.
If you remove mutability, then the problems essentially go away.
It is the combination of referentially transparent functions and immutable values that make functional programming compelling as a better way to write concurrent software.
Almost all the constructs we have invented in 60-odd years of computer programming have been attempts to manage complexity.
Higher-order functions and referential transparency provide very flexible building blocks for composing programs.
Immutability greatly reduces regression bugs, many of which are caused by unintended state changes in one part of a program due to intended changes in another part.
There are other contributors to such non-local effects, but mutability is one of the most important.
It’s common in object-oriented designs to encapsulate access to data structures in objects.
If these structures are mutable, we can’t simply share them with clients.
We have to add special accessor methods to control access, so clients can’t modify them outside our control.
These additions increase code size, which increases the testing and maintenance burden, and they increase the effort required by clients to understand the ad hoc features of our APIs.
In contrast, when we have immutable data structures, many of these problems simply go away.
We can provide access to collections without fear of data loss or corruption.
Of course, the general principles of minimal coupling still apply; should clients care if a Set or List is used, as long foreach is available? Immutable data also implies that lots of copies will be made, which can be expensive.
Functional data structures optimize for this problem (see [Okasaki1998]) and many of the built-in Scala types are efficient at creating new copies from existing copies.
It’s time to dive into the practicalities of functional programming in Scala.
We’ll discuss other aspects and benefits of the approach as we proceed.
Functional Programming in Scala As a hybrid object-functional language, Scala does not require functions to be pure, nor does it require variables to be immutable.
It does, however, encourage you to write your code this way whenever possible.
You have the freedom to use procedural or object-oriented techniques when and where they seem most appropriate.
Though functional languages are all about eliminating side effects, a language that never allowed for side effects would be useless.
Input and output (IO) are inherently about side effects, and IO is essential to all programming tasks.
For this reason, all functional languages provide mechanisms for performing side effects in a controlled way.
Scala doesn’t restrict what you can do, but we encourage you to use immutable values and pure functions and methods whenever possible.
When mutability and side effects are necessary, pursue them in a “principled” way, isolated in well-defined modules and focused on individual tasks.
If you’re new to functional programming, keep in mind that it’s easy to fall back to old habits.
We encourage you to master the functional side of Scala and to learn to use it effectively.
A function that returns Unit implies that the function has pure side effects, meaning that if it does any useful work, that work must be all side effects, since the function doesn’t return anything.
We’ve seen many examples of higher-order functions and composability in Scala.
For example, List.map takes a function to transform each element of the list to something else:
For each argument to the function, you can use _ if the argument is used only once.
We also used the infix operator notation to invoke map.
Here’s an example that “reduces” the same list by multiplying all the elements together:
Both examples successfully “looped” through the list without the use of a mutable counter to track iterations.
Most containers in the Scala library provide functionally pure iteration methods.
In other cases, recursion is the preferred way to traverse a data structure or perform an algorithm.
Function Literals and Closures Let’s expand our previous map example a bit:
We defined a variable, factor, to use as the multiplication factor, and we pulled out the previous anonymous function into a value called multiplier that now uses factor.
Then we map over a list of integers, as we did before.
After the first call to map, we change factor and map again.
Even though multiplier was an immutable function value, its behavior changed when factor changed.
There are two free variables in multiplier: i and factor.
One of them, i, is a formal parameter to the function.
Hence, it is bound to a new value each time multiplier is called.
However, factor is not a formal parameter, but a reference to a variable in the enclosing scope.
Hence, the compiler creates a closure that encompasses (or “closes over”) multiplier and the external context of the unbound variables multiplier references, thereby binding those variables as well.
This is why the behavior of multiplier changed after changing factor.
It references factor and reads its current value each time.
If a function has no external references, then it is trivially closed over itself.
Purity Inside Versus Outside If we called sin(x) thousands of times with the same value of x, it would be wasteful if it calculated the same value every single time.
Even in “pure” functional libraries, it is common to perform internal optimizations like caching previously computed values (sometimes called memoization)
Caching introduces side effects, as the state of the cache is modified.
However, this lack of purity should be opaque to the user (except perhaps in terms of the performance impact)
You can see examples of functional libraries with mutable internals in the Scala library.
The methods in List often use mutable local variables for efficient traversal.
The local variables are thread-safe, as are the traversals, since Lists themselves are immutable.
Recursion Recursion plays a larger role in pure functional programming than in imperative programming, in part because of the restriction that variables are immutable.
For example, you can’t have loop counters, which would change on each pass through a loop.
One way to implement looping in a purely functional way is with recursion.
Both the loop counter j and the result are mutable variables.
For simplicity, we’re ignoring input numbers that are less than or equal to zero.
The output is the same, but now there are no mutable variables.
Recursion not only helps us avoid mutable variables, it is also the most natural way to express some functions, particularly mathematical functions.
The recursive definition in our second factorial is structurally similar to a definition for factorials that you might see in a mathematics book.
However, there are two potential problems with recursion: the performance overhead of repeated function invocations and the risk of stack overflow.
Performance problems in a recursive scenario can sometimes be addressed with memoization, but care should be taken that the space requirements of caching don’t outweigh the performance benefits.
Stack overflow can be avoided by converting the recursive invocation into a loop of some kind.
In fact, the Scala compiler can do this conversion for you for some kinds of recursive invocations, which we describe next.
Tail Calls and Tail-Call Optimization A particular kind of recursion is called tail-call recursion, which occurs when a function calls itself as its final operation.
Tail-call recursion is very important because it is the easiest kind of recursion to optimize by conversion into a loop.
Loops eliminate the potential of a stack overflow, and they improve performance by eliminating the recursive function call overhead.
While tail recursion optimizations are not yet supported natively on the JVM, scalac can do them.
However, our factorial example is not a tail recursion, because factorial calls itself and then does a multiplication with the results.
There is a way to implement factorial in a tail recursive way.
However, that example didn’t use some constructs we’ve learned about since, such as for comprehensions and pattern matching.
So, here’s a new implementation of factorial, calculated with tail-call recursion:
Now, factorial does all the work with a nested method, fact, that is tail recursive because it passes an accumulator argument to hold the computation in progress.
This argument is computed with a multiplication before the recursive call to fact, which is now the very last thing that is done.
In our previous implementation, this multiplication was done after the call to fact.
When we call fact(1), we simply return the accumulated value.
If you call our original non-tail recursive implementation of factorial with a large number—say 10,000—you’ll cause a stack overflow on a typical desktop computer.
The tail-recursive implementation works successfully, returning a very large number.
This idiom of nesting a tail-recursive function that uses an accumulator is a very useful technique for converting many recursive algorithms into tail recursions that can be optimized into loops by scalac.
The tail-call optimization won’t be applied when a method that calls itself might be overridden in a derived type.
The method must be private or final, defined in an object, or nested in another method (like fact earlier)
Trampoline for Tail Calls A trampoline is a loop that works through a list of functions, calling each one in turn.
The metaphor of bouncing the functions off a trampoline is the source of the name.
Consider a kind of recursion where a function A doesn’t call itself recursively, but instead it calls another function B, which calls A, which calls B, etc.
This kind of backand-forth recursion can also be converted into a loop using a trampoline.
Note that trampolines impose a performance overhead, but they are ideal for pure functional recursions (versus an imperative equivalent) that would otherwise exhaust the stack.
Support for this optimization is planned for Scala version 2.8, although it has not yet been implemented at the time of this writing.
Functional Data Structures There are several data structures that are common in functional programming, most of which are containers, like collections.
Languages like Erlang rely on very few types, while other functional languages provide a richer type system.
The common data structures support the same subset of higher-order functions for read-only traversal and access to the elements in the data structures.
These features make them suitable as “protocols” for minimizing the coupling between components, while supporting data exchange.
In fact, these data structures and their operations are so useful that many languages support them, including those that are not considered functional languages, like Java and Ruby.
Lists in Functional Programming Lists are the most common data structure in functional programming.
They are the core of the first functional programming language, Lisp.
In the interest of immutability, a new list is created when you add an element to a list.
It is conventional to prepend the new element to the list, as we’ve seen before:
Because the :: operator binds to the right, the definition of list2 is equivalent to both of the following variations:
We’ll see why when we dive into Scala’s implementation of List in “A Closer Look at Lists” on page 261, after we have learned more about parameterized types in Scala.
Unlike some of the other collections, Scala only defines an immutable List.
However, it also defines some mutable list types, such as ListBuffer and LinkedList.
Maps in Functional Programming Perhaps the second most common data structure is the map, referred to as a hash or dictionary in other languages, and not to be confused with the map function we saw earlier.
Maps are used to hold pairs of keys and values.
In the interest of minimalism, maps could be implemented with lists.
Every even element in the list (counting from zero) could be a key, followed by the value in the next odd position.
In practice, maps are usually implemented in other ways for efficiency.
They define + and - operators for adding and removing elements, and ++ and -- operators for adding and removing elements defined in Iterators of Pairs, where each Pair is a key-value pair.
Sets in Functional Programming Sets are like lists, but they require each element to be unique.
Sets could also be implemented using lists, as long as the equivalent of the list “cons” operator (::) first checks that the element doesn’t already exist in the storage list.
This property means that element insertion would be O(N) if a storage list were used, and the order of the elements in the set wouldn’t necessarily match the order of “insertion” operations.
In practice, sets are usually implemented with more efficient data structures.
Typically, they’re used to provide some convenient feature not supported by a more common functional type.
Traversing, Mapping, Filtering, Folding, and Reducing The functional collections we just discussed—lists, maps, sets, as well as tuples and arrays—all support several common operations based on read-only traversal.
In fact, this uniformity can be exploited if any “container” type also supports these operations.
For example, an Option contains zero or one elements, if it is a None or Some, respectively.
Traversal The standard traversal method for Scala containers is foreach, which is defined by the Iterable traits that the containers mix in.
Here is an example of its use for lists and maps:
Note that for a map, A is actually a tuple, as shown in the example.
Once you have foreach, you can implement all the other traversal operations we’ll discuss next, and more.
A look at Iterable will show that it supports methods for filtering collections, finding elements that match specified criteria, calculating the number of elements, and so forth.
The methods we’ll discuss next are hallmarks of functional programming: mapping, filtering, folding, and reducing.
It returns a new collection of the same size as the original collection.
It is also a member of Iterable, and its signature is:
The passed-in function (f) can transform an original element of type A to a new type B.
Where did the ArrayBuffer come from? It turns out that Iterable.map creates and returns an ArrayBuffer as the new Iterable collection.
This brings up a general conflict between immutable types and object-oriented type hierarchies.
If a base type creates a new instance on modification, how does it know what kind of type to create? You could solve this problem two ways.
First, you could have each type in the hierarchy override methods like map to return an instance of their own type.
This approach is error-prone, though, as it would be easy to forget to override all such methods when a new type is added.
Even if you always remember to override each method, you have the dilemma of how to implement the override.
Do you call the super method to reuse the algorithm, then iterate through the returned instance to create a new instance of the correct type? That would be inefficient.
You could copy and paste the algorithm into each override, but that creates issues of code bloat, maintainability, and skew.
How is the new instance that is returned actually used? Do we really care if it has the “wrong” type? Keep in mind that all we usually care about are the low-level abstractions like lists, maps, and sets.
In the case of functional data structures, the derived types we might implement using objectoriented inheritance are most often implementation optimizations.
The Scala type hierarchy for containers does have a few levels of abstractions at the bottom, e.g., Collection extends Iterable extends AnyRef, but above Collection are Seq (parent of List), Map, Set, etc.
That said, if you really need a Map, you can create one easily enough:
The commented-out line suggests that it would be nice if you could simply pass the new Iterable to Map.apply, but this doesn’t work.
However, we can create an empty map of the right type and then add the new Iterable to it, using the ++ method, which returns a new Map.
So, we can get the Map we want when we must have one.
While it would be nice if methods like map returned the same collection type, we saw that there is no easy way to do this.
Instead, we accept that map and similar methods return an abstraction like Iterable and then rely on the specific subtypes to take Iterables as input arguments for populating the collection.
A related Map operation is flatMap, which can be used to “flatten” a hierarchical data structure, remove “empty” elements, etc.
Hence, unlike map, it may not return a new collection of the same size as the original collection:
We used List[_] because we won’t know what the type parameters are for any embedded lists when we’re traversing the outer list, due to type erasure.
Here is the signature for flatMap, along with map, for comparison:
After going through the collection, flatMap will “flatten” all those Iterables into one collection.
If our function literal leaves nested lists intact, they won’t be flattened for us.
Filtering It is common to traverse a collection and extract a new collection from it with elements that match certain criteria:
There are several different kinds of methods defined in Iterable for filtering or otherwise returning part of the original collection (comments adapted from the Scaladocs):
Returns the longest suffix of this iterable whose first element does // not satisfy the predicate p.
Apply a predicate p to all elements of this iterable object and // return true, iff there is at least one element for which p yields true.
Returns all the elements of this iterable that satisfy the predicate p.
Find and return the first element of the iterable object satisfying a // predicate, if any.
Returns index of the first element satisying a predicate, or -1
Apply a predicate p to all elements of this iterable object and return // true, iff the predicate yields true for all elements.
Returns the index of the first occurence of the specified object in // this iterable object.
Partitions this iterable in two iterables according to a predicate.
Checks if the other iterable object contains the same elements.
Returns an iterable consisting only over the first n elements of this // iterable, or else the whole iterable, if it has less than n elements.
Folding and Reducing We’ll discuss folding and reducing in the same section, as they’re similar.
Both are operations for “shrinking” a collection down to a smaller collection or a single value.
Folding starts with an initial “seed” value and processes each element in the context of that value.
In contrast, reducing doesn’t start with a user-supplied initial value.
Rather, it uses the first element as the initial value:
Reducing can’t work on an empty collection, since there would be nothing to return.
Folding on an empty collection will simply return the seed value.
Here is a “fold” operation that is really a map operation:
Note that we had to call reverse on the result to get back a list in the same order as the input list.
Here are the signatures for the various fold and reduce operations in Iterable:
Combines the elements of this list together using the binary function // op, from right to left, and starting with the value z.
Similar to foldLeft but can be used as an operator with the order of // list and zero arguments reversed.
That is, z /: xs is the same as // xs foldLeft z def /: [B](z : B)(op : (B, A) => B) : B.
Many people consider the operator forms, :\ for foldRight and /: for foldLeft, to be a little too obscure and hard to remember.
Don’t forget the importance of communicating with your readers when writing code.
Why are there left and right forms of fold and reduce? For the first examples we showed, adding and multiplying a list of integers, they would return the same result.
Consider a foldRight version of our last example that used fold to map the integers to strings:
Note also that the arguments to the function literal are reversed compared to the arguments for foldLeft, as required by the definition of foldRight.
Both foldLeft and reduceLeft process the elements from left to right.
It turns out that foldLeft and reduceLeft have one very important advantage over their “right-handed” brethren: they are tail-call recursive, and as such they can benefit from tail-call optimization.
If you stare at the previous breakdowns for multiplying the integers, you can probably see why they are tail-call recursive.
Recall that a tail call must be the last operation in an iteration.
For each line in the foldRight sequence, the outermost multiplication can’t be done until the innermost multiplications all complete, so the operation isn’t tail recursive.
In the following script, the first line prints 1784293664, while the second line causes a stack overflow:
So why have both kinds of recursion? If you’re not worried about overflow, a right recursion might be the most natural fit for the operation you are doing.
Recall that when we used foldLeft to map integers to strings, we had to reverse the result.
That was easy enough to do in that case, but in general, the result of a left recursion might not always be easy to convert to the right form.
Functional Options You’ll find the functional operations we’ve explored throughout the Scala library, and not exclusively on collection classes.
The always handy Option container supports filter, map, flatMap, and other functionally oriented methods that are applied only if the Option isn’t empty (that is, if it’s a Some and not a None)
In this example, we attempt to multiply the contents of two Options by five.
Normally, trying to multiply a null value would result in an error.
But because the implementation of map on Option only applies the passed-in function when it’s non-empty, we don’t have to worry about testing for the presence of a value or handling an exception when we map over the None.
Functional operations on Options save us from extra conditional expressions or pattern matching.
Pattern matching, though, is a powerful tool within the context of functional programming, as we’ll explore in the next section.
Pattern Matching We’ve seen many examples of pattern matching throughout this book.
We got our first taste in “A Taste of Concurrency” on page 16, where we used pattern matching in our Actor that drew geometric shapes.
It’s just as important as polymorphism is in object-oriented programming, although the goals of the two techniques are very different.
Pattern matching is an elegant way to decompose objects into their constituent parts for processing.
On the face of it, pattern matching for this purpose seems to violate the goal of encapsulation that objects provide.
The risk that the parts of an object might be changed outside of the control of the enclosing object is avoided.
For example, if we have a Person class that contains a list of addresses, we don’t mind exposing that list to clients if the list is immutable.
However, exposing constituent parts potentially couples clients to the types of those parts.
We can’t change how the parts are implemented without breaking the clients.
A way to minimize this risk is to expose the lowest-level abstractions possible.
When clients access a person’s addresses, do they really need to know that they are stored in a List, or is it sufficient to know that they are stored in an Iterable or Seq? If so, then we can change the implementation of the addresses as long as they still support those abstractions.
Of course, we’ve known for a long time in object-oriented programming that you should only couple to abstractions, not concrete details (for example, see [Martin2003])
Functional pattern matching and object-oriented polymorphism are powerful complements to each other.
We saw this in the Actor example in “A Taste of Concurrency” on page 16, where we matched on the Shape abstraction, but called the polymorphic draw operation.
Partial Functions You’ve seen partially applied functions, or partial functions, throughout this book.
When you’ve seen an underscore passed to a method, you’ve probably seen partial application at work.
Partial functions are expressions in which not all of the arguments defined in a function are supplied as parameters to the function.
In Scala, partial functions are used to bundle up a function, including its parameters and return type, and assign that function to a variable or pass it as an argument to another function.
This is a bit confusing until we see it in practice:
Calling concatUpper with an underscore ( _ ) turns the method into a function value.
In the first part of the example, we’ve assigned a partially applied version of concatUpper to the value c.
We then apply it, implicitly calling the apply method on c by passing parameters to it directly.
In the second part, we’ve specified the first parameter to concatUpper but not the second, although we have specified the type of the second parameter.
To produce the same output as we saw before, we need only pass in a single value when we apply c2
We’ve seen partially applied functions without the underscore syntax as well:
It’s applied when invoked by mapping over each element in the list.
Because the map operation expects a function as an argument, we don’t need to write map(println _)
The trailing underscore that turns println into a function value is implied, in this context.
Another way of thinking of partial functions is as functions that will inform you when you supply them with parameters that are out of their domain.
This trait defines a method orElse that takes another PartialFunction.
Should the first partial function not apply, the second will be invoked.
In this example, tester is a partial function composed of two other partial functions, truthier and fallback.
In the first println statement, truthier is executed because the partial function’s internal case matches.
In the second, fallback is executed because the value of the expression is outside of the domain of truthier.
The case statements we’ve seen through our exploration of Scala are expanded internally to partially applied functions.
The functions provide the abstract method isDefinedAt, a feature of the PartialFunction trait used to specify the boundaries of a partial function’s domain:
Here, our partial function is a test for the string "pants"
When we inquire as to whether the string "pants" is defined for this function, the result is true.
Were we defining our own partial function, we could provide an implementation of isDefinedAt that performs any arbitrary test for the boundaries of our function.
Currying Just as you encountered partially applied functions before we defined them, you’ve also seen curried functions.
Named after mathematician Haskell Curry (from whom the Haskell language also get its name), currying transforms a function that takes multiple parameters into a chain of functions, each taking a single parameter.
In Scala, curried functions are defined with multiple parameter lists, as follows:
Of course, we could define more than two parameters on a curried function, if we like.
While the previous syntax is more readable, in our estimation, using this syntax eliminates the requirement of a trailing underscore when treating the curried function as a partially applied function.
Calling our curried string concatenation function looks like this in the Scala REPL:
We can also convert methods that take multiple parameters into a curried form with the Function.curried method:
In this example, we transform a function that takes two arguments, cat, into its curried equivalent that takes multiple parameter lists.
If cat had taken three parameters, its curried equivalent would take three lists of arguments, and so on.
The two forms are functionally equivalent, as demonstrated by the equality test, but curryCat can now be used as the basis of a partially applied function as well:
In practice, the primary use for currying is to specialize functions for particular types of data.
You can start with an extremely general case, and use the curried form of a function to narrow down to particular cases.
As a simple example of this approach, the following code provides specialized forms of a base function that handles multiplication:
We start with multiplier, which takes two parameters: an integer, and another integer to multiply the first one by.
We then curry two special cases of multiplier into function values.
Note the trailing underscores, which indicate to the compiler that the preceding expression is to be curried.
In particular, the wildcard underscores indicate that the remaining arguments (in this example, one argument) are unspecified.
As you can see, currying and partially applied functions are closely related concepts.
You may see them referred to almost interchangeably, but what’s important is their application (no pun intended)
Implicits There are times when you have an instance of one type and you need to use it in a context where a different, but perhaps a similar type is required.
For the “one-off” case, you might create an instance of the required type using the state of the instance you already have.
However, for the general case, if there are many such occurrences in the code, you would rather have an automated conversion mechanism.
A similar problem occurs when you call one or more functions repeatedly and have to pass the same value to all the invocations.
You might like a way of specifying a default value for that parameter, so it is not necessary to specify it explicitly all the time.
The Scala keyword implicit can be used to support both needs.
The implicit keyword tells the compiler it can use this method for an “implicit” conversion from a String to a RichString, whenever the latter is required.
The compiler detected an attempt to call a capitalize method, and it determined that RichString.
Then it looked within the current scope for an implicit method that converts String to RichString, finding stringWrapper.
As we’ll see in “Views and View Bounds” on page 263, these conversion methods are sometimes called views, in the sense that our stringWrapper conversion provides a view from String to RichString.
Predef defines many other implicit conversion methods, most of which follow the naming convention old2New, where old is the type of object available and New is the desired type.
However, there is no restriction on the names of conversion methods.
There are also a number of other Rich wrapper classes defined in the scala.runtime package.
Here is a summary of the lookup rules used by the compiler to find and apply conversion methods.
No conversion will be attempted if the object and method combination type check successfully.
Implicit methods aren’t chained to get from the available type, through intermediate types, to the target type.
Only a method that takes a single available type instance and returns a target type instance will be considered.
No conversion is attempted if more than one possible conversion method could be applied.
What if you can’t define a conversion method in a companion object, to satisfy the third rule, perhaps because you can’t modify or create the companion object? In this case, define the method somewhere else and import it.
Normally, you will define an object with just the conversion method(s) needed.
We can’t modify RichString or Predef to add an implicit conversion method for our custom FancyString class.
Instead, we define an object named FancyString2Rich String and define the conversion method in it.
We then import the contents of this object and the converter gets invoked implicitly in the last line.
This pattern for effectively adding new methods to classes has been called Pimp My Library (see [Odersky2006])
There are two other ways to achieve the same effect in all versions of Scala.
The first is to use function currying, as we have seen.
The second way is to define implicit values, using the implicit keyword.
This keyword informs the compiler to seek the value for factor from the surrounding scope, if available, or to use whatever parameter has been explicitly supplied to the function.
We’ve defined our own factor value in scope, and that value is used in the first call to multiplier.
In the second call, we’re explicitly passing in a value for factor and it overrides the value in the surrounding scope.
Essentially, implicit function parameters behave as parameters with a default value, with the key difference being that the value comes from the surrounding scope.
Had our factor value resided in a class or object, we would have had to import it into the local scope.
If the compiler can’t determine the value to use for an implicit parameter, an error of “no implicit argument matching parameter” will occur.
Final Thoughts on Implicits Implicits can be perilously close to “magic.” When used excessively, they obfuscate the code’s behavior for the reader.
Also, be careful about the implementation of a conversion method, especially if the return type is not explicitly declared.
If a future change to the method also changes the return type in some subtle way, the conversion may suddenly fail to work.
In general, implicits can cause mysterious behavior that is hard to debug! When deciding how to implement “default” values for method arguments, a major advantage of using default argument values (in Scala version 2.8) is that the method maintainer decides what to use as the default value.
The implementation is more straightforward and you avoid the “magic” of implicit methods.
However, a disadvantage of using default argument values is that it might be desirable to use a different “default” value based on the context in which the method is being called.
Scala version 2.8 provides some flexibility, as you can use an expression for an argument, not just a constant value.
However, that flexibility might not be enough, in which case implicits are a very flexible and powerful alternative.
Also, consider adding an explicit return type to “non-trivial” conversion methods.
Call by Name, Call by Value Typically, parameters to functions are by-value parameters; that is, the value of the parameter is determined before it is passed to the function.
In most circumstances, this is the behavior we want and expect.
But what if we need to write a function that accepts as a parameter an expression that we don’t want evaluated until it’s called within our function? For this circumstance, Scala offers by-name parameters.
A by-name parameter is specified by omitting the parentheses that normally accompany a function parameter, as follows:
And what’s more, we would have to include those unsightly, empty parentheses in every call to that method.
We can use by-name parameters to implement powerful looping constructs, among other things.
Let’s go crazy and implement our own while loop, throwing currying into the mix:
By delaying evaluation until conditional is called inside our function with a byname parameter, we get the behavior we expect.
Lazy Vals In “Overriding Abstract and Concrete Fields in Traits” on page 114, we showed several scenarios where the order of initialization for fields in override scenarios can be problematic.
Now we discuss the other solution we mentioned previously, lazy vals.
As before, we are using an anonymous inner class that implicitly extends the trait.
The body of the class, which initializes value, is evaluated after the trait’s body.
However, note that inverse is declared lazy, which means that the righthand side will be evaluated only when inverse is actually used.
In this case, that happens in the last println statement.
Only then is inverse initialized, using value, which is properly initialized at this point.
Try uncommenting the println statement at the end of the AbstractT2 body.
This println forces inverse to be evaluated inside the body of AbstractT2, before value is initialized by the class body, thereby reproducing the problem we had before.
This example raises an important point; if other vals use the lazy val in the same class or trait body, they should be declared lazy, too.
Also, watch out for function calls in the body that use the lazy val.
If a val is lazy, make sure all uses of the val are also lazy!
So, how is a lazy val different from a method call? In a method call, the body is executed every time the method is invoked.
For a lazy val, the initialization “body” is evaluated only once, when the variable is used for the first time.
This one-time evaluation makes little sense for a mutable field.
You can also use lazy vals to avoid costly initializations that you may not actually need and to defer initializations that slow down application startup.
They work well in constructors, where it’s clear to other programmers that all the one-time heavy lifting for initializing an instance is done in one place.
Another use for laziness is to manage potentially infinite data structures where only a manageable subset of the data will actually be used.
When we write the Fibonacci sequence, for example, we might write it as an infinite sequence, something like this:
Some pure functional languages are lazy by default, so they mimic this behavior as closely as possible.
This can work without exhausting resources if the user never tries to use more than a finite subset of these values.
Scala is not lazy by default, but it does offer support for working with infinite data structures.
It didn’t really work out that way, except in some rare cases, like the windowing APIs of various platforms.
Why did this not happen? There are certainly many reasons, but a likely source is the fact that simple source or binary interoperability protocols never materialized that would glue these components together.
The richness of object APIs was the very factor that undermined componentization.
Component models that have succeeded are all based on very simple foundations.
Integrated circuits (ICs) in electronics plug into buses with 2n signaling wires that are boolean, either on or off.
From that very simple protocol, the most explosive growth of any industry in human history was born.
With a handful of message types and a very simple standard for message content, it set the stage for the Internet revolution.
RESTful web services built on top of HTTP are also proving successful as components, but they are just complex enough that care is required to ensure that they work successfully.
So, is there hope for a binary or source-level component model? It probably won’t be object-oriented, as we’ve seen.
Components should interoperate by exchanging a few immutable data structures, e.g., lists and maps, that carry both data and “commands.” Such a component model would have the simplicity necessary for success and the richness required to perform real work.
Notice how that sounds a lot like HTTP and REST.
In fact, the Actor model has many of these qualities, as we’ll explore in the next chapter.
Getting a program to do more than one thing at a time has traditionally meant hassling with mutexes, race conditions, lock contention, and the rest of the unpleasant baggage that comes along with multithreading.
Event-based concurrency models alleviate some of these concerns, but can turn large programs into a rat’s nest of callback functions.
No wonder, then, that concurrent programming is a task most programmers dread, or avoid altogether by retreating to multiple independent processes that share data externally (for example, through a database or message queue)
Thankfully, Scala offers a reasonable, flexible approach to concurrency that we’ll explore in this chapter.
Actors Though you may have heard of Scala and Actors in the same breath, Actors aren’t a concept unique to Scala.
Since then, variations on the idea of Actors have appeared in a number of programming languages, most notably in Erlang and Io.
Actors in Abstract Fundamentally, an Actor is an object that receives messages and takes action on those messages.
The order in which messages arrive is unimportant to an Actor, though some Actor implementations (such as Scala’s) queue messages in order.
An Actor might handle a message internally, or it might send a message to another Actor, or it might create another Actor to take action based on the message.
Unlike traditional object systems (which, you might be thinking to yourself, have many of the same properties we’ve described), Actors don’t enforce a sequence or ordering to their actions.
This inherent eschewing of sequentiality, coupled with independence from shared global state, allow Actors to do their work in parallel.
As we’ll see later on, the judicious use of immutable data fits the Actor model ideally, and further aids in safe, comprehensible concurrent programming.
As we can see, an Actor defined in this way must be both instantiated and started, similar to how threads are handled in Java.
It must also implement the abstract method act, which returns Unit.
Once we’ve started this simple Actor, the following sage advice for thespians is printed to the console:
The scala.actors package contains a factory method for creating Actors that avoids much of the setup in the above example.
While a subclass that extends the Actor class must define act in order to be concrete, a factory-produced Actor has no such limitation.
In this shorter example, the body of the method passed to actor is effectively promoted to the act method from our first example.
Illuminating, but we still haven’t shown the essential piece of the Actors puzzle: sending messages.
Sending Messages to Actors Actors can receive any sort of object as a message, from strings of text to numeric types to whatever classes you’ve cooked up in your programs.
For this reason, Actors and pattern matching go hand in hand.
An Actor should only act on messages of familiar types; a pattern match on the class and/or contents of a message is good defensive programming and increases the readability of Actor code:
This example prints the following when run: I got a String: hi there I got an Int: 23 I have no idea what I just got.
The final lines of this example demonstrate use of the ! (exclamation point, or bang) method to send messages to our Actor.
If you’ve ever seen Actors in Erlang, you’ll find this syntax familiar.
The Actor is always on the lefthand side of the bang, and the message being sent to said Actor is always on the right.
If you need a mnemonic for this granule of syntactic sugar, imagine that you’re an irate director shouting commands at your Actors.
The Mailbox Every Actor has a mailbox in which messages sent to that Actor are queued.
Let’s see an example where we inspect the size of an Actor’s mailbox:
This example produces the following output: I've got 3 messages in my mailbox.
Note that the first and second lines of output are identical.
Because our Actor was set up solely to process messages of the string "how many?", those messages didn’t remain in its mailbox.
Only the messages of types we didn’t know about—in this case, Intremained unprocessed.
If you see an Actor’s mailbox size ballooning unexpectedly, you’re probably sending messages of a type that the Actor doesn’t know about.
Include a catchall case ( _ ) when pattern matching messages to find out what’s harassing your Actors.
Actors in Depth Now that we’ve got a basic sense of what Actors are and how they’re used in Scala, let’s put them to work.
The problem is this: a hypothetical barber shop has just one barber with one barber chair, and three chairs in which customers may wait for a haircut.
When a customer arrives, the barber wakes up to cut his hair.
If the barber is busy cutting hair when a customer arrives, the customer sits down in an available chair.
The sleeping barber problem is usually solved with semaphores and mutexes, but we’ve got better tools at our disposal.
Straight away, we see several things to model as Actors: the barber is clearly one, as are the customers.
The barbershop itself could be modeled as an Actor, too; there need not be a real-world parallel to verbal communication in an Actor system, even though we’re sending messages.
Let’s start with the sleeping barber’s customers, as they have the simplest responsibilities:
For the most part, this should look pretty familiar: we declare the package in which this code lives, we import code from the scala.actors package, and we define a class that extends Actor.
First of all, there’s our declaration of case object Haircut.
A common pattern when working with Actors in Scala is to use a case object to represent a message without.
If we wanted to include, say, the time at which the haircut was completed, we’d use a case class instead.
We declare Haircut here because it’s a message type that will be sent solely to customers.
Note as well that we’re storing one bit of mutable state in each Customer: whether or not they’ve gotten a haircut.
In their internal loop, each Customer waits for a Haircut message and, upon receipt of one, we set the shorn boolean to true.
Customer uses the asynchronous react method to respond to incoming messages.
If we needed to return the result of processing the message, we would use receive, but we don’t, and in the process we save some memory and thread use under the hood.
Because there’s only one barber, we could have used the actor factory method technique mentioned earlier to create him.
For testing purposes, we’ve instead defined our own Barber class:
The core of the Barber class looks very much like the Customer.
We loop around react, waiting for a particular type of object.
To keep that loop tight and readable, we call a method, helpCustomer, when a new Customer is sent to the barber.
Within that method we employ a check on the mailbox size to serve as our “chairs” that customers may occupy; we could have the Barber or Shop classes maintain an internal Queue, but why bother when each Actor’s mailbox already is one?
If three or more customers are in the queue, we simply ignore that message; it’s then discarded from the barber’s mailbox.
Otherwise, we simulate a semi-random delay (always at least 100 milliseconds) for the time it takes to cut a customer’s hair, then send off a Haircut message to that customer.
Were we not trying to simulate a real-world scenario, we would of course remove the call to Thread.sleep() and allow our barber to run full tilt.
Next up, we have a simple class to represent the barbershop itself:
Each Shop creates and starts a new Barber, prints a message telling the world that the shop is open, and sits in a loop waiting for customers.
When a Customer comes in, he’s sent to the barber.
We now see an unexpected benefit of Actors: they allow us to describe concurrent business logic in easily understood terms.
After “opening the shop,” we generate a number of Customer objects, assigning a numeric ID to each and storing the lot in an ArrayBuffer.
Next, we “trickle” the customers in by sending them as messages to the shop and sleeping for a semi-random amount of time between loops.
At the end of our simulated day, we tally up the number of customers who got haircuts by filtering out the customers whose internal shorn boolean was set to true and asking for the size of the resulting sequence.
Compile and run the code within the sleepingbarber directory as follows:
Throughout our code, we’ve prefixed console messages with abbreviations for the classes from which the messages were printed.
When we look at an example run of our simulator, it’s easy to see where each message came from:
You’ll find that each run’s output is, predictably, slightly different.
Every time the barber takes a bit longer to cut hair than it does for several customers to enter, the “chairs” (the barber’s mailbox queue) fill up, and new customers simply leave.
Of course, we have to include the standard caveats that come with simple examples.
For one, it’s possible that our example may not be suitably random, particularly if random values are retrieved within a millisecond of one another.
This is a byproduct of the way the JVM generates random numbers, and a good reminder to be careful about randomness in concurrent programs.
Try modifying the code to introduce more customers, additional message types, different delays, or to remove the randomness altogether.
If you’re an experienced multithreaded programmer, you might try writing your own sleeping barber implementation just to compare and contrast.
We’re willing to bet that an implementation in Scala with Actors will be terser and easier to maintain.
Effective Actors To get the most out of Actors, there are few things to remember.
First, note that there are several methods you can use to get different types of behavior out of your Actors.
Table 9-1 should help clarify when to use each method.
If you need the results of processing a message (that is, you need a synchronous response from sending a message to an Actor), use the receiveWithin variant to reduce your chances of blocking indefinitely on an Actor that’s gotten wedged.
Another strategy to keep your Actor-based code asynchronous is the use of futures.
A future is a placeholder object for a value that hasn’t yet been returned from an asynchronous process.
You can send a message to an Actor with the !! method; a variant of this method allows you to pass along a partial function that is applied to the future value.
As you can see from the following example, retrieving a value from a Future is as straightforward as invoking its apply method.
Note that retrieving a value from a Future is a blocking operation:
Instead, think like a director: what are the distinct roles in the “script” of your application, and what’s the least amount of.
Don’t be hesitant to copy data when writing Actor-centric code.
The more immutable your design, the less likely you are to end up with unexpected state.
The more you communicate via messages, the less you have to worry about synchronization.
All those messages and immutable variables might appear to be overly costly.
But, with today’s plentiful hardware, trading memory overhead for clarity and predictability seems more than fair for most applications.
Just because Actors are a great way to handle concurrency in Scala doesn’t mean that they’re the only way, as we’ll see soon.
Traditional threading and locking may better suit write-heavy critical paths for which a messaging approach would incur too much overhead.
In our experience, you can use a purely Actor-based design to prototype a concurrent solution, then use profiling tools to suss out parts of your application that might benefit from a different approach.
Traditional Concurrency in Scala: Threading and Events While Actors are a great way to handle concurrent operations, they’re not the only way to do so in Scala.
As Scala is interoperable with Java, the concurrency concepts that you may be familiar with on the JVM still apply.
One-Off Threads For starters, Scala provides a handy way to run a block of code in a new thread:
A similar construct is available in the scala.concurrent package, as a method on the ops object to run a block asynchronously with spawn:
We’ll use the thread pool to run a simple class, implementing Java’s Runnable interface for thread-friendly classes, that identifies which thread it’s running on:
As is standard in Java concurrency, the run method is where a threaded class starts.
Every time our pool executes a new ThreadIdentifier, its run method is invoked.
You’ll find that your existing knowledge of Java’s approach to multithreading still applies in Scala.
What’s more, you’ll be able to accomplish the same tasks using less code, which should contribute to maintainability and productivity.
Events Threading and Actors aren’t the only way to do concurrency.
Event-based concurrency, a particular approach to asynchronous or non-blocking I/O (NIO), has become a favored way to write servers that need to scale to thousands of simultaneous clients.
Eschewing the traditional one-to-one relationship of threads to clients, this model of.
Typically, the programmer will associate a callback method with each event that’s relevant to her program.
Enter Apache MINA, built atop Java NIO and described on its home page as “a network application framework which helps users develop high performance and high scalability network applications easily” (see [MINA])
While MINA may be easier to use than Java’s built-in NIO libraries, we’ve gotten used to some conveniences of Scala that just aren’t available in MINA.
The open source Naggati library (see [Naggati]) adds a Scala-friendly layer atop MINA that, according to its author, “makes it easy to build protocol filters [using a] sequential style.” Essentially, Naggati is a DSL for parsing network protocols, with MINA’s powerful NIO abilities under the hood.
Let’s use Naggati to write the foundations of an SMTP email server.
To keep things simple, we’re only dealing with two SMTP commands: HELO and QUIT.
The former command identifies a client, and the latter ends the client’s session.
We’ll keep ourselves honest with a test suite, facilitated by the Specs Behavior-Driven Development library (see “Specs” on page 363):
After setting up an environment for each test run, our suite exercises the two SMTP commands we’re interested in.
The doBefore block runs before each test, guaranteeing that mock session and output buffers are in a clean state.
As the QUIT command doesn’t require any additional information from the client, we simply check that data is null.
With our tests in place, let’s implement a basic codec (an encoder and decoder) for SMTP:
We first define a Request case class in which to store request data as it arrives.
Then we specify the encoder portion of our codec, which exists simply to write data out.
A dispose method is defined (but not fleshed out) to fulfill the contract of the ProtocolEncoder trait.
In the case of a HELO command, we also grab the subsequent string on that line.
The results are placed in a Request object and written out to state.
As you might imagine, state stores our progress throughout the parsing process.
Though trivial, the above example demonstrates just how easy it is to parse protocols with Naggati.
Now that we’ve got a working codec, let’s combine Naggati and MINA with Actors to wire up a server.
First, a few lines of setup grunt work to get things going for our SMTP server:
We then initialize an NioSocketAcceptor, a key piece of MINA machinery that accepts new connections from clients.
This is the piece of our server that talks back to the client.
Now that we know what the client is saying, thanks to the decoder, we actually know what to say back!
Straight away, we see the same pattern that we saw in the Actors examples earlier in this chapter: looping around a react block that pattern matches on a limited set of cases.
In SmtpHandler, all of those cases are events provided by MINA.
We’re handed a familiar Request object with each newly received valid message, and we can pattern match on the command field to take appropriate action.
When the client says HELO, we can reply with an acknowledgement.
When the client says QUIT, we say goodbye and disconnect him.
Now that we’ve got all the pieces in place, let’s have a conversation with our server:
A brief conversation, to be sure, but our server works! Now, what happens if we throw something unexpected at it?
Of course, what we’ve built just handles the beginning and end of a complete SMTP conversation.
As an exercise, try filling out the rest of the commands.
Or, to skip ahead to something very much akin to what we’ve built here, check out the open source Mailslot project on GitHub (see [Mailslot])
Recap and What’s Next We learned how to build scalable, robust concurrent applications using Scala’s Actor library that avoid the problems of traditional approaches based on synchronized access to shared, mutable state.
We also demonstrated that Java’s powerful built-in threading model is easily accessible from Scala.
Finally, we learned how to combine Actors with the powerful MINA NIO framework and Naggati to develop event-driven, asynchronous network servers from the ground up in just a few lines of code.
The next chapter examines Scala’s built-in support for working with XML.
The format’s combination of human readability, standardization, and tool support has made working with XML an inevitability for programmers.
Yet, writing code that deals in XML is an unpleasant chore in most programming languages.
As with the Actor functionality we learned about in Chapter 9, Scala’s XML support is implemented partly as a library, with some built-in syntax support.
It feels to the programmer like an entirely natural part of the language.
Convenient operators add a spoonful of syntactic sugar to the task of diving deep into complex document structures, and pattern matching further sweetens the deal.
Unusual in programming languages and particularly handy, Scala allows inline XML.
Most anywhere you might put a string, you can put XML.
This feature makes templating and configuration a breeze, and lets us test our use of XML without so much as opening a file.
First, we’ll look at reading and navigating an XML document.
Finally, we’ll produce XML output programmatically and demonstrate uses for inline XML.
Reading XML We’ll start with the basics: how to turn a string full of XML into a data structure we can work with:
We’ve transformed the string into a NodeSeq, Scala’s type for storing a sequence of XML nodes.
Were our XML document in a file on disk, we could have used the loadFile method from the same package.
Since we’re supplying the XML ourselves, we can skip the XML.loadString step and just assign a chunk of markup to a val or var:
Exploring XML If we paste the previous example into the interpreter, we can explore our sandwich using some handy tools provided by NodeSeq:
That backslash—what the documentation calls a projection function—says, “Find me elements named bread.” We’ll always get a NodeSeq back when using a projection function.
If we’re only interested in what’s between the tags, we can use the text method:
It’s valid syntax to say someXML \ "bread" text, without parentheses or the dot before the call to text.
You’ll still get the same result, but it’s harder to read.
What went wrong? The \ function doesn’t descend into child elements of an XML structure.
To do that, we use its sister function, \\ (two backslashes):
We split the single output line into two lines so it would fit on the page.
We dove into the structure and pulled out the two <condiment> elements.
Looks like one of the condiments has gone bad, though.
We can find out if any of the condiments has expired by extracting its expired attribute.
All it takes is an @ before the attribute name:
Looping and Matching XML The previous bit of code extracted the value of the expired attribute (true, in this case), but it didn’t tell us which condiment is expired.
If we were handed an arbitrary XML sandwich, how would we identify the expired condiments? We can loop through the XML:
We’ll include the original XML document again so you can follow along as we pattern match on XML:
Here, we bind the contents of our <sammich> structure (that is, what’s inside the opening and closing tag) to a variable called ingredients.
Then, as we iterate through the ingredients in a for loop, we assign the elements that are between the <condiments> tags to a temporary variable, cond.
The same tools that let us easily manipulate complex data structures in Scala are readily available for XML processing.
As a readable alternative to XSLT, Scala’s XML library makes reading and parsing XML a breeze.
It also gives us equally powerful tools for writing XML, which we’ll explore in the next section.
Writing XML While some languages construct XML through complex object serialization mechanisms, Scala’s support for XML literals makes writing XML far simpler.
To interpolate variables and expressions, escape out to Scala with curly braces, as we did in the pattern matching examples earlier:
As we can see, the name variable was substituted when we constructed the XML document assigned to bobXML.
That evaluation only occurs once; were name subsequently redefined, the <name> element of bobXML would still contain the string “Bob”
A Real-World Example For a more complete example, let’s say we’re designing that favorite latter-day “hello world,” a blogging system.
We’ll start with a class to represent an Atom-friendly blog post:
Beyond the obvious title and body attributes, we’ve defined several lazily loaded values in our Post class.
These attributes will come in handy when we transmute our posts into an Atom feed, the standard way to syndicate blogs between computers on the Web.
Atom documents are a flavor of XML, and a perfect application for demonstrating the process of outputting XML with Scala.
We’ll define an AtomFeed class that takes a sequence of Post objects as its sole argument:
We’re making heavy use of the ability to escape out to Scala expressions in this example.
Whenever we need a piece of dynamic information—for example, the date of the first post in the sequence, formatted for the Atom standard—we simply escape out and write Scala as we normally would.
In the latter half of the <feed> element, we use a for comprehension to yield successive blocks of dynamically formatted XML.
The write method of AtomFeed demonstrates the use of the saveFull method, provided by the scala.xml library.
Alternately, the save method within the same package will make use of any java.io.Writer variant, should you need buffering, piping, etc.
Writing XML with Scala is straightforward: construct the document you need with inline XML, use interpolation where dynamic content is to be substituted, and make use of the handy convenience methods to write your completed documents to disk or to other output streams.
Recap and What’s Next XML has become ubiquitous in software applications, yet few languages make working with XML a simple task.
We learned how Scala accelerates XML development by making it easy to read and write XML.
In the next chapter, we’ll learn how Scala provides rich support for creating your own Domain-Specific Languages (DSLs)
A Domain-Specific Language is a programming language that mimics the terms, idioms, and expressions used among experts in the targeted domain.
Code written in a DSL reads like structured prose for the domain.
Ideally, a domain expert with little experience in programming can read, understand, and validate this code.
Sometimes, a domain expert might be able to write DSL code, even if he isn’t a professional programmer.
We’ll only touch the surface of DSLs and Scala’s impressive support for them.
The basic build tool we used for the book’s examples, sake, uses a DSL similar to the venerable make and its Ruby cousin rake.
See the README in the code download archive for details.
For some advanced work on DSLs using Scala, [Hofer2008] explores polymorphic substitution of alternative implementations for DSL abstractions, which is useful for analysis, optimization, composition, etc.
A DSL hides implementation details and exposes only those abstractions relevant to the domain.
Efficiency Because implementation details are encapsulated, a DSL optimizes the effort required to write or modify code for application features.
Communication A DSL helps developers understand the domain and domain experts to verify that the implementation meets the requirements.
Quality A DSL minimizes the “impedance mismatch” between feature requirements, as expressed by domain experts, and the implementing source code, thereby minimizing potential bugs.
However, DSLs also have several drawbacks: Difficulties of creating good DSLs.
The latter tend to follow language idioms for API design, where uniformity is important.
Even then, elegant, effective, and easy-to-use APIs are difficult to design.
In contrast, each DSL should reflect the unique language idioms of its domain.
The DSL designer has much greater latitude, which also means it is much harder to determine the “best” design choices.
Long-term maintenance DSLs can require more maintenance over the long term to factor in domain changes.
Also, new developers will require more time to learn how to use and maintain a DSL.
From the implementation point of view, DSLs are often classified as internal and external.
An internal (sometimes called embedded) DSL is an idiomatic way of writing code in a general-purpose programming language, like Scala.
Instead, they are parsed just like any other code written in the language.
In contrast, an external DSL is a custom language with its own custom grammar and parser.
Internal DSLs are easier to create because they don’t require a special-purpose parser.
On the other hand, the constraints of the underlying language limit the options for expressing domain concepts.
You can design the language any way you want, as long as you can write a reliable parser for it.
The downside of external DSLs is the requirement to write and use a custom parser.
For example, internal DSLs written in Lisp are as old as Lisp itself.
Interest in DSLs has surged recently, driven in part by the Ruby community, because they are very easy to implement in Ruby.
As we’ll see, Scala provides excellent support for the creation of internal and external DSLs.
Internal DSLs Let’s create an internal DSL for a payroll application that computes an employee’s paycheck every pay period, which will be two weeks long.
The paycheck will include the employee’s net salary, which is the gross salary minus the deductions for taxes, insurance premiums (at least in some countries), retirement fund contributions, etc.
To better understand the contrasts between code that makes use of DSLs and code that does not, let’s try both techniques on the same problem.
Here’s how the paycheck might be calculated for two employees, without the help of a DSL:
For each employee, the script calculates the gross pay for the pay period, the deductions, and the resulting net.
These values are placed into a Paycheck, which is printed out.
Before we describe the types we are using, notice a few things about the foreach loop that does the work.
A DSL will help us minimize that “noise” and focus on what’s really going on.
We’ll see that our DSLs look similar, but they are more declarative, hiding the work from the user.
Here is the simple Paycheck class used in the script:
The Money type handles arithmetic, rounding to four decimal places, etc.
Proper financial arithmetic is notoriously difficult to do correctly for real-world transactions.
This implementation is not perfectly accurate, but it’s close enough for our purposes.
Each method might use the employee information and the gross salary for the pay period.
In this case, we use very simple algorithms based on just the gross salary, except for insurance premiums, which we treat as a fixed value.
Running the script for the payroll API produces the following output:
A Payroll Internal DSL The previous code works well enough, but suppose we wanted to show it to the Accounting Department to confirm that we’re calculating paychecks correctly.
Most likely, they would get lost in the Scala idioms.
Ideally, we would like to enable the accountants to do these customizations themselves, without our help.
We might achieve these goals if we can express the logic in a DSL that is sufficiently intuitive to an accountant.
Can we morph our API example into such a DSL? Returning to the script for the payroll API, what if we hide most of the explicit references to context information, like the employee, gross salary, and deduction values? Consider the following text:
We have included some “bubble” words (see [Ford2009]) that aid readability but don’t necessarily correspond to anything essential, such as to, an, is, for, of, and which.
We’ll eliminate some of these unnecessary words and keep others in our Scala DSL.
Compared to the version in the payroll API script, there’s a lot less clutter obscuring the essentials of the algorithm.
This is because we have minimized explicit references to the contextual information.
We mention gross five times, but hopefully in “intuitive” ways.
There are many possible internal Scala DSLs we could construct that resemble this ad hoc DSL.
Here is one of them, again in a script, which produces the same output as before:
We’ll go through the implementation step by step, but first, let’s summarize the features of Scala that allow us to implement this DSL.
Infix Operator Notation Consider this line in the definition of payrollCalculator:
Method chaining like this is often implemented where each method returns this so you can continue calling methods on the same instance.
Note that returning this allows those method calls to occur in any order.
If you need to impose a specific ordering, then return an instance of a different type.
Because chaining is so easy, we could have created separate methods for salary, for, minus, and deductions, allowing us to write the following expression:
Note that calls to for are preceded by different calls with very different meanings.
So, if the same instance is used throughout, it would have to track the “flow” internally.
However, since no computations are actually needed between these words, we chose the simpler design where words are joined together, separated by _
Implicit Conversions and User-Defined Types Returning to 2.weeks, since Int doesn’t have a weeks method, we use an implicit conversion to a Duration instance that wraps an Int specifying an amount:
The weeks method multiples that amount by 5 to return the corresponding amount of work days.
Hence, we designed the payroll calculator to work with days as the unit of time.
Should we later add support for work hours, it would be easy to refactor the design to use hours instead.
Duration is one of the ad hoc types that we designed to encapsulate the implicit context, to implement helper methods for the DSL, etc.
We’ll discuss the implicit conversion method we need in a moment.
Apply Methods A number of the implementation objects use apply to invoke behavior.
The rules object encapsulates the process of building the rules for payroll calculation.
Its apply method takes a function literal, Employee => Paycheck.
Payroll Rules DSL Implementation Now let’s explore the implementation, starting with the rules object and working our way down:
The function literal argument for rules.apply is used to construct a PayrollBuilder Rules that will process the specified rules.
It is used at the very beginning of the DSL.
It converts 2 into a Duration instance, which we discussed previously.
The other conversions are used later in the DSL to enable transparent conversion of Doubles, Employees, etc.
Note that the rules object is imported so these conversions are visible in the rest of the current file.
It will also need to be imported in files that use the DSL.
It evaluates the function literal for the whole rule set, wrapped in a try/catch block:
However, we left the exception public for use in catch clauses.
You can decide whether or not you like wrapping a thrown exception in a “domain-specific” exception, as shown.
Note that we have to pass the employee as a “context” instance in the function literal.
We said that it is desirable to make the context as implicit as possible.
An alternative approach would be to store context in singleton objects so other instances can get to them.
To see what we mean concerning the context, consider the part of our script that uses the payroll DSL, where the deductions are specified:
Consider the insurance premiums, for which a flat Money(500) is deducted.
Why didn’t we just write insurancePremiums are 500., instead? It turns out we have to “sneak” the gross instance into the expression somehow.
The name gross implies that it is a Money representing the employee’s salary for the pay period.
Tricksey DSLses!! It is actually another helper instance, DeductionsBuilder, which holds the whole paycheck, including the gross pay, and the employee instance.
The name gross is used merely because it reads well in the places where it is used.
This block is calculating the deductions and deducting them from the gross pay to determine the net pay.
There is no “communication” between the four lines of the function literal.
It would be great if they could be members of DeductionsBuilder or perhaps some other wrapper instance enclosing this scope.
Then each line would be a method call on one or the other wrapper.
Hence, each line must specify the gross instance to maintain continuity.
We jump through various hoops to support the syntax, yet allow gross to be available, as needed.
So, we contrived the convention that “raw” numbers, like the insurance deduction, have to be qualified by the particular currency used for the gross pay.
It is something of a hack, but it reads well and it solves our design problem.
Here is a possible alternative design that would have avoided the problem:
Now the fact that a builder is being used is more explicit, and federalIncomeTax, insurancePremiums, etc.
We opted for a more readable style, with the penalty of a harder implementation.
You’ll sometimes hear the phrase fluent interface used to refer to DSLs that emphasize readability.
Recall that rules defines an implicit conversion from Employee to this type.
GrossPayBuilder initializes the gross and appends new values to it whenever salary_for is called, which assumes we’re adding gross pay in increments of days.
DeductionsBuilder saves the employee from the passed-in GrossPayBuilder, which it doesn’t save as a field.
It also initializes the paycheck using the calculated gross pay.
We don’t need to do anything with the actual currency when this method is invoked.
Instead, it is used to support a design idiom that we’ll discuss shortly.
It invokes the function literal with the individual rules and then returns the completed Paycheck instance, which is ultimately what rules.apply returns.
Our remaining two methods are used to calculate individual deductions.
We used are for the objects with plural names, like insuran cePremiums, and is for the singular objects, like federalIncomeTax.
In fact, since both methods delegate to apply, they are effectively bubble words that the user could omit.
The apply method takes DeductionsBuilder and does nothing with it! In fact, by the time apply is called, the deduction has already been calculated and factored into the paycheck.
By implication, the presence of expressions like federalIncomeTax is are effectively syntactic sugar (at least as this DSL is currently implemented)
They are a fancy form of comments, but at least they have the virtue of type checking the “kinds” of deductions that are allowed.
Of course, as the implementation evolves, these instances might do real work.
Once we have a helper instance, we can call either the in method or the percent_of method.
Every line in the deductions function literal exploits this instance.
Deductions Builder.currency is effectively another bubble word; it simply returns this, but gives a readable idiom for the DSL.
Like any language, testing its robustness can be a challenge.
They will probably not understand the compiler error messages that refer to the internals we’ve hidden behind the DSL.
With an API, you can follow the Scala library conventions for types, method names, etc.
However, with a DSL, you’re trying to imitate the language of a new domain.
A well-designed DSL minimizes the translation effort between requirements and code, thereby improving communications with stakeholders about requirements.
DSLs also facilitate rapid feature change and hide distracting implementation details.
As always, there is a cost/benefit analysis you should make when deciding whether to use a DSL.
Assuming you’ve made the “go” decision, a common problem in DSL design is the finishing problem (see [Ford2009])
How do you know when you’ve finished building up the state of an instance and it’s ready to use? We solved this problem in two ways.
First, we nested the calculation steps in a function literal.
As soon as rules(employee) was invoked, the paycheck was built to completion.
Also, all the steps were evaluated “eagerly.” We didn’t need to put in all the rules, then run them at the end.
Our only ordering requirement was the need to calculate the gross.
We enforced the correct order of invocation using instances of different types.
There are cases in which you can’t evaluate the build steps eagerly.
For example, a DSL that builds up a SQL query string can’t run a query after each step of the build process.
In this case, evaluation has to wait until the query string is completely built.
By contrast, if your DSL steps are stateless, chained method invocation works just fine.
In this case, it doesn’t matter when you stop calling chained methods.
If you chain methods that build up state, you’ll have to add some sort of done method and trust the users to always use it at the end.
External DSLs with Parser Combinators When you write a parser for an external DSL, you can use a parser generator tool like Antlr (see [Antlr])
However, the Scala library includes a powerful parser combinator library that can be used for parsing most external DSLs that have a context-free grammar.
An attractive feature of this library is the way it defines an internal DSL that makes parser definitions look very similar to familiar grammar notations, like EBNF (Extended Backus-Naur Form—see [BNF])
About Parser Combinators Parser combinators are building blocks for parsers.
A combinator framework makes it easy to combine parsers to handle sequential and alternative cases, repetition, optional terms, etc.
We’ll learn more about parsing techniques and terminology as we proceed.
A complete exposition of parsing techniques is beyond our scope, but our example should get you started.
A Payroll External DSL For our parser combinator example, we’ll reuse the example we just discussed for internal DSLs.
We’ll modify the grammar slightly, since our external DSL does not have to be valid Scala syntax.
Compare this example to the internal DSL we defined in “A Payroll Internal DSL” on page 222:
In our new DSL, we insert a specific employee in the script.
We wouldn’t expect a user to copy and paste this script for every employee.
A natural extension that we won’t pursue would allow the user to loop over all salaried employees in a database, for example.
Some of the differences are “gratuitous”; we could have used the same syntax we used previously.
These changes include removing underscores between words in some expressions and expanding camel-case words into space-separated words.
That is, we turned some single words into multi-word expressions.
We made these changes because they will be easy to implement using parser combinators, but using the same multi-word expressions would have added a lot of complexity to the internal DSL’s implementation.
We no longer need “local variables” like employee and gross.
Those words still appear in the DSL, but our parser will keep track of the corresponding instances internally.
It is still convenient to surround the list of deductions with curly braces.
We now use a comma to separate the individual deductions, as that will make the parser’s job easier.
To see how closely the internal DSL for Scala’s parser combinator library resembles the context-free grammar, let’s start with the grammar itself, written in a variation of EBNF.
At this point, we won’t do anything to actually calculate an employee’s paycheck, so we’ll append V1 to the class name:
CombinatorsV1 and the subsequent refinements below would change if we tracked this information.
Would you necessarily keep the parsed strings or track the information some other way? The “or” case is expressed with the | method, just as in the grammar:
The rep method can be used for zero or more repetitions.
We actually use a similar method, repsep, which lets us specify a separator, in our case a comma (,):
Note that deduct combines several features we have just described.
Like repetition, there is an opt method for optional terms, which we aren’t using.
It’s well known that parsing non-trivial grammars with just regular expressions tends to break down pretty quickly.
However, using regular expressions to parse individual terms inside a parsing framework can be very effective.
In our example, we exploit the productions in JavaTokenParsers to parse quoted strings (for the employee’s name), decimal literals, and floating-point literals.
Let’s try it out! Here is a specification that exercises the parser for two cases, without and with deductions:
We invoke the top-level production method, paycheck, and pass its return value as the first argument to parseAll and pass the string to parse as the second argument.
If the parsing process is successful, the result of the parse is returned as an instance of type p.Success[+T], a case class declared in the Parsers trait.
The first is the result of the parse, an instance of type T (assigned to r in the case clause)
The second is the remaining input string to parse, which will be empty after a successful parse (we will have parsed the whole string at this point)
If the parse fails, the returned instance is either a p.Failure or p.Error, which our example handles with a generic case clause.
Both are derived from p.NoSuccess, which contains fields for an error message and the unconsumed input at the point of failure.
A p.Failure in a parser will trigger backtracking so that a retry with a different parser can be invoked by the parser framework, if possible.
An Error result does not trigger backtracking and is used to signal more serious problems.
We have two big unanswered questions: what do the production methods actually return, and what is the type of the result instance returned in the p.Success? The production methods themselves return parsers.
Most of them in our example return p.Parser[String] (again, a path-dependent type)
When this parser is used, it will return a List[String], with one string corresponding to each match in the repetition.
Once in the interpreter, enter the following expressions after the scala> prompt.
Generating Paychecks with the External DSL As we parse the DSL, we want to look up the employee by name, fetch his or her gross salary for the specified pay period, and then calculate the deductions as we go.
When the parser returned by paycheck finishes, we want to return a Pair with the Employee instance and the completed Paycheck.
We will reuse “domain” classes like Employee, Money, Paycheck, etc.
We’ll modify the parsers returned by some of the production methods to return new kinds of parsers.
We’ll also do administrative work like storing running context data, as needed.
We could make it more robust, but doing so isn’t the goal of this exercise.
A real implementation would probably use a data store of some kind.
There are two other fields: currentEmployee, which remembers which employee we are processing, and grossAmount, which remembers the gross amount of pay for the employee for the pay period.
They are set only once per parse, but not when they are declared, only when we parse the input that allows us to calculate them.
No doubt it would be possible to write scripts in the DSL that exploit this bug.
As an exercise, you might try improving the implementation to eliminate these weaknesses.
We have added Javadoc-style @return annotations for most of the productions to make it clear what they are now returning.
In some cases, the productions are unchanged, as the original parser instances are fine as is.
Most of the changes reflect our desire to calculate the paycheck as we go.
To construct the name, the embedded double quotes have to be removed, which is why we start by extracting the substring that tosses the first and last characters.
The name is used to look up the Employee instance in the map, saving the value in the curren tEmployee field.
In general, there is not a lot of “graceful” error handling in PayrollPar serCombinators.
However, the empl method handles the case where no employee is found with the specified name, throwing an UnknownEmployee exception when this occurs.
Sometimes, a parser converts an input string to an Int (e.g., duration) or a Money (e.g., gross)
It folds the list of deductions into a single deduction amount, using addition.
The first has a single argument that specifies the initial value, in this case, zero Money.
In this case, we return the sum of the arguments.
So, foldLeft iterates over the items collection, adding them together.
See “Traversing, Mapping, Filtering, Folding, and Reducing” on page 174 for more information on foldLeft and related operations.
We’re also using stringLiteral, decimalNumber, and floating PointNumber provided by JavaTokenParsers.
They just return a multiplication factor used to determine total days in the pay period in the duration production rule.
There are other combinator methods for applying functions to parser results in different ways.
The following (somewhat incomplete) specification shows the calculation of paychecks when there are no deductions and when there are several deductions:
If you work out what the results should be from the input strings, you’ll see that the implementation correctly calculates the paycheck.
Besides the many small details that differ between this implementation of the external DSL and the previous implementation of the internal DSL, there is one big conceptual difference from the two implementations.
Here we are computing the paycheck as we parse code written in the external DSL.
In the internal DSL case, we generated a paycheck calculator when we parsed the DSL.
Afterward, we used that calculator to compute paychecks for one employee at a time.
We could have generated a paycheck calculator like we did before, but we chose a simpler approach to focus on the construction of the parser itself.
Also, as we discussed earlier, we weren’t as careful about thread safety and other issues in the implementation.
However, a non-trivial DSL can be a challenge to implement and debug.
For the examples in this chapter, the parser combinators implementation was easier to design and write than the implementation for the internal DSL.
However, we found that debugging the internal DSL was easier.
You must also consider how robust the parser must be when handling invalid input.
Depending on the level of sophistication of the users of the DSL, you may need to provide very good feedback when errors occur, especially when your users are nonprogrammers.
The version 2.8 library will also provide support for writing packrat parsers that can implement unambiguous parsing expression grammars (PEGs)
The 2.8 implementation of packrat parsers also supports memoization, which helps improve performance, among other benefits.
If you need a fast parser, a packrat parser will take you further before you need to consider more specialized tools, like parser generators.
DSLs in Scala can be quite fun to work with, but don’t underestimate the effort required to create robust DSLs that meet your clients usability needs, nor long-term maintenance and support issues.
If you choose to write a DSL, you have rich options in Scala.
The syntax is flexible yet powerful enough that an internal DSL may be sufficient.
A internal DSL is an excellent starting point, especially if other programmers will be the primary writers of code in the DSL.
Consider whether the code written in the DSL will need to be processed for other purposes, like generating documentation, spreadsheets, etc.
Since you will have to write a parser for the DSL anyway, it might be straightforward to write others to handle these different purposes.
In the next chapter, we’ll explore the richness of Scala’s type system.
Its type system is one of the most sophisticated in any programming language, in part because it combines comprehensive ideas from functional programming and object-oriented programming.
The type system tries to be logically comprehensive, complete, and consistent.
It exceeds limitations in Java’s type system while containing innovations that appear in Scala for the first time.
However, the type system can be intimidating at first, especially if you come from a dynamically typed language like Ruby or Python.
Most of the time, you don’t need to know the particulars, so we encourage you not to worry that you must master the type system in order to use Scala effectively.
You might choose to skim this chapter if you’re new to Scala, so you’ll know where to look when type-related questions arise later.
Still, the more you know about the type system, the more you will be able to exploit its features in your programs.
This is especially true for library writers, who will want to understand when to use parameterized types versus abstract types, which type parameters should be covariant, contravariant, or invariant under subtyping, and so forth.
Also, some understanding of the type system will help you understand and debug the occasional compilation failure related to typing.
Finally, this understanding will help you make sense of the type information shown in the sources and Scaladocs for Scala libraries.
If you didn’t understand some of the terms we used in the preceding paragraphs, don’t worry.
We’re not going to discuss Scala’s type system in exhaustive detail.
Rather, we want you to come away with a pragmatic understanding of the type system.
You should develop an awareness of the features available, what purposes they serve, and how to read and understand type declarations.
We’ll also highlight similarities with Java’s type system, since it may be a familiar point of reference for you.
Understanding the differences is also useful for interoperability with Java libraries.
To focus the discussion, we won’t cover the .NET type system, except to point out some notable differences that .NET programmers will want to know.
Reflecting on Types Scala supports the same reflection capabilities that Java and .NET support.
First, you can use the same methods you might use in Java or .NET code.
Note that these methods are only available on subtypes of AnyRef.
The classOf[T] method returns the runtime representation for a Scala type.
Using classOf[T] is convenient when you have a type that you want information about, while getClass is convenient for retrieving the same information from an instance of the type.
However, classOf[T] and getClass return slightly different values, reflecting the effect of type erasure on the JVM, in the case of getClass:
Although .NET does not have type erasure, meaning it supports reified types, the .NET version of Scala currently follows the JVM’s erasure model in order to avoid incompatibilities that would require a “forked” implementation.
We’ll discuss a workaround for erasure, called Manifests, after we discuss parameterized types in the next section.
Scala also provides methods for testing whether an object matches a type and also for casting an object to a type.
Once again, type erasure must be considered with parameterized types.
Note that these two operations are methods and not keywords in the language, and their names are deliberately somewhat verbose.
Normally, type checks and casts like these should be avoided.
For casts, consider why a cast is necessary and determine if a refactoring of the design can eliminate the requirement for a cast.
At the time of this writing, there are some experimental features that might appear in the final version 2.8 release in the scala.reflect package.
These features are designed to make reflective examination and invocation of code easier than using the corresponding Java methods.
If you come from a Java or C# background, you probably already have some knowledge of parameterized types and methods.
Now we explore the details of Scala’s sophisticated support for parameterized types.
Scala’s parameterized types are similar to Java and C# generics and C++ templates.
They provide the same capabilities as Java generics, but with significant differences and extensions, reflecting the sophistication of Scala’s type system.
To recap, a declaration like class List[+A] means that List is parameterized by a single type, represented by A.
Sometimes, a parameterized type like List is called a type constructor, because it is used to create specific types.
For example, List is the type constructor for List[String] and List[Int], which are different types (although they are actually implemented with the same byte code due to type erasure)
In fact, it’s more accurate to say that all traits and classes are type constructors.
Manifests There is an experimental feature in Scala (since version 2.7.2), called Manifests, that captures type information that is erased in the byte code.
A Manifest is declared as an implicit argument to a method or type that wants to capture the erased type information.
Unlike most implicit arguments, the user does not need to supply an in-scope Manifest value or method.
Here is an example that illustrates some of the strengths and weaknesses of Manifests:
WhichList tries to determine the type of list passed in.
It uses the value of the Manifest’s toString method to determine this information.
Notice that it works when the list is constructed inside the call to WhichList.apply.
It does not work when a previously constructed list is passed to WhichList.apply.
The compiler exploits the type information it knows in the first case to construct the implicit Manifest with the correct B.
However, when given previously constructed lists, the crucial type information is already lost.
Hence, Manifests can’t “resurrect” type information from byte code, but they can be used to capture and exploit type information before it is erased.
Good examples are the apply methods in companion objects for parameterized classes.
Recall that companion objects are singleton objects associated with a companion class.
There is only one instance of a singleton object, as its name implies, so type parameters would be meaningless.
Let’s consider object List, the companion object for class List[+A]
Here is the definition of the apply method in object List:
The apply methods takes a variable length list of arguments of type A, which will be inferred from the arguments, and returns a list created from the arguments.
Variance Under Inheritance An important difference between Java and Scala generics is how variance under inheritance works.
For example, if a method has an argument of type List[AnyRef], can you pass a List[String] value? In other words, should a List[String] be considered a subtype of List[AnyRef]? If so, this kind of variance is called covariance, because the supertype-subtype relationship of the container (the parameterized type) “goes in the same direction” as the relationship between the type parameters.
In other contexts, you might want contravariant or invariant behavior, which we’ll describe shortly.
In Scala, the variance behavior is defined at the declaration site using variance annotations: +, -, or nothing.
In other words, the type designer decides how the type should vary under inheritance.
Let’s examine the three kinds of variance, summarized in Table 12-1, and understand how to use them effectively.
We’ll assume that Tsup is a supertype of T and Tsub is a subtype of T.
Type variance annotations and their meanings Annotation Java equivalent Description.
The “Java equivalent” column is a bit misleading; we’ll explain why in a moment.
Class List is declared List[+A], which means that List[String] is a subclass of List[AnyRef], so Lists are covariant in the type parameter A.
When a type like List has only one covariant type parameter, you’ll often hear the shorthand expression “Lists are covariant” and similarly for types with a single contravariant type parameter.
The +R is the return type and has the covariant annotation +
The type for the single argument has the contravariant annotation -
For functions with more than one argument, all the argument types have the contravariant annotation.
So, for example, using our T, Tsup, and Tsub types, the following definition would be legal:
So, what does this really mean? Let’s look at an example to understand the variance behavior.
If you run it, it will fail to compile on the last line.
We start by defining a very simple hierarchy of three classes, C and its superclass CSuper and its subtype CSub.
Next we define a var named f on the line with the #1 comment.
It is a function with the signature C => C.
To be clear, the value assigned to f is after the equals sign, (c: C) => new C.
We actually ignore the input c value and just create a new C.
We use whitespace to make the similarities and differences stand out when comparing the original declaration of f and the subsequent reassignments.
We keep reassigning to f because we are just testing what will and won’t compile at this point.
Specifically, we want to know what function values we can legally assign to f: (C) => C.
Similarly, on line #4, we use C as the argument and CSub as the return type, both of which worked fine in the previous lines.
The last line, #5, does not compile because we are attempting to use a covariant argument in a contravariant position.
We’re also attempting to use a contravariant return value where only covariant values are allowed.
Why is the behavior correct in these cases? Here’s where Design by Contract thinking comes in handy.
Let’s see how a client might use some of these definitions of f:
The useF method takes a function C => C as an argument.
We’re just passing function literals now, rather than assigning them to f.
It creates a C (line #1) and passes it to.
You could say that the useF method specifies a contract of behavior.
It expects to be passed a function that can take a C and return a C.
It will call the passed-in function, passing a C instance to it, and it will expect to receive a C back.
In line #5, we pass useF a function that takes a C and returns a C.
In line #6, we pass in a function that is “willing” to accept a CSuper and “promises” to return a CSub.
In effect, it widens the allowed instances by accepting a supertype.
Keep in mind that it will never actually be passed a CSuper by useF, only a C.
However, since it can accept a wider set of instances, it will work fine if it only gets C instances.
Similarly, by “promising” to return a CSub, this anonymous function narrows the possible values returned to useF.
That’s OK, too, because useF will accept any C in return, so if it only gets CSubs, it will be happy.
Applying the same arguments, we can see why the last line in the script, line #7, fails to compile.
Now the anonymous function can only accept a CSub, but useF will pass it a C.
The body of the anonymous function would now break, because it calls c.msub, which doesn’t exist in C.
Similarly, returning a CSuper when a C is expected breaks line #4 in useF, because CSuper doesn’t have the m method.
The same arguments are used to explain how contracts can change under inheritance in Design by Contract.
Note that variance annotations only make sense on the type parameters for parameterized types, not parameterized methods, because the annotations affect the behavior of subtyping.
Methods aren’t subtyped, but the types that contain them might be subtyped.
The + variance annotation means the parameterized type is covariant in the type parameter.
The - variance annotation means the parameterized type is contravariant in the type parameter.
No variance annotation means the parameterized type is invariant in the type parameter.
Finally, the compiler checks your use of variance annotations for problems like the one we just described in the last lines of the examples.
Suppose you attempted to define your own function type this way:
The compiler would throw the following errors for the apply method: ...
Variance of Mutable Types All the parameterized types we’ve discussed so far have been immutable types.
What about the variance behavior of mutable types? The short answer is that only invariance is allowed.
We can make sense of these errors by remembering our discussion of FunctionN type variance under inheritance, where the types of the function arguments are contravariant (i.e., -T1) and the return type is covariant (i.e., +R)
The problem with a mutable type is that at least one of its fields has the equivalent of read and write operations, either through direct access or through accessor methods.
In the first error, we are trying to use a covariant type as an argument to a setter (write) method, but we saw from our discussion of function types that argument types to a method must be contravariant.
A covariant type is fine for the getter (read) method.
Similarly, for the second error, we are trying to use a contravariant type as the return value of a read method, which must be covariant.
Hence, the compiler won’t let us use a variance annotation on a type that is used for a mutable field.
For this reason, all the mutable parameterized types in the Scala library are invariant in their type parameters.
Some of them have corresponding immutable types that have covariant or contravariant parameters.
Variance In Scala Versus Java As we said, the variance behavior is defined at the declaration site in Scala.
The client of a type defines the variance behavior desired (see [Naftalin2006])
In other words, when you use a Java generic and specify the type parameter, you also specify the variance behavior (including invariance, which is the default)
You can’t specify variance behavior at the definition site in Java, although you can use expressions that look similar.
In Java variance specifications, a wildcard ? always appears before the super or extends keyword, as shown earlier in Table 12-1
When we said after the table that the “Java Equivalent” column is a bit misleading, we were referring to the differences between declaration versus call site specifications.
A drawback of call-site variance specifications is that they force the users of Java generics to understand the type system more thoroughly than is necessary for users of Scala parameterized types, who don’t need to specify this behavior when using parameterized types.
If an unknown shape name is passed in, then we return a None<Shape>
Because Scala defines a subtype of all types, Nothing, Scala can define None as case object None extends Option[Nothing]
The Java type system provides no way to implement our Java None in a similar way.
Implementation Notes The implementation of parameterized types and methods is worth noting.
The implementations are generated when the defining source file is compiled.
For each type parameter, the implementation assumes that Any subtype could be specified (Object is used in Java generics)
Type Bounds When defining a parameterized type or method, it may be necessary to specify bounds on the type.
For example, a parameterized type might assume that a particular type parameter contains certain methods.
Upper Type Bounds Consider the overloaded apply methods in object scala.Array that create new arrays.
There are optimized implementations for each of the AnyVal types.
There is another implementation of apply that is parameterized for any type that is a subtype of AnyRef.
The type parameter A <: AnyRef means “any type A that is a subtype of AnyRef.” Note that a type is always a subtype and a supertype of itself, so A could also equal AnyRef.
So the <: operator indicates that the type to the left must be derived from the type to the right, or that they must be the same type.
As we said in “Reserved Words” on page 49, this operator is actually a reserved word in the language.
These bounds are called upper type bounds, following the de facto convention that diagrams of type hierarchies put subtypes below their supertypes.
Without the bound in this case, i.e., if the signature were def apply[A](xs: A*): Array[A], the declaration would be ambiguous with the other apply methods for each of the AnyVal types.
The type signature A <: B says that A must be a subtype of B.
In Java, this would be expressed as A extends B in a type declaration.
This is different from instantiating a type at a call site, where the syntax ? extends B is used in Java, indicating the variance behavior.
Keep in mind the distinction between type variance and type bounds.
For a type like List, the variance behavior describes how actual types instantiated from it, like List[AnyRef] and List[String], are related.
In this case, List[String] is a subtype of List[AnyRef], since String is a subtype of AnyRef.
In contrast, lower and upper type bounds limit the allowed types that can be used for a type parameter when instantiating a type from a parameterized type.
Lower Type Bounds Similarly, there are circumstances when we might want to express that only super types of a particular type are allowed.
Recall that a type is also a supertype of itself.
We call these lower type bounds, again because the allowed type would be above the boundary in a typical type hierarchy diagram.
A particularly interesting example is the :: (“cons”) method in class List[+A]
Recall that this operator is used to create a new list by prepending an element to a list:
The new list will be of type List[B], specifically a scala.::
The :: class (as opposed to the :: method) is derived from List.
The :: method can prepend an object of a different type from A, the type of the elements in the original list.
The compiler will infer the closest common supertype for A and the parameter x.
Here’s an example that prepends a different type of object on a list:
The new list of type List[Any], since Any is the closest common supertype of String and Double.
We started with a list of Strings, so A was String.
Then we prepended a Double, so the compiler inferred B to be Any, the closest (and only) common supertype.
The type signature B >: A says that B must be a supertype of A.
There is no analog in Java; B super A is not supported.
A Closer Look at Lists Putting these features together, it’s worth looking at the implementation of the List class in the Scala library.
It illustrates several useful idioms for functional-style, immutable data structures that are fully type-safe, yet flexible.
We won’t show the entire implementation, and we’ll omit the object List, many methods in the List class, and the comments that are used to generate the Scaladocs.
We encourage you to look at the complete implementation of List, either by downloading the source distribution from the Scala website or by browsing to the implementation through the Scaladocs page for List.
To avoid confusion with scala.List, we’ll use our own package and name, AbbrevList:
A non-empty AbbrevList characterized by a head and a tail.
Notice that while AbbrevList is immutable, the internal implementation uses mutable variables, e.g., in forEach.
AbbrevList (the analog of List) is an abstract trait that declares three abstract methods: isEmpty, head, and tail.
It defines the “cons” operator (::) and a foreach method.
All the other methods found in List could be implemented with these methods, although some methods (like List.length) use different implementation options for efficiency.
It returns true from isEmpty, and it throws an exception from head and tail.
Because AbbrevNil (and Nil) have essentially no state and behavior, having an object rather than a class eliminates unnecessary copies, makes equals fast and simple, etc.
The :: class is the analog of scala.:: derived from List.
Its arguments are the element to become the head of the new list and an existing list, which will be the tail of the new list.
There is no other data structure required to represent the list.
This is why prepending a new element to create a new list is an O(1) operation.
The List class also has a deprecated method + for creating a new list by appending an element to the end of an existing list.
That operation is O(N), where N is the length of the list.
As you build up new lists by prepending elements to other lists, a nested hierarchy of :: instances is created.
Because the lists are immutable, there are no concerns about corruption if one of the :: is changed in some way.
You can see this nesting if you print out a list, exploiting the toString method generated because of the case keyword.
Note the output on the last line, which shows the nesting of (head,tail) elements.
For another example using similar approaches, this time for defining a stack, refer to http://www.scala-lang.org/node/129
Views and View Bounds We’ve seen many examples where an implicit method was used to convert one type to another—for example, to give the appearance of adding new methods to an existing type, the so-called Pimp My Library pattern.
You can also use function values that have the implicit keyword.
A view is an implicit value of function type that converts a type A to B.
An in-scope implicit method with the same signature can also be used as a view, e.g., an implicit method imported from an object.
The term view conveys the sense of having a view from one type (A) to another type (B)
When a type A is used in a context where another type B is expected and there is a view in scope that can convert A to B.
When a non-existent member m of a type A is referenced, but there is an in-scope view that can convert A to a B that has the m member.
For an example of the first circumstance, Predef also defines many views for converting between AnyVal types and for converting an AnyVal type to its corresponding java.lang type.
It allows any type to be used for A if it can be converted to B using a view.
A method or class containing such a type parameter is treated as being equivalent to a corresponding method or class with an extra argument list with one element, a view.
For example, consider the following method definition with a view bound:
The implicit parameter viewAB would be given a unique name by the compiler.
Note that we have an additional argument list, as opposed to an additional argument in the existing argument list.
Why does this transformation work? We said that a valid A must have a view in scope that transforms it to a B.
The implicit viewAB argument will get invoked inside m to convert all A instances to B instances where needed.
For this to work, there must be a view of the correct type in scope to satisfy the implicit argument.
You could also pass a function with the correct signature explicitly as the second argument list when you call m.
However, there is one situation where this won’t work, which we’ll describe after our upcoming example.
For view bounds on types, the implicit view argument list would be added to the primary constructor.
Traits can’t have view bounds for their type parameters, because they can’t have constructor argument lists.
To make this more concrete, let’s use view bounds to implement a LinkedList class that uses Nodes, where each Node has a payload and a reference to the next Node in the list.
This type hierarchy is modeled after List and AbbrevList earlier.
The :: type represents intermediate nodes, and NilNode is analogous to Nil for Lists.
We also override toString to give us convenient output, which we’ll examine shortly.
The following script defines a LinkedList type that uses Nodes:
It starts with a definition of a parameterized implicit method, any2Node, that converts A to Node[A]
It will be used as the implicit view argument when we work with Linked Lists.
It creates a “leaf” node using a bounds.:: node with a reference to NilNode as the “next” element in the list.
Otherwise, the script would run the same, except that some of the temporary lists would be using Node[Any] rather than Node[Int]
It defines a view bound on A and takes a single argument, the head Node of the list (which may be the head of a chain of Nodes)
As we see later in the script, even though the constructor expects a Node[A] argument, we can pass it an A and the implicit view any2Node will get invoked.
The beauty of this approach is that a client never has to worry about proper construction of Nodes.
As we saw for List and AbbrevList, the lower bound allows us to prepend items of different types from the original A type.
This method will have its own implicit view argument, but our parameterized, implicit method, any2Node, will be used for this argument, too.
We mentioned previously that if you don’t have a view in scope, you could pass a “nonimplicit” converter as the second argument list explicitly.
This actually won’t work in our example, because the constructor and :: method in LinkedList take Node[A] arguments, but we call them with Ints and Strings.
We would have to call them with Node[Int] and Node[String] arguments explicitly.
The grammar for type parameters, including view bounds, is the following (see [ScalaSpec2009]):
Again, all the bounds expressions apply to the first id (B) or the underscore ( _ )
Finally, we create a LinkedList in the script, prepend some values to create new lists, and then print them out:
To recap, the view bounds let us work with “payloads” of Ints and Strings while the implementation handled the necessary conversions to Nodes.
View bounds are not used as often as upper and lower bounds, but they provide an elegant mechanism for those times when automatic coercion from one type into another is useful.
As always, use implicits with caution; implicit conversions are far from obvious when reading code and debugging mysterious behavior.
Nothing and Null In “The Scala Type Hierarchy” on page 155, we mentioned that Null is a subtype of all AnyRef types and Nothing is a subtype of all types, including Null.
Null is declared as a final trait (so it can’t be subtyped), and it has only one instance, null.
Since Null is a subtype of all AnyRef types, you can always assign null as an instance of any of those types.
Java, in contrast, simply treats null as a keyword with special handling by the compiler.
However, Java’s null actually behaves as if it were a subtype of all reference types, just like Scala’s Null.
On the other hand, since Null is not a subtype of AnyVal, it is not possible to assign null to an Int, for example, which is also consistent with the primitive semantics in Java.
Nothing is also a final trait, but it has no instances.
The best example is Nil, the empty list, which is a case object.
Because lists are covariant in Scala, as we saw earlier, this makes Nil an instance of List[T], for any type T.
We also exploited this feature in our Abbrev List and LinkedList implementations.
Understanding Abstract Types Besides parameterized types, which are common in statically typed, object-oriented languages, Scala also supports abstract types, which are common in functional languages.
Technically, you could implement almost all the idioms that parameterized types support using abstract types and vice versa.
However, in practice, each feature is a natural fit for different design problems.
Recall our version of Observer that uses abstract types in Chapter 6:
We’ll have more to say about structural and function types later in this chapter.
We can also use type bounds when we declare or refine the declaration of abstract types.
That is, t had an upper type bound (superclass) of AnyRef.
We can also have lower type bounds (subclasses), and we can use most of the value types (see “Value Types” on page 275) in the bounds expressions.
The rest of the expression is telling us the bounds of t2
That doesn’t mean that you can leave out the declaration of t3
It has to be there, but it also has to show a consistent type bound with the one implied in the t2 declaration.
When we revisit the Observer Pattern in “Self-Type Annotations and Abstract Type Members” on page 317, we’ll see another example of type bounds used on abstract types.
We’ll see a problem they can cause, along with an elegant solution.
Remember that the abstract types are members of the enclosing type, not type parameters, as for parameterized types.
The enclosing type may have an inheritance relationship with other types, but member types behave just like member methods and variables.
They don’t affect the inheritance relationships of their enclosing type.
However, they can also be refined in subtypes without being fully defined, unlike variables and methods.
Of course, instances can only be created when the abstract types are given concrete definitions.
Parameterized Types Versus Abstract Types When should you use parameterized types versus abstract types? Parameterized types are the most natural fit for parameterized container types like List and Option.
What should be the type of the field x? We can’t use A because it’s not in scope at the point of the constructor argument.
We could use Any, but that defeats the value of having appropriately typed declarations.
If a type will have constructor arguments declared using a “placeholder” type that has not yet been defined, then parameterized types are the only good solution (short of using Any or AnyRef)
You can use abstract types as method arguments and return values within a function.
First, you can run into problems with pathdependent types (discussed in “Path-Dependent Types” on page 272), where the compiler thinks you are trying to use an incompatible type in a particular context, when in fact they are paths to compatible types.
Second, it’s awkward to express methods like List.:: (“cons”) using abstract types where type changes (expansion in this case) can occur:
Also, if you want to express variance under inheritance that is tied to the type abstractions, then parameterized types with variance annotations make these behaviors obvious and explicit.
These limitations of abstract types really reflect the tension between object-oriented inheritance and the origin of abstract types in pure functional programming type systems, which don’t have inheritance.
Parameterized types are more popular in objectoriented languages because they handle inheritance more naturally in most circumstances.
On the other hand, sometimes it’s useful to refer to a type abstraction as a member of another type, as opposed to a parameter used to construct new types from a parameterized type.
Refining an abstract type declaration through a series of enclosing type refinements can be quite elegant:
This example also shows that abstract types are often used to declare abstract variables of the same type.
When the abstract variables are eventually made concrete, they can either be defined inside the type body, much as they were originally declared, or they can be initialized through constructor arguments.
Using constructor arguments lets the user decide on the actual values, while initializing them in the body lets the type designer decide on the appropriate value.
We used constructor arguments in the brief BulkReader example we presented in “Abstract Types And Parameterized Types” on page 47:
If you come from an object-oriented background, you will naturally tend to use parameterized types more often than abstract types.
The Scala standard library tends to emphasize parameterized types, too.
Still, you should learn the merits of abstract types and use them when they make sense.
Path-Dependent Types Languages that let you nest types provide ways to refer to those type paths.
Although you will probably use them rarely, it’s useful to understand the basics, as compiler errors often contain type paths.
In Scala, that means that the nested Logger type is unique for each of the service types.
In this case, the easiest solution is to move the declaration of Logger outside of Service, thereby removing the path dependency.
In other cases, it’s possible to qualify the type so that it resolves to what you want.
C.this For a class C, you can use C.this or this inside the body to refer to the current instance:
Inside a type body and outside a method definition, this refers to the type itself:
The this in the expression this.C refers to the trait T1
C.super You can refer specifically to the parent of a type with super:
If you want to refer specifically to one of the parents of a type, you can qualify super with the type, as shown in setX5
This is particularly useful for the case where a type mixes in several traits, each of which overrides the same method.
If you need access to one of the methods in a specific trait, you can qualify super.
What if you are calling super in a class with several mixins and it extends another type? To which type does super bind? Without the qualification, the rules of linearization determine the target of super (see “Linearization of an Object’s Hierarchy” on page 159)
Just as for this, you can use super to refer to the parent type in a type body outside a method:
The elements of a type path must be stable, which roughly means that all elements in the path must be packages, singleton objects, or type declarations that alias the same.
The last element in the path can be a class or trait.
Value Types Because Scala is strongly and statically typed, every value has a type.
The term value types refers to all the different forms these types take, so it encompasses many forms that are now familiar to us, plus a few new ones we haven’t encountered until now.
We are using the term value type here in the same way the term is used by [ScalaSpec2009]
However, elsewhere in the book we also follow the specification’s overloaded use of the term to refer to all subtypes of AnyVal.
Type Designators The conventional type IDs we commonly use are called type designators:
They are actually a shorthand syntax for type projections, which we cover later.
Parameterized Types When we create a type from a parameterized type, e.g., List[Int] and List[String] from List[A], the types List[Int] and List[String] are value types, because they are associated with declared values, e.g., val names = List[String]()
Scala allows you to declare instances of these types using an infix notation, e.g., a Either b.
The attempt method will evaluate the call-by-name parameter operation and return its Boolean result, wrapped in a Right, or any Throwable that is caught, wrapped in a Left.
Recall from “The Scala Type Hierarchy” on page 155 that when using this exception-handling idiom with Either, it is conventional to use Left for the exception and Right for the normal return value.
Function Types The functions we have been writing are also typed.
When there is only one argument, you can drop the parentheses: T => R.
We used a call-by-name argument in our attempt example in the previous section.
Recall that everything in Scala is an object, even functions.
As we discussed in “Variance Under Inheritance” on page 251, the FunctionN traits are contravariant in the type parameters for the arguments and covariant in the return type parameter.
Recall that when you reference any object followed by an argument list, Scala calls the apply method on the object.
In this way, any object with an apply method can also be considered a function, providing a nice symmetry with the object-oriented nature of Scala.
When you define a function value, the compiler instantiates the appropriate FunctionN object and uses your definition of the function as the body of apply:
The capitalizer and capitalizer2 function values are effectively the same, where the latter mimics the compiler’s output.
It returns a new function with N argument lists, each of which has a single argument taken from the original argument list of N arguments.
In the first part of the script, we define a Function3 value f that does Double arithmetic.
We create a new function value fc by currying f.
Then we call both functions with the same arguments and print out the results.
There are no concerns about rounding errors in the comparison here; recall that both functions call the same apply method, so they must return the same value.
In the second part of the script, we exploit the feature of curried functions that we can partially apply arguments, creating new functions, until we apply all the arguments.
The example also helps us make sense of the declaration of curry in Function3
Finally, since functions are instances of traits, you can use the traits as parents of other types.
Type Projections Type projections are a way to refer to a type declaration nested in another type:
You can also reference the abstract type T#t, but you can’t use it in a declaration because it is abstract.
Singleton Types If you have a value v of a subtype of AnyRef, including null, you can get its singleton type using the expression v.type.
In these cases an object may have a path-dependent type that appears to be incompatible with another path-dependent type, when in fact they are compatible.
Using the v.type expression retrieves the singleton type, a “unique” type that eliminates the path dependency.
This example uses the singleton type for one value in a declaration of another:
Self-Type Annotations You can use this in a method to refer to the enclosing type, which is useful for referencing a member of the type.
Using this is not usually necessary for this purpose, but it’s useful occasionally for disambiguating a reference when several values are in scope with the same name.
By default, the type of this is the same as the enclosing type, but this is not really essential.
Self-type annotations let you specify additional type expectations for this, and they can be used to create aliases for this.
We could use self within any method inside the body of C1 or its nested types.
Note that the name self is arbitrary, but it is somewhat conventional.
In fact, you could say this =>, but it would be completely redundant.
If the self-type annotation has types in the annotation, we get some very different benefits:
This script shows a schematic layout for an App (application) infrastructure supporting several tiers/components, persistent storage, midtier, and UI.
For now, we just care about the role of self types.
Each abstract trait declares a “start” method that does the work of initializing the tier.
We’re ignoring issues like success versus failure of startup, etc.
Each abstract tier is implemented by a corresponding concrete trait (not a class, so we can use them as mixins)
We have traits for database persistence, some sort of computation cluster to do the heavy lifting for the business logic, and a web-based UI.
For example, it does the work of starting the tiers in the run method.
Note the self-type annotation, self: Persistence with Midtier with UI =>
The body of the trait can assume it is an instance of Persistence, Midtier, and UI, so it can call methods defined in those types, whether or not they are actually defined at this point.
The concrete type that mixes in this trait must also mix in these three other traits or descendants of them.
In other words, the self type in App specifies dependencies on other components.
These dependencies are satisfied in MyApp, which mixes in the concrete traits for the three tiers.
As we said, the self-type annotation lets the App assume it is of type Persistence, etc.
That’s exactly what happens when you mix in a trait, too.
Why, then, are self types useful if they appear to be equivalent to inheritance? There are some theoretical reasons and a few special cases where self-type annotations offer unique benefits.
In practice, you could use inheritance for almost all cases.
By convention, people use inheritance when they want to imply that a type behaves as (inherits from) another type, and they use self-type annotations when they want to express a dependency between a type and other types (see [McIver2009])
In our case, we don’t really think of an App as being a UI, database, etc.
We think of an App as being composed of those things.
Note that in most object-oriented languages, you would express this compositional dependency with member fields, especially if your language doesn’t support mixin composition, like Java.
For example, you might write App in Java this way:
We nested the component interfaces inside JavaApp to avoid creating separate files for each one.
However, the selftype approach turns programmatic dependency resolution, i.e., passing dependencies to constructors or setter methods at runtime, into declarative dependency resolution at compile time, which catches errors earlier.
Structural Types You can think of structural types as a type-safe approach to duck typing, the popular name for the way method resolution works in dynamically typed languages.
That method, if found, might have been defined in the class used to instantiate starFighter or one of its parents or “included” modules.
The method might also have been added to the object using the metaprogramming facility of Ruby.
Finally, the object might override the catch-all method_missing method and do something reasonable when the object receives the shootWeapons “message.” Scala doesn’t support this kind of method resolution, Instead, Scala allows you to specify that an object must adhere to a certain structure: that it contains certain types, fields, or methods, without concern for the actual type of the object.
Here is the example we saw then, a variation of the Observer Pattern:
On the other hand, if the name is a universal convention in some sense, then coupling to it has more merit.
For example, foreach is very common name in the Scala library with a particular meaning, so defining a structural type based on foreach might be better for conveying intent to the user, rather than using an anonymous function of some kind.
Existential Types Existential types are a way of abstracting over types.
They let you “acknowledge” that there is a type involved without specifying exactly what it is, usually because you don’t know what it is and you don’t need that knowledge in the current context.
Existential types are particularly useful for interfacing to Java’s type system for three cases:
The type parameters of generics are “erased” at the byte code level (called type erasure)
For example, when a List[Int] is created, the Int type is not available in the byte code.
You might encounter “raw” types, such as pre-Java 5 libraries where collections had no type parameters.
When Java uses wildcards in generics to express variance behavior when the generics are used, the actual type is unknown.
If you compile this with the -unchecked flag on the JVM, you’ll get warnings that the type parameters like Int are unchecked, because of type erasure.
Hence, we can’t distinguish between any of the list types shown.
The Manifests that we discussed previously won’t work either, because they can’t recover the erased type of B.
We’ve already learned that the best we can do in pattern matching is to focus on the fact that we have a list and not try to determine the “lost” type parameter for the list instance.
For type safety, we have to specify that a list has a parameter, but since we don’t know what it is, we use the wildcard _ character for the type parameter, e.g.:
Although we said that variance behavior in Scala is defined at the declaration site, you can use existential type expressions in Scala to define call-site variance behavior.
It is not recommended, for the reasons discussed previously, but you have that option.
You won’t see the forSome existential type syntax very often in Scala code, because existential types exist primarily to support Java generics while preserving correctness in Scala’s type system.
Type inference hides the details from us in most contexts.
When working with Scala types, the other type constructs we have discussed in this chapter are preferred to existential types.
In functional languages that are lazy by default, like Haskell, laziness makes it easy to support infinite data structures.
For example, consider the following Scala method fib that calculates the Fibonacci number for n in the infinite Fibonacci sequence:
If Scala were purely lazy, we could imagine a definition of the Fibonacci sequence like the following and it wouldn’t create an infinite loop:
Scala isn’t lazy by default (and there is no infinity value or keyword…), but the library contains a Stream class that supports lazy evaluation and hence it can support infinite data structures.
We’ll show an implementation of the Fibonacci sequence in a moment.
First, here is a simpler example that uses streams to represent all positive integers, all positive odd integers, and all positive even integers:
The from method is recursive and never terminates! We use it to define the ints by calling from(0)
Streams.cons is an object with an apply method that is analogous to the :: (“cons”) method on List.
It returns a new stream with the first argument as the head and the second argument, another stream, as the tail.
The odds and evens infinite streams are computed by filtering ints.
Once we have defined the streams, the take method returns a new stream of the fixed size specified, 10 in this case.
When we print this stream with the print method, it prints the 10 elements followed by Stream.empty when it hits the end of the stream.
Returning to the Fibonacci sequence, there is a famous definition using infinite, lazy sequences that exploits the zip operation (see, e.g., [Abelson1996])
To get back to a single integer for each position in the stream, we map the stream of tuples to a stream of Ints by adding the tuple elements.
Note that each second element is the next number in the Fibonacci sequence after the first element in the tuple.
Another lazy Scala type, albeit a finite one, is Range.
Range is lazy, so very large ranges don’t consume too many resources.
However, this feature can lead to subtle problems unless you are careful, as documented by [Smith2009b] and commenters.
Using the example described there, consider this function for returning a Seq of three random integers:
Calling first on the sequence does not always return the same value! The reason is that the range at the beginning of the for comprehension effectively forces the whole sequence to be lazy.
Hence, it is reevaluated with each call to first, and the first value in the sequence actually changes, since Random returns a different number each time (at least, it will if there is a sufficient time delta between calls)
However, calling toList on the sequence forces it to evaluate the whole range and create a strict list.
Finally, Scala version 2.8 will include a force method on all collections that will force them to be strict.
Recap and What’s Next It’s important to remember that you don’t have to master the intricacies of Scala’s rich type system to use Scala effectively.
As you use Scala more and more, mastering the type system will help you create powerful, sophisticated libraries that accelerate your productivity.
The effort is worthwhile if you want a deep understanding of the type system.
There are also a multitude of papers on Scala’s type system.
You can find links to many of them on the official http://scala-lang.org website.
The next two chapters cover the pragmatics of application design and Scala’s development tools and libraries.
In this chapter, we take a pragmatic look at developing applications in Scala.
We discuss a few language and API features that we haven’t covered before, examine common design patterns and idioms, and revisit traits with an eye toward structuring our code effectively.
Annotations Like Java and .NET, Scala supports annotations for adding metadata to declarations.
Annotations are used by a variety of tools in typical enterprise and Internet applications.
For example, there are annotations that provide directives to the compiler, and some Object-Relational Mapping (ORM) frameworks use annotations on types and type members to indicate persistence mapping information.
While some uses for annotations in the Java and .NET worlds can be accomplished through other means in Scala, annotations can be essential for interoperating with Java and .NET libraries that rely heavily on them.
Fortunately, Java and .NET annotations can be used in Scala code.
The interpretation of Scala annotations depends on the runtime environment.
In this section, we will focus on the JDK environment.
In Java, annotations are declared using special conventions, e.g., declaring annotations with the @interface keyword instead of the class or interface keyword.
The @Pre annotation is used to specify “preconditions” that must be satisfied when entering a method or constructor, or before using a parameter passed to a method or constructor.
The conditions are specified as a string that is actually a snippet of source code that evaluates to true or false.
The source languages supported for these snippets are scripting languages like Groovy and JRuby.
The name of the variable for this string, value, is a conventional name for the most important field in the annotation.
The other field is an optional message to use when reporting failures.
Here is a Scala example that uses @Pre and shows several ways to specify the value and message parameters:
In the Person class, the @Pre annotation on name has a simple string argument: the “precondition” that users must satisfy when passing in a name.
This value can’t be null, and it can’t be of zero length.
As in Java, if a single argument is given to the annotation, it is assigned to the value field.
A similar @Pre annotation is used for the third argument, the ssn (Social Security number)
In both cases, the message defaults to the empty string specified in the definition of Pre.
The @Pre annotation for the age shows one way to specify values for more than one field.
The syntax for each field looks like a val declaration, without any type information, since the types can always be inferred! This syntax allows you to use the shorthand syntax for the value and still specify values for other fields.
If Person were a Java class, this annotation expression would look identical, except there would be no val keywords and parentheses would be used.
The @Pre annotation on the constructor parameter for the SSN class shows the alternative syntax for specifying values for more than one field.
The value field is specified as before with a one-element parameter list.
The message is initialized in a follow-on block in curly braces.
Testing this code would require the Contract4J library, build setup, etc.
This approach eliminates a “special case” in the language, but it also means that some of the features provided by Java annotations aren’t supported, as we will see.
Here is an example annotation from the Scala library, SerialVersionUID (again with the comments removed for clarity):
The @SerialVersionUID annotation is applied to a class to define a globally unique ID as a Long.
When the annotation is used, the ID is specified as a constructor argument.
This annotation serves the same purpose as a static field named serialVersionUID in a Java class.
This is one example of a Scala annotation that maps to a “non-annotation” construct in Java.
Did you notice that there is no val on uid? Why isn’t uid a field? The reason is that the annotation’s data is not intended for use by the program.
Recall that it is metadata designed for external tools to use, such as scalac.
This also means that Scala annotations have no way to define default values in version 2.7.X, as implicit arguments don’t work.
However, the new default arguments feature in version 2.8.0 may work.
It is not yet implemented at the time of this writing.
Like Java (and .NET) annotations, a Scala annotation clause applies to the definition it precedes.
You can have as many annotation clauses as you want, and the order in which they appear is not significant.
All the constructor parameters must be constant expressions, including strings, class literals, Java enumerations, numerical expressions and one-dimensional arrays of the same.
However, the compiler also allows annotation clauses with other arguments, such as boolean values and maps, as shown in this example:
Curiously, if you attempt to use the standard Map literal syntax that is shown in the comments, you get a compilation error that the -> method doesn’t exist for String.
The implicit conversion to ArrowAssoc that we discussed in “The Predef Object” on page 145 isn’t invoked.
Instead, you have to use a list of Tuples, which Map.apply actually expects.
It is supposed to be used for annotations that should have runtime retention, i.e., the annotations should be visible in the class file so they are available at runtime.
However, actually using it with the JDK version of Scala results in compiler errors like the following:
If that is what you want, you must write the annotation class in Java.
Hence, if you want runtime visibility, you have to implement the annotation in Java.
This works fine, since you can use any Java annotation in Scala code.
For Scala version 2.7.X, another important limitation to keep in mind is that annotations can’t be nested.
This causes problems when using JPA annotations in Scala code, for example, as discussed in [JPAScala]
We start with the direct children of Annotation, followed by the children of StaticAnnotation.
The parent trait for annotations that should be retained in the class file for runtime access, but it doesn’t actually work on the JDK!
BeanDescription BeanDescriptor (class) An annotation for JavaBean types or members that associates a short description (provided as the annotation argument) that will be included when generating bean information.
BeanDisplayName BeanDescriptor (class) An annotation for JavaBean types or members that associates a name (provided as the annotation argument) that will be included when generating bean information.
BeanInfo BeanInfo (class) A marker that indicates that a BeanInfo class should be generated for the marked Scala class.
A marker that indicates that bean information should not be generated for the annotated member.
The parent trait of annotations that should be visible across compilation units and define “static” metadata.
An annotation trait that can be applied to other annotations that define constraints on a type, relying only on information defined within the type itself, as opposed to external context information where the type is defined or used.
The compiler can exploit this restriction to rewrite the constraint.
There are currently no library annotations that use this trait.
BeanProperty JavaBean convention A marker for a field (including a constructor argument with the val or var keyword) that tells the compiler to generate a JavaBean-style “getter” and “setter” method.
A class marker indicating that a class can be cloned.
The compiler will issue a warning when the item is used.
A method marker telling the compiler that it should try “especially hard” to inline the method.
The method body will not be generated by the compiler, but usage of the method will be type checked.
A method marker that prevents the compiler from inlining the method, even when it appears to be safe to do so.
A class marker indicating that the class can be invoked from a remote JVM.
A class marker indicating that the class can be serialized.
The annotation’s constructor takes a Long argument for the UID.
It tells the compiler to generate optimized versions of the type or method for the AnyVal types corresponding to platform primitive types.
Optionally, you can limit the AnyVal types for which specialized implementations will be generated.
If it is present, the compiler will issue an error if the method cannot be optimized into a loop.
This happens, for example, when the method is not private or final, when it could be overridden, and when recursive invocations are not true tail calls.
A marker for a value that is assumed to be stable even though its type is volatile (i.e., annotated with @volatile)
A marker for a type argument that is volatile, when it is used in a parameterized type, to suppress variance checking.
A marker for an individual field or a whole type, which affects all fields, indicating that the field may be modified by a separate thread.
Note that fib, which calculates Fibonacci numbers, is recursive, but it isn’t tail-call recursive, because the call to itself is not the very last thing that happens in the second case clause.
Hence, a tail-call optimization can’t be performed on this method.
This time we annotate the i in the match statement.
This annotation causes the compiler to raise an error if it can’t generate a switch construct in byte code from the cases in the match statement.
Normally, you would want to add a case for None.
However, if you want to suppress the warning message in situations like this, change the method as follows:
With the @unchecked annotation applied to x as shown, the warning will be suppressed.
However, if x is ever None, then a MatchError will be thrown.
It is a pragmatic solution to a tradeoff between space efficiency and performance.
In Java and Scala, the implementation of a parameterized type or method is generated at the point of the declaration (as we discussed in “Understanding Parameterized Types” on page 249)
In contrast, in C++, a template is used to generate an implementation for the actual type parameters where the template is used.
The C++ approach has the advantage of allowing optimized implementations to be generated for primitive types, while it has the disadvantage of resulting in code bloat from all the instantiations of templates.
In JVM-related languages, the “on-demand” generation of implementations isn’t suitable, primarily because there is no “link” step as in compiled languages, where every required instantiation of a template can be determined.
By default, a Scala parameterized type or method will be translated to a single implementation assuming Any for the type parameters (in part due to type erasure at the byte code level)
However, if a particular use of the type or method uses one of the AnyVal types, say Int, then we get inefficient boxing and unboxing operations in the implementation.
The alternative would be to generate a separate implementation for every AnyVal corresponding to a primitive type, but this would lead to code bloat, especially since it would be rare that an application would use all those implementations.
It lets the user tell the compiler that runtime efficiency is more important than space efficiency, so the compiler will generate the separate implementations for each primitive corresponding to an AnyVal.
Here is an example of how the annotation is used:
At the time of this writing, the implementation in the version 2.8 “nightly” build only supports generation of specialized implementations for Int and Double.
For the final version 2.8 library, it is planned that the other AnyVal types will be supported.
There are also plans to allow the user to specify the types for which optimized implementations are generated so that unused implementations for the other AnyVals are avoided.
See the final 2.8 Scaladocs for details on the final feature set.
It will be a directive interpreted by a compiler plugin that will trigger generation of continuation-based byte code for method invocation, rather than the default stack frame byte code.
The annotation will have no effect unless the corresponding scalac.
Consult the release documentation for more information on this feature, when it becomes available.
To understand the @throws annotation, it’s important to remember that Scala does not have checked exceptions, in contrast with Java.
There is also no throws clause available for Scala method declarations.
This is not a problem if a Scala method calls a Java method that is declared to throw a checked exception.
However, suppose the Scala method in question doesn’t catch the exception, but lets it pass through.
The following Scala class prints out the contents of a java.io.File:
The Java IO API methods used by print and the private method loop might throw this exception.
By the way, notice that loop uses functional-style tail recursion, rather than a loop.
Now, returning to the FilePrinter class, suppose you comment out the @throws line.
So, the purpose of @throws is to insert the information on thrown checked exceptions into the byte code that javac will read.
In a mixed Java-Scala environment, consider adding the @throws annotation for all your Scala methods that can throw Java checked exceptions.
Eventually, some Java code will probably call one of those methods.
Enumerations Versus Pattern Matching Enumerations are a way of defining a finite set of constant values.
You can reference the values directly, iterate through them, index into them with integer indices, etc.
Just as for annotations, Scala’s form of enumerations are class-based, with a particular set of idioms, rather than relying on special keywords for defining them, as is used for enumerations in Java and .NET.
However, you can also use enumerations defined in those languages.
There are several ways to construct and use an enumeration.
We’ll demonstrate one idiom that most closely matches the Java and .NET forms you may already know.
We defined the set of HTTP 1.1 methods using a sealed case class hierarchy:
In that example, each method had a body attribute for the message body.
We’ll assume here that the body is handled through other means and we only care about identifying the kind of HTTP method.
So, here is a Scala Enumeration class for the HTTP 1.1 methods:
When pattern matching on enumeration values, the compiler can’t tell if the match is “exhaustive.”
You might wonder why we hardcoded strings like “Connect” in the println statements in the case clauses.
Unfortunately, we can’t get those names from the values in the Scala enumeration, at least given the way that we declared HttpMethod.
However, there are two ways we can change the implementation to get name strings.
In the first approach, we pass the name to Value when creating the fields:
It is a bit redundant to have to use the same word twice in declarations like val Connect = Value("Connect")
Note that we have a redundant list of name strings and names of the vals.
It is up to you to keep the items in the list and their order consistent with the declared values! This version has fewer characters, but it is more error-prone.
Internally, Enumeration pairs the strings with the corresponding Value instances as they are created.
The output when printing the whole HttpMethod object is better for either alternative implementation.
When the values have names, their toString returns the name.
In fact, our final two examples have become quite artificial because we now have identical statements for each case clause! Of course, in a real implementation, you would handle the different HTTP methods differently.
Thoughts On Annotations and Enumerations For both annotations and enumerations, there are advantages and disadvantages to the Scala approach, where we use regular class-based mechanisms, rather than inventing custom keywords and syntax.
Classes and traits are used in more or less the same ways they are used for “normal” code.
The disadvantages include the need to understand and use ad hoc conventions that are not always as convenient to use as the custom syntax mechanisms required in Java and .NET.
So, should the Scala community relent and implement ad hoc, but more full-featured mechanisms for annotations and enumerations? Maybe not.
Many of the features provided by Java and .NET annotations and enumerations can be implemented in Scala by other means.
For enumerations, sealed case classes and pattern matching provide a more flexible solution, in many cases.
Enumerations Versus Case Classes and Pattern Matching Let’s revisit the HTTP method script, which uses a sealed case class hierarchy versus the version we wrote previously that uses an Enumeration.
Since the enumeration version doesn’t handle the message body, let’s write a modified version of the sealed case class version that is closer to the enumeration version, i.e., it also doesn’t hold the message body and it has name and id methods:
Note that we used case object for all the concrete subclasses, to have a true set of constants.
To mimic the enumeration id, we added a field explicitly, but now it’s up to us to pass in valid, unique values! The handle methods in the two implementations are nearly identical.
The object names are ugly, but we could parse the string and remove the substring we really care about.
Both approaches support the concept of a finite and fixed set of values, as long as the case class hierarchy is sealed.
An additional advantage of a sealed case class hierarchy is the fact that the compiler will warn you if pattern matching statements aren’t.
Try removing one of the case clauses and you’ll get the usual warning.
The compiler can’t do this with enumerations, as we saw.
The enumeration format is more succinct, despite the name duplication we had to use, and it also supports the ability to iterate through the values.
We had to do that manually in the case clause implementation.
The case class implementation naturally accommodates other fields, e.g., the body, as in the original implementation, while enumerations can only accommodate constant Values with associated names and IDs.
For cases where you need only a simple list of constants by name or ID number, use enumerations.
For fixed sets of more complex, constant objects, use sealed case objects.
Scala has to support null, because null is supported on both the JVM and .NET and other libraries use null.
What if null were not available? How would that change your designs? The Map API offers some useful examples.
A map may not have a value for a particular key.
Both of these methods avoid returning null in that case.
Concrete implementations of get in subclasses return a None if no value exists for the key.
The method signature tells you that a value might not exist, and it forces you to handle that situation gracefully:
You have to specify a default value for when a key isn’t in the map.
Note that the default value can actually be an instance of a supertype relative to the map’s value type:
A lot of Java and .NET APIs allow null method arguments and can return null values.
You can write Scala wrappers around them to implement an appropriate strategy for handling nulls.
We’ll refactor our FilePrinter class and the Java driver into a combined script.
ScalaIOException is a subclass of RuntimeException, so it is unchecked.
The script ends with a loop over the input arguments, which are stored automatically in the args variable.
Each argument is treated as a file name to be printed.
Consider this first version of a script that uses Options in a for comprehension:
Imagine this code is used in some sort of social networking site.
New users submit profile data, which is passed to this service in bulk for processing.
For example, we hardcoded a list of submitted profiles, where each profile data set is a map.
The map might have been copied from an HTTP session.
The service filters out incomplete profiles (missing fields), shown with the #1 comments, and creates new user objects from the complete profiles.
Running the script prints out three new users from the five submitted profiles:
Before you rerun the script, what do you expect to happen? Will it print five lines with some fields empty (or containing other kinds of values)? It prints the same thing! How did it do the filtering we wanted without the explicit conditional? The answer lies in the way that for comprehensions are implemented.
Here are a couple of simple for comprehensions followed by their translations (see [ScalaSpec2009])
First, we’ll look at a single generator with a yield:
With more than one generator, map is replaced with flatMap in the yield expressions, but foreach is unchanged:
Note that the second through the Nth generators become nested for comprehensions that need translating.
There are similar translations for conditional statements (which become calls to filter) and val assignments.
We won’t show them here, since our primary purpose is to describe just enough of the implementation details so you can understand how Options and for comprehensions work together.
If you follow this translation process on our example, you get the following expansion:
Note that we have flatMap calls until the most nested case, where map is used (flatMap and map behave equivalently in this case)
Now we can understand why the big conditional was unnecessary.
The key is the behavior of flatMap defined on Option, which lets us treat Options like other collections.
Hence, the nested iterations simply stop and never get to the line marked with the comment #1, where the User is created.
On a multi-element collection like this, the behavior of flatMap is similar to map, but it flattens the new.
Using Options with for comprehensions eliminate the need for most “null/empty” checks.
Exceptions and the Alternatives If nulls are the “billion dollar mistake” as we discussed in “Option, Some, and None: Avoiding nulls” on page 41, then what about exceptions? You can argue that nulls should never occur and you can design a language and libraries that never use them.
However, exceptions have a legitimate place because they separate the concerns of normal program flow from “exceptional” program flow.
For example, if a user mistypes his username, is that normal or exceptional? Another question is where should the exception be caught and handled? Java’s checked exceptions were designed to document for the API user what exceptions might be thrown by a method.
The flaw was that it encouraged handling of the exception in ways that are often suboptimal.
If one method calls another method that might throw a checked exception, the calling method is forced to either handle the exception or declare that it also throws the exception.
More often than not, the calling method is the wrong place to handle the exception.
It is too common for methods to simply “eat” an exception that should really be passed up the stack and handled in a more appropriate context.
Otherwise, throws declarations are required up the stack of method calls.
This is not only tedious, but it pollutes the intermediate contexts with exception names that often have no connection to the local context.
Any exception can propagate to the point where it is most appropriate to handle it.
However, design discipline is required to implement handlers in the appropriate places for all exceptions for which recovery is possible! Every now and then, an argument erupts among developers in a particular language community about whether or not it’s OK to use exceptions as a control-flow mechanism for normal processing.
Sometimes this use of exceptions is seen as a useful longjump or non-local goto mechanism for exiting out of a deeply nested scope.
One reason this debate pops up is that this use of exceptions is sometimes more efficient than a more “conventional” implementation.
For example, you might implement Iterable.foreach to blindly traverse a collection and stop when it catches whatever exception indicates it went past the end of the collection.
When it comes to application design, communicating intent is very important.
Using exceptions as a goto mechanism breaks the principle of least surprise.
It will be rare that the performance gain will justify the loss of clarity, so we encourage you to use exceptions only for truly “exceptional” conditions.
In Ruby the keywords throw and catch are actually reserved for this purpose, while raise and rescue are the keywords for raising an exception and handling it.
Whatever your view on the proper use of exceptions, when you design APIs, minimizing the possibility of raising an exception will benefit your users.
This is the flip side of an exception handling strategy, preventing them in the first place.
Returning null in this case isn’t an option, because the sequence could have null elements! In contrast, the firstOption method returns an Option, so it returns None if the sequence is empty, which is unambiguous.
You could argue that the Seq API would be more robust if it only had a “first” method that returned an Option.
It’s useful to ask yourself, “How can I prevent the user from ever failing?” When “failure” can’t be prevented, use Option or a similar construct to document for the user that a failure mode is possible.
Thinking in terms of valid state transformations, the first method, while convenient, isn’t really valid for a sequence in an empty state.
Should the “first” methods not exist for this reason? This choice is probably too draconian, but by returning Option from firstOption, the API communicates to the user that there are circumstances when the method can’t satisfy the request and it’s up to the user to recover gracefully.
In this sense, firstOption treats an empty sequence as a non-exceptional situation.
We discussed two methods on Option for retrieving the value an instance wraps (when the instance is actually a Some)
The get method throws an exception if there is no value, i.e., the Option instance is actually None.
The other method, getOrElse, takes a second argument, a default value to return if the Option is actually None.
Part of the original intent of checked versus unchecked exceptions was to distinguish between potentially recoverable problems and catastrophic failures, like out-of-memory errors.
However, the alternative methods in Seq and Option show a way to “encourage” the user of an API to consider the consequences of a possible failure, like asking for the first element in an empty sequence.
The user can specify the contingency in the event that a failure occurs.
Minimizing the possibility of exceptions will improve the robustness of your Scala libraries and the applications that use them.
Scalable Abstractions It has been a goal for some time in our industry to create reusable components.
Unfortunately, there is little agreement on the meaning of the term component, nor on a related term, module (which some people consider synonymous with component)
Proposed definitions usually start with assumptions about the platform, granularity, deployment and configuration scenarios, versioning issues, etc.
We’ll avoid that discussion and use the term component informally to refer to a grouping of types and packages that exposes coherent abstractions (preferably just one) for the services it offers, that has minimal coupling to other components, and that is internally cohesive.
All languages offer mechanisms for defining components, at least to some degree.
However, objects alone aren’t enough, because we quickly find that objects naturally cluster together into more coarse-grained aggregates, especially as our applications grow.
Generally speaking, an object isn’t necessarily a component, and a component may contain many objects.
Ruby modules serve a similar purpose, as do C# and C++ namespaces.
A common problem is that they don’t clearly define what is publicly visible outside the component boundary and what is internal to the component.
For example, in Java, any public type or public method on a public type is visible outside the package boundary to every other component.
You can make types and methods “package private,” but then they are invisible to other packages encapsulated in the component.
Scala provides a number of mechanisms that improve this situation.
Fine-Grained Visibility Rules We saw in “Visibility Rules” on page 96 that Scala provides more fine-grained visibility rules than most other languages.
You can control the visibility of types and methods outside type and package boundaries.
Consider the following example of a component in package encodedstring:
This example encapsulates handling of strings encoding comma-separated values (CSVs) or tab-separated values (TSVs)
The encodedstring package exposes a trait EncodedString that is visible to clients.
The trait defines two abstract val fields: one to hold the encoded string, which is protected.
Recall from Chapter 6 that abstract fields, like abstract types and methods, must be initialized in concrete instances.
In this case, string will be defined through a concrete constructor, and the separator is defined explicitly in the concrete classes, CSV and TSV.
The toString method on EncodedString prints the string as a “normal” string.
By hiding the string value and the concrete classes, we have complete freedom in how the string is actually stored.
For example, for extremely large strings, we might want to split them on the delimiter and store the tokens in a data structure.
This could save space if the strings are large enough and we can share tokens between strings.
Also, we might find this storage useful for various searching, sorting, and other manipulation tasks.
The package also exposes an object with an Enumeration for the known separators, an apply factory method to construct new encoded strings, and an unapply extractor method to decompose the encoded string into its enclosed string and the delimiter.
In this case, the unapply method looks trivial, but if we stored the strings in a different way, this method could transparently reconstitute the original string.
So, clients of this component only know about the EncodedString abstraction and the enumeration representing the supported types of encoded strings.
All the actual implementation types and details are private to the encodedstring package.
We put them in the same file for convenience, but normally you would kept them separate.
Hence, the boundary is clear between the exposed abstractions and the internal implementation details.
It produces the following output: EncodedString: Scala,is,great! token: Scala token: is token: great! EncodedString: Scala    is      great! token: Scala token: is token: great!
In this simple example, it wasn’t essential to make the concrete types private to the component.
However, we have a very minimal interface to clients of the component, and we are free to modify the implementation as we see fit with little risk of forcing client code modifications.
A common cause of maintenance paralysis in mature applications is the presence of too many dependencies between concrete types, which become difficult to modify since they force changes to client code.
So, for larger, more sophisticated components, this clear separation of abstraction from implementation can keep the code maintainable and reusable for a long time.
Mixin Composition We saw in Chapter 4 how traits promote mixin composition.
A class can focus on its primary domain, and other responsibilities can be implemented separately in traits.
When instances are constructed, classes and traits can be combined to compose the full range of required behaviors.
For example, in “Overriding Abstract Types” on page 120, we discussed our second version of the Observer Pattern:
We used this version to observe button “clicks” in a UI.
Let’s revisit this implementation and resolve a few limitations, using our next tool for scalable abstractions, self-type annotations combined with abstract type members.
To be clear, S and O are “placeholders” at this point, while Subject and Observer are abstract traits defined in SubjectObserver.
By the way, declaring SubjectObserver as an abstract class versus a trait is somewhat arbitrary.
We need SubjectObserver primarily so we have a type to “hold” our abstract type members S and O.
However, if you attempt to compile this code as currently written, you get the following error:
In the nested Observer trait, receiveUpdate is expecting an instance of type S, but we are passing it this, which is of type Subject.
In other words, we are passing an instance of a parent type of the type expected.
One solution would be to change the signature to just expect the parent type, Subject.
We just mentioned that our concrete observers need the more specific type, the actual concrete type we’ll eventually define for S, so they can call methods on it.
For example, when observing UI CheckBoxes, the observers will want to read whether or not a box is checked.
We don’t want to force the observers to use unsafe casts.
Let’s use this feature now to solve our current compilation problem.
Here is the same code again with a self-type annotation:
We can now use self as an alias for this, but whenever it appears, the type will be assumed to be S, not Subject.
It is as if we’re telling Subject to impersonate another type, but in a type-safe way, as we’ll see.
Actually, we could have used this instead of self in the annotation, but self is somewhat conventional.
A different name also reminds us that we’re working with a different type.
Are self-type annotations a safe thing to use? When an actual concrete SubjectObserver is defined, S and O will be specified and type checking will be performed to ensure that the concrete S and O are compatible with Subject and Observer.
In this case, because we also defined S to be a subtype of Subject and O to be a subtype of Observer, any concrete types derived from Subject and Observer, respectively, will work.
Comment #2 shows that we pass self instead of this to receiveUpdate.
Now that we have a generic implementation of the pattern, let’s specialize it for observing button clicks:
We use an object now so that we can refer to the nested types easily, as we’ll see shortly.
However, ButtonObserver is still an abstract trait, because receiveUpdate is not defined.
Notice that the argument to receiveUpdate is now an ObservableButton, the value assigned to S.
The final piece of the puzzle is to define a concrete observer.
However, to emphasize the value of having the specific type of instance passed to the observer, a Button in this case, we’ll enhance the observer to track clicks for multiple buttons using a hash map with the button labels as the keys.
Note that it is now impossible to call receiveUpdate with a normal Button.
This restriction eliminates bugs where we don’t get the notifications we expected.
We also have access to any “enhanced” features that ObservableButton may have.
We create three buttons and one observer for all of them.
Finally, we confirm that the clicks were properly counted for each button.
We see again how abstract types combined with self-type annotations provide a reusable abstraction that is easy to extend in a type-safe way for particular needs.
Even though we defined a general protocol for observing an “event” after it happened, we were able to define subtypes specific to Buttons without resorting to unsafe casts from Subject abstractions.
The Scala compiler itself is implemented using these mechanisms (see [Odersky2005]) to make it modular in useful ways.
For example, it is relatively straightforward to implement compiler plugins.
Effective Design of Traits One of the reasons that many languages (like Java) do not implement multiple inheritance is because of the problems observed with multiple inheritance in C++
One of those problems is the so-called diamond of death, which is illustrated in Figure 13-1
Here is a recap of these types, collected together into a single script that also exercises the code:
Note that maxAllowed is a constant, but it can be overridden when instantiating each instance.
So, two instances could differ only by the value of maxAllowed.
In general, a single trait can either be stateless, i.e., it contributes no new dimensions of state to the instance, or it can contribute orthogonal state dimensions to the instance, i.e., dimensions that are independent of the state contributions from other traits and the parent class.
For example, the Observer Pattern traits we used in Chapter 4 contained methods for managing their lists of observers.
Independent of whether a trait contributes state dimensions, a trait can also modify the possible values for a dimension contributed by a different trait or the parent class.
To see an example, let’s refactor the script to move the click count to the Clickable trait:
Now Clickable contributes one state dimension for count (which is now a method that returns the value of the private clicks field)
We might say that VetoableClicks is invasive, because it changes the behavior of other mixins.
For example, if you have a test suite that Button passes, will a Button with VetoableClicks instance pass the same.
There are different “specifications” for these two kinds of buttons.
This difference is expressed by the Liskov Substitution Principle (see [Martin2003])
An instance of a Button with VetoableClicks won’t be substitutable in every situation where a regular Button instance is used.
This is a consequence of the invasive nature of VetoableClicks.
When a trait adds only orthogonal state and behavior, without affecting the rest of the state and behavior of the instance, it makes reuse and composition much easier, as well as reducing the potential for bugs.
The Observer Pattern implementations we have seen are quite reusable.
The only requirement for reuse is to provide some “glue” to adapt the generic subject and observer traits to particular circumstances.
This does not mean that invasive mixins are bad, just that they should be used wisely.
Indeed, some of the Gang of Four patterns (see [GOF1995]) are not really needed in Scala, as native features provide better substitutes.
Other patterns are part of the language itself, so no special coding is needed.
Of course, patterns are frequently misused, but that’s not the fault of the patterns themselves.
We think the criticisms often overlook an important point: the distinction between an idea and how it is implemented and used in a particular situation.
These ideas are part of the vocabulary that software developers use to describe their designs.
Some common patterns are native language features in Scala, like singleton objects that eliminate the need for a Singleton Pattern ([GOF1995]) implementation like you often use in Java code.
The Iterator Pattern ([GOF1995]) is so pervasive in programming that most languages include iteration mechanisms for any type that can be treated like a collection.
For example, in Scala you can iterate through the characters in a String with foreach:
We’ll discuss a better alternative to the Visitor Pattern ([GOF1995]) in a moment.
Finally, still other patterns can be implemented in Scala and remain very useful.
For example, the Observer Pattern that we discussed earlier in this chapter and in Chapter 4 is a very useful pattern for many design problems.
We won’t discuss all the well known patterns, such as those in [GOF1995]
We’ll start by discussing a replacement for the Visitor Pattern that uses functional idioms and implicit conversions.
Then we’ll discuss a powerful way of implementing dependency injection in Scala using the Cake Pattern.
The Visitor Pattern: A Better Alternative The Visitor Pattern solves the problem of adding a new operation to a class hierarchy without editing the source code for the classes in the hierarchy.
For a number of practical reasons, it may not be feasible or desirable to edit the hierarchy to support the new operation.
Let’s look at an example of the pattern using the Shape class hierarchy we have used previously.
We’ll start with the case class version from “Case Classes” on page 136:
Suppose we don’t want the draw method in the classes.
This is a reasonable design choice, since the drawing method will be highly dependent on the particular context of use, such as details of the graphics libraries on the platforms the application will run on.
For greater reusability, we would like drawing to be an operation we decouple from the shapes themselves.
First, we refactor the Shape hierarchy to support the Visitor Pattern, following the example in [GOF1995]:
We define a ShapeVisitor trait, which has one method for each concrete class in the hierarchy, e.g., visit(circle: Circle)
Each such method takes one parameter of the corresponding type to visit.
Concrete derived classes will implement each method to do the appropriate operation for the particular type passed in.
The Visitor Pattern requires a one-time modification to the class hierarchy.
An overridden method named accept must be added, which takes a Visitor parameter.
It calls the corresponding method defined on the visitor instance, passing this as the argument.
It won’t help us prevent some bugs in the Visitor Pattern implementation, but it will prove useful shortly.
Here is a concrete visitor that supports our original draw operation:
For each visit method, it “draws” the Shape instance appropriately.
Visitor has been criticized for being somewhat inelegant and for breaking the OpenClosed Principle (OCP; see [Martin2003]), because if the hierarchy changes, you are forced to edit (and test and redeploy) all the visitors for that hierarchy.
Note that every ShapeVisitor trait has methods that hardcode information about every Shape derived type.
In languages with “open types,” like Ruby, an alternative to the Visitor Pattern is to create a new source file that reopens all the types in the hierarchy and inserts an appropriate method implementation in each one.
Scala does not support open types, of course, but it offers a few alternatives.
The first approach we’ll discuss combines pattern matching with implicit conversions.
Let’s begin by refactoring the ShapeVisitor code to remove the Visitor Pattern logic:
If we would like to invoke draw as a method on any Shape, then we will have to use an implicit conversion to a wrapper class with the draw method:
When draw is called, the shape is pattern matched based on its type to determine the appropriate way to draw it.
A companion object declares an implicit conversion that wraps a Shape in a ShapeDrawer.
It produces the same output as the example using the Visitor Pattern.
This implementation of ShapeDrawer has some similarities with the Visitor Pattern, but it is more concise, elegant, and requires no code modifications to the original Shape hierarchy.
Technically, the implementation has the same OCP issue as the Visitor Pattern.
Changing the Shape hierarchy requires a change to the pattern matching expression.
However, the required changes are isolated to one place and they are more succinct.
In fact, all the logic for drawing is now contained in one place, rather than separated into draw methods in each Shape class and potentially scattered across different files.
Note that because we sealed the hierarchy, a compilation error in draw will occur if we forget to change it when the hierarchy changes.
If we don’t like the pattern matching in the draw method, we could implement a separate “drawer” class and a separate implicit conversion for each Shape class.
That would allow us to keep each shape drawing operation in a separate file, for modularity, with the drawback of more code and files to manage.
If, on the other hand, we don’t care about using the object-oriented shape.draw syntax, we could eliminate the implicit conversion and do the same pattern matching that is done in ShapeDrawer.draw.
This approach could be simpler, especially when the extra behavior can be isolated to one place.
Indeed, this approach would be a conventional functional approach, as illustrated in the following script:
We define two function values and assign them to variables, drawText and drawXML, respectively.
Each drawX function takes an input Shape, pattern matches it to the correct type, and “draws” it appropriately.
We also define a helper method to convert a Point to XML in the format we want.
The first time, we pass drawText as the argument to foreach.
Running this script reproduces the previous results for “text” output, followed by new XML output:
Any of these idioms provides a powerful way to add additional, special-purpose functionality that may not be needed “everywhere” in the application.
It’s a great way to remove methods from objects that don’t absolutely have to be there.
A drawing application should only need to know how to do input and output of shapes in one place, whether it is serializing shapes to a textual format for storage or rendering shapes to the screen.
We can separate the drawing “concern” from the rest of the functionality for shapes, and we can isolate the logic for drawing, all without modifying the Shape hierarchy or any of the places where it is used in the application.
Pattern gives us some of this separation and isolation, but we are required to add visitor implementation logic to each Shape.
Let’s conclude with a discussion of one other option that may be applicable in some contexts.
If you have complete control over how shapes are constructed, e.g., through a single factory, you can modify the factory to mix in traits that add new behaviors as needed:
We define a Drawing trait and concrete derived traits for each Shape class.
Then we define a ShapeFactory object with a makeShape factory method that takes a variablelength list of arguments.
A match is done on the first argument to determine which shape to make.
The trailing arguments are cast to appropriate types to construct each shape, with the corresponding drawing trait mixed in.
The variable-length list of Any values, heavy use of casting, and minimal error checking were done for expediency.
Compared to our previous scripts, the list of shapes is now constructed using the factory.
When we want to draw the shapes in the foreach statement, we simply call draw on each shape.
There is one subtlety with this approach that we should discuss.
If it did that, it would not be able to call draw on the instance! In this script, Scala inferred a slightly different common supertype for the parameterized list.
You can see that type if you use the :load command to load the script while inside the interactive scala interpreter, as in the following session:
This line was printed after the list of shapes was parsed.
Product is a trait mixed into all case classes, such as our concrete subclasses of Shape.
Recall that to avoid case-class inheritance, Shape itself is not a case class.
See “Case Classes” on page 136 for details on why case class inheritance should be avoided.
So, our common supertype is an anonymous class that incorporates Shape, Product, and the Drawing trait.
If you want to assign one of these drawable shapes to a variable and still be able to invoke draw, use a declaration like the following (shown as a continuation of the same interactive scala session):
Dependency Injection in Scala: The Cake Pattern Dependency injection (DI), a form of inversion of control (IoC), is a powerful technique for resolving dependencies between “components” in larger applications.
It supports minimizing the coupling between these components, so it is relatively easy to substitute different components for different circumstances.
It used to be that when a client object needed a database “accessor” object, for example, it would just instantiate the accessor itself.
While convenient, this approach makes unit testing very difficult because you have to test with a real database.
It also compromises reuse, for those alternative situations where another persistence mechanism (or none) is required.
Inversion of control solves this problem by reversing responsibility for satisfying the dependency between the object and the database connection.
Instead of instantiating an accessor object, the client object asks JDNI to provide one.
The client doesn’t care what actual type of accessor is returned.
Hence, the client object is no longer coupled to a concrete implementation of the dependency.
It only depends on an appropriate abstraction of a persistence accessor, i.e., a Java interface or Scala trait.
Instead, an external mechanism with system-wide knowledge “injects” the appropriate accessor object using a constructor argument or a setter method.
The second class is a case class, Tweet, that encapsulates a single “tweet” (a Twitter message, limited to 140 characters by the Twitter service)
Besides the message string, it encapsulates the user who sent the tweet and the date and time when it was sent.
We made this class a case class for the convenient support case classes provide for creating objects and pattern matching on them.
We didn’t make the profile class a case class, because it is more likely to be used as the parent of more detailed profile classes.
Next is the Tweeter trait that declares one method, tweet.
Each one declares a nested trait or class that encapsulates the component’s behavior.
Each one also declares a val with one instance of the nested type.
This is common in other languages, too, using their equivalent of a package, e.g., a module or a namespace.
Here we define a more precise notion of a component, and a trait is the best vehicle for it, because traits are designed for mixin composition.
This class has a client field that must be initialized with a Tweeter instance.
The first is sendTweet, which is defined to call the client object.
This method would be used by the UI to call the client when the user sends a new tweet.
It is called whenever a new tweet is to be displayed, e.g., from another user.
It is abstract, pending the “decision” of the kind of UI to use.
Instances with this trait save tweets to the local persistent cache when saveTweet is called.
Its nested type has a method, sendTweet, that sends a new tweet to Twitter.
It also has a history method that retrieves all the tweets for the current user.
Its one tweet method sends new tweets to the Twitter service.
If successful, it sends the tweet back to the UI and to the local persistent cache.
This composition will be realized by mixing in these components, which are traits, when we create concrete clients, as we will see shortly.
The self-type annotation also means we can reference the vals declared in these components.
In fact, they are in scope, because of the self-type annotation.
Notice also that all the methods that call other components are concrete.
The abstractions are directed “outward,” toward the graphical user interface, a caching mechanism, etc.
Let’s now define a concrete Twitter client that uses a textual (command-line) UI, an in-memory local cache, and fakes the interaction with the Twitter service:
The TextClient constructor takes one argument, a user profile, which will be passed onto the nested client class.
For the client, it simply creates a new TwitterClient, passing it the userProfile.
For the ui, it instantiates an anonymous class derived from TwitterClientUI.
For the localCache, it instantiates an anonymous class derived from TwitterLocalCache.
Finally, for the service, it instantiates an anonymous class derived from TwitterService.
This “fake” defines sendTweet to print out a message and to return an empty list for the history.
We instantiate a TextClient for the user “BuckTrends.” Old Buck sends three insightful tweets through the UI.
We finish by reprinting the history of tweets, in reverse order, that are cached locally.
Design by Contract has been around for about 20 years.
It has fallen somewhat out of favor, but it is still very useful for thinking about design.
When considering the “contract” of a module, you can specify three types of conditions.
First, you can specify the required inputs for a module to successfully perform a service (e.g., when a method is called)
They can also include system requirements, e.g., global data (which you should normally avoid, of course)
You can also specify the results the module guarantees to deliver, the postconditions, if the preconditions were satisfied.
Finally, you can specify invariants that must be true before and after an invocation of a service.
The specific addition that Design by Contract brings is the idea that these contractual constraints should be specified as executable code, so they can be enforced automatically at runtime, but usually only during testing.
A constraint failure should terminate execution immediately, forcing you to fix the bug.
Scala doesn’t provide explicit support for Design by Contract, but there are several methods in Predef that can be used for this purpose.
The following example shows how to use require and assume for contract enforcement:
The class BankAccount uses require to ensure that a non-negative balance is specified for the constructor.
Similarly, the debit and credit methods use require to ensure that a positive amount is specified.
The specification in Example 13-1 confirms that the “contract” is obeyed.
Similarly, the same kind of exception is thrown if the debit amount is less than zero.
The assume method, which is used to ensure that overdrafts don’t occur, is functionally almost identical to require.
Both require and assume come in two forms: one that takes just a boolean condition, and the other that also takes an error message string.
There is also an assert pair of methods that behave identically to assume, except for a slight change in the generated failure message.
Pick assert or assume depending on which of these “names” provides a better conceptual fit in a given context.
Predef also defines an Ensuring class that can be used to generalize the capabilities of these methods.
Ensuring has one overloaded method, ensure, some versions of which take a function literal as a “predicate.” A drawback of using these methods and Ensuring is that you can’t disable these checks in production.
It may not be acceptable to terminate abruptly if a condition fails, although if the system is allowed to “limp along,” it might crash later and the problem would be harder to debug.
The performance overhead may be another reason to disable contract checks at runtime.
These days, the goals of Design by Contract are largely met by Test-Driven Development (TDD)
However, thinking in terms of Design by Contract will complement the design benefits of TDD.
If you decide to use Design by Contract in your code, consider creating a custom module that lets you disable the tests for production code.
Recap and What’s Next We learned a number of pragmatic techniques, patterns, and idioms for effective application development using Scala.
Good tools and libraries are important for building applications in any language.
The next chapter provides more details about Scala’s command-line tools, describes the state of Scala IDE support, and introduces you to some important Scala libraries.
In the previous chapter, Chapter 13, we looked at how to design scalable applications in Scala.
In this chapter, we discuss tools and libraries that are essential for Scala application developers.
Now we explore these tools in greater detail and learn about other tools that are essential for the Scala developer.
We’ll discuss language-aware plugins for editors and IDEs, testing tools, and various libraries and frameworks.
We won’t cover these topics in exhaustive detail, but we will tell you where to look for more information.
Command-Line Tools Even if you do most of your work with IDEs, understanding how the command-line tools work gives you additional flexibility, as well as a fallback should the graphical tools fail you.
In this chapter, we’ll give you some practical advice for interacting with these tools.
All the command-line tools are installed in the scala-home/bin directory (see “Installing Scala” on page 8)
In contrast with Java requirements, the source file name doesn’t have to match the public class name in the file.
In fact, you can define as many public classes in the file as you want.
You can also use arbitrary package declarations without putting the files in corresponding directories.
However, in order to conform to JVM requirements, a separate class file will be generated for each type with a name that corresponds to the type’s name (sometimes encoded, e.g., for nested type definitions)
Also, the class files will be written to directories corresponding to the package declarations.
We’ll see an example of the types of class files generated in the next section, when we discuss the scala command.
The scalac command is just a shell-script wrapper around the java command, passing it the name of the Scala compiler’s Main object.
It adds Scala JAR files to the CLASSPATH and it defines several Scala-related system properties.
For example, we used the following scalac invocation command in “A Taste of Scala” on page 10, where we created a simple command-line tool to convert input strings to uppercase:
Table 14-1 shows the list of the scalac options, as reported by scalac -help.
We recommend routine use of the -deprecation and -unchecked options.
They help prevent some bugs and encourage you to eliminate use of obsolete libraries.
The advanced -X options control verbose output, fine-tune the compiler behavior, including use of experimental extensions and plugins, etc.
We’ll discuss the -Xscript option when we discuss the scala command in the next section.
Similarly, the -Xexperimental option enables experimental changes and issues warnings for potentially risky behavior changes.
See “Overriding Abstract and Concrete Fields in Traits” on page 114 for details.
An important feature of scalac is its plugin architecture, which has been significantly enhanced in version 2.8
Compiler plugins can be inserted in all phases of the compilation, enabling code transformations, analysis, etc.
Other plugins that are under development include an “effects” analyzer, useful for determining whether functions are truly side-effect-free, whether or not variables are modified, etc.
Finally, the preliminary sxr documentation tool (see [SXR]) uses a compiler plugin to generate hyperlinked documentation of Scala code.
In particular, Table 14-4 shows an example sbaz command that installs the scala-devel-docs documentation.
If you have your own collections implementations, they may require changes.
The scala Command-Line Tool The scala command is also a shell-script wrapper around the java command.
It adds Scala JAR files to the CLASSPATH, and it defines several Scala-related system properties.
Upper is the class name with a main method to run.
The command decides what to do based on the script-or-object specified.
If you don’t specify a script or object, scala runs as an interactive interpreter.
You type in code that is evaluated on the fly, a setup sometimes referred to as a REPL (Read, Evaluate, Print, Loop)
There are a few special commands available in the interactive mode.
The version 2.8 REPL adds many enhancements, including code completion.
Our Upper example demonstrates the case where you specify a fully qualified object name (or Java class name)
In this case, scala behaves just like the java command; it searches the CLASSPATH for the corresponding code.
It will expect to find a main method in the type.
Recall that for Scala types, you have to define main methods in objects.
Any arguments are passed as arguments to the main method.
If you specify a Scala source file for script-or-object, scala interprets the file as a script (i.e., compiles and runs it)
Many of the examples in the book are invoked this way.
Any arguments are made available to the script in the args array.
Here is an example script that implements the same “upper” feature:
If we run this script with the following command, scala upper.scala Hello World, we get the same output we got before, HELLO WORLD.
Finally, if you invoke scala without a script file or object name argument, scala runs in interpreted mode.
The scala command options (in addition to the scalac options)
Use the -i file option in the interactive mode when you want to preload a file before typing commands.
Once in the shell, you can also load a file using the command :load filename.
Table 14-3 lists the special :X commands available within the interactive mode.
The new “power user mode” adds additional commands for viewing in-memory data, such as the abstract syntax tree and interpreter properties, and for doing other operations.
For batch-mode invocation, use the -e argument option to specify Scala code to interpret.
Invoking scripts with scala is tedious when you use these scripts frequently.
On Windows and Unix-like systems, you can create standalone Scala scripts that don’t require you to use the scala script-file-name invocation.
For Unix-like systems, the following example demonstrates how to make an executable script.
Remember that you have to make the permissions executable, e.g., chmod +x secho:
Here is how you might use it: $ secho Hello World You entered: Hello World.
There are some limitations when running a source file with scala versus compiling it with scalac.
Any scripts executed with scala are wrapped in an anonymous object that looks more or less like the following example:
As of this writing, Scala objects cannot embed package declarations, and as such you can’t declare packages in scripts.
This is why the examples in this book that declare packages must be compiled and executed separately, such as this example from Chapter 2:
Conversely, there are valid scripts that can’t be compiled with scalac, unless a special -X option is used.
For example, function definitions and function invocations outside of types are not allowed.
However, if you try to compile the script with scalac (without the -Xscript option), you get the following errors:
The script itself describes the solution; to compile this script with scalac you must add the option -Xscript name, where name is the name you want to give the compiled class file.
For example, using MessagePrinter for name will result in the creation of several class files with the name prefix MessagePrinter:
You can now run the compiled code with the command: scala -classpath.
What are all those files? MessagePrinter and MessagePrinter$ are wrappers generated by scalac to provide the entry point for the script as an “application.” Recall that we specified MessagePrinter as the name argument for -Xscript.
The printMessage method in the script is a method in this class.
If you want to see what’s inside these class files, use one of the decompilers, which we describe next.
The scalap, javap, and jad Command-Line Tools When you are learning Scala and you want to understand how Scala constructs are mapped to the runtime, there are several decompilers that are very useful.
They are especially useful when you need to invoke Scala code from Java and you want to know how Scala names are mangled into JVM-compatible names, or you want to understand how the scala compiler translates Scala features into valid byte code.
Since the class files generated by scalac contain valid JVM byte codes, you can use Java decompilers tools:
It outputs declarations as they would appear in Scala source code.
It outputs declarations as they would appear in Java source code.
Therefore, running javap on Scala-generated class files is a good way to see how Scala definitions are mapped to valid byte code.
It attempts to reconstruct an entire Java source file from the class file, including method definitions, as well as the declarations.
Note that the first method inside object MessagePrinter is the main method.
The $tag method was originally introduced to optimize pattern matching, but it is now deprecated and it may be removed in a forthcoming release of Scala.
Let’s compare the scalap output to what we get when we run javap -classpath.
Now we see the declaration of main as we would typically see it in a Java source file.
Finally, to use jad, you simply give it the file name of the class file.
It generates a corresponding output file with the .jad extension.
You will also get several warnings that jad could not fully decompile some methods.
We won’t reproduce the output here, but the .jad file will print normal Java statements interspersed with several sections of JVM byte code instructions, where it could not decompile the byte code.
The Mac and Linux distributions also include a man page.
Finally, as an exercise, compile the following very simple Complex class, representing complex numbers.
Then run scalap, javap, and jad on the resulting class files:
How are the + and - methods encoded? What are the names of the reader methods for the real and imaginary fields? What Java types are used for the fields?
The scaladoc Command-Line Tool The scaladoc command is analogous to javadoc.
It is used to generate documentation from Scala source files, called Scaladocs.
If you use scaladoc for your documentation, you might want to investigate vscaladoc, an improved scaladoc tool that is available at http://code.google.com/p/vscaladoc/
The sbaz Command-Line Tool The Scala Bazaar System (sbaz) is a packaging system that helps automate maintenance of a Scala installation.
It is analogous to the gem packaging system for Ruby, CPAN for Perl, etc.
There is a nice summary of how to use sbaz on the scala-lang.org website.
Note that a remote repository used by sbaz is called a “universe.”
The fsc Command-Line Tool The fast (offline) scala compiler runs as a daemon process to enable faster invocations of the compiler, mostly by eliminating the startup overhead.
It is particularly useful when running scripts repeatedly (for example, when re-running a test suite until a bug can be reproduced)
In fact, fsc is invoked automatically by the scala command.
Build Tools Scala plugins have been implemented for several, commonly used build tools, including Ant, Maven, and Buildr.
There are also several build tools written in Scala and aimed specifically at Scala development.
Perhaps the best known example of these tools is SBT (simple build tool—see [SBT])
These plugins and tools are documented very well on their respective websites, so we refer you to those sites for details.
The Scala distribution includes Ant tasks for scalac, fsc, and scaladoc.
They are used very much like the corresponding Java Ant tasks.
It does not require Scala to be installed, as it will download Scala for you.
Several third-party Scala projects, such as Lift (see “Lift” on page 367), use Maven.
It is aimed at JVM applications written in any language, with built-in support for Scala and Groovy as well as Java.
Since build scripts are written in Ruby, they tend to be much more succinct than corresponding Maven files.
Buildr is also useful for testing JVM applications with Ruby testing tools, like RSpec and Cucumber, if you use JRuby to run your builds.
The Scala-oriented SBT, available at http://code.google.com/p/simple-build-tool/, has some similarities to Buildr.
It is also compatible with Maven, but it uses Scala as the language for writing build scripts.
It also has built-in support for generating Scaladocs and for testing with ScalaTest, Specs, and ScalaCheck.
Integration with IDEs If you come from a Java background, you are probably a little bit spoiled by the rich features of today’s Java IDEs.
Scala IDE support is not yet as good, but it is evolving rapidly in Eclipse, IntelliJ IDEA, and NetBeans.
At the time of this writing, all the Scala plugins for these IDEs support syntax highlighting, project management, limited support for automated refactorings, etc.
This section describes how to use the Scala support available in Eclipse, IntelliJ IDEA, and NetBeans.
We assume you already know how to use each IDE for development in other languages, like Java.
Eclipse For details on the Eclipse Scala plugin, start at this web page, http://www.scala-lang .org/node/94
If you are interested in contributing to the development of the plugin, see this web page, http://lampsvn.epfl.ch/trac/scala/wiki/EclipsePlugin.
To install the plugin, invoke the “Software Updates” command in the Help menu.
Click the Available Software tab and click the “Add Site…” button on the righthand side.
Enter the URL that is shown in the figure, http://www.scala-lang.org/scala-eclipse-plu gin.
It is easy to be confused by the poor usability of the Software Updates dialog.
After finding the plugin on the update site, an Install dialog is presented.
Click through the sequence of screens to complete the installation.
You will be asked to restart Eclipse when the installation completes.
Scroll down to the Scala plugin, as shown in Figure 14-4
Right-click the Scala plugin name and select “Download and Install” from the menu.
You will have to restart IDEA for the plugins to be enabled.
After IDEA restarts, confirm that the two plugins were installed correctly by reopening the Plugin Manager.
Click the Installed tab and scroll down to find the two Scala plugins.
They should be listed with a black font, and the checkboxes next to them should be checked, as seen in Figure 14-5
If the font is red or the checkboxes are not checked, refer to the Scala plugin web page above for debugging help.
Click through to the screen titled “Please Select Desired Technology.” Check the “Scala” checkbox, and check the “New Scala SDK” checkbox.
You will only need to specify the SDK the first time you create a project or when you install a new SDK in a different location.
You will be prompted to create either a project or an application.
Select “Application” if you want to share this project with other Scala projects on the same machine.
Now you can work with your Scala project using most of the same commands you would use with a typical Java project.
For example, you can create a new Scala trait, object, or class using the context menu, as for Java projects.
The IntelliJ IDEA Scala plugin is still beta-quality, but Scala developers using IDEA should find it acceptable for their daily needs.
NetBeans 6.5 or a more recent nightly build is required.
The Scala plugin contains a version of the Scala SDK.
The wiki page provides instructions for using a different SDK, when desired.
Back in the Plugins dialog, make sure the checkboxes for all the new plugins are checked.
Click through the installation dialog and restart NetBeans when finished.
Fill in the project name, location, etc., and click Finish.
Once the project is created, you can work with it using most of the same commands you would use with a typical Java project.
For example, when you invoke the New item in the context menu, the submenu does not show items for creating new Scala types.
Instead, you have invoke the Other… menu item and work through a dialog.
Despite some minor issues like this, the NetBeans Scala plugin is mature enough for regular use.
Text Editors The sbaz tool manages the scala-tool-support package that includes Scala plugins for several editors, including Emacs, Vim, TextMate and others.
Like sbaz, the scala-toolsupport package is also included with the language installation.
Most of the editorspecific directories contain instructions for installing the plugin.
In other cases, consult your editor’s instructions for installing third-party plugins.
If you want to contribute to the Scala community, please consider improving the quality of the existing plugins or contributing new plugins.
At the time of this writing, there are several variations of a Scala “bundle” for the TextMate editor, which is a popular text editor for Mac OS X.
These bundles are currently being managed by Paul Phillips on the GitHub website.
Hopefully, the best features of each bundle will be unified into an “authoritative” bundle and integrated back into the scala-tool-support package.
Test-Driven Development in Scala One of the most important developer practices introduced in the last decade is TestDriven Development (TDD)
The Scala community has created several tools to support TDD.
If you work in a “pure” Java shop, consider introducing one or more of these Scala testing tools to test-drive your Java code.
This approach is a low-risk way to introduce Scala to your environment, so you can gain experience with it before making the commitment to Scala as your production code language.
In particular, you might experiment with ScalaTest (see “ScalaTest” next), which can be used with JUnit ([JUnit]) and TestNG ([TestNG])
You might also consider ScalaCheck or Reductio (see “ScalaCheck” on page 365), which offer innovations that may not be available in Java testing frameworks.
All of the tools we describe here integrate with Java testing and build tools, like JUnit, TestNG, various mocking libraries, Ant ([Ant]), and Maven ([Maven])
All of them also offer convenient Scala DSLs for testing.
ScalaTest Scala’s version of the venerable XUnit tool is ScalaTest, available at http://www.artima .com/scalatest/
You can drive your tests using the built-in Runner or use the provided integration with JUnit or TestNG.
ScalaTest also comes with an Ant task and it works with the ScalaCheck testing tool (described later)
Besides supporting the traditional XUnit-style syntax with test methods and assertions, ScalaTest provides a Behavior-Driven Development ([BDD]) syntax that is becoming increasingly popular.
The ScalaTest website provides examples for these and other options.
Here is an example ScalaTest test for the simple Complex class we used in “The scalap, javap, and jad Command-Line Tools” on page 350:
This particular example uses the “function value” syntax for each test that is provided by the FunSuite parent trait.
Each call to test receives as arguments a descriptive string and a function literal with the actual test code.
The downloadable distribution of the code examples is organized this way:
We used a \ to continue the long command on a second line.
Total number of tests run was: 4 All tests passed.
Again, we wrapped the long output lines with a \
In a nutshell, the goal of BDD is to recast traditional test syntax into a form that better emphasizes the role of TDD as a process that drives design, which in turn should implement the requirements “specification.” The syntax of traditional TDD tools, like the XUnit frameworks, tend to emphasize the testing role of TDD.
With the syntax realigned, it is believed that the developer will be more likely to stay focused on the primary role of TDD: driving application design.
Here is another example for the simple Complex class we showed previously:
Here, as before, we assume the Specs JAR is in the ../lib directory and we assume the compiled class files are in the build directory.
Complex addition should + return a new number where the real and imaginary parts are the sums of the input values real and imaginary parts, respectively.
Complex subtraction should + return a new number where the real and imaginary parts are the differences of the.
Note that the strings in the specification are written in a form that reads somewhat like a requirements specification:
There are many ways to run specifications, including using an Ant task or using the built-in integration with ScalaTest or JUnit.
JUnit is the best approach for running specifications in some IDEs.
These and other options are described in the User’s Guide at http://code.google.com/p/specs/wiki/RunningSpecs.
ScalaCheck can be installed using sbaz, i.e., sbaz install scalacheck.
Using ScalaCheck (or QuickCheck for Haskell), conditions for a type are specified that should be true for any instances of the type.
The tool tries the conditions using automatically generated instances of the type and verifies that the conditions are satisfied.
The toD function just converts an Int to a Double by dividing by 0.1
It’s useful to convert an Int index provided by ScalaCheck into Double values that we will use to construct Complex instances.
We also need an implicit conversion visible in the scope of the test that generates new Complex values.
An Arbitrary[Complex] object (part of the ScalaCheck API) is returned by this method.
We provide a function literal that assigns a passed-in Int value to a variable s.
Fortunately, you don’t have to define implicit conversions or generators for most of the commonly used Scala and Java types.
This object defines a few helper methods, additionTest and subtractionTest, that each return true if the conditions they define are true.
For additionTest, if a new Complex number is the sum of two other Complex numbers, then its real part must equal the sum of the real parts of the two original numbers.
Likewise, a similar condition must hold for the imaginary part of the numbers.
For subtractionTest, the same conditions must hold with subtraction substituted for addition.
Two more specify classes assert that the conditions should also hold for any pair of Complex numbers.
We can run the check using the following command (once again assuming that Complex is already compiled into the build directory):
Note that ScalaCheck tried each specify case with 100 different inputs.
Rather than going through the process of writing enough “example” test cases with representative data, which is tedious and error-prone, we define reusable “generators,” like the arbitrary Complex function, to produce an appropriate range of instances of the type under test.
Then we write property specifications that should hold for any instances.
ScalaCheck does the work of testing the properties against a random sample of the instances produced by the generators.
You can find more examples of ScalaCheck usage in the online code examples.
Some of the types used in the payroll example in “Internal DSLs” on page 218 were tested with ScalaCheck.
Finally, note that there is another port of QuickCheck called Reductio.
Reductio is less widely used than ScalaCheck, but it offers a “native” Java API as well as a Scala API, so it would be more convenient for “pure” Java teams.
Other Notable Scala Libraries and Tools While Scala benefits from the rich legacy of Java and .NET libraries, there is a growing collection of libraries written specifically for Scala.
Lift Lift is the leading web application framework written in Scala.
Lift has been used for a number of commercial websites.
Scalaz Scalaz is a library that fills in gaps in the standard library.
Among its features are enhancements to several core Scala types, such as Boolean, Unit, String, and Option, plus support for functional control abstractions, such as FoldLeft, FoldRight, and Monad, that expand upon what is available in the standard library.
Scalax Scalax is another third-party library effort to supplement the Scala core library.
Metaprogramming features tend to be weaker in statically typed languages than in dynamically typed languages.
Also, the JVM and .NET CLR impose their own constraints on metaprogramming.
Many of the features of Scala obviate the need for metaprogramming, compared to languages like Ruby, but sometimes metaprogramming is still useful.
MetaScala attempts to address those needs more fully than Scala’s built-in reflection support.
JavaRebel JavaRebel is a commercial tool that permits dynamic reloading of classes in a running JVM (written in any language), beyond the limited support provided natively by the “HotSwap” feature of the JVM.
JavaRebel is designed to offer the developer faster turnaround for changes, providing an experience more like the rapid turnaround that users of dynamic languages enjoy.
Miscellaneous Smaller Libraries Finally, Table 14-5 is a list of several Scala-specific libraries you might find useful for your applications.
Configgy Managing configuration files and logging for “daemons” written in Scala (http://www.lag.net/configgy/)
Akka A project to implement a platform for building fault-tolerant, distributed applications based on REST, Actors, etc.
We’ll discuss using Scala with several well-known Java libraries after we discuss Java interoperability, next.
Java Interoperability Of all the alternative JVM languages, Scala’s interoperability with Java source code is among the most seamless.
This section begins with a discussion of interoperability with code written in Java.
Once you understand the details, they can be generalized to address interoperability with other JVM languages, such as JRuby or Groovy.
For example, if you already know how to use JRuby and Java together, and you know how to use Java and Scala together, then you can generalize to using JRuby and Scala together.
Because Scala syntax is primarily a superset of Java syntax, invoking Java code from Scala is usually straightforward.
Going the other direction requires that you understand how some Scala features are encoded in ways that satisfy the JVM specification.
Instantiating Java generic types is straightforward in Scala (since Scala version 2.7.0)
We can instantiate it from Scala, specifying the type parameter, as shown in Example 14-1
Since Scala version 2.7.2, you can also use Scala generics from Java.
Consider the following JUnit 4 test, which shows some of the idiosyncrasies you might encounter:
For simplicity, Name has public firstName and lastName fields and a constructor.
However, the first test has two compile-time warnings, indicated by the // warning comments, while the second test compiles without warnings:
In the compiled Scala library, the return type of Map.get is Option with no type parameter, or effectively Option<Object>
Using Scala Functions in Java Continuing with our previous SMapTest example, we can explore invoking Scala code from Java where Scala functions are required:
The empty function object is needed when we use Map.getOrElse in the test method, usingMapGetOrElse.
Where A is the key type parameter, B is the value type parameter, and B2 is a supertype of B or the same as B.
Note that by-name parameters are implemented as scala.Func tion0 objects.
So, we can’t simply pass in the static object emptyName.
The second test, usingFilterKeys, requires a Function1 object, which has an apply method that takes one argument.
We use this Function1 object as a filter passed to Map.filterKeys.
The Java code here is considerably more involved than the equivalent Scala code would be! Not only do we have to define the apply and $tag methods, we must also define methods used for function composition, compose and andThen.
Fortunately, we can delegate to objects that are already in the Scala library, as shown.
Finally, recall that in “Companion Objects and Java Static Methods” on page 133, we discussed that methods defined in companion objects are not visible as static methods to Java code.
For example, main methods defined in companion objects can’t be used to run the application.
So, using Scala function objects from Java can be challenging.
If you find it necessary to use them frequently, you could define Java utility classes that handle the boilerplate for all the methods except apply.
However, there are times when you need JavaBeans accessor methods.
Now we add the annotation to each constructor argument, which is a field in the case class:
If you compile this class, then decompile it with javap -classpath ...
Now compare this output with the result of decompiling the original Complex.class file:
The order of the methods shown may be different when you run javap on these files.
We reordered them so the two listings would match as closely as possible.
Note that the only differences are the names of the classes and the presence of getImaginary and getReal methods in the ComplexBean case.
We would also have corresponding setter methods if the real and imaginary fields were declared as vars instead of vals.
You can call them, but as the Scaladoc page goes on to say, you should use the Scala-style writer (and reader) methods instead.
AnyVal Types and Java Primitives Notice also in the previous Complex example that the Doubles were converted to Java primitive doubles.
All the AnyVal types are converted to their corresponding Java primitives.
These characters are encoded (or “mangled,” if you prefer) to satisfy the tighter constraints of the JVM specification.
You can see this at work in the following contrived trait, where each character is used to declare an abstract method that takes no arguments and returns Unit:
Note that we doubled up some of the characters to get them to compile as method names, where using single characters would have been ambiguous.
Compiling this file and decompiling the resulting class file with javap AllOpChars yields the following Java.
We have rearranged the output order of the methods to match the order in the original Scala file.
To conclude, interoperability between Java and Scala works very well, but there are a few things you must remember when invoking Scala code from Java.
If you’re uncertain about how a Scala identifier is encoded or a Scala method is translated to valid byte code, use javap to find out.
Because they are widely used in “enterprise” and Internet Java applications, successful interoperability with Scala is important.
AspectJ AspectJ ([AspectJ]) is an extension of Java that supports aspect-oriented programming (AOP), also known as aspect-oriented software development ([AOSD])
The goal of AOP is to enable systemic changes of the same kind across many modules, while avoiding copying and pasting the same code over and over into each location.
Avoiding this duplication not only improves productivity, it greatly reduces bugs.
For example, if you want all field changes to all “domain model” objects to be persisted automatically after the changes occur, you can write an aspect that observes those changes and triggers a persistence write after each change.
AspectJ supports AOP by providing a pointcut language for specifying in a declarative way all the “execution points” in a program for which a particular behavior modification (called advice) is required.
In AspectJ parlance, each execution point is called a join point, and a particular query over join points is a pointcut.
Hence the pointcut language is a query language, of sorts.
For a given pointcut, AspectJ incorporates the desired behavior modifications into each join point.
An aspect encapsulates pointcuts and advices, much the way a class encapsulates member fields and methods.
For a detailed introduction to AspectJ with many practical examples, refer to [Laddad2009]
There are two issues that must be considered when using AspectJ with Scala.
The first issue is how to reference Scala execution points using AspectJ’s pointcut language, e.g., Scala types and methods.
The second issue is how to invoke Scala code as advice.
Let’s look at an aspect that logs method calls to the Complex class we used previously in this chapter.
We’ll add a package declaration this time to provide some scope:
Next, here is an AspectJ aspect that defines one pointcut for the creation of Complex instances and another pointcut for invocations of the + method:
We won’t explain all the details of AspectJ syntax here.
See the AspectJ document at [AspectJ] and [Laddad2009] for those details.
We’ll limit ourselves to a “conceptual” overview of this aspect.
The first pointcut, newInstances, matches on executions of the constructor calls, using the syntax Complex.new to refer to the constructor.
As we saw previously, scala.Double occurrences are converted to Java primitive doubles when generating byte code.
The args clause “binds” the values of the arguments passed in, so we can refer to them in advice.
The second pointcut, plusInvocations, matches on “executions” of the + method, which is actually $plus in the byte code.
The self and other parameters are bound to the object on which the + method is invoked (using the this clause) and the argument to it (using the args clause), respectively.
The first before advice is executed for the newInstances pointcut, that is, before we actually enter the constructor.
We “log” the call, displaying the actual real and imaginary values passed in.
The next before advice is executed for the plusInvocations pointcut, that is, before the + method is executed.
We log the value of self (i.e., this instance) and the other number.
Finally, an after returning advice is also executed for the plusInvocations pointcut, that is, after the + method returns.
We capture the return value in the variable c and we log it.
If you have AspectJ installed in an aspectj-home directory, you can compile this file as follows:
This is one line; we used the \ to indicate a line wrap.
To run this code with the LogComplex aspect, we use load-time weaving.
We’ll invoke Java with an agent that “weaves” the advice from LogComplex into Complex.
The META-INF directory should be on the class path; we’ll assume it’s in the current working directory.
This file tells the weaver which aspects to use (the aspect tag) and which classes to target for weaving (the include tag), and it also enables verbose output, which is useful for debugging purposes.
Finally, we can run the application with the following command:
You get several lines of messages logging the weaving process.
We added this additional behavior without manually inserting these statements in Complex itself! Recall we said that the second issue you might encounter when using AspectJ is how to invoke Scala code from advice.
In our LogComplex aspect, the statements inside our different before and after advices are really just Java code.
Therefore, we can just as easily invoke Scala code, applying the same lessons we have already learned for invoking Scala from Java.
This powerful technique lets you implement a form of aspect advice.
When you need to affect a set of join points that don’t share a common supertype, you’ll need the capabilities of AspectJ.
However, if you find yourself in that situation, you should consider if you can refactor your code to extract a common trait that provides the “hooks” you need for advice implemented using traits.
The Spring Framework The Spring Framework (see [SpringFramework]) is an open source, modular Java enterprise framework that provides a “pure” Java AOP API, integrated support for AspectJ, a dependency injection (DI) container, uniform and well-designed APIs for invoking a variety of other Java third-party APIs, and additional components for security, web development, etc.
Here we focus on dependency injection, as interoperability issues with the other parts of the Spring Framework boil down to either Java or AspectJ issues, which we covered earlier.
We discussed the concept of DI in “Dependency Injection in Scala: The Cake Pattern” on page 334, where we showed elegant patterns for injecting dependencies using Scala itself.
However, if you are in a mixed Java/Scala environment, it might be necessary to use a DI framework like the one provided by Spring to manage dependencies.
In Spring DI, dependencies are specified using a combination of XML configuration files and source-code annotations.
The Spring API resolves these dependencies as classes are instantiated.
Spring expects these classes to follow JavaBean conventions (see [JavaBeansSpec])
Well-designed classes will only depend on abstractions, i.e., Java interfaces or Scala traits, and the concrete instances satisfying those dependencies will be given to the bean through constructor arguments or through JavaBean setter methods.
The annotation is not needed when you use constructor injection.
Not only does this choice eliminate the need to use the @BeanProperty annotation, it leaves each instances in a known good state when the construction process is finished.
However, if you inject dependencies into Scala objects, you must use setter injection, as you have no way to define constructor parameters and you have no control over the construction process.
One other point; remember that Spring will expect Java-compatible names, so you must use encoded names for methods and objects, as needed.
Here is an example that illustrates “wiring together” objects with Spring:
The case class FactoryUsingBean is a simple type with a dependency on a Factory abstraction that we want to inject using constructor injection.
It has a make method to create instances of some kind.
To demonstrate setter injection on objects, we also give it a nameOf Factory field.
Scala requires us to initialize nameOfFactory with a value, but we will use Spring to set the real value.
We have to use the @BeanProperty annotation to generate the setNameOf Factory method Spring will expect to find.
The concrete make method in NamedObjectFactory creates a new NamedObject.
It is a simple case class with a name field.
Note that none of these types depend on the Spring API.
You can compile this file without any Spring JAR files.
Next, we define the dependency “wiring” using a standard Spring XML configuration file:
Note that we have to append a $ to the end of the name, the actual name of the object in the byte code.
We can’t control instantiation of objects, so we have to inject the correct dependency after construction completes.
Since this is a class, we can use constructor injection.
The constructor tag specifies that the factory bean is used to satisfy the dependency at construction time.
Finally, here is a script that uses these types to demonstrate Spring DI with Scala:
This context object is our gateway to the DI container.
We have to cast the returned AnyRef (i.e., Java Object) to the correct type.
We print out the factory’s name, to see if it is correct.
Next, we ask the bean’s factory to make “something” with the string "Dean Wampler"
When we print the returned object, it should be a NamedObject.
If you have Spring installed in a spring-home directory, you can run this script with the following command:
The current working directory “.” is needed in the classpath to find the XML file.
This example required a number of files and configuration details to get working.
For a moderately large Java application, the effort is justified.
However, Scala gives you new and simpler ways to implement dependency injection in Scala code without configuration files and a DI container.
Hadoop MapReduce is a divide-and-conquer programming model for processing large data sets in parallel.
In the “map” phase, a data set is divided into N subsets of approximately equal size, where N is chosen to optimize the amount of work that can be done in parallel.
For example, N might be close to the total number of processor cores available.
A few cores might be left idle as “backups” or for doing other processing.
The “reduce” phase combines the results of the subset calculations into a final result.
Therefore, a functional language like Scala is ideally suited for writing MapReduce applications.
MapReduce frameworks provide tools for mapping and reducing data sets, managing all phases of the computation, including the processing nodes, restarting operations that fail for some reason, etc.
The user of a MapReduce framework only has to write the algorithms for mapping (subdividing) the input data, the computations with the data subsets, and reducing the results.
The name of the Google framework has become a de facto standard for these frameworks.
Hadoop (see [Hadoop]) is an open source MapReduce framework created and maintained by Yahoo!
Both examples demonstrate the great reduction in code size when using Scala.
Recap and What’s Next This chapter filled in the details of the Scala command-line tools that you will use every day.
We also surveyed the available support for Scala in various text editors and IDEs.
We discussed a number of important libraries, such as testing APIs.
Finally, we discussed interoperability between Scala and other JVM languages and libraries.
This completes our survey of the world of Scala programming.
The next chapter is a list of references for further exploration, followed by a glossary of terms that we have used throughout the book.
It is currently used to optimize pattern matching, but it may be removed in a future release of Scala.
While normally invisible to Scala code (it is generated automatically by the compiler), Java code that extends some Scala traits and classes may need to implement this method.
Abstraction The outwardly visible state, state transformations, and other operations supported by a type.
This is separate from the encapsulated implementation (fields and methods) of the abstraction.
Scala traits and abstract classes are often used to define abstractions and optionally implement them.
A class or trait with one or more methods, fields, or types declared, but undefined.
A type declaration within an class or trait that is abstract.
Actor An autonomous sender and receiver of messages in the Actor model of concurrency.
Actor Model of Concurrency A concurrency model where autonomous Actors coordinate work by exchanging messages.
An Actor’s messages are stored in a mailbox until the Actor processes them.
Annotated Type Any type that has one or more @ annotations applied to it.
Annotation A way of attaching “metadata” to a declaration that can be exploited by the compiler and other tools for code generation, verification and validation, etc.
One or more additions to a type declaration that specify behaviors like variance under inheritance, bounds, and views.
Application In Scala, any object with a main routine that is invoked by the JVM or .NET CLR at the start of a new process.
In AOP terms, the execution points are called join points; a particular set of them is called a pointcut; and the new behavior that is executed before, after, or “around” a join point is called advice.
Scala traits can be used to implement some aspect-like functionality.
AspectJ ([AspectJ]) supports two forms of syntax: an extended Java-based syntax, and a “pure” Java syntax that uses Java annotations to indicate the pointcuts and advices of an aspect.
The aspect behaviors (advices) can be incorporated into the target code at compile time, as a post-compile “weaving” step, or at load time.
Attribute Another name for a field, used by convention in many object-oriented programming languages.
Scala follows Java’s convention of preferring the term field over attribute.
Auxiliary Constructor A secondary constructor of a class, declared as a method named this with no return type.
An auxiliary constructor must invoke the primary constructor or a previously defined auxiliary constructor as the first or only statement in its method body.
Behavior-Driven Development A style of Test-Driven Development (TDD) that emphasizes TDD’s role in driving the understanding of requirements for the code.
You follow the same process as in TDD, where the “tests” are written before the code.
The difference is that the automated tests are written in a format that looks more like a requirements (or behavioral) specification and less like a test of the code’s conformance to the requirements.
However, the specification format is still executable and it still provides the verification, validation, and regression testing service that TDD tests provide.
Bound Variable A variable that is declared as an argument to a function literal.
It is “bound” to a value when the closure created from the function literal is invoked.
By-Value Parameter A by-value parameter is the usual kind of method parameter that is evaluated before it is passed to the method.
Case The keyword used in pattern matching expressions for testing an object against an extractor, type check, etc.
The Scala compiler automatically defines equals, hashCode and toString methods for the class and creates a companion object with an apply factory method and an unapply extractor method.
Case classes are particularly convenient for use with pattern matching (case) expressions.
Child Type A class or trait that inherits from a parent class or trait.
Client An informal term used throughout the book to indicate a section of software that uses another as an API, etc.
Class A template for instances that will have the same fields, representing state values, and the same methods.
Scala classes support single inheritance and zero or more mixin traits.
Closure In Scala, an instance that has been created from a function literal with all the free variables referenced in the function literal bound to variables of the same name in the enclosing scope where the function literal was defined.
In other words, the instance is “closed” in the sense that the free variables are bound.
They can be passed to other functions to customize their behavior.
For example, List.foreach takes a closure that is applied to each element in the list.
Companion Class A class declared with the same name as an object and defined in the same source file.
Companion Object An object declared with the same name as a class (called its companion class) and defined in the same source file.
Companion objects are where methods and fields are defined that would be statics in Java classes, such as factory methods, apply and unapply for pattern matching, etc.
Component For our purposes, an aggregation of cohesive types that expose services through welldefined abstractions, while encapsulating implementation details and minimizing coupling to other components.
There is a wide-range of definitions for component in computer science and industry.
Concrete Type A class, trait, or object with all methods, fields, and types defined.
Contract The protocol and requirements that exist between a module (e.g., class, trait, object, or even function or method) and clients of the module.
Context-Free Grammar A kind of language grammar for which each nonterminal can be specified as a production without reference to additional context information.
That is, each nonterminal can appear by itself on the lefthand side of the production the specifies it.
Contravariance or Contravariant In the context of the variance behavior of parameterized types under inheritance, if a parameter A is contravariant in a parameterized type T[-A], then the - is the variance annotation, and a type T[B] is a supertype of T[A] if B is a subtype of A.
Covariance or Covariant In the context of the variance behavior of parameterized types under inheritance, if a parameter A is covariant in a parameterized type T[+A], then the + is the variance annotation, and a type T[B] is a subtype of T[A] if B is a subtype of A.
Cross-Cutting Concerns “Concerns” (kinds of requirements, design or coding issues) that don’t fit in the same boundaries as the primary modularity decomposition.
The same behaviors must be invoked consistently at specific execution points over a range of objects and functions.
For example, the same ORM (ObjectRelational Mapping) persistence strategy needs to be used consistently for a set of classes, not just a single class.
Supporting these concerns should not involve duplication of code, etc.
Currying Converting an N argument function into a sequence of N functions of one argument, where each function except for the last returns a new function that takes a single argument that returns a new function, etc., until the last function that takes a single argument and returns a value.
Declaration Site In reference to how the variance behavior of parameterized types is specified, in Scala, this is done when types are declared, i.e., at the declaration site.
In Java, it is done when types are called (that is, used), i.e., at the call site.
Declarative Programming The quality of many functional programs and Domain-Specific Languages where the code consists of statements that declare relationships between values and types, rather than directing the system to take a particular sequence of action.
The ability to define a default value for a method argument that will be used if the caller does not specify a value.
Dependency Injection A form of inversion of control where an object’s external dependencies are given to it, either programmatically or through a DI framework that is driven by configuration information.
Hence, the object remains “passive,” rather than taking an active role in resolving dependencies.
The injection mechanism uses constructor arguments or field setters provided by the object.
Design By Contract An approach to class and module design invented by Bertrand Meyer for the Eiffel language.
For each entry point, valid inputs are specified in a programmatic way, so they can be validated during testing.
Similarly, assuming the preconditions are specified, specifications on the guaranteed results are called postconditions and are also specified in an executable way.
Invariants can also be specified that should be true on entry and on exit.
Design Pattern A solution to a problem in a context.
Domain-Specific Language A custom programming language that resembles the terms, idioms, and expressions of a particular domain.
An internal DSL is an idiomatic form of a general-purpose programming language.
That is, no specialpurpose parser is created for the language.
Instead, DSL code is written in the generalpurpose language and parsed just like any other code.
An external DSL is a language with its own grammar and parser.
Duck Typing A term used in languages with dynamic typing for the way method resolution works.
As long as an object accepts a method call (message send), the runtime is satisfied.
Dynamic Typing Loosely speaking, late binding of type information, sometimes referred to as binding to the value a reference is assigned to, rather than to the reference itself.
Encapsulation Restricting the visibility of members of a type so they are not visible to clients of the type when they shouldn’t be.
This is a way of exposing only the abstraction supported by the type, while hiding implementation details, which prevents unwanted access to them from clients and keeps the abstraction exposed by the type consistent and minimal.
Event The notification of a state change in eventbased concurrency.
Event-Based Concurrency A high-performance form of concurrency where events are used to signal important state changes and handlers are used to respond to the events.
Existential Types A way of expressing the presence of a type without knowing its concrete value, sometimes, because it can’t be known.
It is used primarily to support aspects of Java’s type system within Scala’s type system, including type erasure, “raw” types (e.g., pre-Java 5 collections), and call site type variance.
Extractor An unapply method defined in a companion object that is used to extract the constituent values for fields in an object.
Field A val or var in a type that represents part, if not all, of the state of a corresponding instance of the type.
For type members, final prevents users from overriding the members.
First Class An adjective indicating that the applicable “thing” is a first-class value in the language, meaning you can assign instances to variables, pass them as function parameters, and return them from functions.
Often used to refer to functions, which are first-class values in Scala and other functional programming languages.
Formal Parameter Another name for a function argument, used in the context of binding the free variables in the function.
Free Variable A variable that is referenced in a function literal but is not passed in as an argument.
Therefore, it must be “bound” to a defined variable of the same name in the context where the function literal is defined, to form a closure.
Function In Scala, the term function is used for a function that is not tied to a particular object or class.
Functions are instances of FunctionN types, where N is the arity of the function.
Function Literal Scala’s term for an anonymous function expression, from which closures are created.
Functional Programming A form of programming that mimics the way mathematical functions and variables work.
Mathematical functions are side-effect-free, and they are composable from other functions.
Functions can be assigned to variables and returned from other functions.
Each pass through the loop generates a new val i taken from the list listO fInts, in this example.
Generics Another term for parameterized types, used more often in Java than Scala.
Higher-Order Functions Functions that take other functions as arguments or return a function value.
Immutable Value A value that can’t be changed after it has been initialized.
Imperative Programming The quality of many object-oriented and “procedural” programs where the code consists of statements directing the system to.
Implicit A Scala keyword used to mark a method or function value as eligible for use as an implicit type conversion.
The keyword is also used to mark an implicit argument.
Implicit Type Conversion A method or function value that is marked with the implicit keyword, marking it as eligible for use as an implicit type conversion, whenever it is in scope and conversion is needed (e.g., for the Pimp My Library pattern)
Implicit Argument Method arguments that are optional for the user to specify and indicated with the implicit keyword.
If the user does not specify a value for the argument, a default value is used instead, which is either an in-scope value of the same type or the result of calling an in-scope, no-argument method that returns an instance of the same type.
Infinite Data Structure A data structure that represents a nonterminating collection of values, but which is capable of doing so without exhausting system resources.
The values are not computed until the data structure is asked to produce them.
As long as only a finite subset of the values are requested, resource exhaustion is avoided.
Infix Notation A syntax supported by the compiler for methods with one argument.
The method can be invoked without the period between the object and the method name and without the parentheses around the argument.
When used for methods named with operator characters, the syntax provides a form of operator overloading.
Infix Type When a parameterized type of the form Op[A,B] is used to instantiate a type, it can also be written as A Op B.
Inheritance A strong relationship between one class or trait and another class or trait.
The inheriting (derived) class or trait incorporates the members of the parent class or trait, as if they were defined within the derivative.
Instances of a derivative are substitutable for instances of the parent.
Instance or Instantiate An object created by invoking a class constructor.
The word object is synonymous in most object-oriented languages, but we use the term object to refer to an explicitly declared Scala object, and we use the term instance (and the verb instantiate) for the more general case.
Instantiation can also refer to creating a concrete type from a parameterized type by specifying concrete types for the parameters.
Invariance and Invariant In the context of the variance behavior of parameterized types under inheritance, if a parameter A is invariant in a parameterized type T[A], then there is no variance annotation, and a type T[B] is a subtype of T[A] if and only if B equals A.
In the context of Design by Contract, an assertion that should be true before and after a method is executed.
Inversion of Control The idea that an object should not instantiate its own copies of external dependencies, but rather rely on other mechanisms to supply those dependencies.
IoC promotes better decoupling and testability, as the object only knows about the abstractions of its dependencies, not specific concrete implementers of them.
A weak form of IoC is when an object calls a factory, service locator, etc., to obtain the dependents.
Hence, the object still has an active role and it has a dependency on the “provider.” The strongest form of IoC is dependency injection, where the object remains “passive.”
Lazy Immutable variables (vals) can be declared lazy, meaning they will only be evaluated when they are read.
This feature is useful for expensive evaluations that may not be needed.
Lazy data structures can also be used to define infinite data structures that won’t exhaust system resources as long as only a finite subset of the structure is evaluated.
Linearization The algorithm used for a type to resolve member lookup, such as overridden methods, including calls to super.
Mailbox The queue where an Actor’s messages are stored until the Actor processes them in the Actor model of concurrency.
Main The entry function for an application that is invoked by the runtime is called main.
In Scala, a main method must be defined in an object.
Java, by way of contrast, requires a main method to be defined as a static method of a class.
MapReduce A divide-and-conquer strategy for processing large data sets in parallel.
The “reduce” phase combines the results of the subset calculations into a final result.
MapReduce frameworks handle the details of managing the operations and the nodes they run on, including restarting operations that fail for some reason.
The user of the framework only has to write the algorithms for mapping and reducing the data sets and computing with the subsets.
Member A generic term for a type, field, or method declared in a class or trait.
The results from a function’s invocations are saved so that when repeated invocations with the same inputs are made, the cached results can be returned instead of reinvoking the function.
Message In the Actor model of concurrency, messages are exchanged between Actors to coordinate their work.
In object-oriented programming, method invocation is sometimes referred to as “sending a message to an object,” especially in certain languages, like Smalltalk and, to some extent, Ruby.
Method A function that is associated exclusively with an instance, either defined in a class, trait, or object definition.
Mixin A narrowly focused encapsulation of state and behavior that is more useful as an adjunct to another object’s state and behavior, rather than standing on its own.
Multiple Inheritance In some languages, but not Scala, a type can extend more than one parent class.
Mutable Value A value that can be changed after it has been initialized.
The ability to refer to a method argument by name when calling the method.
It is useful in combination with default argument values for minimizing the number of arguments that have to be specified by the caller.
Nonterminal An item in a grammar that requires further decomposition into one or more nonterminals (including possibly a recursive reference to itself) and terminals.
Object A cohesive unit with a particular state, possible state transitions, and behaviors.
In Scala, the keyword object is used to declare a singleton explicitly, using the same syntax as class declarations, except for the lack of constructor parameters and auxiliary parameters (because objects are instantiated by the Scala runtime, not by user code)
To avoid confusion with objects, we use the term instance to refer to instances of classes and objects generically.
Object-Oriented Programming A form of programming that encapsulates state values and operations on that state, exposing a cohesive abstraction to clients of the object while hiding internal implementation details.
In Scala, a form of operator overloading is supported by allowing operator characters to be used as normal method names and by allowing methods with one argument to be invoked with infix notation.
Overloaded Functions Two or more functions defined in the same scope (e.g., as methods in a type or as “bare” functions) that have the same name but different signatures.
Packrat Parsers Parsers for parsing expression grammars (PEGs; see [Ford])
They have several benefits, such as lack of ambiguity and good performance characteristics.
The forthcoming Scala version 2.8 parser combinator library will add support for creating packrat parsers.
Parameterized types are defined with placeholder parameters for types they use.
When an instance of a parameterized type is created, specific types must be specified to replace all the type parameters.
Parent Type A class or trait from which another class or trait is derived.
Parsing expression grammars (PEGs) An alternative to context-free grammars that provide guaranteed linear-time parsing using memoization and unambiguous grammars ([PEG])
Partial Application Associated with currying, where a subset of a curried function’s arguments are applied, yielding a new function that takes the remaining arguments.
Partial Function A function that is not valid over the whole range of its arguments.
Pattern matching expressions can be converted to partial functions by the compiler in some contexts.
Path-Dependent Type A nested type T is unique based on its “path,” the hierarchical, period-delimited list of the enclosing packages, the enclosing types, and finally the type T itself.
For example, if T is nested in a trait and the trait appears in the linearizations of different types, then the instances in those Ts will have different types.
Pattern Matching Case expressions, usually in a match expression, that compare an object against possible types, type extractors, regular expressions, etc., to determine the appropriate handling.
Pimp My Library The name of a design pattern that appears to add new methods to a type.
It uses an implicit type conversion to automatically wrap the type in a wrapper type, where the wrapper type has the desired methods.
Precondition An assertion that should be true on entry to a method or other entry point.
Postcondition An assertion that should be true on exit from a method or other boundary point.
Postfix Notation A syntax supported by the compiler for methods with no argument, sometimes called nullary methods.
The method can be invoked without the period between the object and the method name.
Primary Constructor The main constructor of a class, consisting of the class body with the parameter list specified after the name of the class.
Primitive Type A non-object type on the underlying runtime platform (e.g., JVM and .NET)
Scala does not have primitive types at the source code level.
Rather, it uses value types, which are subclasses of AnyVal, to wrap runtime primitives, providing object semantics at the code level, while using boxing and unboxing of primitives at the byte code level to optimize performance.
Production A term used for each part of a grammar that decomposes a specific nonterminal into other nonterminals (perhaps including a recursive reference to the original nonterminal) and terminals.
Pure Used in the context of functions to mean that they are side-effect-free.
Recursion When a function calls itself as part of its computation.
A termination condition is required to prevent an infinite recursion.
Reference Type A type whose instances are implemented as objects on the runtime platform.
Referential Transparency The property of an expression, such as a function, where it can be replaced with its value without changing the behavior of the code.
This can be done with side-effect-free functions when the inputs are the same.
The primary benefit of referential transparency is that it is easy to reason about the behavior of a function, without having to understand the context in which it is invoked.
That makes the function easier to test, refactor, and reuse.
Refinement The term used for adding or overriding members in a type body for a compound type.
Reified Types Where the specific types used when instantiating a generic type are retained in the byte code, so the information is available at runtime.
This is a property of .NET byte code, but not JVM byte code, which uses type erasure.
To minimize incompatibilities, both the Java and .NET Scala versions use type erasure.
Scaladocs The API documentation generated from Scala source code using the scaladoc tool, analogous to Java’s Javadocs.
Scope A defined boundary of visibility, constraining what types and their members are visible within it.
Sealed Keyword for parent classes when all the direct subclasses allowed are defined in the same source file.
Self-Type Annotation A declaration in a trait or class that changes its type, sometimes with an alias for this.
A self type can be used to indicate dependencies on other traits that will have to be mixed into a concrete instance to resolve the dependency.
In some cases, these dependencies are used to ensure that an instance of the current type can be used as an instance of a dependent type in certain contexts (e.g., as used in the Observer Pattern in “Self-Type Annotations and Abstract Type Members” on page 317)
Side-Effect-Free Functions or expressions that have no side effects, meaning they modify no global or “object” state.
Signature For a function: the name, parameter list types, and return value.
For a method: also includes the type that defines the method.
Single Inheritance A class, object, or trait can extend one parent class.
In Scala, singletons are declared using the keyword object instead of class.
Singleton Types The unique type designator that excludes path dependencies.
Singleton types are not specifically the types of singleton objects, but singleton objects do have singleton types.
Stable Types Used in the context of path-dependent types, all but the last elements in the path must be stable, which roughly means that they are either packages, singleton objects, or type declarations that alias the same.
State As in, “the state of an object,” where it informally means the set of all the current values of an object’s fields.
Static Typing Loosely speaking, early binding of type information, sometimes referred to as binding to a reference, rather than the value to which the reference is assigned.
Strict Used to refer to data structures that are not lazy, i.e., they are defined “eagerly” by the expressions used to construct them.
Structural Type A structural type is like an anonymous type, where only the “structure” a candidate type must support is specified, such as members that must be present.
Structural types do not name the candidate types that can match, nor do any matching types need to share a common parent trait or class with the structural type.
Hence, structural types are a type-safe analog to duck typing in dynamically typed languages, like Ruby.
Literal symbols are written starting with a single “right quote,” e.g., 'name.
Tail-Call Recursion A form of recursion where a function calls itself as the last thing it does, i.e., it does no additional computations with the result of the recursive call.
The Scala compiler will optimize tail-call recursions into a loop.
Test-Driven Development A development discipline where no new functionality is implemented until a test has been written that will pass once the functionality is implemented.
Terminal A token in a grammar, such as a keyword, that requires no further decomposition.
Test Double When testing the behavior of one object, a test double is another object that satisfies a dependency in the object under test.
The test double may assist in the testing process, provide controlled test data and behaviors, and modify the interaction between the object under test and the test double.
Specific types of test doubles include “fakes,” “mocks,” and “stubs.”
Trait A class-like encapsulation of state (fields) and behavior (methods) that is used for mixin composition.
Zero or more traits can be mixed into class declarations or when creating instances directly, effectively creating an anonymous class.
Trampoline A loop that iterates through a list of functions, invoking each in turn.
The metaphor of bouncing the functions off a trampoline is the source of the name.
It can be used to rewrite a form of recursion where a function doesn’t call itself, but rather calls a different function that invokes the original function, and so forth, back and forth.
There is a proposal for the Scala version 2.8 compiler to include a trampoline implementation.
Tuple A grouping of two or more items of arbitrary types into a “Cartesian product,” without first defining a class to hold them.
They are first-class values, so you can assign them to variables, pass them as values, and return them from functions.
Type A categorization of allowed states and operations on those states, including transformations from one state to another.
The type of an instance is the combination of its declared class (explicitly named or anonymous), mixed-in traits, and the specific types used to resolve any parameters if the.
When indicated in the text, we sometimes use the term type to refer to a class, object, or trait generically.
Type Annotation An explicit declaration of the type of a value, e.g., count: Int, where Int is the type annotation.
A type annotation is required when type inference can’t be used.
In Scala, function parameters require type annotations, and annotations are required in some other contexts where the type can’t be inferred, e.g., for return values of some functions.
Type Bounds Constraints on the allowed types that can be used for a parameter in a parameterized type or assigned to an abstract type.
In Scala, the expression A <: B defines an upper bound on A; it must be a subtype or the same as B.
The expression A >: B defines a lower bound on A; it must be a supertype or the same as B.
Type Erasure A property of the generics type model on the JVM.
When a type is created from a generic, the information about the specific types substituted for the type parameters is not stored in the byte code and is therefore not available at runtime.
Type Inference Inferring the type of a value based on the context in which it is used, rather than relying on explicit type annotations.
Type Projections A way to refer to a type nested within another type.
For example, if a type t is declared in a class C, then the type projection for t is C#t.
Type Variance When a parameterized type is declared, the variance behavior under inheritance of each type parameter can be specified using a type variance annotation on the type symbol.
Type Variance Annotation On a type parameter in a parameterized types, a + prefixed to the type symbol is used to indicate covariance.
A - prefix on the type symbol is used to indicate contravariance.
No variance annotation is used to indicate invariance (the default)
Value The actual state of an instance, usually in the context of a variable that refers to the instance.
Value Type A subclass of AnyVal that wraps a corresponding non-object “primitive” type on the runtime platform (e.g., JVM and .NET)
All are declared abstract final so they can’t be used in new V expressions.
Instead, programs specify literal values, e.g., 3.14 for a Double or use methods that return new values.
All the instances of value types are immutable value objects.
The term value type is also used to mean the categories of types for instances.
That is, the type of every instance must fall into one of several categories: annotated types, compound types, function types, infix types, parameterized types, tuples, type designators, type projections, and singleton types.
If the variable is declared with the val keyword, a new value can’t be assigned to the variable.
If the variable is declared with the var keyword, a new value can be assigned to the variable.
The value a variable references must be typecompatible with the declared or inferred type of the variable.
View An implicit value of function type that converts a type A to B.
In the later case, the (=> A) is a by-name parameter.
An in-scope implicit type conversion method with the same signature can also be used as a view.
View Bounds A type specification of the form A <% B, which says that any type can be used for A as long as an in-scope view exists that can convert an A to a B.
Visibility The scope in which a declared type or type member is visible to other types and members.
We’d like to hear your suggestions for improving our indexes.
He specializes in Scala, Java, and Ruby, and works with clients on application design strategies that combine object-oriented programming, functional programming, and aspectoriented programming.
He also consults on Agile methods, such as Lean and XP.
Dean is a frequent speaker at industry and academic conferences on these topics.
Alex Payne is Platform Lead at Twitter, Inc., where he develops services that enable programmers to build atop the popular social messaging service.
Alex has previously built web applications for political campaigns, non-profits, and early-stage startups, and supported information security efforts for military and intelligence customers.
In his free time, Alex studies, speaks, and writes about the history, present use, and evolution of programming languages, as well as minimalist art and design.
Colophon The animal on the cover of Programming Scala is a Malayan tapir (Tapirus indicus), also called an Asian tapir.
It is a black-and-white hoofed mammal with a round, stocky body similar to that of a pig.
The Malayan tapir’s appearance is striking: its front half and hind legs are solid black, and its midsection is marked with a white saddle.
This pattern provides perfect camouflage for the tapir in a moonlit jungle.
Other physical characteristics include a thick hide, a stumpy tail, and a short, flexible snout.
Despite its body shape, the Malayan tapir is an agile climber and a fast runner.
It tends to have very poor vision, so it relies on smell and hearing as it roams large territories in search of food, tracking other tapirs’ scents and communicating via high-pitched whistles.
The Malayan tapir’s predators are tigers, leopards, and humans, and it is considered endangered due to habitat destruction and overhunting.
