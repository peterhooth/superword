The publisher offers discounts on this book when ordered in quantity.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in the book, and Manning Publications was aware of a trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning’s policy to have the books we publish printed on acid-free paper, and we exert our best efforts to that end.
Recognizing also our responsibility to conserve the resources of our planet, Manning books are printed on paper that is at least 15 percent recycled and processed without the use of elemental chlorine.
Your neck is straining as you bend your head back as far as it will go to take it all in.
If you squint, you can barely see something moving around at the top.
You’ve heard from people you trust that it’s worth climbing this wall.
But, you’re damned sure going to hurt yourself on the way up.
You can already see some of the jagged edges jutting out.
And what if it turns out that you don’t like what you see when you get there?
Learning difficult things is like this—and make no mistake: Scala is difficult to learn.
And you may very well not like what you see when you get to the top.
I’d guess that only a small fraction of developers learning a language like Scala ever put it to use.
But it’s almost always the climb that makes a challenge worth the effort.
By the time you reach the top, you’ll understand why those features exist, how they make your Scala programs better, and, more important, how they make you a more effective programmer.
You’ll still be sore from the bumps along the way but that pain will help you remember the lessons learned.
You may even find yourself happily and productively working full-time in Scala for years to come!
As worthwhile as a journey like this may be, you don’t want to climb a mountain this high alone, if you can help it.
When covering unfamiliar—even alien—territory you want a guide who can make it look easy.
He has a way of putting people at ease when describing complex subjects.
Scala itself isn’t that complex—it’s really just a bunch of simple pieces that join to form a deceptively.
Nilanjan has a talent for making us believe that those pieces really are simple and are there for unearthing the underlying principles that bind them together.
Indeed, even for the nuts and bolts of installation, configuration, and project compilation, reading this book is like having an experienced mentor accompany you every step of the way.
Some of the concepts in Scala in Action are going to be more foreign than others.
Musicians don’t become great by playing the songs they know over and over.
If you approach this climb properly, you’ll reach the top sharper, more openminded, and, best of all, less afraid.
Scala in Action targets developers who not only want to learn the language but also want to build real-world applications using Scala.
This book, in other words, covers not only the language and its latest features but also its ecosystem.
My goal was to pack a sufficient number of real-world examples along with the right mix of theory so readers can easily become comfortable with the language.
Scala is a feature-rich language and it is not possible to cover all of its features in one book, at least one of a reasonable size.
For that reason, I deliberately avoided some of the more advanced features of Scala.
I encourage you to think of this book as your first on Scala, a foundation on which to build, before you dive into the more advanced features Scala has to offer.
I had a great time writing this book and I hope you have a great time learning this xv.
I know you had a choice when it comes to Scala books; thank you for choosing this one.
I also thank all of the members of the Scala community.
Without their help and comments, I never could have imagined writing this book.
Special thanks go to Ivan Kirkpatrick and Clint Combs, my technical proofreaders, for their in-depth feedback on the text and the code.
Their corrections and comments on the manuscript as it was being written were invaluable.
I extend a special thanks to Lutz Hankewitz for his help during the writing process.
Without his thoughtful feedback, this book would have been incomplete.
Special thanks also to Chad Fowler for contributing the foreword and for endorsing my work.
Last but definitely not least, I would like to thank my wife Manisha for her support and patience as I spent countless weekends working on this book while she took care of the family without any complaints.
And if you are still undecided, please read the first chapter in this book, and I am sure that will change your mind.
The programming languages we use shape the way we think and how we solve programming issues.
And when faced with new programming languages and paradigms we try to map them to the languages we know.
I would discourage you from doing that when reading Scala in Action.
Scala is a new programming language that brings myriad new ideas to the Java virtual machine platform.
It is a multi-paradigm programming language that combines both functional and object-oriented languages.
It has its own set of best practices and idioms and by the end of this book what you have learned will also be helpful in other xvii.
Scala in Action has been updated to reflect the newest changes in Scala version 2.10
Who should read this book? This book is for all developers and hobbyists who like programming.
Most of the concepts discussed can be easily absorbed without any knowledge of Java, but having a basic knowledge of Java is a definite plus.
The book assumes that you are at least familiar with the JVM and its ecosystem.
There are plenty of available resources for the JVM and its toolset that augment this book.
Part 2 makes use of the concepts and shows how to use them in real world.
It is recommended that you read the book from beginning to end.
Having said that, if some chapters interest you more than others, feel free to jump ahead, but make certain you are familiar with the concepts introduced in the first five chapters (part 1)
Chapter 6 is also important because it introduces the build tool used to compile and build the accompanying code samples.
When reading this book, and working with its examples, I recommend that you keep the Scala interpreter (REPL) open at all times.
This is a programming book so keep programming as you read.
Part 1 introduces Scala and the programming paradigms it supports.
Chapter 1 explores why Scala should be your next programming language.
Picking up and learning a new language is a lot of work and you should read this chapter to understand what Scala has to offer and why Scala should be your next programming language.
Chapter 2 introduces basic Scala features you need to get started with the language.
This chapter also introduces one of the most important Scala tools, the Scala REPL.
If you have never used the REPL, this chapter will prepare you.
Chapter 3 explores the object-oriented programming side of things in Scala.
It introduces traits, case classes, and companion objects, all new innovations in OOP.
Chapter 4 focuses on the Scala collection library, one of the most powerful features of Scala.
The collection is one of the things that attracted me to this language.
This chapter will introduce new concepts gently so that you can start using Scala collection classes as soon as possible.
I promise once you get used to them there is no going back.
This chapter introduces what and why functional programming is important to learn.
Even if you don’t use Scala in your projects, some of the concepts introduced here can be used in any programming language.
Chapter 6 takes the first stab at building a large web application using Scala.
This chapter will show you how to build and organize a Scala project and it introduces the popular Simple Build Tool (SBT)
Chapter 7, a continuation of the previous chapter, introduces Scala frameworks you can use to connect to the database.
No Scala book could be complete without exploration of Scala’s type system.
But it’s no fun to talk about types unless you learn their value in design applications in the real world.
This chapter introduces types available in Scala and how you can use them to build reusable components.
An actor is a high-level abstraction over threads, allowing you to build and design concurrent applications.
Chapter 10 focuses on testing Scala applications and how you can use patterns to make your code more testable.
If you are thinking of taking your Scala application to production you need to learn to write automated tests and to understand dependency injections.
Chapter 11 demonstrates integration with Java, one of the core features of Scala.
Using Scala doesn’t necessarily mean you have to use only Scala frameworks.
In this chapter you will take Java frameworks like Spring, Hibernate, and Maven and use them with Scala.
At the end of the chapter you will build a large distributed and scalable application using Akka.
If you are interested in concurrent and parallel programming, this chapter is for you.
Code convention and downloads This book contains numerous code examples.
All the code is in a fixed-width font like this to separate it from ordinary text.
Code members such as method names, class names, and so on are also in a fixed-width font.
Source code examples in this book are fairly close to the samples that you’ll find online, but for the sake of brevity, we may have removed material such as comments from the code to fit it well within the text.
Code annotations accompany many of the source code listings, highlighting important concepts.
In some cases, numbered bullets link to explanations that follow the listing.
To run the samples, you’ll need to download some of the tools and languages used in this book.
Links in the text point you to places where you can get the relevant files.
Software requirements You can use any platform of your choice as long as you have a Java runtime version 1.5 or later running.
I have used Java 6 on Mac OS X for running and building all the code examples.
This page provides information on how to get on the forum once you are registered, what kind of help is available, and the rules of conduct on the forum.
Manning’s commitment to our readers is to provide a venue where a meaningful dialogue between individual readers and between readers and authors can take place.
It’s not a commitment to any specific amount of participation on the part of the author, whose contribution to the forum remains voluntary (and unpaid)
We suggest you try asking the author some challenging questions, lest his interest stray! The Author Online forum and archives of previous discussions will be accessible from the publisher’s website as long as the book is in print.
The book includes finely colored illustrations of figures from different regions of Croatia, accompanied by descriptions of the costumes and of everyday life.
Senj is the oldest town on the upper Adriatic coast, founded in the time before the Romans, some 3,000 years ago, on the hill Kuk, overlooking the sea.
Through the centuries, Senj was a prosperous seaport which was ruled by many different tribes and nation states in its long history.
The colors and embroidery patterns would tell the story of a person’s class and family affiliation.
The woman on the cover is wearing a richly embroidered blue dress and vest and a fancy lace apron and headscarf, which would have signaled both her status and her family ties.
Dress codes and lifestyles have changed over the last 200 years, and the diversity by region, town, or family, so rich at the time, has faded away.
It is now hard to tell apart the inhabitants of different continents, let alone of different hamlets or towns separated by only a few miles.
Perhaps we have traded cultural diversity for a more varied personal life—certainly for a more varied and fast-paced technological life.
In Scala in Action, chapter 1 focuses on Scala and why you should pick it as.
You’ll learn how Scala’s high-level features compare with programming languages you may be very familiar with.
If you’re an objectoriented programmer, you’ll quickly get comfortable with Scala; if you’ve used a functional programming language, Scala won’t look much different because Scala supports both programming paradigms.
Scala is one of those rare languages that successfully integrates both objectoriented and functional language features.
This makes it powerful because it gives you more in your toolbox to solve programming problems.
If you have existing Java applications and are looking for a language that will improve your productivity and at the same time reuse your existing Java codebase, you’ll like Scala’s Java integration and the fact that Scala runs on the JVM platform.
It’s important, when learning something new, to become comfortable in the heretofore unknown environment.
Chapter 2 stays within the middle of the road, helping you become comfortable with the basics of Scala and its environment so you can start working with it and writing Scala programs.
Early on, the focus is on only the Scala interpreter and its REPL environment to keep things simple, but you’ll also learn about the basic Scala types, functions, for-comprehension, pattern matching, among other things.
Chapter 3 introduces the object-oriented features of Scala, including some not available in other statically typed languages.
You’ll build a Scala driver for MongoDB, a scalable document-oriented database.
You’ll explore how to use traits when building Scala applications, and learn about the importance of Scala case classes.
To understand and benefit from Scala collections, you need to know two concepts: type parameterization and higher-order functions.
Type parameterization allows you to create types that take another type as a parameter (similar to Java generics)
Higher-order functions let you create functions that take other functions as parameters.
These two concepts allow you to create generic and reusable components, like Scala collections.
The Scala collection is one of Scala’s most powerful features.
The library implements all the common data structures you need, making it essential for every Scala developer.
A recent addition to the collection library is parallel collections.
Scala parallel collections allow you to solve data parallelism problems in Scala with ease.
In some cases functional programming is obvious; other times it is mixed with object-oriented constructs of Scala.
To become a useful and productive developer, you also need to be familiar with all the toggles and gizmos that make up the language infrastructure.
Before I make the case for why Scala should be your next programming language, it’s important to understand what Scala is.
It’s a feature-rich language that’s used in various types of applications, starting with building a large messaging layer for social networking sites such as Twitter1 to creating an application.
Scala is a general-purpose programming language that runs on Java Virtual Machine (JVM) and .NET platforms.
But the recent explosion of programming languages on JVM, .NET, and other platforms raises a question that every developer.
Because of this scala-bility, the name of the language is Scala.
This chapter explores the high-level features of the language and shows how they compare to the programming languages you may be very familiar with.
This will help you to choose Scala as your next programming language.
If you’re an object-oriented programmer, you’ll quickly get comfortable with the language; if you’ve used a functional programming language, Scala won’t look much different because Scala supports both programming paradigms.
Scala is one of those rare languages that successfully integrates both object-oriented and functional language features.
This makes Scala powerful because it gives you more in your toolbox to solve programming problems.
If you have existing Java applications and are looking for a language that will improve your productivity and at the same time reuse your existing Java codebase, you’ll like Scala’s Java integration and the fact that Scala runs on the JVM platform.
Even though Scala is fairly new in the language space, it has gained the support of the programming community, which is growing every day.
Scala is a rich language in terms of features available to programmers, so without wasting time let’s dive into some of them.
The main difficulty to overcome is that Scala programs make heavy use of the Java JDK, which is not available out of the box in the .Net platform.
The examples in this book are tested only on a JVM.
The popularity of programming languages such as Java, C#, and Ruby has made objectoriented programming (OOP) widely acceptable to the majority of programmers.
What’s Scala? as its name implies, is a programming paradigm that uses objects.
Think of objects as data structures that consist of fields and methods.
Object orientation helps to provide structure to your application using classes and objects.
It also facilitates composition so you can create large applications from smaller building blocks.
There are many OOP languages in the wild, but only a few are fit to be defined as pure object-oriented languages.
What makes a language purely object-oriented? Although the exact definition of the term depends on whom you ask, most will agree a pure object-oriented language should have the following characteristics:
In Scala this expression is interpreted as 1.+(2) by the Scala compiler.
An identifier in Scala is either a sequence of letters and digits starting with a letter or a sequence of operator characters.
Along with the pure object-oriented features, Scala has made some innovations on OOP space:
Modular mixin composition—This feature of Scala has traits in common with both Java interfaces and abstract classes.
You can define contracts using one or more traits and provide implementations for some or all of the methods.
Self-type—A mixin doesn’t depend on any methods or fields of the class that it’s mixed into, but sometimes it’s useful to use fields or methods of the class it’s mixed into, and this feature of Scala is called self-type.
Type abstraction—There are two principle forms of abstraction in programming languages: parameterization and abstract members.
Scala supports both forms of abstraction uniformly for types and values.
A mixin could also be viewed as an interface with implemented methods.
Before I describe Scala as a functional language, I’ll define functional programming in case you’re not familiar with it.
Functional programming is a programming paradigm that treats computation as the evaluation of mathematical functions and avoids state and mutable data.
Functional programming takes more of a mathematical view of the world, where programs are composed of functions that take certain input and produce values and possibly other functions.
The building blocks of functional programming are neither objects nor procedures (C programming style) but functions.
The simple definition of functional programming is programming with functions.
It’s important to understand what is meant by function here.
A function relates every value of the domain (the input) to exactly one value of the codomain (the output)
Figure 1.1 depicts a function that maps values of type X to exactly one value of Y.
Another aspect of functional programming is that it doesn’t have side effects or mutability.
The benefits of not having mutability and side effects in functional programs are that the programs are much easier to understand (it has no side effects), reason about, and test because the activity of the function is completely local and it has no external effects.
Another huge benefit of functional programming is ease of concurrent programming.
Concurrency becomes a nonissue because there’s no change (immutability) to coordinate.
In the case of an immutable object, the contents of the object can’t be altered if you have a reference to it.
It’s easy to create a mutable object; all you have to do is provide access to the mutable state of the object.
The disadvantage of mutable objects is keeping track of the changes.
For immutable objects, you don’t have to worry about these situations.
Figure 1.1 A pure function that maps values of X to exactly one value of Y.
Functional programming languages that support this style of programming provide at least some of the following features:
Some of these features are probably unfamiliar if you haven’t done functional programming before.
How do mathematical functions relate to functions in programming? In mathematics, a function is a relation between a given set of elements called the domain (in programming we call this input) and a set of elements called the codomain (in programming we call this output)
The function associates each element in the domain with exactly one element in the codomain.
For example, f(x) = y could be interpreted as x has a relationship f with y or x maps to y via f.
If you write your functions keeping in mind the definition of the mathematical function, then for a given input your function should always return the same output.
One other interesting property of a mathematical function is referential transparency, which means that an expression can be replaced with its result.
In the case of addFunction, we could replace all the calls made to it with the output value, and the behavior of the program wouldn’t change.
Side effects A function or expression is said to have a side effect if, in addition to producing a value, it modifies some state or has an observable interaction with calling functions or the outside world.
A function might modify a global or a static variable, modify one of its arguments, raise an exception, write data to a display or file, read data, or call other functions having side effects.
In the presence of side effects, a program’s behavior depends on its history of execution.
That means that in Scala, every function is a value (like some integer value 1 or some string value "foo"), and like any values, you can pass them as parameters and return them from other functions.
Here val represents a single assignment variable (like Java final variables) with a value that can’t be changed after the assignment.
In the following example you’ll see how to pass functions as parameters to another function and get the result:
Based on the output you can see that map is invoking the given function for each element in the list.
Don’t worry about the syntax right now; you’ll learn about it in detail in later chapters.
Scala is a multi-paradigm language because it supports both functional and OOP programming.
Scala is the first to unify functional programming and OOP in a statically typed language for the JVM.
The obvious question is why we need more than one style of programming.
The goal of multi-paradigm computing is to provide a number of problem-solving styles so a programmer can select the solution that best matches the characteristics of the problem to be solved.
This provides a framework where you can work in a variety of.
Is Scala a pure functional language? Scala, put simply, is not a pure functional language.
In a pure functional language modifications are excluded, and variables are used in a mathematical sense, with identifiers referring to immutable and persistent values.
Scala supports both types of variables: single-assignment variables (also called values) that don’t change their value throughout their lifetime and variables that point to a mutable state or could be reassigned to other objects.
Even though you should use immutable objects whenever possible, Scala as a language doesn’t provide any restrictions.
A good rule of thumb is to always default to val and use variables when it’s absolutely necessary.
To me, the fundamental property of a functional language is treating functions as values, and Scala does that well.
What’s Scala? styles and mix the constructs from different ones.
Functional programming makes it easy to build interesting things from simple parts (functions), and OOP makes it easy to adopt and extend complex systems using inheritance, classes, and so on.
According to researcher Timothy Budd,5 “Research results from the psychology of programming indicate that expertise in programming is far more strongly related to the number of different programming styles understood by an individual than it is the number of years of experience in programming.”
How can Scala combine these two different and almost opposite paradigms into one programming language? In the case of OOP, building blocks are objects, and in functional programming building blocks are functions.
One of the benefits of combining functional programming with object-oriented programming in Scala is treating functions as objects.
Scala, being a functional language, treats functions as values, and you saw one example of assigning a function to a variable.
Because all values in Scala are objects, it follows that functions are objects too.
When the compiler encounters such a call, it replaces the function parameter with an object, as in the following:
What’s going on here? Without diving in too deeply for now, when the Scala compiler encounters functions with one parameter, it replaces that call with an instance of class scala.Function1, which implements a method called apply.
If you look carefully, you’ll see that the body of the function is translated into the apply method.
Likewise, Scala has Function objects for functions with more than one parameter.
As the popularity of multi-paradigm programming increases, the line between functional and object-oriented programming will fade away.6 As we continue to explore Scala, you will see how we blend both functional programming and OOP to solve problems.
Scala stands for scalable language.7 One of the design goals of Scala is to create a language that will grow and scale with your demand.
Scala is suitable for use as a scripting language, as well as for large enterprise applications.
Scala also provides a unique combination of language mechanisms that makes it easy to add new language constructs in the form of libraries.
You could use any method as an infix or postfix operator, and closures in Scala can be passed as “pass by name” arguments to other functions (see the next listing)
These features make it easier for developers to define new constructs.
Let’s create a new looping construct called loopTill, which is similar to the while loop in the following listing.
In this code you’re creating a new loopTill construct by declaring a method called loopTill that takes two parameters.
As long as the condition evaluates to true, the loopTill function will execute the given closure.
Even though it’s defined outside the closure, you could still use it inside.
The second parameter in the loopTill example is a closure, and in Scala that’s represented as an object of type scala.Function0
Extending a language with a library is much easier than extending the language itself because you don’t have to worry about backward compatibility.
For example, Scala actor implementation (defined in section 1.2.2) is provided as a library and isn’t part of the Scala language.
When the first actor implementation didn’t scale that well, another actor implementation was added to Scala without breaking anything.
The best thing about Java is not the language but the JVM.
A JVM is a fine piece of machinery, and the Hotspot team has done a good job in improving its performance over the years.
Being a JVM language, Scala integrates well with Java and its ecosystem, including tools, libraries, and IDEs.
Now most of the IDEs ship with the Scala plug-in so that you can build, run, and test Scala applications inside the IDE.
The current crisis don’t have to get rid of all the investments you’ve made in Java so far.
Instead you can reuse them and keep your ROI coming.
Scala compiles to Java byte code, and at the byte-code level you can’t distinguish between Java code and Scala code.
You could use the Java class file disassembler javap to disassemble Scala byte code (chapter 11 looks into this in more detail) as you could for Java classes.
Another advantage of running Scala on a JVM is that it can harness all the benefits of JVM-like performance and stability out of the box.
And being a statically typed language, Scala programs run as fast as Java programs.
I go through all these features of Scala in more detail throughout the book, but I still haven’t answered the question—why Scala?
An interesting phenomenon known as “Andy giveth, and Bill taketh away” comes from the fact that no matter how fast processors become, we software people find a way to use up that speed.
With software you’re solving more and more complex problems, and this trend will keep growing.
The key question is whether processor manufacturers will be able to keep up with the demand for speed and processor power.
According to Moore’s law, the number of transistors per square inch on a chip will double every 18 months.
Unfortunately, Intel and other CPU manufacturers are finally hitting the wall8 with Moore’s law and instead are taking the route of multicore processors.
The good news is that processors are going to continue to become more powerful, but the bad news is that our current applications and programming environments need to change to take advantage of multicore CPUs.
How can you take advantage of the new multicore processor revolution? Concurrency.
Concurrency will be, if it isn’t already, the way we can write software.
Who doesn’t want efficient and good performance from their applications? We all do.
A few people have been doing parallel and concurrent programming for a long time, but it still isn’t mainstream or common among enterprise developers.
One reason is that concurrent programming has its own set of challenges.
In the traditional thread-based concurrency model, the execution of the program is split into multiple concurrently running tasks (threads), and each operates on shared memory.
This leads to hard-to-find race conditions and deadlock issues that can take weeks and.
It’s not the threads but the shared memory that’s the root of all the concurrency problems.
The current concurrency model is too hard for developers to grok, and we need a better concurrent programming model that will help developers easily write and maintain concurrent programs.
Scala takes a totally different approach to concurrency: the Actor model.
The basic Actor architecture relies on a shared-nothing policy and is lightweight in nature.
It’s not analogous to a Java thread; it’s more like an event object that gets scheduled and executed by a thread.
The Scala Actor model is a better way to handle concurrency issues.
Its shared-nothing architecture and asynchronous message-passing techniques make it an easy alternative to existing thread-based solutions.
Traditionally, programming multicore processors is more complex than programming uniprocessors and it requires platform-specific knowledge.
To make parallel programming easier, Scala provides higher abstractions in the form of a parallel collections library that hides parallel algorithms.
For example, to square up each element of a List in parallel, you can use parallel collections like the following:
In this case the .par transforms the List into a parallel collection that implements the map method using a parallel algorithm.
Behind the scenes a parallel collections library will fork threads necessary to execute the map method using all the cores available in a given host machine.
The parallel collections library is a new addition to Scala and provides parallel versions of most collection types.
Erlang was the first programming language to implement the Actor model.
Erlang is a general-purpose concurrent programming language with dynamic typing.
After the success of the Erlang Actor model at Ericsson, Facebook, and Yahoo!, it became a good alternative for handling concurrency problems, and Scala inherited it.
In Scala, actors are implemented as a library that allows developers to have their own implementation.
Java made object-oriented programming easier for developers, compared with C/C++, and was quickly adopted into the industry.
Every new feature added to the language brings with it more boilerplate code for the programmer; even small programs can become bloated with annotations, templates, and type information.
Java developers are always looking for new ways to improve productivity using third-party libraries and tools.
But is that the answer to the problem? Why not have a more productive programming language?
Adding libraries and tools to solve the productivity problem sometimes backfires, adding complexity to applications and reducing productivity.
I’m not saying that you shouldn’t rely on libraries; you should whenever it makes sense.
But what if you had a language built from the ground up from ideas like flexibility, extensibility, scalability—a language that grows with you?
Developers’ needs today are much different than they used to be.
In the world of Web 2.0 and agile development, flexibility and extensibility in the programming environment are important.
Developers need a language that can scale and grow with them.
It will make you productive, and it will allow you to do more with less code and without the boilerplate code.
To see the succinctness of Scala, you have to dive into the code.
The next two listings provide a simple example of finding an uppercase character in a given string, comparing Scala and Java code.
In this code you’re iterating through each character in the given string name and checking whether the character is uppercase.
If it’s uppercase, you set the hasUpperCase flag to true and exit the loop.
Now let’s see how we could do it in Scala.
Listing 1.2 Finding an uppercase character in a string using Java.
In Scala you can solve this problem with one line of code.
Even though it’s doing the same amount of work, most of the boilerplate code is taken out of the programmer’s hands.
In this case you’re calling a function called exists on name, which is a string, by passing a predicate that checks whether the character is true, and that character is represented by _
This demonstrates the brevity of the Scala language and its readability.
Now let’s look at the following listing, where you create a class called Programmer with the properties name, language, and favDrink.
This is a simple POJO (plain old Java object) with three properties—nothing much to it.
In Scala you could create a similar class in one line, as in the following listing.
In this example you’re creating a similar class called Programmer in Scala but with something called a primary constructor (similar to a default constructor in Java) that takes three arguments.
Yes, you can define a constructor along with the class declarationanother example of succinctness in Scala.
The var prefix to each parameter makes the Scala compiler generate a getter and setter for each field in the class.
That’s impressive, right? You’ll look into more interesting examples throughout the book.
Listing 1.3 Finding an uppercase character in a string using Scala.
Coming from a dynamic language when you go deeper into Scala.
For now, it’s clear that with Scala you can do more with fewer lines of code.
You could argue that the IDE will automatically generate some of this boilerplate code, and that’s not a problem.
But I’d argue that you’d still have to maintain the generated code.
Scala’s succinctness will be more apparent when you look into much more involved examples.
It’s hard to find developers these days who haven’t heard of or played with Ruby, Groovy, or Python.
The biggest complaint from the dynamic language camp about statically typed languages is that they don’t help the productivity of the programmer and they reduce productivity by forcing programmers to write boilerplate code.
And when dynamically typed languages are compared with Java, obvious things like closures and extensibility of the language are cited everywhere.
Before going into the issue of static versus dynamically typed languages, let’s look into Scala’s support for closures and mixin.
The following listing shows how to count the number of lines in a given file in Ruby.
Simple! The following listing shows how you can do this in Scala.
You could solve this in many ways in Scala; here you’re using the map method to return 1 for each line, then using the sum method to calculate the total count.
Scala supports mixin composition with something called traits, which are similar to an abstract class with partial implementation.
For example, you can create a new type of collection which allows users to access file contents as iterable, by mixing the Scala Iterable trait.
Now if you mix in the Scala Iterable, your new FileAsIterable will become a Scala Iterable and will start supporting all the Iterable methods:
Listing 1.6 Counting the number of lines in a file in Ruby.
Listing 1.7 Counting the number of lines in a file in Scala.
In this case you’re using the foreach method defined in the Iterable trait and printing each line in the file.
Using this feature you can dynamically add methods and fields to a type at runtime.
This is very similar to the method_missing feature of Ruby and is quite useful if you’re building a domain-specific language (DSL)
For example, Scala map is a collection of key value pairs and if you want to access the value associated with a key you can do something like the following:
Using Dynamic we can easily change that so that we can access the keys as if they were part of a type:
The magic ingredient in this case is the selectDynamic method.
When the Scala compiler checks that foo is not part of the type it doesn’t give up immediately.
If the type is a subtype of Dynamic it looks for the selectDynamic method and invokes it.
If the method is not provided, you will get a compilation error.
Scala also supports something called implicit conversion, which is similar to Ruby open classes but scoped and compile time checked.
With all that said and done, Scala is still a statically typed language.
But if you’ve gone through the examples in the previous section, you’ve probably already figured out that Scala’s static typing doesn’t get in your face, and it almost feels like a dynamically typed language.
A number variable can’t hold anything other than a number.
Types are determined and enforced at compile time or declaration time.
It’s possible to successively put a number and a string inside the same variable.
The size and the complexity of the software you’re building are growing every day, and having a compiler do the type checking for you is great.
It reduces the time you need to spend fixing and debugging type errors.
In a statically typed language like Scala, if you try to invoke a length method on a number field, the Scala compiler will give you a compilation error.
In a dynamically typed language you’ll get a runtime error.
Another benefit of a statically typed language is that it allows you to have powerful tools like refactoring and IDEs.
Having an IDE might not interest you because of powerful editing tools like Emacs and TextMate, but having refactoring support is great when working on large codebases.
Statically typed languages are more constraining than dynamically typed languages, and some force you to provide additional type information when you declare or call a function.
But having constraints is useful when building a large application because they allow you to enforce a certain set of rules across the codebase.
Scala, being a type-inferred language, takes care of most of the boilerplate code for the programmer (that’s what compilers are good for, right?) and takes you close to a dynamically typed language, but with all the benefits of a statically typed language.
The compiler can deduce that the variable s in s="Hello" will have the type string because "hello" is a string.
The type inference ensures the absence of any runtime type errors without putting a declaration burden on the programmer.
To demonstrate how type inference works, create an array of maps in Scala:
If you run this Scala code in the Scala REPL, you’ll see the following output: computers:
Even though you only specified an array of maps with key and value, the Scala compiler was smart enough to deduce the type of the array and the map.
And the best part is that now if you try to assign the value of name to some integer type variable somewhere in your codebase, the compiler will complain about the type mismatch, saying that you can’t assign String to an integer-type variable.
Scala is the first statically typed language to fuse functional and OOP into one language for the JVM.
Scala has made some innovations in OOP (mentioned previously) so that you can create better component abstractions.
Scala inherits lots of ideas from various programming languages of the past and present.
To start with, Scala adopts its syntax from Java/C# and supports both JVM and Common Language Runtime (CLR)
Some would argue that Scala’s syntax is more dissimilar than similar to that of Java/C#
You saw some Scala code in previous sections, so you can be the judge of that.
In Scala every value is an object, and every operation is a method call.
Scala also supports universal nesting and uniform access principles (see the following listing), and these are borrowed from Algol/Simula and Eiffel, respectively.
In Scala variables and functions without parameters are accessed the same way.
Here you’re accessing a field and a method of the instance of the UAPExample class, and to the caller of the class it’s transparent.
Scala’s functional programming constructs are similar to those of the metalanguage (ML) family of languages, and Scala’s Actor library is influenced by Erlang’s Actor model.
Based on this list you may realize that Scala is a rich language in terms of features and functionality.
You won’t be disappointed by Scala and will enjoy learning this language.
In this chapter I quickly covered many concepts, but don’t worry because I’m going to reiterate these concepts throughout the book with plenty of examples so that you can relate them to real-world problems.
You learned what Scala is and why you should consider learning Scala as your next programming language.
Scala’s extensible and scalable features make it a language that you can use for small to large programming problems.
Its multi-paradigm model provides programmers with the power of abstractions from both functional and OOP models.
Functional programming and actors will make your concurrent programming easy and maintainable.
Scala’s type inference takes care of the pain of boilerplate code so that you can focus on solving problems.
In the next chapter you’ll set up your development environment and get your hands dirty with Scala code and syntax.
Some of the concepts in this chapter, like pattern matching and forcomprehensions, are functional programming concepts.
But because Scala is a multi-paradigm language, I cover them with the rest of the nonfunctional programming concepts.
The objective of this chapter is for you to become comfortable with the basics of the Scala language and its environment so you can start writing Scala programs.
Early on, I focus on only the Scala interpreter and its REPL environment (you’ll see it in the next section) to keep things simple.
In chapter 1 you learned what Scala is and how it compares to other programming languages.
I described the concepts at a high level, and in the next few chapters you’re going to revisit those concepts and explore them in detail using examples.
Before going any farther, make sure your Scala installation is working.
Throughout the chapter you’re going to work with various Scala examples, and it would be better if you tried them in the Scala interpreter at the same time.
The easiest way to get started with Scala is by using the Scala interpreter, an interactive shell for writing Scala expressions and programs.
To start the Scala interpreter in interactive mode, type scala at the command prompt.
If everything goes fine with your installation, you’ll see something like the following:
I’m running Scala version 2.10.0, and all the code examples should work for this version and above.
At the Scala prompt type 42 and press Enter, and you should see something like this:
If you type the variable name, in this case res0, at the prompt, you’ll get similar output:
You could loop readevaluate-print steps repeatedly inside the Scala interpreter.
Now you’ll write your first “Hello world” program in Scala:
You’re evaluating the println function by passing the "Hello world" string as a parameter, and Scala outputs the same string.
Scala Predef (part of the standard library) maps println to Console.println for you so you don’t have to prefix it with Console when using it.
In chapter 1, I mentioned that Scala integrates well with Java but didn’t provide an example.
If you don’t remember all the methods that you could possibly invoke on myList, don’t worry because the Scala interpreter will help you with that.
Type in myList, followed by a period, and press Tab; the Scala interpreter lists all the methods you can invoke.
Not only does it list all the methods associated with a data type, it also autocompletes variable names and class names that are known to the interpreter.
I encourage you to spend some time with the Scala interpreter, explore the available options, and keep it handy when working through the examples in this book.
Consider REPL as an essential part of learning a new language because it gives quick feedback.
In this section I round out basic Scala information with examples so you can gradually get comfortable with the language.
You’ll use the Scala REPL to try out the examples, but you can use any of the development environments mentioned in the previous section that suit you.
You’ll learn about two types of Scala variables, var and val, how they’re used, and how they’re different.
You’ll also learn about the Scala functions, how to define them, and ways you can invoke them.
For example, :cp tools/junit.jar will try to find a JUnit JAR file relative to your current location and, if found, it will add the JAR file to your classpath so that you can refer to the classes inside the JAR file.
If you want to investigate existing Scala code, you could load the file into the Scala interpreter, and all the definitions will be accessible to you.
If you’re a Java programmer, you’ll be glad to know that Scala supports all the basic value types (primitives): Byte, Short, Int, Float, Double, Boolean, and Char.
Table 2.2 shows all eight basic value types supported by Scala.
In Scala all the basic types are objects, and they’re defined under the scala package.
In Scala all the basic types are declared using initial caps.
Instead of declaring something as int, in Scala it’s declared as Int.
In earlier versions of Scala, programmers could use lowercase and uppercase interchangeably, but from version 2.8 on, if you declare any variable with int, you’ll get a compiler error:
Even though the full qualifying name of Int is scala.Int, you can use only Int.
A small fact about Scala Predef The Scala compiler implicitly imports java.lang, the scala package, and an object called scala.Predef to every compilation unit or Scala program.
In the case of .NET instead of java.lang, it imports the system package.
The Predef object defines standard functions and type aliases for Scala programs.
Because this object is imported automatically, all members of this object are available to you.
Predef is interesting, and you can learn a lot about Scala by looking at the scaladoc or source of the scala.Predef object.
To see all packages that are automatically imported, use the :imports command inside REPL:
In this case, java.lang, scala, and scala.Predef packages are automatically imported when you start a REPL session.
Of the basic types defined in table 2.2, Byte, Short, Int, Long, and Char are collectively called integral types.
The integer literals can represent decimal, hexadecimal, and octal numbers.
You could create a string object by using the string literal "one" and also using the new keyword, as in new String("one")
Because integer literals are usually integers, Scala infers the type as integer, but if you want a Long type, you could add the suffix L or l:
One thing to note while using hexadecimal and octal numbers is that the Scala interpreter always evaluates the result as a decimal number.
You can specify the type of variable (sometimes called only value; more on this later) that you want when you think Scala type inference isn’t producing the result you are seeking.
If you declare a variable with an integer literal, for example, Scala creates a variable of type integer unless it can’t fit the value within the range of integer values.
What do you do if you want a Byte type variable? You declare the type Byte when creating the variable:
Floating-point literals are composed of digits with a decimal point and an exponent part.
But both the decimal point and the exponent part are optional.
Floating-point literals are of type Float when they’re suffixed with F or f and are Double otherwise:
You can also create a Double variable with an exponent part.
A character literal is a single character enclosed in quotes.
The character can be a printable Unicode character or an escape sequence:
You can also assign special character literal escape sequences to Char type variables:
This raises an interesting corner case when invoking methods on floating literals.
If you try it as 1.toString without the space, it invokes the toString method defined in the Int object.
This is necessary only when the method name starts with a letter.
Scala takes programming with Unicode characters to the next level.
You can use not only literals but also printable Unicode characters as variable and method names.
To create a variable name ans with the value 42 using Unicode characters, you would do something like the following:
Using Unicode characters for naming variables or functions in a program is a way to get yelled at by your peers, but in some contexts it improves the readability of the code.2 In the following example Unicode characters are used in the variable and method name:
Before trying out these examples, make sure your editor supports Unicode encoding.
A string literal is a sequence of characters in double quotes.
The characters are either printable Unicode characters or escape sequences.
If the string literal contains a double-quote character, it must be escaped with a slash (\):
The value of the string literal is of type String.
The sequence of characters is arbitrary, except that it may not contain a triple quote, and it doesn’t even necessarily have to be printable:
The output of the multiLine variable has leading whitespaces, and maybe you don’t want that.
There’s an easy fix—invoking a method called stripMargin strips out the margin for you:
The first | (vertical bar) or margin is added by the interpreter when you press Enter without completing the expression, and the second one is the one that you added as a margin for the multiline string.
When the stripMargin method finds those margin characters, it strips out the leading whitespaces.
I find multiline strings helpful when creating data sets for unit tests.
Treating a string as an immutable collection makes sense because it’s a collection of characters, and a string is an immutable object.
Scala still has Rich type wrappers for other basic types like RichInt, RichBoolean, RichDouble, and so on.
Typically, working with XML means using third-party parsers and libraries, but in Scala it’s part of the language.
Scala supports XML literals, where you can have XML fragments as part of the code:
When you type this expression into the Scala interpreter, you’ll get the following:
As you can see, Scala executes the code inside the curly braces and replaces it with the output of the code.
The code defined within curly braces is called Scala code blocks.
In Scala there are two ways you can define variables: val and var.
A val is a single assignment variable, sometimes called value.
Once initialized a val can’t be changed or reassigned to some other value (similar to final variables in Java)
On the other hand, var is reassignable; you can change the value of the variable over and over again after initial assignment:
The Scala interpreter does a good job of inferring the type of variable based on the value, but there are times when you’d like to, or have to, specify the type.
You can specify the type of variable after the variable name, separating it by a colon (:)
There’ll be situations where you want to declare a variable without assigning a value because you don’t yet know the value.
In cases like these you can use the Scala placeholder character (_) to assign a default value:
Sometimes you may want to declare a type whose value gets calculated based on some time-consuming operation, and you don’t want to do that when you declare the variable; you want to initialize it lazily because by default Scala evaluates the value assigned to var or val when it’s declared.
In the last line, typing b forces the evaluation of the b, and because it wasn’t evaluated when b was declared, it uses the latest value of a.
The lazy keyword is allowed only with val; you can’t declare lazy var variables in Scala.
The variable declaration can sometimes have a pattern on the left side.
Say you want to extract the first element of a List and assign it to a variable.
You can do that using a pattern on the left side along with the variable declaration:
The :: (called cons) is a method defined in List.
I cover more about pattern matching later in this chapter.
Earlier I made the argument for immutability and why you should always prefer immutability to mutability.
Keeping that in mind, always start with val when declaring variables in Scala and change to var when it’s absolutely necessary.
Functions are building blocks in Scala, and in this section you’re going to explore that topic.
To define a function in Scala, use the def keyword followed by the method name, parameters, optional return type, =, and the method body.
Figure 2.1 shows the syntax of the Scala function declaration.
Use a colon (:) to separate the parameter list from the return type.
The equals sign (=) is used as a separator between the method signature and the method body.
Let’s drop the parameter for the time being; you’ll come back to parameters later.
The return type of a Scala function is optional because Scala infers the return type of a function automatically.
There are situations where it doesn’t work, but don’t worry about that until later.
The significance of = after the method signature isn’t only to separate the signature from the method body but also to tell the Scala compiler to infer the return type of your function.
If you omit that, Scala won’t infer your return type:
In this case when you invoke the function using the function name and (), you’ll get no result.
Unit in Scala is like void in Java, and it means that the method doesn’t return anything.
If you’re creating a library and plan to expose your functions as a public API, it’s a good practice to specify the return type for the users of the library.
In any case, if you think it’s not clear from the function what its return type is, either try to improve the name so that it communicates its purpose better or specify the return type.
Your myFirstMethod is simple: it returns the string "exciting times ahead" and when you have a function like that, you also drop the curly braces from the method body:
If you invoke the function, you’ll get the same result.
In Scala it’s always possible to take out unnecessary syntax noise from the code.
Because you aren’t passing any parameters, you can take out the unused () from the declaration, and it almost looks like a variable declaration, except that instead of using var or val you’re using def:
When calling the function you can also leave out the parentheses:
If the function has side effects, the common convention is to use “()” even though it isn’t required.
Returning to function parameters, the following function called max takes two parameters and returns the one that’s the greater of the two:
By now you probably have figured out that specifying return is optional in Scala.
You don’t have to specify the return keyword to return anything from the function.
In the previous case, if the if condition evaluates to true, then a is the last expression that is executed, so a is returned; otherwise b is returned.
Even though the return type is optional, you do have to specify the type of the parameters when defining functions.
Scala type inference will figure out the type of parameters when you invoke the function but not during the function declaration.4,5
Sometimes it becomes necessary to create a function that will take an input and create a List from it.
But the problem is you can’t determine the type of input yet.
Someone could use your function to create a List of Int, and another person could use it to create a List of String.
In cases like this you create a function in Scala by parameterized type.
The parameter type will be decided when you invoke the function:
When declaring the function, you denote the unknown parameterized type as A.
Now when your toList is invoked, it replaces the A with the parameter type of the given parameter.
In the method body you create an instance of immutable List by passing the parameter, and from the REPL output it’s clear that List is also using a parameterized type.
Daniel Spiewak, posted at Code Commit, “What is Hindley-Milner? (and why is it cool?),” undated, http://
Type inference If you have a background in Haskell, OCaml, or any other type of inferred programming language, the way Scala parameters are defined could feel a bit weird.
The reason is that Scala doesn’t use the Hindley-Milner algorithm to infer type; instead Scala’s type inference is based on declaration-local information, also known as local type inference.
Type inference is out of the scope of this book, but if you’re interested you can read about the Hindley-Milner type inference algorithm and why it’s useful.
The only difference to remember for now is that Java uses angle brackets (<>) and Scala uses square brackets ([])
Another Scala convention for naming the parameterized types is that they normally start at A and go up to Z as necessary.
In Scala you can also pass a function as a parameter to another function, and most of the time in those cases I provide an inline definition of the function.
This passing of functions as a parameter is sometimes loosely called closure (passing a function isn’t always necessarily closure; you’ll look into that in chapter 4)
Scala provides a shorthand way to create a function in which you write only the function body, called function literals.
In this test you want to add all the elements of a List using function literals.
This demonstrates a simple use of function literals in Scala.
The foldLeft method takes two parameters: an initial value and a binary operation.
It applies the binary operation to the given initial value and all the elements of the list.
It expects the binary operation as a function that takes two parameters of its own to perform the operation, which in our case will be addition.
If you can create a function that will take two parameters and add them, you’re finished with the test.
The foldLeft function will call your function for every element in the List, starting with the initial value:
In this case the function (a: Int, b:Int) => a + b is called an anonymous function, or a function without a predefined name.
You can improve your function by taking advantage of Scala’s type inference:
Usually you have to specify the type of the parameter for top-level functions because Scala can’t infer the parameter types when declared, but for anonymous functions Scala inference can figure out the type from the context.
In this case you’re using a list of integers and 0 as your initial value, and based on that Scala can easily infer the type of a and b as an integer.
Scala allows you to go even further with your anonymous function: you can drop the parameters and only have the method body to make it a function literal.
But in this case the parameters will be replaced with underscores (_)
An underscore has a special meaning in Scala, and in this context it’s a placeholder for a parameter; in your case, use two underscores:
You’ve already seen another use of the underscore when assigning a default value to variables.
In Scala you can use underscores in various places, and their meaning is determined solely by the context and where they’re used.
Sometimes it gets a little confusing, so always remember that the value of the underscore is based on where it’s being used.
Function literals are a common idiom in Scala, and you’ll find occurrences of them in Scala libraries and codebases.
In chapter 1 you saw the following example but without enough explanation of what’s going on with the code.
Now, with your new knowledge of function literals, it should be pretty obvious that _.isUpper is a function literal:
In this case you’re invoking the given function literals for each character in the name string; when it finds an uppercase character, it will exit.
The underscore in this context represents a character of name string.
Before moving to the next section, leaving the “defining function” section without a small working example of closure would be unfair.
A closure is any function that closes over the environment in which it’s defined.
For example, closure will keep track of any variable changes outside the function that are being referred to inside the function.
In the example you’ll try to add support for the word break.
I haven’t talked about Scala keywords yet, but Scala doesn’t have break or continue.
Once you get comfortable with Scala, you won’t miss them because Scala’s support of functional programming style reduces the need for break or continue.
But assume you have a situation where you think having break would be helpful.
Scala is an extensible programming language, so you can extend it to support break.
Use the Scala exception-handling mechanism to implement break in Scala.
Throwing an exception will help you break the sequence of execution, and the catch block will help you reach the end of the call.
Because break isn’t a keyword, you can use it to define your function so that it will throw an exception:
Another subject not yet covered is exception handling, but if you’ve used it in Java, C#, or Ruby, it should be easy to follow.
In any case, you’ll read about exception handling in a later part of the chapter.
Now create the main function that will take the operation that needs a breakable feature.
The right side of the => defines the return type of the function—in this case it’s Unit (similar to Java void)—and op is.
Because you haven’t specified anything on the left side of the arrow, it means that the function you’re expecting as a parameter doesn’t take any parameter for itself.
But if you expect a function parameter that takes two parameters, such as foldLeft, you have to define it as follows:
The breakable function that you declared takes a no-parameter function and returns Unit.
Now, using these two functions, you could simulate the break.
Let’s look at an example function that needs to break when the environment variable SCALA_HOME isn’t set; otherwise, it must do the work:
Now inside the breakable function we need to catch the exception that will get raised when break is called from the install function:
In Scala if the last argument of a function is of function type, you can pass it as closure.
In the next chapter you’ll look into how closures are converted into objects; remember, everything in Scala is an object.
You should use Breaks if you have a need for break.
Again, I’d argue that once you look into functional programming with Scala in detail, you’ll probably never have a need for break.
Chapter 4 is dedicated to data structures, but until then I’ll introduce List and Array so you can start writing useful Scala scripts.
In Scala, array is an instance of the scala.Array class and is similar to the Java array:
Always remember that in Scala the type information or parameterization is provided using square brackets.
The type parameterization is a way to configure an instance with type information when creating the instance.
Now iterating through each element in the array is easy; call the foreach method:
You’re passing a function literal to the foreach method to print all the elements in the array.
The most obvious question in your mind right now is probably why we have to check ArrayLike, which is a different class than the Array used to check methods that are available for an Array instance in Scala.
Scala Predef provides additional array functionality dynamically using ArrayLike when you create an instance of an Array.
Again, Predef is a great place to start to understand the Scala Library.
ArrayOps is the subclass of ArrayLike, so ArrayLike is more like the interface for all the additional methods available to Array type collections.
When writing Scala scripts, you sometimes have to take command-like arguments.
You can do that implicitly as a val type variable called args.
In the following example you’ll create a Scala script that takes input from a user and prints it to the console:
Open a command prompt to the location where the file is saved and run the following command:
You’re using the same command you used to start the Scala REPL environment.
But in this case you’re executing a Scala script by specifying the script filename and three parameters: my, first, and script.
You’ll see another script example at the end of this chapter.
The Array is a mutable data structure; by adding each element to the array, you’re mutating the array instance and producing a side effect.
The only effect a method is allowed to have is to compute a value and return that value without mutating the instance.
An immutable and more functional alternative to a sequence of objects like Array is List.
In Scala, List is immutable and makes functional-style programming easy.
Creating a list of elements is as easy as the following:
In Scala, :: is a valid identifier, and you could use it to name a class.
Nil represents an empty list, and scala.:: represents any nonempty list.
Most of us are used to mutable collections where, when we add or remove elements, the collection instance expands or shrinks (mutates), but immutable collections never change.
Instead, adding or removing an element from an immutable collection creates a new modified collection instance without modifying the existing one.
The following example adds an element to an existing List:
The job of the :: method is to create a new List with all the existing elements plus the new element added at the front of the List.
To add at the end of the List, invoke the :+ method:
Scala provides a special object called Nil to represent an empty List, and you can use it to create new lists easily:
In this example you’re using a new instance of List every time you add an element.
To delete an element from a List, you could use the - method, but it’s deprecated.
A better way would be to use the filterNot method, which takes a predicate and selects all the elements that don’t satisfy the predicate.
To delete 3 from the newList, you can do something like the following:
In the meantime I encourage you to look into methods defined for List and play with them.
It’s a little hard to get into useful programming or scripting with Scala without the loops and ifs.
In Scala, if and else blocks work as they do in any other programming language.
If the expression inside the if evaluates to true, then the if block gets executed; otherwise, the else block is executed.
The interesting part about Scala is that every statement is an expression, and its value is determined by the last expression within the statement.
Assigning a value depending on some condition in Scala could look like this:
Scala doesn’t support the ? operator as Java does, but I don’t think you’ll miss it in Scala.
You can nest if/else blocks, and you can combine multiple if/else blocks using else if.
Loops in Scala have all the usual suspects like the while loop and do-while, but the most interesting looping construct is for or for-comprehensions.
The while and do-while loops are pretty standard, and in Scala they aren’t any different from Java or C#
A for-comprehension in Scala is like a Swiss Army knife: you can do many things with it using basic simple elements.
The for expression in Scala consists of a for keyword followed by one or more enumerators surrounded by parentheses and followed by an expression block or yield expression (see figure 2.2)
Before I go into yield expression, let’s look into the more traditional form of the for loop.
The common pattern used in a for loop is to iterate through a collection.
To print all the files in a directory that end with the .scala extension, for example, you have to do something like the following:
In Scala this is called a generator, and the job of a generator is to iterate through a collection.
The right side of the <- represents the collection—in this case, files.
For each element in the collection (file in this case) it executes the for block.
This is similar to the way you define a for loop in Java:
In the case of Scala, you don’t have to specify the type of file object because Scala type inference will take care of it.
Apart from the generator, you can use other ingredients in a Scala for loop to simplify the previous example.
Scala for loops allow you to specify definitions and guard clauses inside the loop.
In this case you’re defining a filename variable and checking whether the filename ends with .scala.
The loop will execute when the given guard clause is true, so you’ll get the same result as the previous example.
Note that all the variables created inside a for expression are of the val type, so they can’t be modified and hence reduce the possibility of side effects.
As mentioned earlier, it’s possible to specify more than one generator in a Scala for loop control.
The following example executes the loop for each generator and adds them:
The generators in this case are aList and bList, and when you have multiple generators, each generator is repeated for the other generator.
I used curly braces to surround the for expression, but you don’t have to; you could use ()
I tend to use curly braces when I have more than one generator and guard clause.
You’ve already seen one form in the previous examples: the imperative form.
In this form you specify the statements that will get executed by the loop, and it doesn’t return anything.
In the functional form, you tend to work with values rather than execute statements, and it does return a value.
Instead of printing the value of a + b, you’re returning the value of the addition from the loop using the yield keyword.
You’re using the same aList and bList instances in the loop control, and it returns the result as a List.
Now if you want to print the result, as in the previous example, you have to loop through the result List:
It does look like the functional form is more verbose than the imperative form, but think about it.
You’ve separated the computation (the adding of two numbers) from how you’re using it—in this case, printing the result.
This improves the reusability and compatibility of functions or computations, which is one of the benefits of functional programming.
In the following example you reuse the result produced by the foryield loop to create an XML node:
It takes each element in the List and concatenates each element with whatever separator you provide—in this case, a comma.
Even though it doesn’t make sense, what if you try to print inside the yield expression? What will happen? Remember, everything in Scala is an expression and has a return value.
If you try the following, you’ll still get a result, but the result won’t be useful because it will be a collection of units.
A unit is the equivalent of void in Java, and it’s the result value of a println function used inside the yield expression:
The next section moves into another functional concept: pattern matching.
Pattern matching is another functional programming concept introduced in Scala.
To start with, Scala pattern matching looks similar to switch case in Java.
The example in the following listing, showing the similarity between Scala and Java, takes an integer and prints its ordinal number.
Here the argument of the program is parsed to the integer value, and the ordinal method returns the ordinal text of a given number.
Here you’re doing something similar to the previous Java example: taking an input integer value from a command like args and determining the ordinal value of the number.
Because Scala can also be used as a scripting language, you don’t have to define an entry point like the main method.
And you no longer need to provide a break for each case because in Scala you can’t overflow into other case clauses (causing multiple matches) as in Java, and there’s no default statement.
In Scala, default is replaced with case _ to match everything else.
To run the Ordinal.scala script, execute the following command from a command prompt:
The wildcard case is optional and works like a safe fallback option.
If you remove it, and none of the existing cases match, you get a match error:
This is great because it tells you that you’re missing a case clause, unlike in Java, where if you remove the default and none of the existing cases match, it ignores it without providing any sort of feedback.
The similarity between Java and Scala pattern matching ends here because Scala takes pattern matching to the next level.
In Java you can only use a switch statement with primitives and enums, but in Scala you can pattern match strings and complex values, types, variables, constants, and constructors.
More pattern-matching concepts are in the next chapter, particularly constructor matching, but look at an example of a type match.
The following example defines a method that takes an input and checks the type of the given object:
In this example you’re using a Scala type pattern consisting of a variable and a type.
This pattern matches any value matched by the type pattern—in this case, String, List[AnyRef], Array[AnyRef], and java.util.Date.
You could do that in Java using the instanceof operator and casting, but this is a more elegant solution.
Save this method into the file printType.scala and load the file into the Scala REPL:
Now try a printType function with various types of input:
Scala also allows the infix operation pattern, in which you can specify an infix operator in your pattern.
In the following example, you’re extracting the first and the second elements from the List:
Sometimes you need to have a guard clause along with the case statement to have more flexibility during pattern matching.
In the following example you’re determining the range in which the given number belongs:
Here in the new implementation of ordinal you’re using range, which is a collection of integer values between a given start and end.
You’re calling the contains method on the range to check whether the number belongs to the range.
You can access elements of a List like array using index positions in the List.
You’ll revisit pattern matching in chapter 3 after looking at case classes.
It’s time to move on to the last topic of this chapter: exception handling.
You caught a glimpse of Scala exception handling in the breakable example.
Scala allows you a single try/catch block, and in the single catch block you can use pattern matching to catch exceptions.
The catch block is a match block under the surface, so all the pattern-matching techniques that you learned in the previous section are applicable to a catch block.
Modify the rangeMatcher example to throw an exception when it’s beyond 100:
Now when calling this method you can surround it with a try/catch block and catch the exception:
The case statement isn’t any different from the type pattern matching used in the printType example.
Scala doesn’t have any concept like a checked exception; all exceptions are unchecked.
This way is more powerful and flexible because as a programmer you’re free to decide whether or not you want to catch an exception.
Even though Scala exception handling is implemented differently, it behaves exactly like Java, with exceptions being unchecked, and it allows Scala to easily interoperate with existing Java libraries.
You’ll see the use of Scala exception handling in examples throughout the book.
In this section you’ll build a command-line-based REST client in Scala script.
To make a REST call to a RESTful service, you have to be aware of the operations supported by the service.
To test your client you need a RESTful web service.
You could use free public web services to test the client, but to have better control of the operations on the service you’ll create one.
You could use any REST tool or a framework to build the REST service.
I’ll use a Java servlet (Java developers are familiar with it) to build the service to test the REST client.
Understanding how the service is implemented isn’t important for this example.
The simple way to create a RESTful service for now is to use a Java servlet, as shown in the following listing.
It’s software architectural style for distributed hypermedia systems like the World Wide Web.
The term first appeared in “Architectural Styles and the Design of Network based Software Architectures,”7 the doctoral dissertation paper by Roy Fielding, one of the principal authors of the HTTP specification.
Systems that follow Fielding’s REST principles are often referred to as RESTful.
These methods are simple and return the request parameters and headers in response, which is perfect when testing your REST client.
The two helper methods I added are parameters and headers.
The parameters method is responsible for parsing the HTTP request object for parameters that are passed from the client; in this case, it’s the REST client.
The headers method retrieves all the header values from the request object.
Once the servlet is built, you must deploy the WAR file to a Java web container.
The purpose of HttpClient is to transmit and receive HTTP messages.
It’s not a browser, and it doesn’t execute JavaScript or try to guess the content type or other functionality unrelated to the HTTP transport.
The most essential function of HttpClient is to execute HTTP methods.
Users are supposed to provide a request object like HttpPost or HttpGet, and the HttpClient is expected to transmit the request to the target server and return the corresponding response object, or throw an exception if the execution is unsuccessful.
In this script you’re going to use four types of requests: GET, POST, DELETE, and OPTIONS.11 The previous example implemented only GET, POST, and DELETE because the web container will automatically implement the OPTIONS method.
HttpClient provides a default client and is good enough for our purpose.
To execute an HTTP DELETE method you have to do the following:
The HTTP POST method is a little different because, according to the HTTP specification, it’s one of the two entity-enclosing methods.
To work with entities HttpClient provides multiple options, but in the example you’re going to use the URL-encoded form entity.
It’s similar to what happens when you POST a form submission.
To use the HttpClient in your script, you have to import all the necessary classes.
I haven’t talked about import, but for now think of it as similar to Java import except that Scala uses _ for importing all classes in a package, as in the following:
You can do other interesting things with Scala imports, but that’s in the next chapter.
Now, because the service is up and running, you can focus on the client script.
To make the script useful, you have to know the type of operation (GET or POST), request parameters, header parameters, and the URL to the service.
The request parameters and header parameters are optional, but you need an operation and a URL to make any successful REST call:
You’re using a require function defined in Predef to check the size of the input.
Remember that the command-line inputs are represented by an args array.
The require function throws an exception when the predicate evaluates to false.
In this case, because you expect at least two parameters, anything less than that will result in an exception.
The first parameter to the script is the command, and the next ones are the request and header parameters.
The input to the script will look something like the following:
The request parameters and header parameters are determined by a prefix parameter, –d or –h.
One way to define a parseArgs method to parse request and header parameters is shown in the following listing.
Scala allows nested functions, and nested functions can access variables defined in the outer scope functionin this case, the args parameter of the parseArgs function.
Nested functions allow you to encapsulate smaller methods and break computation in interesting ways.
Here the nested function nameValuePair takes the parameter name, –d or –h, and creates a list of name-value pairs of request or header parameters.
The next interesting thing about the nameValuePair function is the return type.
Listing 2.6 Parsing headers and parameters passed to the program.
Tuple is immutable like List, but unlike List it can contain different types of elements; in this case, it contains a String and a List.
Scala provides syntax sugar for creating a Tuple by wrapping elements with parentheses ():
The last interesting thing I’d like to mention about the parseArgs method is the Map.
A Map is an immutable collection of keys and values.
In this example you’re creating a Map of parameter name(-d or –h) and listing all the parameters as values.
When you pass a tuple of two elements to Map, it takes the first element of the tuple as the key and the second element as the value:
For now you’ll support only four types of REST operations: POST, GET, DELETE, and OPTIONS, but I encourage you to implement other HTTP methods like PUT and HEAD.
To check what type of operation is requested, you can use simple pattern matching:
For example, in the case of a GET call, you’ll pass the request parameters as query parameters to the URL.
Look at the handleGetRequest method, shown in the following listing.
Listing 2.7 Preparing a GET request and invoking the REST service.
In this method you’re retrieving all the request parameters from the Map params and creating the query string.
Then you create the HttpGet method with the given URL and query string.
The DefaultHttpClient is executing the httpget request and giving the response.
The handlePostRequest method is a little more involved because it needs to create a form entity object, as shown in the following listing.
The Scala List and the Java List are two different types of objects and aren’t directly compatible with each other.
The special :_* tells the Scala compiler to send the result of toArray as a variable argument to the Arrays.asList method; otherwise, asList will create a Java List with one element.
Listing 2.8 Preparing a POST request and invoking the REST service.
In this complete example you implemented the support for four types of HTTP requests: POST, GET, DELETE, and OPTIONS.
The require function call B ensures that your script is invoked with at least two parameters: the action type and the URL of the REST service.
The pattern-matching block at the end of the script C selects the appropriate action handler for a given action name.
The parseArgs function handles the additional arguments provided to the script, such as request parameters or headers, and returns a Map containing all the name-value pairs.
The formEntity function is interesting because the URL encodes the request parameters when the http request type is POST, because in POST request parameters are sent as part of the request body and they need to be encoded.
To run the REST client you can use any build tool that can build Scala code.
This example uses a build tool called simple build tool (SBT)
You’ll learn about this tool in detail in chapter 6, but for now go ahead and install the tool following the instructions from the SBT wiki (http://www.scala-sbt.org)
Take a look at the codebase for this chapter for an example.
This chapter covered most of the basic Scala concepts like data types, variables, and functions.
Most importantly, you learned how to define functions, an important building block in Scala, and functional concepts including pattern matching and for-comprehension.
You also learned about exception handling and how Scala uses the same pattern-matching techniques for exception handling.
This chapter also provided a basic introduction to List and Array collection types so you can start building useful Scala scripts.
You worked with the Scala REPL throughout the chapter when trying out examples.
The Scala REPL is an important and handy tool, and you’ll use it throughout the book.
The chapter finished by building a complete REST client using most of the concepts you learned in it.
The example also demonstrated the flexibility Scala provides when building scripts.
It’s now time to move on to Scala classes and objects.
You’ll build this driver incrementally using the object-oriented constructs provided by Scala, and along the way I’ll explain each concept in detail.
Scala has made some object-oriented innovations, and one of them is the trait.
You’ll explore how to use traits when building Scala applications.
As you move through the chapter, you’ll learn about Scala case classes.
Case classes are useful when it comes to building immutable classes, particularly in the context of concurrency and data transfer objects.
Up to this point the book has been focusing on Scala’s fundamentals.
Without wasting any more time, let’s learn OOP programming in Scala by building a MongoDB driver.
To explore Scala’s object-oriented constructs and use, let’s build a MongoDB driver.
While building this example driver, you’ll dive deep into concepts for a thorough understanding.
You won’t need to start from scratch because you’ll use an existing Java driver for MongoDB.
You’ll build a Scala wrapper over the Java MongoDB driver.
That way, you don’t have to deal with the low-level MongoDB API and can focus on your objective of learning Scala.
The user stories you’ll be implementing in your Scala wrapper driver are as follows:
As a developer, I want an easier way to connect to my MongoDB server and access document databases.
As a developer, I want to query and manage documents.
WHAT’S A USER STORY? A good way to think about a user story is as a reminder to have a conversation with your customer (in Agile, project stakeholders are called customers), which is another way to say it’s a reminder to do some just-in-time analysis.
In short, user stories are slim and high-level requirements artifacts.
MongoDB is a scalable, high-performance, open source, schema-free, documentoriented database written in C++.1 MongoDB is a document-based database that uses JSON (JavaScript Object Notation)
The schema-free feature lets MongoDB store any kind of data of any structure.
You don’t have to define your database tables and attributes up front.
You can add or remove attributes whenever you need them.
Unlike relational databases, in a document-based model records are stored as documents in which any number of fields of any length can be stored.
For example, you could have the following JSON documents in a single collection (a collection in MongoDB is like a table in a traditional RDBMS):
In a schema-free environment, the concept of schema moves more toward the application than to the database.
Like any other tool, there are pros and cons for using a schema-free database, and it depends on the solution you’re trying to solve.
The format of the document in which the information is stored in MongoDB is BSON (binary JSON)
Other document-based databases like Lotus Notes (IBM) and SimpleDB (Amazon.com) use XML for information storage.
Classes and constructors advantage when working with web-based applications because JSON content can be easily consumed with little transformation.
A great place to get a feel for MongoDB is http://try.mongodb.org.
Then, unpack it and run the following command to start the MongoDB server:
To connect to the MongoDB server, use the client shell that ships with the distribution of MongoDB:
At this point you should be ready to start building the Scala wrapper driver.
To connect to the already running MongoDB server, create a Mongo client class with a hostname and port number:
The class declaration looks different from the way you declare in Java or C#—you’re not only declaring the class, but also its primary constructor.
The primary constructor is a constructor that needs to be called directly or indirectly from overloaded constructors when creating the instance MongoClient.
In Scala, the primary constructor for a class is coded inline with the class definition.
In this case, the constructor takes two parameters: host and port.
The host parameter specifies the address of the server, and port specifies the port in which the MongoDB server is waiting for the connection.
Because all the constructor parameters are preceded by val, Scala will create immutable instance values for each of them.
The following example creates an instance of a Mongo client and accesses its properties:
Like Java or C#, Scala also uses the new keyword for creating instances of a class.
But wait a minute—where’s the body of the MongoClient class? In Scala that’s optional.
Creating a class like a JavaBean with only a getter and setter would be easy in Scala, as in the following:
When parameters are prefixed with var, Scala creates mutable instance variables.
The val and var prefixes are optional, and when both of them are missing, they’re treated as private instance values, not accessible to anyone outside the class:
Note that when Scala creates instance values or variables, it also creates accessors for them.
At no point in time are you accessing the field directly.
The following MongoClient definition is equivalent to the class MongoClient(val host:String, val port:Int) definition.
The reason I’m using private (you’ll learn about access levels later in this chapter) is so the Scala compiler doesn’t generate accessors by default.
What val and var do is define a field and a getter for that field, and in the case of var an additional setter method is also created.
Wouldn’t it be nice to have an additional zero-argument constructor that defaults the host and port number so you don’t have to specify them every time? How about this:
To overload a constructor, name it this followed by the parameters.
Constructor definition is similar to method definition except that you use the name this.
Classes and constructors can’t specify a return type as you can with other methods.
The first statement in the overloaded constructors has to invoke either other overloaded constructors or the primary constructor.
When you compile this with scalac, you get the following compilation errors:
This poses an interesting challenge when you have to do some operation before you can invoke the other constructor.
Later in this chapter, you’ll look into a companion object and see how it addresses this limitation.
How do you add a setter method to a class? To add a setter, you have to suffix your setter method with _=
In the following Person class, age is private so I’ll add both a getter and a setter:
When Scala encounters an assignment like x = e, it checks whether there’s any method defined like x_= and if so, it invokes the method.
The assignment interpretation is interesting in Scala, and it can mean different things based on context.
For example, assignment to a function application like f(args) = e is interpreted as f.update(args)
To run the Scala Mongo wrapper code you’re going to develop in this chapter, you need to have the Java driver .jar file available in the classpath.
The underlying instance value will hold the connection to MongoDB.
When Scala generates the constructor, it instantiates the underlying instance value too.
Because of Scala’s scripting nature, you can write code inside the class like a script, which will get executed when the instance of the class is created (kind of like Ruby)
The following example creates a class called MyScript that validates and prints the constructor input:
How is Scala doing this? Scala puts any inline code defined inside the class into the primary constructor of the class.
If you want to validate constructor parameters, you could do that inside the class (usually at the top of the class)
Right now the MongoClient is using an underlying instance to hold the connection to MongoDB.
To extend or inherit from a superclass, you have to use the extends keyword.
The following code demonstrates how it would look if you extended from the Mongo class provided by the Java driver:
As shown in the previous example, you can also inline the definition of the primary constructor of a superclass.
One drawback of this approach is that you can no longer validate the parameters of the primary constructor before handing it over to the superclass.
Even though extending Mongo as a superclass is a completely valid way to implement this driver, you’ll continue to use the earlier implementation because that will give you more control over what you want to expose from the Scala driver wrapper, which will be a trimmed-down version of the complete Mongo Java API.
Before going any further, I’ll talk about Scala imports and packages.
This will help you to work with the Mongo library and structure your code.
A package is a special object that defines a set of member classes and objects.
The Scala package lets you segregate code into logical groupings or namespaces so that they don’t conflict with each other.
In Java you’re only allowed to have package at the top of the .java file, and the declaration defines the scope across the file.
You can still use the traditional Java approach and define package at the top of the Scala file, or use a scoping approach, as demonstrated in the following listing.
The previous code is exactly equivalent to the following code, where you’re declaring the package in traditional Java style:
You can also use curly braces with top-level package declarations like the following:
It’s a matter of style; you can use either one of them.
The scoping approach shown in listing 3.1 provides more flexibility and a concise way to lay out your code in different packages.
But it might quickly become confusing if you start to define multiple packages in the same file.
The most widely used way in Scala code bases is the traditional way of declaring a package at the top of the Scala file.
The only large, open source project I know of that uses the package-scoping approach is the Lift web framework (http://liftweb.net)
One more interesting point to note here is that Scala package declaration doesn’t have to match the folder structure of your filesystem.
You’re free to declare multiple packages in the same file:
If you save this code in a file called Packages.scala and compile it using the Scala compiler (scalac Packages.scala), you’ll notice that the Scala compiler generates class files in appropriate folders to match your package declaration.
You’ve already seen some examples of import in previous chapters, but I haven’t discussed it.
At first glance, the Scala import looks similar to Java imports, and it’s true they’re similar, but Scala adds some coolness to it.
To import all the classes under the package com.mongodb, you have to declare the import as follows:
Here’s another use for _, and in this context it means you’re importing all the classes under the com.mongodb package.
In Scala, import doesn’t have to be declared at the top of the file; you could use import almost anywhere:
In this case you’re importing the Random class defined in the scala.util package in the Scala code block, and it’s lexically scoped inside the block and won’t be available outside it.
Because the Scala package is automatically imported to all Scala programs, you could rewrite the block by relatively importing the util.Random class:
In Scala, when you import a package, Scala makes its members, including subpackages, available to you.
To import members of a class, you have to put ._ after the class name:
Building Scala code Scalac2 is the compiler that comes bundled with the Scala distribution.
If you’ve installed Scala as specified in chapter 2, you should have it available in your path.
The Scala compiler provides lots of standard options, like deprecation, verbose, and classpath, and additional advanced options.
For example, to compile the MongoClient you have to do the following:
Ant and Maven are standard tools for building Java projects.
You can easily use them to build Scala projects too, but the standard build tool for Scala projects is SBT3
Chapter 5 discusses how to use build tools to build Scala projects.
Here you’re invoking the nanoTime method defined in the System class without a prefix because you’ve imported the members of the System class.
This is similar to static imports in Java (Scala doesn’t have the static keyword)
Because imports are relatively loaded, you could import the System class in the following way as well:
Scala also lets you map a class name to another class name while importing—you’ll see an example of that soon.
If you try to compile this code, you’ll get an error saying that type IOMonad isn’t available.
That’s because Scala is looking for the IOMonad type in the io.monads package, not in another top-level package called monads.
To specify a top-level package you have to use _root_:
You can’t import an empty package, but the members of an empty package can see each other.
The java.sql.Date is imported as SqlDate to reduce confusion with java.util .Date.
You can also hide a class using import with the help of the underscore:
The Date class from the java.sql package is no longer visible for use.
To finish the functionality required for the first user story, you still need to add.
To achieve that you’ll add the methods shown in the following listing.
Everything in this code should be familiar to you except the createDB and db methods.
I haven’t yet introduced DB objects (I do that in the next section)
The createDB and db method implementations are identical because the getDB method defined in the Java driver creates a db if one isn’t found, but I wanted to create two separate methods for readability.
Before I show you the DB class used in the previous example, let’s explore Scala objects.
Scala doesn’t provide any static modifier, and that has to do with the design goal of building a pure object-oriented language where every value is an object, every operation is a method call, and every variable is a member of some object.
Having static doesn’t fit well with that goal, and along with that there are plenty of downsides4 to using static in the code.
A singleton object allows you to restrict the instantiation of a class to one object.5 Implementing a singleton pattern in Scala is as simple as the following:
The object declaration is similar to a class declaration except instead of class you’re using the object keyword.
To invoke the new p method, you have to prefix it with the class name, as you’d invoke static methods in Java or C#:
You can import and use all the members of the RichConsole object as follows:
The DB object introduced in listing 3.2 is nothing but a factory to create DB instances representing a database in MongoDB:
Scala provides syntactic sugar that allows you to use objects as function calls.
Scala achieves this by translating these calls into the apply method, which matches the given parameters defined in the object or class.
If there’s no matching apply method, it will result in a compilation error.
Even though calling an apply method explicitly is valid, a more common practice is the one I’m using in the example.
Note also that an object is always evaluated lazily, which means that an object will be created when its first member is accessed.
The Factory pattern in Scala When discussing constructors I mentioned that sometimes working with constructors could create some limitations like processing or validating parameters because in overloaded constructors the first line has to be a call to another constructor or the primary constructor.
Using Scala objects we could easily address that problem because the apply method has no such limitation.
Here you’ll create multiple Role classes, and based on the role name you’ll create an appropriate role instance:
Inside the apply method you’re creating an instance of the DB class.
In Scala, both a class and an object can share the same name.
When an object shares a name with a class, it’s called a companion object, and the class is called a companion class.
First, the DB class constructor is marked as private so that nothing other than a companion object can use it.
In Scala, companion objects can access private members of the companion class, which otherwise aren’t accessible to anything outside the class.
In the example, this might look like overkill, but there are times when creating an instance of classes through a companion object is helpful (look at the sidebar for the factory pattern)
The second interesting thing in the previous code is the mongodb import statement.
Because of the name conflict, you’re remapping the DB class defined by the Java driver to MongoDB.
Now you can use the role object as a factory to create instances of various roles: val root = Role("root") val analyst = Role("analyst")
In MongoDB, a database is divided into multiple collections of documents.
Shortly you’ll see how you can create a new collection inside a database, but for now add a method to retrieve all the collection names to the DB class:
The only thing that looks somewhat new is the Wrappers object.
You’re using utility objects provided by Wrappers to convert a java.util.Set to a Scala set so you can use a Scala for-comprehension.
To try out the mongodb driver, write this sample client code:
Package object The only things you can put in a package are classes, traits, and objects.
But with the help of the package object, you can put any kind of definition in a package, such as a class.
For example, you can add a helper method in a package object that will be available to all members of the package.
Normally you would put your package object in a separate file, called package.scala, in the package that it corresponds to.
You can also use the nested package syntax, but that’s unusual:
The following example uses verifyAge defined inside the package object:
The main use case for package objects is when you need definitions in various places inside your package, as well as outside the package when you use the API defined by the package.
This sample client creates a database called mydb and prints the names of all the collections under the database.
If you run the code, it will print test and system .indexes, because by default MongoDB creates these two collections for you.
Now you’re going to expose CRUD (create, read, update, delete) operations in the Scala driver so that users of your driver can work with documents.
The following listing shows the Scala driver code you’ve written so far.
Using MongoClient, your driver will be able to connect to the running MongoDB server, to a given host and port, or to the local MongoDB server.
You also added methods like dropDB, createDB, and db to manage MongoDB databases.
The following listing shows the DB class you created to wrap the underlying MongoDB database.
So far, you haven’t added much functionality to the DB class.
The only thing it provides is an easier way to access names of the collections of a given database.
A trait is like an abstract class meant to be added to other classes as a mixin.
Traits can be used in all contexts where other abstract classes could appear, but only traits can be used as mixin.
In OOP languages, a mixin is a class that provides certain functionality.
You can also view a trait as an interface with implemented methods.
You’ll see shortly how Scala traits will help you in implementing the second user story.
Both can take type parameters, which I discuss in the next chapter.
The second user story you need to implement in your driver is an ability to create, delete, and find documents in a MongoDB database.
MongoDB stores documents in a collection, and a database could contain multiple collections.
You need to create a component that will represent a MongoDB collection.
The common use case is to retrieve documents from a collection; another use case would be to perform administrative functions like creating and deleting documents.
The Java Mongo driver provides a DBCollection class that exposes all the methods to operate on the collection, but you’re going to take it and slice it into multiple views.
In this implementation you’ll wrap the existing DBCollection and provide three kinds of interfaces: a read-only collection, an administrable collection, and an updatable collection.
The following listing shows how the read-only collection interface will look.
The only abstract member defined in this trait is underlying, which is an abstract value.
In Scala, it’s possible to declare abstract fields like abstract methods that need to be inherited by subclasses.
It’s not necessary to have an abstract member in a trait, but usually traits contain one or more abstract members.
Note that you’re invoking the findOne or getCount method on the underlying collection without using the.
Scala allows you to treat any method as you would the infix operator (+, -, and so on)
The DBObject parameter is nothing but a key-value map provided by the Mongo Java driver, and you’re going to use the class directly.
In the full-blown driver implementation, you’ll probably want to wrap that class too, but for the toy driver you can live with this bit of leaky abstraction.
I’ll talk about the details of these methods shortly when you test the methods.
The next two traits you’re going to look at are Administrable and Updatable.
In the Administrable trait, you’ll expose methods for drop collection and indexes; and in the Updatable trait you’ll allow create and remove operations on documents—see the following listing.
Both traits extend the ReadOnly trait because you also want to provide all the features of a read-only collection.
If your trait extends another trait or class, then that trait can only be mixed into a class that also extends the same superclass or trait.
This makes sense because you want to make sure that someone else implements the abstract members that your trait depends on.
As with abstract classes, you can’t create an instance of a trait; you need to mix it with other concrete classes.
You’re overriding the underlying abstract value with whatever value will be passed to the primary constructor when creating the instance of DBCollection.
Note that the override modifier is mandatory when overriding members of a superclass.
The following adds three methods that return different flavors of the collection:
Here you’re getting the underlying collection by name and wrapping it into a DBCollection instance.
When building the administrable and updatable collection, you’re mixing in the corresponding traits using a with clause.
Using the with keyword, you can mix one or more traits into an existing concrete class.
Another way of thinking about Scala mixins is as decorators.
That allows you to widen a thin interface with additional traits when needed, as you did with the ReadOnly, Administrable, and Updatable traits.
If you’ve done any Ruby programming, you’ll find lots of similarity with Ruby modules.
One advantage of traits compared to module systems available in other languages is that the trait mixin is checked at compile time.
If you make mistakes while stacking traits, the compiler will complain.
Now you’ll build a client to demonstrate that the driver works.
Ideally, you should always write unit tests to make sure your code works.
For now, the following listing shows the small client that validates your driver.
In the test client you’re creating collections using the methods exposed by the DB class.
You’re using BasicDBObject provided by the underlying MongoDB driver to test the find method.
BasicDBObject is nothing but a wrapper around a Java map.
MongoDB being a schema-free database, you can put any key-value pair on it and save.
At the end of the test, you’re using the same BasicDBObject to query the database C.
To run the test client, make sure you have the Mongo Java driver .jar file in the classpath.
To specify the classpath to the Scala interpreter, use the –cp option.
After the release of your driver, all the users are happy.
But it turns out that the driver is slow in fetching documents, and users are asking whether there’s any way we could improve the performance.
One way to solve this problem immediately is by memoization.6 To speed things up, you’ll remember the calls made to the find method and avoid making the same call to the underlying collection again.
The easiest way to implement the solution is to create another trait and mix it in with the other traits.
By nature Scala traits are stackable, meaning one trait can modify or decorate the behavior of another trait down the stack.
You’re keeping track of all the resulting DBObjects, and when the same request is made a second time, you’re not going to make a call to MongoDB—instead, you’ll return from the map.
The getOrElseUpdate method is interesting; it allows you to get the value for the given key, and if it doesn’t exist, it invokes the function provided in the second parameter.
Then it stores the value with the key in the map and returns the result.
You saved a complete if and else block with a single method.
In the case of the parameterless findOne method, you’re using -1 as the key because the method doesn’t take a parameter.
To use this memoizer trait, you have to modify the existing DB class as follows:
Now whenever the findOne method is invoked, the overridden version will be called, and the result will be cached.
If you remove documents from a collection, the Memoizer will still have them and return them.
The next section discusses how stackable traits are implemented in Scala.
If you’ve worked with C++ or Common Lisp, then the mixin of traits will look like multiple inheritance.
The next question is how Scala handles the infamous diamond problem (http://en.wikipedia.org/wiki/Diamond_problem)
Scala solves this problem using a something called class linearization.
Linearization specifies a single linear path for all the ancestors of a class, including both the regular superclass chain and the traits.
This is a two-step process in which it resolves method invocation by first using right-first, depth-first search and then removing all but the last occurrence of each class in the hierarchy.
First, all classes in Scala extend scala.AnyRef, which in turn inherits from the scala.Any class.
The linearization of the ReadOnly trait is simple because it doesn’t involve multiple inheritance:
Now if you add the Memoizer trait into the mix, it will show up before Updatable because it’s the rightmost element:
To recap, you’ve used a Scala trait as an interface using ReadOnly.
You’ve used it as a decorator to expand the functionality of DBCollection using the Updatable and Administrable traits.
And you’ve used traits as a stack where you’ve overridden the functionality of a ReadOnly trait with Memoizer.
The stackable feature of a trait is useful when it comes to modifying the behavior of existing components or building reusable components.
Chapter 7 explores abstractions provided by Scala in building reusable components.
For now, let’s look at another example and explore stackable traits in more detail.
You have another requirement for your driver; this time it’s related to locale.
The Scala Mongo driver is so successful that it’s now used internationally.
Luckily, all the non-English documents have a field called locale.
Now if only you could change your find to use that, you could address this problem.
You could change your find method in the ReadOnly trait to find by locale, but that would break all your users looking for English documents.
If you build another trait and mix it with ReadOnly, you could create a new kind of Collection that will find documents using locale:
Now when creating a new Collection, you could mix in this trait:
The traits could be reordered as follows, with the same result:
As you can see, it’s easy to use traits in a stack to add or modify the behavior of existing classes or traits.
This kind of use is common in Scala code bases, and you’ll see more on them throughout the second part of the book.
Before we leave traits, there’s one more thing I’d like to mention: the use of super.
As you can see, when creating a trait you can’t tell how your trait will get used and who will be above you.
All you know is that it has to be of a type that your trait extends.
In the previous code, you could mix in the LocaleAware trait before or after Memoizer, and in each case super would mean something different.
The interpretation of super in traits is dynamically resolved in Scala.
Case classes are a special kind of class created using the keyword case.
When the Scala compiler sees a case class, it automatically generates  boilerplate code so you don’t have to do it.
In this code example, you’re creating a Person case class with firstName and lastName parameters.
But when you prefix a class with case, the following things will happen automatically:
Scala prefixes all the parameters with val, and that will make them public value.
But remember that you still never access the value directly; you always access through accessors.
Both equals and hashCode are implemented for you based on the given parameters.
The compiler implements the toString method that returns the class name and its parameters.
Every case class has a method named copy that allows you to easily create a modified copy of the class’s instance.
A companion object is created with the appropriate apply method, which takes the same arguments as declared in the class.
The compiler adds a method called unapply, which allows the class name to be used as an extractor for pattern matching (more on this later)
ScalaObject trait When discussing class linearization, I didn’t give you the complete picture.
Now think about how many times you’ve created a data transfer object (DTO) with only accessors for the purpose of wrapping some data.
Scala’s case classes will make that easier for you the next time.
Both equals and hashCode implementations also make it safer to use with collections.
Like any other class, a case class can extend other classes, including trait and case classes.
When you declare an abstract case class, Scala won’t generate the apply method in the companion object.
That makes sense because you can’t create an instance of an abstract class.
You can also create case objects that are singleton and serializable:
Scala case classes and objects make it easy to send serializable messages over the network.
You’ll see a lot of them when you learn about Scala actors.
If you have a need, you can declare your case class without a parameter.
Use () as a parameter list or use the case object.
Let’s put your recently gained knowledge of case classes to use in the MongoDB driver.
So far, you’ve implemented basic find methods in your driver.
It’s great, but you could do one more thing to the driver to make it more useful.
MongoDB supports multiple query options like Sort, Skip, and Limit that you don’t support in your driver.
Using case classes and a little pattern matching, you could do this easily.
You’ll add a new finder method to the collection to find by query and query options.
But first, let’s define the query options you’re going to support:
The NoOption case is used when no option is provided for the query.
Each query option could have another query option because you’ll support multiple query options at the same time.
The Sort option takes another DBObject in which users can specify sorting criteria.
Note that all the option case classes extend an empty trait, and it’s marked as sealed.
I’ll talk about modifiers in detail later in the chapter, but for now a sealed modifier stops everyone from extending the trait, with a small exception.
To extend a sealed trait, all the classes need to be in the same source file.
For the Query class, you’ll wrap your good old friend DBObject and expose methods like sort, skip, and limit so that users can specify query options:
Here each method creates a new instance of a query object with an appropriate query option so that, like a fluent interface (http://martinfowler.com/bliki/Fluent Interface.html), you can chain the methods together as in the following:
The most extraordinary part of the code is the last parameter of the Query class: option: QueryOption = NoOption.
Here you’re assigning a default value to the parameter so that when the second parameter isn’t specified, as in the previous snippet, the default value will be used.
I’m sure that, as a focused reader, you’ve already spotted the use of the companion object that Scala generates for case classes.
When creating an instance of a case class, you don’t have to use new because of the companion object.
To use the new query class, add the following new method to the ReadOnly trait:
Before discussing implementation of the find-by-query method, let’s see how case classes help in pattern matching.
You learned about pattern matching in chapter 2, but I haven’t discussed case classes and how they could be used with pattern matching.
One of the most common reasons for creating case classes is the pattern-matching feature that comes free with case classes.
Let’s take the Person case class once again, but this time you’ll extract firstName and lastName from the object using pattern matching:
Look how you extracted the first and last names from the object using pattern matching.
The case clause should be familiar to you; here you’re using a variable pattern in which the matching values get assigned to the first and last variables.
Under the hood, Scala handles this pattern matching using a method called unapply.
If you have to handcode the companion object that gets generated for Person, it will look like following:
The apply method is simple; it returns an instance of the Person class and it is called when you create an instance of a case class.
The unapply method gets called when the case instance is used for pattern matching.
Typically, the unapply method is supposed to unwrap the case instance and return the elements (parameters used to create the instance) of the case class.
I’ll talk about the Option type in Scala in detail in the next chapter, but for now think of it as a container that holds a value.
If a case class has one element, the Option container holds that value.
But because you have more than one, you have to return a tuple of two elements.
In the discussion of for-comprehensions in chapter 2, I didn’t mention that the generator part of for-comprehensions uses pattern matching.
Here you’re creating a list of persons and looping through them using pattern matching:
You’ll see more examples of extractors and pattern matching throughout the book.
Before we leave this section, I still owe you the implementation of the find-by-query method, so here you go (see the following listing)
Here you’re using pattern matching to apply each query option to the result returned by the find method—in this case, DBCursor.
The nested applyOptions function is applied recursively because each query option could wrap another query option identified by the next variable, and you bail out when it matches NoOption.
When it comes to overload methods (methods with the same name), you have to specify the return type; otherwise, the code won’t compile.
Scala type inference can’t infer the type of recursive methods or functions.
In case of type errors, it’s always helpful to add type information.
Using the test client in the following listing, you could test your new finder method.
When you run this client, you’ll see output similar to the following:
The id values in the output might be different for you because they’re autogenerated by MongoDB.
Scala lets you specify method arguments using a named style.
When you have methods or class constructors taking similar types of arguments, it becomes difficult to detect errors if you swap them by mistake.
Instead of passing in an order of first name, last name, if we swap the order, Scala won’t complain:
Unfortunately, both parameters are of the same type, and the compiler can’t detect the mistake at compile time.
But now, using named style arguments, you can avoid the problem:
Common arguments against pattern matching Pattern matching is common in functional programming languages, but not in the world of OOP languages.
Common arguments against pattern matching by objectoriented developers are that pattern matching could be replaced by a Visitor pattern, pattern matching isn’t extensible, and pattern matching breaks encapsulation.
First, pattern matching reduces lots of boilerplate code when compared to the Visitor pattern.
The extensibility argument enters the picture when pattern matching is supported only for basic datatypes like Int, Long, or String.
But Scala takes pattern matching much further and beyond basic datatypes with case classes.
Pattern matching implemented for case classes matches only the constructor parameters provided for the case classes.
This way, you don’t have to expose hidden fields of the class, and you ensure encapsulation.
The named arguments use the same syntax as a variable assignment, and when you use named arguments, the order doesn’t matter.
You can mix the named arguments with positional arguments, but that’s usually a bad idea.
When going for named arguments, always try to use a named style for all the arguments.
The following example uses a named style for the first argument but not for the second.
When using a named style, if the parameter name doesn’t match, the Scala compiler will complain about the value not being found.
But when you override a method from a superclass, the parameters’ names don’t have to match the names in the superclass method.
In this case, the static type of the method determines which names have to be used.
Consider this example, where you have the Person trait and SalesPerson overriding the grade method and changing the parameter name in the process from years to yrs:
Here years won’t work because the type of the s instance is SalesPerson.
If you force the type variable to Person, then you can use years as a named argument.
I know this is a little tricky to remember, so watch out for errors like this:
The value of the named argument could be an expression like a method or block of code, and every time the method is called, the expression is evaluated:
You’ve already seen one example of a default argument in the query example, where the last argument of the case class defaulted to NoOption:
The default argument has the form arg: Type = expression, and the expression part is evaluated every time the method uses the default parameter.
If you create a Query instance using Skip, the default won’t be used:
One of the interesting uses of default arguments in Scala is in the copy method of case classes.
Starting from Scala 2.8 on, along with the usual goodies, every case class has an additional method called copy to create a modified instance of the class.
This method isn’t generated if any member exists with the same name in the class or in one of its parent classes.
The following example uses the copy method to create another instance of the skip query option, but with a Limit option instead of NoOption:
The copy method is using a named argument to specify the parameter that you’d like to change.
The copy method generated by the Scala compiler for the Skip case class looks like the following:
As you can see, in the generated method all the parameters are defaulted to the value provided to the constructor of the class, and you can pick and choose the parameter value you want to change during the copy.
If no parameters are specified, copy will create another instance with the same values:
In Scala, invoking the == method is the same as calling the equals method.
The == method is defined in the scala.Any class, which is the parent class for all classes in Scala.
You’ve already seen a few modifiers in action, but let’s look deeper into them.
Along with standard modifiers like private and protected, Scala has more modifiers and new abilities.
The private modifier can be used with any definition, which means it’s only accessible in an enclosed class, its companion object, or a companion class.
In Scala, you can qualify a modifier with a class or a package name.
In the following example, the private modifier is qualified by class and package name:
Here, access to the f method can appear anywhere within the Outer class, but not outside it.
You could use it to make your methods available for unit tests in the same package.
Because the h method is qualified with outerpkg, it can appear anywhere within outerpkg and its subpackages.
Scala also lets you qualify the private modifier with this: private[this]
And object private is only accessible to the object in which it’s defined.
When members are marked with private without a qualifier, they’re called class-private.
It’s also accessible to a companion object of the defining class and companion objects of all the subclasses.
Like the private modifier, you can also qualify the protected modifier with class, package, and this.
By default, when you don’t specify any modifier, everything is public.
Scala doesn’t provide any modifier to mark members as public.
Like Java, Scala also provides an override modifier, but the main difference is that in Scala the override modifier is mandatory when you override a concrete member definition from the parent class.
The override modifier can be combined with an abstract modifier, and the combination is allowed only for members of traits.
This modifier means that the member in question must be mixed with a class that provides the concrete implementation.
The following creates a DogMood trait (dogs are moody, you know) with an abstract greet method and an AngryMood trait that overrides it:
You can’t invoke the super greet method because it’s abstract.
But super calls are important if you want your trait to be stackable so that it can get mixed in with other traits.
In cases like this, you can mark the method with abstract override, which means it should be mixed in with some class that has the concrete definition of the greet method.
To make this code compile, you have to add abstract along with override, as in the following:
Scala has introduced a new modifier called sealed, which applies only to class definitions.
It’s a little different from the final modifier; classes marked final in Scala can’t be overridden by subclasses.
But classes marked sealed can be overridden as long as the subclasses belong to the same source file.
You used sealed in a previous section when you created QueryOption case classes:
This is a common pattern when you want to create a defined set of subclasses but don’t want others to subclass it.
Starting with version 2.10, Scala allows user-defined value classes (which could be case classes as well) that extend AnyVal.
Value classes are a new mechanism to avoid runtime allocation of the objects.
To create a value class you need to abide by some important rules, including:
These are big constraints, so why bother? Value classes allow you to add extension methods to a type without the runtime overhead of creating instances.
Here Wrapper is a custom value class that wraps the name parameter and exposes an up() method.
To invoke the up method create the instance of the Wrapper class as in the following:
So what is going on? Behind the scenes the Scala compiler has generated a companion object for the value class and rerouted the w.up() calls to the up$extension method in the companion object.
The "$extension" is the suffix added to all the methods extracted from the companion class.
The contents of the up$extension method are the same as the up() method except all the references to name are changed to use the parameter.
Here is an equivalent implementation of a Wrapper companion object:
A value class can only extend a universal trait, one that extends Any (normally traits by default extend AnyRef)
Universal traits can only have def members and no initialization code:
Even though now you can invoke the p method on a Wrapper instance at runtime an instance will also be created because the implementation of the p method prints the type.
There are limitations when allocation is necessary; if you assign a value class to an array, the optimization will fail.
Nonetheless this is a very nice way to add extension methods to an existing type.
We will see examples of value classes later in the book.
For now let’s explore the rest of the Scala type hierarchy.
Implicit conversion is a method that takes one type of parameter and returns another type.
Here is an example of a conversion from Double to Int:
Usually you cannot assign a Double value to an Int type, but here we are explicitly converting Double to Int using the double2Int method before then assigning it to someInt.
We can make the conversion implicit by using the implicit keyword:
The advantage of implicit conversion is that the compiler will find the appropriate implicit conversion and invoke it for you:
What is going on here? The first time we assigned a Double value to an Int type variable it failed, but it succeeds the second time.
When the compiler encounters a type error, it doesn’t immediately give up; instead, it looks for any implicit conversions that might fix the error.
In this case, double2Int is used to convert Double to Int.
This conversion happens at compile time, and, if no appropriate conversion method is found, the compiler throws a compilation error.
The compiler will also throw an error if there is ambiguity in an implicit resolution; for example, more than one implicit conversion is found that matches the given criteria.
It is quite safe compared to the runtime extension available in some dynamically typed languages.
One of the common uses of implicit conversion is to add extension methods to existing types.
For example, we know we can create a range by using the to method:
But what if we want to create a range of numbers using the --> method?
This will fail because there is no --> method defined for Int.
We can easily fix this by following two simple steps:
Let’s create a class that defines a --> method for Int and creates a Range of integers:
Here the left operand becomes the constructor parameter and the right operand the parameter to the --> method.
By default, the Scala compiler always evaluates expressions from left to right.
Since implicit conversion is so commonly used by libraries and applications, Scala provides implicit classes.
Implicit classes reduce boilerplate code by combining the steps required for implicit conversion.
We can combine the RangeMaker and conversion methods by making the class implicit:
Behind the scenes, the compiler will “desugar” the implicit class into a simple class and an implicit conversion method, as we did earlier.
Note that implicit classes must have a primary constructor with one argument.
Looking up an appropriate implicit conversion takes time, but it’s not much of an issue because it happens at compile time.
The only runtime cost comes from creating an additional instance of RangeMaker for each implicit conversion.
The good news is that we can avoid the runtime cost by turning our implicit classes into value classes:
Implicit conversion is a very powerful language feature, but overusing it can reduce the readability and maintenance of the code base.
The root class in the hierarchy is the class scala.Any.
Every class in Scala inherits directly or indirectly from this class.
All the values that are represented by an object in the underlying host system (JVM or CLR) are a subclass of AnyRef.
The subclasses for AnyVal aren’t represented as objects in the underlying host system.
This is interesting because all this while I’ve been saying that everything in Scala is an object.
It’s still true that everything in Scala is an object, but not at the host system level.
When Scala compiles to Java bytecode, it takes advantage of the Java primitive types for efficiency, but converts them to objects when required by the Scala application.
In figure 3.3, along with the subtypes, you have views.
Views are implicit type converters that allow Scala to convert from Char to Int to Long and so on.
Type Scala.Null is the subtype of all reference types, and its only instance is the null reference.
The following code creates an instance of scala.Null, and the only way to create an instance of Null is by assigning null to an instance:
You’ll still get an exception when you try to access any method on a Null object.
Because it’s subclassed from AnyRef, you can’t assign null to any value type in Scala:
On the other hand, scala.Nothing is at the bottom of the Scala hierarchy, and it’s a subtype of everything in Scala.
But you can’t create an instance of Nothing, and there’s no instance of this type in Scala.
You’ll learn about Nothing in more detail in chapter 4 because it does solve some interesting problems for Scala.
This chapter covered a lot of ground, including how Scala empowers traditional OOP.
You learned about new features introduced in Scala 2.8, like named and default arguments.
On the one hand, you worked with traits and classes that provide interesting ways to structure your code in Scala; on the other hand, I introduced you to case classes that are handy and useful for building immutable data objects.
You’ll work with case classes again when you learn about Actors and concurrency.
You also learned about singleton objects and companion objects and how they’re used in Scala.
For the first time you also explored the Scala hierarchy and some of the important base classes.
This knowledge will help you easily browse scaladoc when looking for library classes and how they fit into the Scala hierarchy.
This chapter provided a foundation for the things that will follow in consecutive chapters.
You’ll revisit object-oriented concepts in Scala in chapter 7, where you’ll explore other abstraction techniques provided by Scala.
Remember, a great way to familiarize yourself with Scala concepts is to load up the Scala REPL and try out all the features for yourself.
The interactive approach is a good way to learn Scala and understand its concepts.
The next chapter should be interesting, because you begin to.
View Figure 3.3 Class hierarchy of Scala with subtypes and views tackle the various functional data structures in Scala.
To understand and benefit from Scala collections, you need to know two concepts: type parameterization and higher-order functions.
Type parameterization allows you to create types that take another type as a parameter (similar to Java generics)
Higher-order functions let you create functions that take other functions as parameters.
These two concepts allow you to create generic and reusable components, like Scala collections.
The Scala collection is one of Scala’s most powerful features.
The library implements all the common data structures you need, making it essential for every Scala developer.
A recent addition to the collections library is parallel collections.
Scala parallel collections allow you to solve data parallelism problems in Scala with ease.
You’ll see how the Scala parallel collections library helps when working with large datasets, so buckle up! This will be a fun and exciting ride.
In programming, type parameterization allows you to define methods or classes in terms of a type that will be specified later.
Parameterized types should not be a new concept to you if you’ve worked with Java or C# generics.
Scala provides additional extensions to generics because of its sound and powerful type system.
In chapter 3 you implemented the query interface to the MongoDB driver, and one of the methods you exposed was findOne, which retrieves a single document from the collection.
The problem with that method is that it returns null when the collection is empty, but this little fact isn’t clear from the method declaration.
One way to solve the problem would be to add a comment, although adding a comment to a method is the weakest form of documentation.
Wouldn’t it be better if you could explicitly communicate that sometimes the method might not work as intended?
In a situation like this, Scala developers use something called an Option.
Unlike other collection types, an Option contains a maximum of one element.
It represents one of two possible values: None and Some.
None means “no value” and Some represents “some value.” By returning Option from a method, you can communicate that the method might not return a value at all times.
For this section, forget that Scala provides an Option type so that you can build something similar on your own.
You’ll create a function that will return the index position of an element in a list.
It also needs to work for all kinds of lists.
How can you build a function that works for all types of lists? Using type parameterization, you can make the type information configurable.
Here A denotes a type that could only be determined when the function is invoked.
Both List and the value you’re looking for need to be of the same type.
Unlike Java and C#, Scala uses square brackets ([]) to declare the type parameter.
When this function is invoked with a list of integers, A represents the type  Int.
Similarly if you invoke it with a list of strings, the type parameter A is replaced with  String.
Now test your position method with different types of parameters:
Even though you can explicitly specify the type value for the type parameter as in the last example, it’s optional.
Scala type inference determines the value of the type parameter based on the arguments passed to the function.
Currently your position function returns -1 when there’s no matching element.
Here, instead of returning the Int result, you’ll return a new type that clearly expresses the behavior of the method.
You’ll create a container called Maybe that will wrap the result.
Most of this code should be familiar to you, except the type parameter part.
The Maybe class is declared as a covariant on type A.
Type variance complements type parameterization with a mechanism to specify constraints like covariant and contravariant to the type system.
When using type parameters for classes or traits, you can use a + sign along with the type parameter to make it covariant (like the Maybe class in the previous example)
Covariance allows subclasses to override and use narrower types than their superclass in covariant positions such as the return value.
Here the Nil object is a subclass of Maybe with scala.Nothing as a type parameter.
The reason you’re using scala.Nothing here is that the get method in the Nil object throws an exception.
Because the A type of Maybe is covariant, you can return a narrower type from its subclasses.
There’s no narrower type than Nothing in Scala because it’s at the bottom of the hierarchy.
In Scala, an immutable List is covariant in its type parameter, so List[String] is a subtype of List[Any]
You can take an instance, List[String], and assign it to a List of Any:
Usefulness of Nothing in Scala In Scala all methods and functions need to have a return type.
In situations where you have methods that don’t return anything, you use scala.Unit as a return type of the method.
Scala uses scala.Nothing as a return type in situations where the return type isn’t clear, such as when an exception is thrown:
When you see a method returning Nothing, that means that method won’t return successfully.
Here’s another example of invoking exit (System.exit) to terminate the runtime:
Traversable is the parent trait for all the collection types in Scala, and the ++ method is only defined in this trait.
In the preceding example Seq, Iterable, and List inherit the definition from Traversable.
Still, depending on the type of collection you’re dealing with, it returns the same type of collection back, because Traversable is defined with a covariant parameter.
In the case of covariance, subtyping can go downward, as you saw in the example of List, but in contravariance it’s the opposite: subtypes go upward.
Contravariance comes in handy when you have a mutable data structure.
Mutable objects need to be invariant A type parameter is invariant when it’s neither covariant nor contravariant.
An example can explain why mutable objects need to be invariant.
Because ListBuffer is mutable, it’s declared as invariant as follows:
Even though String is a subtype of scala.Any, Scala still doesn’t let you assign mxs to everything.
To understand why, assume ListBuffer is covariant and the following code snippet works without any compilation problem:
Can you spot the problem? Because everything is of the type Any, you can store an integer value into a collection of strings.
To avoid these kinds of problems, it’s always a good idea to make mutable objects invariant.
The next question is what happens in case of an immutable object for collections.
It turns out that for immutable objects, covariance isn’t a problem at all.
If you replace ListBuffer with the immutable List, you can take an instance of List[String] and assign it to List[Any] without a problem.
The best way to understand contravariance is to see the problem that comes when it’s absent.
Try to spot the problem in the following Java code example:
You end up assigning the string to an integer array.
Scala stops these kinds of errors at compile time by forcing parameter types to be either contravariant or invariant.
A type parameter is invariant when it’s neither covariant nor contravariant.
A good example of contravariance in action is the Function1 trait defined in Scala:
Scala uses the minus sign (-) to denote contravariance and the plus sign (+) for covariance.
In this case, Function1 is contravariant in P and covariant in R.
For example, Function1 type represents any function that takes one parameter.
The important question is why the Function1 trait is contravariant for the parameter and covariant for the return type.
To answer, consider what will happen when the opposite is the case—for instance, covariant for parameter and contravariant for return value.
Now imagine you have a covariant parameter, as in the following example:
Because Int is a subtype of scala.Any, the covariant parameter should allow the preceding code to compile.
But the loophole in this code is that you can now invoke addOne with any type of parameter as long as it’s a subtype of scala.Any.
Doing so could cause all sorts of problems for a function that expects only Int.
Scala, being a type-safe language, doesn’t allow you to do that.
The only other option you have is to declare the parameter type as invariant, but that would make the Function1 trait inflexible.
A contravariant parameter type is the only viable option to create a type-safe function.
The only reason this assignment is safe is because List is immutable.
You can add 1 to xs List, and it will return a new List of type Any.
Again, this addition is safe because the cons(::) method always returns a new List, and its type is determined by the type of elements in the List.
The only type that could store an integer value and reference value is scala.Any.
This is an important property to remember about type variance when dealing with mutable/ immutable objects.
You can’t use a contravariant return type because you again get into a problem.
This code is not  valid because Any is a super type of Int, and a contravariant allows you to go from a narrower type to a more generic type.
The following piece of code should also be valid then, right?
But you end up adding 20 to a string value, and that could be problematic.
Scala’s strong type system implementation stops you from making these kinds of mistakes when dealing with parameter types and return types.
To have a flexible and type-safe Function1 trait, the only possible implementation would be to have the parameter contravariant and the return type covariant.
In the next section you’ll learn to set bounds for type parameters—another important concept associated with type parameters.
In Scala, type parameters may be constrained by type bound.
Such type bounds limit the concrete values of the type variables.
The position function in the following listing throws an exception if you invoke get method on the Nil object:
Wouldn’t it better to pass a default value in case the element isn’t found? That way you can control the outcome in case of error.
You can add a new method to the Maybe abstract class that will take a default callback:
Here getOrElse returns the default value if the isEmpty method returns true.
In case of a Nil instance, isEmpty will always return true.
But if you try to compile this code, you’ll get the following compiler error:
Because A is a covariant type, Scala doesn’t allow the covariant type as an input parameter.
You’ll also get the following compilation error if A is contravariant because it’s used as a return type for get:
You could solve this problem in two ways: make the Maybe class an invariant and lose all the subtyping with Just and Nil, or use type bound.
I’m not willing to give up nice subtyping, so let’s go with the local type bound.
Scala provides two types of type bound: lower and upper.
An upper type bound T <: A declares that type variable T is a subtype of a type A, and A is the upper bound.
To create a function that can only take subclasses of Maybe, you can write something like the following:
The function defaultToNull takes parameter A, and it’s constrained to one of the subtypes of Maybe.
Because Maybe takes a type parameter, you have to declare the type parameter when defining the upper type bound.
If you don’t care about the type parameter, you can use the _ placeholder as in the last example.
A lower bound sets the lower limit of the type parameter.
The lower bound T >: A declares that the type parameter T is constrained to some super type of type A.
You can use the lower type bounds to implement the getOrElse method.
The lower bound helps you restrict the type of the parameter to A to some super type.
The Maybe class is defined with a covariant parameter type A so that its subclasses can return more specialized types.
The Just subclass holds the success value of an operation.
Nil is opposite of Just and denotes an error condition.
Because the default value is taken as a parameter, you have to set the lower bound to A to satisfy the contravariant rule.
Just and Nil are the two subclasses that represent success and error situations.
The sealed modifier restricts anyone else from creating a subclass of Maybe (modifiers are covered in chapter 3)
When getting the index, you can always invoke the getOrElse method and avoid any unnecessary exceptions:
Scala’s type system prevents you from making mistakes that could be easily caught during the compilation phase.
You’ve learned the important role that covariance and contravariance play in building nice, type-safe applications.
But remember that the approaches discussed here work only for immutable objects.
When it comes to mutable objects, it’s always safer to make them invariant.
But I think you have enough to understand the data structures discussed in this chapter.
You’ll see more examples and explanations of Scala’s type system throughout the book.
This method allows you to build a new list by applying a function to all elements of a given List:
Here A represents the type of List in which the map is defined, and B is the type of the new List.
To define a function argument, you have to use the => sign.
In this case, f is a function that takes a single parameter of type A and returns a result of type B.
In the following examples you’re creating a new List by adding 1 to all the elements of a given list in various ways:
In the first case you’re passing an anonymous function that takes x as a parameter and adds 1 to it.
In the second case you’re using a function literal, where a placeholder represents the parameter.
In the last example, you’re passing an existing function without referring to the parameters that are passed between the map function and the addOne function.
This is a good example of pointfree-style1 programming and functional composition.
It’s also an example of a call-by-name invocation, where the parameters of the function are wrapped inside another function.
The function addOne is invoked every time map accesses the function.
You haven’t looked at an example where a function returns another function.
Let’s fix that by refactoring the addOne function to add a nested function that abstracts our increment to 1 operation:
Call-by-value, call-by-reference, and call-by-name method invocation Java supports two types of method invocation: call-by-reference and call-by-value.
In call-by-reference invocation, a function is invoked with a reference to an object.
In the case of Scala, it’s any subtype of AnyRef.
In the case of call-by-value invocation, the function is invoked with a value type.
In Scala these are value types like Int or Float.
Remember, Scala unboxes and boxes value types from and to an object depending on how it’s used in the code.
Along with these standard ones, Scala also provides additional method invocation mechanisms called call-by-name and call-by-need.
In call-by-name method invocation, Scala passes a method parameter as a function.
In the following example, you have a log function, which logs a message when log is enabled:
Here the parameter will always be evaluated even if log isn’t enabled, and maybe you don’t want that.
But Scala will pass the parameter as a function that will be evaluated every time the parameter is accessed.
If the log isn’t enabled, the parameter will never be evaluated.
Later in this chapter you’ll see how lazy collections like Stream use the call-by-name method invocation pattern.
Here the nested function ++ returns another function that takes Int as a parameter and returns Int.
If you evaluate the ++ function in REPL, you’ll see that the return type of the function is Int => Int:
How can you implement a function like map that works for any type of list? You have a couple of ways to implement the map function—one uses recursion and the other uses a for-comprehension.
The following is the implementation based on recursion, where using pattern matching you’re extracting elements from the list and applying the supplied function:
You’re returning Nil (empty List) when the List is empty, and when the List isn’t empty you’re using pattern matching to separate List into head and tail.
The head is assigned to the first element in the List, and the tail is the remaining list.
You’re using cons (::) to append the result of the f function recursively.
At this point the empty list will match the first case, and the final expression will look like this:
Now the result of the each function f will be prepended to the empty list, resulting in a new list:
How does head :: tail work? This pattern is called the Infix Operation pattern, where the expression head :: tail is shorthand for the constructor pattern ::(head, tail)
The immutable List in Scala is defined as an abstract sealed class, and its only two subclasses are Nil and ::
As you already know, one of the biggest benefits of Scala case classes is that they can participate in pattern matching.
So ::(head, tail) matches the constructor of the :: case class that takes head as the first element and the list as the second element, which in this case is tail.
The associativity of the cons ( :: ) is right instead of left.
In Scala, the associativity of an operator is determined by the operator’s last character.
Always remember that associativity is determined by the last character of the operator.
Another simple way to implement the map function is to use a for-comprehension; here’s the implementation:
This method builds a new collection by applying a function to all elements of this list and concatenating the result.
The following shows how the flatMap method is declared in the List class:
GenTraversableOnce represents all collection types that could be iterated either sequentially or in parallel.
All the traits that start with Gen are introduced to the collection library to implement operations that are applicable to both sequential and parallel collections.
Here you’re focusing on sequential collections—later you’ll learn about parallel collections.
The flatMap method works similarly to the map method with the added ability to flatten a collection of collections into a single collection.
In the following example you’re creating a list of characters from a list of strings:
As mentioned earlier, String is treated as a Seq-like collection in Scala, and it exposes a method called toList to convert to a List.
In the previous example you invoked toList on each element in the list.
If you use map instead of flatMap, you get the following result:
As you can see, flatMap takes the result of the map and flattens it to a single list.
Here’s how you could implement the flatMap function for List:
Here  flatMap is implemented by combining map with a new function called flatten that takes a List of List and flattens it to a single List.
The ::: is another method defined for List that appends the contents of one List to another.
The flatMap function in that example is declared a little differently, as if it has two sets of parameters, one for the List and another for the f parameter:
What’s the difference between a lambda and a closure? A lambda is an anonymous function—a function without a name.
Closure is any function that closes over the environment in which it’s defined.
Let’s use an example to explore this fact, applying a percentage to a list of amounts:
In this case the function you’re passing to the map function is a lambda.
Now assume that the percentage value could change over time, so save the current percentage value in a variable.
In this case applyPercentage is a closure because it keeps track of the environment in which it’s created, like a percentage variable:
Lambdas and closures are different concepts, but they’re closely related.
The downside of using a recursive solution is that it can throw a stack overflow error on large datasets.
An alternative is to implement the function using tail recursion so that the Scala compiler could do tail call optimization and transform the recursion into a loop.
In tail recursion you perform your calculation first and then execute the recursive call by passing the result of the current step to the next step.
Here’s the implementation of the flatten function using tail recursion:
In this case the flatten function is implemented using a nested function that uses the tail recursion pattern.
The result of newList ::: head is passed as a parameter to the function so that the Scala compiler can optimize it.
You’ll learn more about tail call recursion in the next chapter.
In the next section you’ll explore another new concept called fold that allows you to process a data structure in some order and build a return value.
Two other interesting methods defined in List are foldLeft and foldRight.
These two operations allow you to perform binary operations on all the elements of the List.
Both of these methods in List are declared as follows:
The main difference between these methods is the way the parameters are passed to the function f.
In foldLeft, z represents the start value, and it’s passed as the first parameter to f.
For foldRight the start value is passed as the second parameter.
The function f is a combining function that takes two parameters and produces a single result value.
To understand how fold works, look at the previous flatten implementation one more time.
Notice that it’s similar to the recursive implementation of map:
There’s a common pattern in these functions: you do one thing when List is empty and something else when List isn’t empty.
The only thing that’s different between these functions is the binary operation.
The startValue is set to an empty List, and the combining function lets you apply the binary operator to all elements.
Both the cases start with an empty List and invoke either :: or ::: on the parameters.
I’m sure by now you’re familiar with the underscore (_) as a placeholder for parameters.
This is a good example of how having higher-order functions helps libraries build common operations that can be used in many different ways without duplicating code.
Avoid foldRight as much as possible as it uses recursion and can potentially throw a stack overflow error.
In some cases the Scala compiler transforms recursive functions to loops—you’ll learn about them in the next chapter.
One alternative approach is to use foldLeft and reverse the result.
For example, you can implement map2 using foldLeft and reverse the result:
The foldLeft method applies a binary operator to a start value and all elements of a List going from left to right.
Here’s how to use foldLeft to calculate the sum and length of a List:
The first example calculates the sum of all the elements, and the second calculates the length of the List.
The second example can’t use underscore because you aren’t using all the function parameters to compute the length of the List.
Both foldLeft and foldRight have an alias version, /: (foldLeft) and :\ (foldRight), that you can use instead.
But Scala programmers tend to prefer foldLeft and foldRight to symbolic versions.
Folding is a great way to process collections of data to build a return value.
Where map and flatMap allow you to transform contents of a collection, folding allows you to do structural transformations.
You can use folding in many interesting ways to solve your day-to-day programming problems.
The following example uses foldLeft to determine whether an element exists in a List:
Using foldLeft and foldRight, you can perform any type of binary operation without creating additional functions.
Next time you feel the need to create a loop, step back and think about ways you could use higher abstractions like maps or folds.
Just because the Scala List collection is used for all the examples doesn’t mean it should be your go-to collection type.
For a given problem, always try to choose the right collection type.
Later in this chapter I introduce you to other Scala collection types.
All the goodness of higher-order functions is possible in Scala because of its function objects.
The next section explores how function objects work in Scala.
A function object is an object that you can use as a function.
That doesn’t help you much, so why don’t I show you an example? Here you create a function object that wraps the foldLeft method you saw in the previous example:
Now you can use the foldl function object like any function you’ve used before:
You’ve already seen a few examples of apply and how to use them.
To treat an object as a function object, all you have to do is declare an apply method.
Because you’ve defined the parameter Traversable, the parent trait for all collection types in Scala, you can pass any collection type as a parameter.
The expression (defaultValue /: xs)(op) might look a little cryptic, but the idea is to demonstrate the alternative syntax for foldLeft, /:
Remember that when an operator ends with :, the right associativeness kicks in.
When declaring function objects, it’s a good idea to extend one of the Function traits defined in the Scala library.
Here 1 stands for “function with one parameter.” Similarly, Scala defines a function trait for two or more parameters.
In the following example, you’re building an increment function that takes an integer parameter and increments its value by 1:
The shorthand and equivalent implementation of this function would be:
There’s one more way you could define your function object: using the alternative notation of function =>
You use a similar syntactic notation when declaring higher-order functions.
You can use function objects anywhere you’ve used lambdas or closures before.
Here you’re invoking your map function with your new ++ function:
This is the same as invoking it with an anonymous function:
When passing an existing function (not a function object) as a parameter, Scala creates a new anonymous function object with an apply method, which invokes the original function.
One interesting subtrait of the Function1[-P, +R] trait is PartialFunction, which allows you to define a function that’s not defined for all P and allows you to compose with other partial functions to create a complete function that’s defined for all values of input parameter(s)
You’ll explore PartialFunction in the next chapter, where you’ll have a good use case for it.
Function traits also let you compose two functions to create a new function.
This is important because in functional programming you tend to solve problem by combining functions.
The following example composes the same function twice to create a new one:
Composing the addOne and addTwo functions together creates the addThree function.
The compose method allows you to chain functions together to create new functions.
You’ll look into function composition in much more detail in the next chapter.
Now it’s time for you to explore the Scala collection hierarchy.
The Scala collection classes are part of the scala.collection package and its subpackages.
You won’t find any reference to this trait in scaladoc because this type alias is defined inside the Predef class as follows:
I discuss type variables at length in chapter 6, where we’ll explore using them to create abstract members in Scala.
Scala collection hierarchy provide side-effect operations that could change the state of the collection in place.
The immutable collections are sometimes called persistent data structures because you can be certain that accessing the same collection will yield the same result over time.
Here persistent has nothing to do with a database or anything like that, but over time an immutable collection stays unchanged during the current program execution.
Any change in an immutable collection produces a new collection without touching the existing collection.
The generic subpackage provides building blocks for implementing various collections.
Typically, collection classes in mutable and immutable packages use the classes in the generic package for implementation of some of their operations.
Normally you don’t have to deal with the classes in the generic package unless you’re planning to create custom collection classes on your own.
A collection class defined in the package scala.collection can be either mutable or immutable.
Generally, the root collections in the package scala.collection define the same interface as the immutable collections, and the mutable collections add additional methods on top of the immutable collections that allow mutation of the collection.
In the case of mutable map, it provides methods like += and -= that add and remove elements from the collection and hence change the collection in the process.
Even though you can use a root collection type as a reference to an immutable collection, it’s always better to be explicit about the type of collection (mutable or immutable) when dealing with collections so that users can figure out the code easily.
Here you’re assigning both mutable and immutable collections to the collection.Map type value, and it’s not clear from the type of the mapping whether or not the map it refers to can be changed by others:
Figure 4.1 shows a simplified version of the Scala collection library.
This base trait implements the behavior common to all collection types in Scala (see table 4.1)
The only abstract method defined in the Traversable trait is foreach:
Let’s see an example where you can use this knowledge.
You’ve already seen examples of how to use Java collections with Scala, but here’s another simple example where you could wrap any Java collection to Traversable so that you could use it in Scala freely:
Figure 4.1 Scala collection hierarchy with three main types of collections: Set, Seq, and Map.
You’re providing a concrete implementation of only an abstract method in the Traversable trait, foreach.
Now with just one concrete method, you can use all the methods defined in the Traversable trait, such as map, foldLeft, or filter:
In Scala you can define a traversable object as finite or infinite; hasDefiniteSize determines whether a collection is finite or infinite.
You’ll see examples of an infinite collection later in this chapter.
A collection in a scala.collection can be both mutable and immutable.
You read about the difference between mutable and immutable collection classes in the previous section.
But before I start looking into specific collection classes, let’s peek at the Iterable trait.
It provides the implementation of foreach that you learned in the last section and it exposes a new abstract method called iterator.
It also adds methods like takeRight and dropRight along with the methods defined in the Traversable trait.
The most interesting things about the Iterable trait are its three base classes: Seq, Set, and Map.
One thing common among these subclasses is that they all implement the PartialFunction trait, and that means that all of them have the apply method.
You’ll now explore these base classes and their subclasses in detail.
Because Seq also implements PartialFunction, it has an apply method (the Function trait defines an apply method), and it’s a partial function from Int to an element.
The reason it’s partial is because for all values of Int you may not have elements in the collection.
In the following example you’re trying to access an element that exists in the sequence and one that doesn’t:
If the sequence is mutable like ListBuffer, then along with the apply method it offers an update method.
In the following code snippet you’re creating a ListBuffer and updating it:
Using a collection as PartialFunction Along with the standard apply method, the PartialFunction trait in Scala defines a couple of interesting methods, including andThen and orElse.
For example, to avoid situations where your Seq doesn’t have elements, you could use orElse, which works as a fallback when the partial function isn’t defined for a given value.
Now if you try to access an index that doesn’t exist, your default is used instead.
This is a good example of function composition and how to use it in Scala.
You’ll explore PartialFunction and other function composition parts and their usage throughout the book.
The assignment at B and the update method called at C are identical.
The two main subclasses for Seq are LinearSeq and Vector.
Vector provides efficient apply and length operations, whereas LinearSeq has efficient head and tail operations.
The most common subclasses of LinearSeq are List and Stream.
Buffers are always mutable, and most of the collections I talk about here are internally built using Buffers.
Set is an iterable collection type that contains no duplicate elements.
Set provides the contains method to check whether the given element exists in the Set, and the apply method that does the same thing.
To add or remove elements to or from an immutable Set, use + and –, respectively.
Using these methods for a mutable Set isn’t a good idea because it will create a new.
What type of collection should I use? Scala collections provide various types of collections, and every collection type has different performance characteristics.
Making sure that you select the appropriate collection type to solve your problem is important.
In case you’re not sure what type of collection to use, always reach out for Vector.
Overall, Vector has better performance characteristics compared to other collection types.
A better way to change mutable Sets is using the += and -= methods (for other useful methods available for Set, see table 4.2)
Here are some examples of adding and removing elements from both immutable and mutable Sets:
Along with the add and remove methods, you can perform other Set operations, such as union, intersect, and diff.
When iterator or foreach is called on SortedSet, it produces its elements in a certain order.
In the following code snippet you’re adding two sets, one using Set and the other using SortedSet.
In the case of SortedSet, the order of the elements is maintained:
Table 4.2 Useful methods defined in immutable and mutable Sets.
The key-value pair is represented by scala.Tuple2, a tuple of two elements.
Unlike other collections, a Tuple is a heterogeneous collection where you can store various types of elements.
Here’s how you can create an instance of an immutable Map with two key-value pairs:
An alternative way to create a similar Map would be to use the key -> value syntax:
Most of the operations of Map are similar to those of Set.
In the case of Map, the apply method returns the value of a given key, and if the value doesn’t exist, it throws an exception.
Here’s an example of how to use the apply method:
The better way to retrieve a value associated with a key is to use the get method defined in Map.
Instead of returning the value, it wraps the value in a container called Option:
Option is similar to the Maybe construct you created at the beginning of the chapter.
The Scala Option provides more features than you built for the Maybe.
You can think of Option as a List of one element.
When an element exists in the Map, it returns Some of the elements; otherwise, it returns None.
In the following example you’re using get to retrieve the value for a key:
You can use get to extract the element from Option or use getOrElse to retrieve a value from Option.
To get all the keys and values from the Map, you can use m.keys and m.values (table 4.3 lists some of the useful methods defined in mutable and immutable maps), and both of them return Iterator.
Scala Map also defines methods like filter, which takes a predicate and returns a Map of all the key values for which the predicate was true.
In the following code snippet, you’re filtering out all the rock artists:
The filter method in Map calls the predicate by passing the key-value pair as an instance of scala.Tuple2
Another way you could filter out all the rock artists would be to use for-comprehension, like so:
This brings up an interesting point of how a for-comprehension is similar to the map, filter, and foreach methods you’ve learned about so far.
In the next section you’ll see how a for-comprehension is translated in Scala.
You’ve learned that for-comprehensions are made up of generators, value definitions, and guards.
But I haven’t talked about the under-the-hood translation that happens.
Table 4.3 Useful methods defined in immutable and mutable Map.
Otherwise, update ms with the mapping  k -> d and return d.
Working with List and ListBuffer and how a for-comprehension combines pattern matching with filter, map, flatMap, and foreach.
This knowledge will help you understand how to combine simple functions to create something powerful.
As always, a better way is to look at what’s going on underneath.
This time you’ll create a case class to represent the artists and use a for-comprehension to create a list of rock artists.
Under the hood, this for-comprehension will get translated to something like the following:
For-comprehensions without yield (imperative version) are translated into a foreach method call on the output of the filter.
When you have multiple generators in the for-comprehension, things become a little more involved and interesting.
Let’s say that along with the artists you also like to store.
Create another case class to store the artists with their albums, and using a for-comprehension you can easily filter out all the rock albums.
Why use withFilter but not filter? The answer lies in strict versus nonstrict processing of the filter.
In this example you have a list of numbers, and you want to control the processing based on a flag:
The reason is that prior to Scala 2.8, forcomprehensions were translated into something like the following:
As you can see, when the filter processes the elements, go is true; hence it returns all the elements.
The fact that you’re making the go flag false has no effect on the filter because the filter is already done.
When withFilter is used, the condition is evaluated every time an element is accessed inside a map method.
For each artist you’re iterating through all the albums and checking to see if the genre is Rock.
When you have multiple generators, Scala uses flatMap instead of map.
The reason you use flatMap here is because you have to flatten the output of map for each generator.
Before leaving Scala collections, I’d like to discuss one more thing, and that’s the usage of Scala Option.
You’ve already seen some of the methods defined in Scala collections that return Option, and it’s time to see what they are.
If you’ve worked with Java or a Ruby-like language, then you understand the pain of working with null or nil (in the case of Ruby) in code.
In Ruby, things are a little better in the sense that Nil is a singleton object, and you can invoke methods on Nil.
To avoid the issue many programmers clutter their codebase with null checks and make the code difficult to read.
Scala takes a different approach to solving this problem, using something called Option.5 Option implements the Null Object pattern.
I talk about Monads at length in the next chapter, but for now think of a Monad as a simple container.
Option is an abstract class in Scala and defines two subclasses, Some and None.
Every now and then you encounter a situation where a method needs to return a value or nothing.
In Java or Ruby you typically do this by returning null or nil.
But in the case of Scala, Option is the recommended way to go when you have a function return an instance of Some or otherwise return None.
When the given key exists, it returns the value wrapped in Some or returns None when the key doesn’t exist.
You can use Option  with pattern matching in Scala, and it also defines methods like map, filter, and flatMap so that you can easily use it in a for-comprehension.
To understand lazy collections, step back and examine their opposite—strict collections.
So far you’ve looked at collections that are strict, meaning they evaluate their elements eagerly.
The following example adds 1 to all the elements of the List but only returns the head from the List:
When should you use Either rather than Option? scala.Either represents one of the two possible meaningful results, unlike Option, which returns a single meaningful result or Nothing.
By convention, Left represents failure and Right is akin to Some.
In the following code you’re trying to make a socket connection, and as you know, it might fail or return a connection based on whether a server is available to accept it.
You could easily wrap these kinds of operations in a function called throwableToLeft:
When creating a new Socket connection, you can wrap using throwableToLeft as in the following:
When an exception occurs, you create an instance of Left otherwise Right.
Most programmers are used to throwing exceptions, and using Either could take some getting used to.
In particular, throwing an exception isn’t a good idea in concurrent programming, and using Either like a pattern is a good way to send and receive failures between processes and threads.
To make it clearer, break the previous line like this:
Sometimes this isn’t a big issue, but other times you may want to save space and time by not creating intermediate collections and operations unless required.
Scala offers a couple of interesting and useful ways to create more on-demand collections using View and Stream.
The more technical term for on-demand collections is nonstrict collections.
Sometimes nonstrict collections are called lazy collections, but lazy usually refers to nonstrict functions that cache results.
This is one of the migration issues you may face when moving to Views from Projections.
Almost all collections expose a method called view, which returns a nonstrict view of the collection you’re working on.
To process the previous List example on demand, you could do the following:
In this case, a call to map produces another view without doing the calculation, and the calculation is deferred until you invoke head on it.
But one of the elements will result in a divide-by-zero error:
Even though you’re interested in only the first element of the list, the entire collection is processed, causing the exception for the third element.
Using View would avoid the exception when accessing the first element:
You can skip the error element and process the other elements, but the moment you access the error element you’ll get an exception:
The nonstrict method of processing collection elements is a useful and handy way to improve performance, especially when the operation is time-consuming.
In lazy functional languages like Haskell and Clean, almost every construct is evaluated lazily.
But because Scala isn’t a lazy functional programming language, you have to take extra steps to simulate the equivalent type of laziness by using call-by-name functions or partial functions.
The following code snippet has a time-consuming operation called tweets, which takes a handle and searches for Twitter messages that have the handle name:
Using Source you get the Twitter search result in XML and create an XML node instance from it.
Even though it doesn’t take that much time, for argument’s sake let’s consider this operation to be expensive and time-consuming.
Now you need to process these Twitter search results for multiple users.
The most obvious solution would be to create a Map that stores the tweets with the handle name, as in the following:
Working with lazy collections: views and streams processing tweets for ManningBooks processing tweets for bubbl_scala.
The problem with this approach is that while creating the Map, you’re invoking the tweets function for all users.
But because the tweets function is time-consuming, you’d like to invoke it only when you need it for a user.
An alternative way to solve the problem would be to use a partial function, as discussed previously:
In this case you’ve created the map using a partial function.
A function becomes a partial function when you don’t specify all the arguments it needs.
For example, if you invoke tweets with an argument you get messages, but if you omit the argument you get a function back:
To omit an argument you have to specify _; otherwise, you’ll get an error from Scala:
In the example if you use view, you can achieve the laziness you’re looking for, and your tweets function will get called when you need the value:
Inside a Map, values are stored as Tuple2, a tuple of two elements.
You’re invoking the tweets function by passing the handle name.
If you want to process the messages for Manning Books, you can use a for-comprehension, as in the following:
Note that starting with Scala 2.8, for-comprehensions are now nonstrict for standard operations.
The class Stream implements lazy lists in Scala where elements are evaluated only when they’re needed.
Stream is like List, in which elements are stored in two parts, head and.
If you want, you can build an infinite list in Scala using Stream, and it will consume memory based on your use.
Because Stream extends from LinearSeq, you have almost all the List methods available to you.
The following example zips each element of the List with its index:
Scala Streams also enjoy the same benefits as views when it comes to memory consumption and performance.
Let’s look at an example to demonstrate that, using the Fibonacci sequence.7 In mathematics, the Fibonacci numbers are the numbers in the following sequence:
The most common way to implement the Fibonacci sequence is to use recursion; here’s the code:
This function will return the Fibonacci number for a given n value.
To clearly understand why it’s not an efficient solution, look at what happens for fib 8:
As you can see, the calculation is growing exponentially, and unfortunately many of the steps are computed repeatedly.
One way to implement the Fibonacci sequence is to build it using an infinite stream.
You’re using the apply method of the cons object with a head of 0 and another Stream as a tail.
Now if you compare the timing of both implementations, you’ll see that the stream-based solution performs better than the previous one.
The question that remains is how Stream can manage to evaluate the tail when it’s required and not eagerly evaluated.
A close look at the signature of the apply method in the cons object will show that the second parameter is declared as a function that takes no arguments and returns a stream:
Here t1 is a call-by-name parameter, which is encoded by turning it into a no-arg function.
Note that when pattern matching a Stream, the usual cons(::) doesn’t work; you have to use #::
So far you’ve looked at collections that use either an eager or lazy evaluation strategy, and the elements of the collection are processed sequentially.
Now you’ll learn about Scala parallel collections, in which elements of a collection are evaluated in parallel.
The Scala parallel collections are implemented in terms of split and combine operations.
The split operation divides parallel collections into small Iterable collections until it hits some threshold defined for a given collection.
Then a set of tasks is created to perform the operation in parallel on small Iterable collections.
These tasks are implemented in terms of a Fork/Join framework.8 The Fork/Join framework figures out the number of CPU cores available to perform the operation and uses threads to execute the task.
At the end, the output of the each task combines to produce the final result.
Figure 4.2 shows how a map operation is performed on ParVector, a parallel version of Vector collection.
The threshold operation defined for each parallel collection provides an estimate on the minimum number of elements the collection has before the splitting stops.
Once the split operation is over, each collection is handed over to a task to perform the operation.
At the end, the output of each task is combined into the final result.
Each parallel collection type provides a combiner that knows how to combine smaller collections to produce the output.
To figure how many workers are used to perform the operation, you could try the following snippet inside the REPL:
I’ve changed the map method a little bit to print out the name of the thread executing the operation.
Because I’m running on a quad-core Macbook Pro, the Fork/Join framework used four different worker threads to execute the map operation in parallel.
The details of Fork/Join are completely taken care of for you so you can focus on solving problems.
Figure 4.2 Parallel collections implemented in terms of split and combine operations using the Fork/Join framework.
Configuring parallel collections The engine responsible for scheduling the parallel collection operations is called TaskSupport.
Each parallel collection is configured with a task support object that is responsible for keeping track of the thread pool, load-balancing, and scheduling of tasks.
Scala provides a few implementations of task support out of the box:
Parallel collections are well-suited for data parallel problems and improve the performance of the overall operation considerably without your worrying about concurrency.
The map operation is a perfect example of an operation that you could parallelize because it doesn’t depend on the order in which elements of a collection are processed.
On the other hand, foldLeft isn’t suited for parallel collections because the elements need to be processed in a certain order.
The following example demonstrates that foldLeft is executed by a single thread even if performed on ParVector:
Note that in the absence of side effects, parallel collections have the same semantics as the sequential collections.
The next section explores the types of available parallel collections.
Parallel collections were added to the collections library in Scala 2.9
All the parallel collection classes are implemented in a separate class hierarchy (see figure 4.3)
At the top of the parallel collection library is ParIterable.
This trait implements the common behavior of all the parallel collections.
This is implemented in the scala.concurrent package and uses the fork-join thread pool behind the scene.
To change the task support associated with a given collection, simply change the taskSupport property as in the following:
In this case tasksupport is changed to ForkJoinTask with four working threads.
These classes form a hierarchy similar to the class hierarchy in figure 4.1
If you want your code to not care whether it receives a parallel or sequential collection, you should prefix it with Gen: GenTraversable, GenIterable, GenSeq, and so on.
The Scala parallel collections library implements parallel versions of almost all the collection types available in the scala.collection package, both mutable and immutable types.
I say almost because you won’t find parallel implementation of LinearSeq type collections like List because they aren’t well-suited for parallel execution.
You don’t always have to start with a parallel collection implementation; you can easily switch between sequential and parallel as you need them.
Scala collections provide the par method to all sequential collection types to create a parallel version of the collection.
And on the other side, all the parallel collection types implement the seq method to create sequential versions of the collection.
The following example filters out all the odd numbers by converting the sequential version of Vector to ParVector using the par method:
In this case the output will be an instance of ParVector of even numbers.
To get back the sequential version of Vector, you have to invoke the seq method.
The following example converts Vector to its parallel counterpart to perform a filter operation and then converts back to Vector again:
This kind of conversion has the additional benefit that you can optimize part of a codebase to use parallel collections without changing the type of the collection used across the code.
You shouldn’t expect that everything will perform better by switching to parallel collections.
In fact, in some cases it might perform worse than the sequential version.
First, not all operations are parallelizable, so switching to parallel collection won’t improve the performance of these operations.
An ideal candidate would be the one that doesn’t assume any order of execution and doesn’t have any side effects.
Operations like map, flatMap, filter, and forall are good examples of methods that would be easily parallelized.
Second, there’s an overhead of creating a parallel version of a sequential collection and using the Fork/Join framework.
If it takes less time to perform the operation than to create a parallel collection, then using the parallel version will reduce your performance.
It also depends on the type of collection you’re using.
Converting Seq to ParSeq is much faster than converting List to Vector because there’s no parallel List implementation, so when you invoke par on List you get Vector back.
The knowledge of type parameterization helped in exploring type-variance concepts and the type-safety features of Scala.
Understanding this concept is important for building generic, typesafe, reusable components in Scala.
The chapter also explored the use and importance of higher-order functions such as map and filter and how they help users of the Scala library—in particular the collection library that provides rich and useful APIs.
Using higher-order functions, you can easily encapsulate common programming idioms.
This chapter also introduced the Scala collections library and the new changes made to the API starting with Scala 2.8
The Scala collections library is a vast set of APIs, and you saw only the most important and commonly used ones.
You need to explore the scaladoc for other Collection classes and APIs, because almost all common, useful functions are already encoded in the library.
When working with collections it’s also important to understand the performance and memory requirements of individual collection types.
Knowing the difference between strict and nonstrict processing will help you decide which type of collection would be useful and when.
You’ll learn what functional programming is and how to do functional programming in Scala.
Understanding functional programming will help you build functional, immutable, and simple solutions.
The goal of the chapter is to make you comfortable with functional programming and help you to write code in functional programming style.
You’re already doing functional programming using Scala if you’ve been following the examples in the book.
In some cases it’s explicitly mentioned or visible and in other cases it’s mixed with object-oriented constructs of the Scala language.
What is functional programming? formal mathematical system to investigate functions, function application, and function recursion.
In lambda calculus functions are first-class values; functions can take other functions as a parameter and return functions as an output (higher-order functions)
A function that adds two input parameters could be written like the following in lambda calculus:
Functional programming languages implement lambda calculus with some constraints and types.
Not all programming languages have features like first-class functions, pattern matching, and so on, but it’s possible to do functional programming in almost all programming languages.
There’s nothing much to it except understanding what’s meant by function in this context.
A function relates every value of type X to exactly one value of Y (see figure 5.1)
In Scala you could write the signature of such a function as follows:
A function provides the predictability that for a given input you will always get the same output.
Here the function add: (Int, Int) => Int fits the definition of the function because for a given input, it’s always going to return the same result.
But what about the functions that depend on some external state and don’t return the same result all the time? They’re functions but they’re not pure functions.
Any observable behavior change after the function finishes is considered a side effect.
Figure 5.1 A pure function where each value of X is mapped to exactly one value of Y.
The behavior of a pure function doesn’t depend on any external behavior or state.
You’re also not allowed to mutate the arguments to the function or invoke a function that has side effects.
The add function is a perfect example of a pure function, but the following weather function isn’t a pure function because the weather will change based on when you invoke the function:
Here I use the Weather Underground API3 to get the weather information for a given ZIP code.
Why care about pure functions? What’s the value of programming with pure functions?
Referential transparency is a property whereby an expression could be replaced by its value without affecting the program.
Assume the following is a part of a functional program:
Why should you care about referential transparency? What advantage does it give you?
Referential transparency provides the ability to reason about your code.
You can provide proof that your program works correctly by replacing pure functions with their values and reducing a complex expression to a simpler expression.
Sometimes you can even compute the result of a program in your head.
This ability to reason about code helps programmers to debug and solve complex problems easily.
With varying difficulty you can do functional programming in any programming language.
The essence of functional programming is referential transparency, and its benefit is referential transparency—the safety net that allows you to easily find and fix problems.
When you add that to the fact that Scala is a type-safe language, you can catch lots of problem ahead of time during compilation.
In Scala, functional programming is baked in with Scala’s object-oriented features, so sometimes it’s difficult to distinguish it when you have a language that allows you to define both methods and functions.
Moving from OOP to functional programming remember that methods in Scala don’t have any type; type is only associated with the enclosing class, whereas functions are represented by a type and object.
Unfortunately, coming up with a definition of a functional programming language4 is still hard.
Everyone has his or her own definition, and I’m sure you’ll also come up with your own one day.
But even if functional programming is possible in all languages, it doesn’t necessarily mean you should use it.
It can be like trying to do OOP in a procedural language—it’s possible, but probably hard and painful.
There is good news: writing functional programs in Scala is easy.
A pure functional program is a single referentially transparent expression.
An expression is constructed with a combination of subexpressions created using the language.
The following is an example of a pure functional program:
Here the main method is the entry point to our purely functional program, and the rest of the program is defined by a single expression that takes a collection of strings and returns a Tuple of two collections based on some partition criteria.
If you can start thinking about your program as a collection of subexpressions combined into one single referentially transparent expression, you have achieved a purely functional program.
Yes, you still may have to read inputs from the console or from the filesystem, but think of those as implementation details.
It doesn’t matter how the inputs are read—the behavior of your purely functional program should not change.
Programmers in Java and C# as well as C++ are already familiar with the concepts of classes and objects and are probably comfortable with OOP.
But how can one transition to a more functional programming style from a more OOP experience? Scala is a great language for this because it allows you to combine the styles in an elegant and well-engineered fashion.
It’s perfectly okay to start with Scala and focus on only its object-oriented features, but as you become more comfortable with the language and its library, you may slowly transition to a more functional programming style as discussed in the previous section.
In this section I highlight a few techniques that you can.
At first glance it may seem odd to compare object-oriented and functional programming at a pure versus impure level.
Although it’s true that you can write object-oriented code without side effects, in practice OOP easily becomes riddled with undesirable side effects.
Typically, OO-style applications are built around the idea of mutable state (produces side effects) managed by various objects inside the application.
Object-oriented solutions are modeled around classes and objects where data tends to carry collections of methods, and these methods share and at times mutate the data.
A functional programming style only deals with values where problems are solved by the application of functions to data.
Because data is only represented by value, each application of a function results in a new value without any side effects.
Another way to differentiate them is that functional programming raises the abstraction level over OOP.
If you only work with values, then how your functional program is interpreted and executed becomes irrelevant.
Remember, you can compute the result of a purely functional program using paper and pen; running it using a machine is an implementation detail.
In languages where you have only functions, such as Haskell and Clojure, you don’t have to worry about impurity.
But Scala bakes both object-oriented and functional programming into one language, so you have to be extra careful about side effects.
In Scala you still have to define classes (or traits) and objects to group your methods and function values.
And it’s your responsibility as a developer to make sure you don’t rely on mutable data defined inside the class.
Take the following example, where you have a class that represents a square with one method that computes the area:
The problem with this class is that the side property is mutable, and the area method depends on the value of the side to compute the area of the square.
It’s clearly not a pure solution because the result of the area method depends on some external state—in this case, the value of the side property.
It’s also hard to reason about the area method because now you have to keep track of the value of the side property at a given point in time.
To implement Square in a pure functional way, you’ll use a Successor Value pattern5 (sometimes called a functional object), where each change of state returns a copy of itself with the new state:
In this new solution, every time the side property is modified, a new copy of PureSquare is returned, so you don’t have to worry about a mutable state and the result of the area method because now it’s associated with a new object, PureSquare.
This common pattern is used when you have to model a state that could change over time.
The Java String class we’ve been using throughout the book is an example of a Functional Object pattern.
Now your challenge is to design all your objects in a similar fashion because it’s easy to introduce side effects unintentionally.
Watch out for all the vars and setters that your methods depend on and make them as pure as possible.
Remember going forward: referential transparency is a criterion of a good design.
Design patterns are just as useful in functional programming as they are in OOP as tools for communication.
Some design patterns like Singleton, Factory, and Visitor are already implemented as part of the language.
You can easily implement Singleton and Factory patterns using Scala objects.
You could implement the Visitor pattern using the pattern-matching feature of Scala.
Take a look at the Strategy pattern.6 This pattern allows you to select algorithms at runtime and can easily be implemented using a higher-order function:
Because taxingStrategy is defined as a function, you can pass different implementations of the strategy.
Similarly you can also implement the template method pattern using the higher-order functions.
Higher-order functions are also useful when dealing with dependency injection (DI)
For example, you can define a type for tax strategy and have a function that calculates a tax based on strategy and the product code:
This function takes an instance of TaxStrategy and returns a function of type String => Double, which encapsulates the taxing strategy.
With this setup you can easily create new functions by injecting different types of tax strategy:
Your knowledge of OO design patterns is still valuable in Scala, but its style of implementation has changed.
Functional programming brings new sets of patterns that you haven’t encountered before, and those are mainly related to recursive programming.
So far, I’ve been focusing on implementing pure functional solutions, but what if you have to deal with side effects like writing to a socket or to a database? You can’t avoid that, and in fact any useful program you write for an enterprise probably has a side effect.
Are you doomed? No! The trick is to push the side effects as far down as possible.
You can create an impure layer, as shown in figure 5.2, and keep the rest of your application as pure and functional as possible.
In section 5.5 you’ll learn how to build abstractions around code that by necessity include side effects.
To demonstrate how this works, you’re going to build a simple HTTP server that only serves files from a directory in which the server is started.
Like any server, this HTTP server is full of side effects, like writing to a socket, reading files from the filesystem, and so on.
Here are your design goals for the server you’re building:
The essence of the problem is to parse the request to figure out the name of the requested file, locate the resource, and return the response.
The side-effecting code should form a thin layer around the application.
Similarly the response could also be represented as a collection of characters.
The resource locator type should be able to check whether the file exists, retrieve the file contents, and check the content length.
The ResourceLocator is a function type that takes the name of a resource and returns the resource.
The resource is represented by a trait Resource and provides all the methods you need to create the appropriate HTTP response.
The important point here is that you’re building an abstract layer that will allow you to design your application using values and pure functions.
The GET method first retrieves the requested filename, then locates the file based on the given locator.
Now that the core of the server is implemented, you need to hook it to the real world so it can be useful and practical.
First you have to open a server socket at an appropriate port and listen for requests.
You also have to handle each request in a separate thread so that you can listen for a new request while you’re processing the old one.
Here let’s focus on the implementation of Resource and ResourceLocator:
The only thing left now is reading and writing to the socket.
You’ve successfully separated the side effects from pure functional code.
This is an important technique to remember when designing your application: push the side effects to the edge of the world.
You can refer to the nano-http-server project in the code base of this chapter for the complete implementation.
To run the example, you need to install the simple build tool covered in the next chapter.
In the next section you’ll explore various types of functions and their applications.
Functional programming is all about functions, and Scala lets you create functions in various forms.
That means you can treat functions like Int or String as type values in Scala.
You can create them as a value, pass them to functions as parameters, and compose them to create new functions.
The common form of a function in Scala is defined as a member of a class.
Here use is a method defined in the class UseResource.
One downside of using methods is that it’s easy to depend on the state defined by the enclosing class without explicitly passing the dependencies as parameters—be careful about that because that will take you away from having pure methods.
Unlike functions, methods don’t have any type associated with them.
Scala infuses functional programming with OOP by transforming functions into objects.
For example, you can assign a function literal (anonymous function) to a value like the following:
Here succ is associated with a function Int => Int and it’s nothing but a shorthand definition of the following:
Functions in Scala are represented by a type and object, but methods aren’t.
The good news is that Scala lets you convert methods to functions using a transform process called Eta expansion.
You can take any existing method and append _ (underscore) to create the function.
The following code creates a function version of the use method from the previous example:
It’s common in Scala to convert methods into functions and pass them around to other functions.
In the next section you’ll look into examples of higher-order functions and how they help you in solving problems.
Higher-order functions are those that take functions as parameters or return functions as a return value.
You’ve already seen plenty of examples of higher-order functions throughout the book.
In the Scala collections you’ll notice the use of higher-order functions everywhere.
For example, to filter out all the even numbers from a List, you write something like the following:
Now let’s see how you can use higher-order functions to solve a day-to-day problem.
One of the most common programming problems is managing resources.
For example, to send data over a TCP connection, you have to open a socket, send data, and, hopefully, remember to close the socket.
Similarly, to read data from a file in a filesystem, you have to open the file, read data, and then close the file handle.
The typical way to manage these resources is to wrap them in a try-finally block like the following:
You get a handle to the resource (socket or file) and use the resource in the tryfinally (sometimes catch) block to dispose of the resource once you’re done using it.
Here the use function is taking care of the resource management, and the function parameter f allows you to use the resource without worrying about releasing or disposing of it.
Now the code for sending data through a socket will be like the following:
This abstraction for managing resources will remove duplication from your code and centralize the way you manage and dispose of resources after use, without cluttering the code base with try-finally blocks.
To make this work you have to define a common type like Resource so you can create abstraction around the implementation.
This is a common pattern in Scala known as the Loan pattern7 (the object-oriented counterpart is called the Template Method pattern)
Another example will demonstrate the power of higher-order functions in terms of the design flexibility it provides to programmers.
One of the common pieces of logic that you’ll see across code bases goes something like the following:
Use the instance in the other parts of the code.
Let’s take a look at the following pseudo Scala code:
But the problem with that approach is that you might not have everything that’s accessible to the context of the caller namespace.
For example, the reference to mailer in the previous code snippet is only available to the context of the caller and not available inside the Person instance.
One way you can address this problem is to use higher-order functions.
This function will apply the side-effecting function to the instance and return the instance.
With this new tap function, your code will get some structure:
This is better than what you had, but you can still improve on it using Scala’s implicit conversion.
Because this is so common, you’ll make this function available to all the types.
Compare the code inside the main method B with the pseudo code you started with, and you should see the difference.
Now the code is more concise and well-structured, and the side effects are not leaking through.
The best part of this pattern is that it lets you compose without relying on sequences of instructions.
The combinatory logic is beyond the scope of this book, but I highly recommend To Mock a Mockingbird.
I’d like to highlight that once you start thinking in higher-order functions, you’ll see opportunities for extracting reusable code that you never thought possible.
Think, for example, about foldRight and foldLeft defined in the Scala collection library, which let you apply any binary function.
The next section discusses partial functions and how they help in the composition of functions.
Function currying is a technique for transforming a function that takes multiple parameters into a function that takes a single parameter.
Look at the following function definition that takes two parameters:
The taxIt function takes TaxStrategy and String parameters and returns Double value.
To turn this function into a curried function, you can invoke the built-in curried method defined for function types:
It turned the taxIt function into a function that takes one parameter and returns another function that takes the second parameter:
What’s the benefit of using function currying? It allows you to turn generalized functions into specialized ones.
For example, turning the taxIt function to function currying allows you to create a more specialized function like taxFree.
Here I injected the taxStrategy as a dependency to the curried function to create a new function that uses the dependency.
You can also turn existing methods to curried functions using an underscore (_)
The following code example redefines the taxIt function as a method and then converts it to a curried function:
You can also define methods in currying style using multiple parameters set:
You’ve used multiple parameters set for higherorder functions to pass anonymous function like closures, but now you’ve learned another benefit of function currying: dependency injection.
A partial function is a function that’s only defined for a subset of input values.
This is different from the definition of a pure function (see section 5.1), which is defined for all input parameters.
Like all function types, PartialFunction declares the apply method and an additional method called def isDefinedAt(a: A):Boolean.
This isDefinedAt method determines whether the given partial function is defined for a given parameter.
The easiest way to create a partial function is by defining an anonymous function with pattern matching.
The following code example defines the partial function shown in figure 5.3:
In this case the Scala compiler will translate the preceding code snippet to something like the following:
The PartialFunction trait provides two interesting combinatory methods called orElse and andThen.
The orElse method lets you combine this partial function with another partial function.
It’s much like if-else, where if the current partial function isn’t defined for a given case, then the other is invoked.
You can chain multiples of them to create if-else if patterns.
On the other hand, andThen lets you compose transformation functions with a partial function that works on the result produced by the partial function.
They let you write smaller functions, keeping in mind the single responsibility principle, and then compose them together to create a complete function that provides the solution.
When composing partial functions, always remember that the isDefinedAt method of each composing partial function might get invoked multiple times.9
Assume you’re building a pricing system for all the patient claims for which you need to invoice your providers.
Typically these systems are complicated, so I’ll simplify things a little.
The pricing depends on the type of the claim and location.
Functions in all shapes and forms more, the location is divided by state codes or ZIP codes.
Each of these factors could influence the final price you’ll charge your providers.
Also, not all claims have specific pricing logic associated with them, so you have to have a catch-all default so that you can always calculate the price.
I’m sure this sounds similar to some of the business rules you implement for your enterprise.
First define the claim types you’re going to work with:
Each claim takes a claimId that uniquely identifies it in the system and optionally some additional properties associated with the claim.
Understanding how each claim is different isn’t important for this exercise, but remember that they’re different.
To request a price, the requestor has to provide the claim information, the location, and the product identifier.
In this application you can easily represent that using case classes:
Except for Generic claim, the pricing for each claim is determined by specific business logic, and all the calculations start with some base prices associated with the product.
To determine the final price of a product and the claim, you have to provide the request information and the base price.
You can capture that with a type variable called PC (Pricing Criteria):
Here the Option[Double] represents the base price of the product.
The following code example implements the business logic associated with each Full and Partial claim:
Similarly, the final price to the provider is also influenced by the location of the claim.
Both state code and ZIP code could change the price.
The separate location-based logic could also be implemented as separate partial functions, as in the following:
To create the final solution to calculate the price for a provider, you can combine these smaller partial functions and be done with it.
According to the business rules, you should first determine the price based on the claim and further refine the calculated price based on location.
You can easily combine these functions with the orElse and andThen combinators you learned at the beginning of this section:
The preceding code implements the business rules the way they’ve been described.
Calculate the price using the claim, then refine it based on location.
As the business rules or new claim types get added to the system, you can easily modify the combinations and add new partial functions.
For example, you aren’t handling the Generic claim type yet.
You can easily add it to the final solution by adding another orElse block to the claimHandlers.
The partial functions are applicable to more situations than you might think.
For example, throwing exceptions from a function or a method could be considered a partial function.
The function that’s throwing an exception isn’t defined for the case that raises the exception.
Instead of throwing an exception, you could consider making the function partial and combining it with some other function that knows how to handle the exception case.
In Scala, partial functions are powerful when it comes to function composition.
Recursion is a useful tool in your functional programming toolbox.
Think of recursion as the assembly language of functional programming.
One of the main benefits of recursion is that it lets you create solutions without mutation.
In the next small exercise, you have to calculate the sum of all the elements of a List without using any mutation.
You can solve the problem in many ways using library functions, but let’s try to build it from scratch.
The imperative solution to this problem looks something like the following:
You declare a mutating variable and accumulate the result by iterating through all the elements of the collection.
And the recursion-based solution would look something like the following:
The difference is that a recursion-based solution doesn’t use any mutable temporary variables and it lets you break the problem into smaller pieces.
A typical way to implement recursive functions in Scala is to use pattern matching.
Pattern matching helps you break the problem into subproblems where each case potentially represents a subproblem.
Recursion always looks easy when someone else is doing it, but it can be hard when you have to do it.
The next section explains how you can start thinking recursively by following simple steps.
Suppose you’re given a list of elements, and your job is to remove the duplicates.
I’m going to show you a step-by-step process to come up with a recursion-based solution.11
Thinking in terms of type will help you think about the input parameter and the return value of the function.
Don’t generalize yet, but think about what you have and what you want to achieve.
The type of the removeDups function will look like the following:
The next step is to declare all the cases that you need to handle.
In the case of removeDups, you have to handle the following cases:
The first case checks for an empty list, the second case is for a duplicate element in the list, and the third case is for a nonduplicate element.
Depending on the type of the problem you’re trying to solve, you might end up with many cases.
Don’t worryyou’ll refactor the solution later into a more elegant solution after it’s working.
Here you have only one simple case, and that’s case Nil.
Because empty lists can’t have any duplicates, you can safely return an empty list back:
The next step is to implement the other case(s) when you have a nonempty list.
For this step, it’s useful to consider which constructs and functions you have that you could use to implement these cases.
For the second case, you want to throw out the x because it’s a duplicate and continue with the processing for the rest of the elements in the list.
The easiest way to do that is to invoke removeDups again by passing the ys as a parameter.
For the last case you want to continue with the rest of the list and append the nonduplicate element to the list:
The final step is to generalize and simplify the solution.
Start with your type signature and see whether you can generalize the solution.
In this case, you started with List[Int] => List[Int], but do you need to specify Int here? Are you using anything that’s specific to the Int?
In the removeDups solution, you don’t care about the type of List as long as there is a way to compare two elements.
You can generalize the type signature of removeDups as in the following:
In this case, it looks simple so you don’t need to go any farther.
The best way to get better at recursion is practice.
Once you become comfortable, these steps will come naturally to you, but until then let them guide you in how to implement recursive solutions.
Before I talk about tail recursion, let me explain how head recursion works.
Head recursion is the more traditional way of doing recursion, where you perform the recursive call first and then take the return value from the recursive function and calculate the result.
In tail recursion you perform your calculation first and then execute the recursive call by passing the result of the current step to the next step.
I’ll provide an example shortly to demonstrate how you can write recursive functions, keeping tail recursion in mind.
But first it’s useful to consider why recursion is avoided in Java and other similar languages.
Generally when you call a function an entry is added to the call stack of the currently running thread.
The downside is that the call stack has a defined size, and once you violate that boundary you get a StackOverflowError exception.
This is why Java developers prefer to iterate rather than recurse.
Because Scala runs on the JVM, Scala programs also suffer from this problem.
But the good news is that starting with Scala 2.8.1, Scala overcomes this limitation by doing tail call optimization.
In the following code snippet you’re calculating the length of a given List:
This is a classic example of head recursion where you call length recursively and then add all the ones at the end.
If you try to execute this function with a large List (100,000 elements, for example), you will get a StackOverflowError.
In this version you aren’t doing any calculation after the recursive call.
You do the calculation at each step and pass the result to the next step of the recursion.
The question is, which one should you prefer? For Scala, you should always prefer the version that uses tail recursion because Scala tries to optimize tail recursive functions.
Scala does tail call optimization at compile time, and the compiler transforms a tail recursive function into a loop.
This way tail recursive functions don’t add additional entries to the call stack and don’t blow up.
Scala can’t optimize every tail recursion—it can optimize functions but not nonfinal methods.
The best way to know whether the Scala compiler can optimize your tail recursion is to use the @tailrec annotation because that way you’ll get a compiler warning if the Scala compiler fails to optimize your tail recursive functions or methods.
The following listing has the complete length function with the @tailrec annotation.
The common pattern for implementing a tail recursive function is to use a local function like _length.
This local function approach allows you to have an additional parameter by which you can pass the result of the current step to the next step.
Always remember when going for tailrec optimization that your recursion should be the last step in your function or final method.
A data type in general is a set of values (think of Int as a type that identifies all the integer values)
You can define an algebraic type by enumerating all the values of the set, except each value could have its own constructor.
It also allows you to decompose the type using pattern matching.
If that sounds like an abstract concept, let’s look at an example.
So far you’ve learned that ADT is a kind of type that represents a set of values.
ADTs could represent a finite or an infinite set of values.
First, look at an example of a closed ADT (finite set of values) that explores why they’re valuable.
The easiest way to define an algebraic type in Scala is to use case classes.
The following example code defines an Account type and its possible values:
Here you’ve defined three account types, each with its own constructor taking various numbers of parameters.
It also declares Account trait as sealed, and that means no one else can extend the trait and create a new type of Account unless it’s defined in the same source file.
You’ve managed to create a finite ADT, but why case classes are a good implementation choice for ADTs is still not clear.
ADTs become much easier to deal with if they’re implemented as case classes because pattern matching works out of the box.
Why does functional programming matter? Along with the values and constructors, ADTs also come with a way to decompose the type through pattern matching so that you can easily use them in your functions.
A powerful concept: once you create an algebraic data type, you get pattern matching support to readily use them in functions.
Another advantage of using the finite algebraic data type is that it provides hints to the Scala compiler to check whether the functions are handling all possible values of algebraic data types.
There are two ways you can get rid of the warning: provide the case for the PremiumAccount or make the Account type nonsealed.
The downside of removing the sealed keyword is that anybody can extend Account trait and create a new account type.
A total function is one that knows how to handle all the values of an algebraic data type and always produces a result.
That means you know at compile time that the function will work for all inputs.
You’ve been using ADT types in this book for a while now without knowing it.
A couple of well-known examples of ADT in Scala are scala.Either and scala.Option.
Functional programming, as I’ve mentioned, is different from imperative programming.
Why bother to learn this new technique? What will this buy you?
First, it’s good to learn a new programming paradigm because it makes you a better programmer (see section 1.1.4)
But this reason alone isn’t good enough—what other benefits does functional programming provide? The popular answer is concurrency and multicore programming.
Functional programming helps you write concurrent applications more effectively and easily.
Enterprise software developers have to deal with complex business problems and large-scale software development, and although concurrency is an important part of it, it’s not enough to convince all developers.
In the rest of the section I make a case for why functional programming matters and how it can help you better handle complexity.
John Hughes, a renowned computer scientist, in his excellent paper on why functional programming matters12 describes how functional programming helps in.
In fact, the content and the title of this section were influenced by this paper.
Unix pipes are analogous to a pipeline where processes are chained together (by their input/output streams) to form a pipeline.
The | is the Unix pipe command, which indicates the output from one process is to be “piped” as input to the next one.
I’m pretty sure the authors of curl and wc would have never thought that someone would combine these processes to perform an action.
In fact, you can almost take any number of Unix processes and combine them to create new commands.
This is one of the most useful and powerful ideas of the Unix-like OSs.
What’s the design philosophy behind all these Unix processes?13 All Unix processes follow two simple rules:
So what do you gain by following these simple rules? The answer is composability.
You can pick them in any order, combine them to create new processes, name these processes, build new processes on top of them, and so on.
How does all of this map to functional programming? A Unix pipe is like a functional programming language.
If you think of each process as a function, a Unix pipe lets you compose these functions using | notation; in Scala it’s functional composition.
Similarly, let’s say the following set of functions is available to you in Scala:
These functions are like Unix processes in that each one does exactly one thing.
The even function returns true if the given element is an even integer value.
The filter function, on the other hand, takes a Traversable type collection (super trait for all traversable collection types in Scala) and additional criteria function to return a collection with all the elements that match the criteria.
The map function traverses the collection and applies the given function f.
Now, suppose you have a problem where you have to find all the even numbers in a given collection and double them.
With the functions at your disposal, you can easily build the solution by composing them into a multistep.
First build a filter that knows how to find even elements and a function that can double a value:
In the case of evenFilter I’m using function currying to create a specific version of the filter that knows how to filter even numbers.
To compose the two functions together, Scala provides a method called andThen, available to all function types except those with zero arguments.
This andThen method behaves similarly to Unix pipes—it combines two functions in sequence and creates one function.
Because all Scala functions get compiled to a scala.Function trait, you’ll use this compose method to join two functions.
To filter out odd elements and double them, create the following function:
Here evenFilter creates a collection of even elements, and the map function invokes the double function for each element in the collection.
But what if you have to double all the odd numbers? You have all the ingredients you need, just compose them slightly differently:
Here for the odd function I use another combinatory method defined for function types, called compose.
The only difference between andThen and compose is that the order of evaluation for compose is right to left.
This breaking down of problems into smaller digestible pieces happens across all the layers of software development.
Functional composability lets you build these mathematical microworlds in your application.
In these microworlds you can be certain about things because they’re all made of pure functions, and you can easily build them using function composition.
The good news is that you can implement most of the application complexity in these microworlds, and functional composability gives you a clear, well-defined path to break the problem into smaller functions and compose them later.
You no longer have to worry about the sequence of events that happened before the problem occurred because there’s no side effect.
You also don’t have to worry about the sequence in which the functions are executed because the behavior of the function is only driven by the set of input parameters.
It’s much easier to find defects in the functional programming world than in the imperative programming world.
To make all this possible, follow the Unix design philosophy:
The second rule is a by-product of following the first rule.
Keeping functions small and pure automatically helps you compose them together with other functions, as you saw in the previous example.
One way to keep functions small is to have them only take one parameter (although the reality is you’re going to have functions that take multiple parameters and do a combination of things)
The second rule is to write functions while keeping function currying in mind, or use partial functions.
When declaring functions, make sure you order your parameters from most specific to most generic.
It will help others to use functions in more places and to replace the generic parameters—or even better, have functions that take only a single parameter.
As an object functional language, Scala offers the benefits of both object-oriented and functional programming.
Functional programming gives you the additional benefit of composition that makes the core and more complex parts of your application easier to write and maintain.
So what about those functions with side effects? Are they hopeless in terms of composition? No.
In the next section I show how you can build abstractions around them so that they can participate in composition.
If you come from an OOP background you’ve probably encountered design patterns.
In this section I describe a functional programming design pattern called monads.
The problem with monads is the mysticism that comes along with them.
The general misconceptions about monads are that they’re hard to understand and that you need a good mathematical background to fully appreciate them.
It’s true that monads originated from tenets of category theory,14 a branch of mathematics that formalizes abstract mathematical concepts into collections and arrows.
But they provide a nice abstraction layer (like design patterns) to help structure your code.
Many implementations of monads exist, and each solves a specific kind of problem.
You’ve already used monads, in fact—the two common ones so far are List and Option.
The Option monad abstracts out the computation that may not return anything (Some or None)
Monads are usually considered an advanced functional programming concept, but I feel strongly enough about them to put them in this book because they have enough practical benefits that you as an application developer should know them.
Monads let you compose functions that don’t compose well, such as functions that have side effects.
Monads let you order computation within functional programming so that you can model sequences of actions.
Both of these are critical and powerful properties to be aware of when designing applications using functional programming techniques.
First I’ll explore the second benefit because it’s commonly used even if you’re building smaller mathematical microworlds without side effects.
In the later part of this section I show you how to compose side-effecting functions in functional programming style.
When I introduced functional programming I mentioned that it doesn’t care about the sequencing of functions or operations, because functions are pure.
This application needs to calculate a price for a product by following a sequence of steps:
Apply a product-specific discount to the result of the previous step.
Apply tax to the result of the previous step to get the final price.
It’s clear that you have to maintain the price generated by each action and pass it to the next action in the sequence.
How will you do that? The imperative answer is to use a mutable variable shared by each action—bad idea, for all the reasons I’ve mentioned throughout the book.
What about implementing all these actions in one function? Yikes! That could result in a large function, because each step could potentially be 10–20 lines of code.
A better answer would be to implement each step as a function and pipe the result of the current action to the next action.
I stubbed out the uninteresting parts of the code into a file called Stubs so that it doesn’t clutter the code example.
The most interesting feature in listing 5.4 is the calculate price method B.
It invokes each individual function and wires them together in a sequence by passing the result of one function to the next.
Naming variables with a, b, and c is not a good idea, but it nicely shows how the instance of PriceState is passed around.
This solution works and is implemented using a functional programming style, but the API of each individual function looks ugly.
The last line of each apply method in listing 5.4 shows the problem.
It’s easy to get something wrong while managing PriceState on your own, while wiring each individual method.
In more complicated problems, this kind of approach becomes messy.
Surely a higher abstraction that takes care of this state management would be helpful.
It is called a State monad because it threads the changing state across multiple operations transparently.
In this case, you’ll implement a State monad so that you don’t have to manage the PriceState across multiple method calls.
But you’ll have enough of a generic implementation so that you can use it in other places where you have similar problems.
The Scalaz15 library provides implementations of lots of monads—consider using them without reinventing the wheel.
All these methods now take an instance of PriceState and return the calculated price.
Your job is to implement a State monad so you can sequence these methods or actions to calculate the final price.
The State monad encapsulates a transition function from an initial state to a (newState, value) pair.
It could be easily represented as a trait in Scala, as in the following:
To implement this trait all you have to do is provide a function that takes S and returns (S,A)
You’ll use this object as a place to keep all the handy methods you need to work with your State monad.
While you’re there, add a couple of methods to the State object to make life easier:
The init method lets you create State monads with a skeleton implementation of a transition function (s => (s, s))
Think of it as a default constructor of the State monad.
It lets you modify the current state inside the monad with a new state and return the given function and value part of the (S, A) pair with Unit.
To treat the State trait as a first-class monad you also have to implement map and flatMap methods.
Why will be clear once you’re done with it, but for now remember that map and flatMap are critical parts of the monad interface—without them, no function can become a monad in Scala.
Implementing map and flatMap is easy because you know how to create an instance of the State monad.
The following listing shows the trait that represents the State monad.
The map method of the State monad helps transform the value inside the State monad.
On the other hand, flatMap helps transition from one state to another.
If all this feels a little abstract, don’t worry—it will make sense when you use these constructs to implement the solution.
So far you’ve learned that State monads take care of threading the changing state across method calls without your being worried about it.
But you still have to invoke individual pricing methods in sequence as specified by the business rules.
The best place to sequence a series of method calls is in a for-comprehension.
This guarantees that the steps you specify inside them will execute in sequence; it’s also a great way to isolate things that need to be run in a sequence.
In this case it will be something like the following:
Lots of things are going on in this small code example, so let me take it from the top.
The modifyPriceState method is a handy method that takes one of the pricing methods and lifts it to a function so that you can invoke the modify method inside the State object.
Each modifyPriceState method creates an instance of a State monad.
When you invoke them inside the for-comprehension, you get a State monad back that encapsulates this sequence of method calls and knows how to create a price state with a final price.
Note that now stateMonad holds a transition function that’s a composition of all the pricing methods defined inside the for-comprehension.
And the benefit of this approach is that the threading of state is almost invisible from the application code and is hidden inside the monad.
You can pass around this instance of a State monad, and when the time comes you can calculate the final price by passing the initial state:
How does this work? The magic is in map and flatMap.
You’ve used for-comprehensions for List and Option—because they both implement map and flatMap.
Chapter 4 looked into this in great detail, and in this section I’ll dissect the previous for-comprehension and show you how it gets translated to the map/flatMap combination.
Note in the previous code example those underscores on the left side of the forcomprehension.
They represent the value part of the pair, and you don’t need to worry about them for this example.
I’ll show you another example where this value would be used effectively—the following listing shows the complete reimplementation of the retail pricing using StateMonad.
State monad is a general abstraction layer that allows you to build computations for sequences of actions that require a shared state.
You’ve yet to see the relevance of having the pair of state and value in your State monad implementation.
Although it’s true that you don’t always need them, if you have a computation that relies on the current state of the monad, you can use the state and value pair to do your computation.
Suppose you need to implement logging for the retail-pricing example and log the result of each step.
To implement logging, you need to expose one more method to the State object, called gets.
This method lets you access the current state so you can create the log message and save it as a value inside the monad.
It’s similar to the modify method but allows you to provide a function that takes S and returns A.
The gets method also creates a new instance of State monad with the value returned from the given function f.
Now you can sequence the log steps after each pricing action inside the for-comprehension, as shown in the following listing, to log all the steps.
First you create a logStep method to wrap the gets method to provide a little more readability.
Secondly you’ve sequenced the logStep after each state modification so you can track the state changes.
Finally you’re combining the log of each step as part of the yield to create a list of log messages.
See how easy it is to add behavior that relies on changing state using State monad?
The building blocks for monads in Scala are the flatMap and map combination.
If you think of a monad as a container, then flatMap and map are the only two possible ways to get into the value currently stored inside the container.
Both flatMap and map take a function as a parameter and create a new instance of a monad by applying the.
But in both cases you end up with another instance of a monad.
To retrieve the value from the monad, you have to use a different technique.
For our example I used the apply method defined inside the StateMonad.
For example, scala.Option is a monad, and you use pattern matching to retrieve the value from the Some instance.
The important part is to understand why you need both flatMap and map methods.
To clearly understand why both are important, reimplement the calculatePrice method from listing 5.6 without the forcomprehension:
Here price state is chained together using flatMap without the syntactic sugar of forcomprehension.
As you can see, I used both flatMap and map together.
This is exactly how Scala will also translate the for-comprehension from listing 5.6
Now compare the previous code using the following signature of map and flatMap:
The map method lets you create an instance of State monad, and flatMap lets you flatten the nested state.
Without flatMap you end up with a nested instance of State monad because each invocation of modifyPriceState returns back an instance of State monad.
Try to change the previous code to use map instead of flatMap to see the difference.
Here’s the recipe for building a monad on your own:
Decide on a way to get the value of the monad (pattern matching or apply)
Monads are almost everywhere—you’ve been using them without knowing it.
One common monad you haven’t seen here is the I/O monad.
Now you know how to create monads on your own and identify them if you see them in the wild.
Monads are a great way to raise the abstraction level that is composable.
You could have invented monads17 (and maybe you already have)
Even though you’ve been using functional programming constructs provided by Scala, for the first time I explained functional programming in detail in this chapter.
You looked into the roots of functional programming and into the example of a purely functional program.
Enterprise developers find it hard to not worry about side effects because any interesting program out there somehow has to talk to the outside world.
You learned how you could still build pure functional modules and push the side effects as far as possible from the core code, which will help build the confidence in and correctness of your applications.
Now no more hours of debugging will be required to isolate the mutable state that’s causing your application to misbehave.
The critical benefit of functional programming is composition, the fundamental property that you apply when you construct large programs.
And with the power of type abstractions available in Scala, you can finally build reusable functions and components.
You also learned about a functional programming design pattern called Monad.
Monads let you compose in the face of side effects.
At first they appear to be complex, but once you start using them you’ll see this pattern in many places, including the standard Scala library.
Using Monads you’ve merely scratched the surface of functional programming design patterns and concepts.
You can use the concepts you’ve learned here in any programming language.
Always remember the key is to create a referentially transparent expression, and if that’s possible in your language then go for it.
In short, one language is more functional than another if it makes composing functions easier.
Chapter 6 explores how you can start taking advantage of the benefits of your Java code bases by integrating them with Scala.
These five chapters in part 2 focus on working with Scala.
You’ll build applications using the Simple Build Tool (SBT), connect to a database using Squeryl, build scalable and reusable components in Scala, and use actors to make concurrent programming easy.
In chapter 6 you’ll learn how to create a simple web application using the SBT and the Scalaz HTTP module.
But the application you set out to build in chapter 6 won’t be complete by chapter’s end because to build a functional Kanban application, your application needs to store information such as stories and its status information into persistent storage.
In chapter 7 you will complete the weKanban application and learn how to retrieve and store information in a relational database.
In chapter 8, in which you will learn how to build simple and reusable components, your focus will be on the type system—types of types.
You’ll also learn about a new kind of polymorphism using type classes that allows you to express and create abstractions that are easy to extend and scale—a powerful construct to help you solve your day-to-day programming problems.
Think of an actor as an object that processes a message (your request) and encapsulates state (state is not shared with other actors)
The ability to perform action in response to an incoming message is what makes an object an actor.
At a high level, actors are the way you should do OOP.
The important thing to remember is that the actor model encourages no shared state architecture.
I explain why that’s an important property to have for any concurrent program.
The goal in chapter 10 is to make you comfortable writing automated tests in Scala so that you can build production-quality software.
The path to writing well-crafted code is the path where you write tests for your code.
Another goal is to dispel the common perception that writing tests is hard.
Your first steps will be getting started with practices like test-driven development (TDD) and continuous integration for your Scala project.
But this chapter introduces you to an interesting library called Scalaz (http://code.google.com/p/scalaz/)
This simple library will allow you to focus on building a web application in functional style without worrying about the complexity of a full-stack web framework.
There are quite a few similarities between web applications and functional programming.
Think of a web application as a collection of functions that takes an Building web applications in functional style.
Each URL endpoint is mapped to a function that knows how to handle the request.
Because you’re building in functional programming style, the web application state (like user sessions) is explicitly specified in each request.
The benefit of thinking in this style is that you can build web applications by composing functions or using higher-order combinators.
Frameworks built using this strategy usually are stateless and scalable.
In this chapter you’ll learn to use functional programming to build a web application.
You’re also quickly reaching a point where you have to start thinking about a build tool to compile and run Scala applications.
Even though you could use almost any existing build tool for Scala applications, the de facto standard is SBT (introduced in chapter 1)
This chapter will explore this tool and will show you how to configure and build Scala web projects using SBT.
Get yourself a nice coffee and a sandwich before diving in to build your first web application in Scala.
The word Kanban is derived from the Japanese language and it means “card-signal.” In Kanban, the card-signaling is used to trigger action for new work.
This mechanism is also known as a pull system because new work is pulled in only when there’s available capacity to handle the work.
The essential idea behind the Kanban system is limiting the work in progress.2 Stop starting and start finishing is an important mantra aimed at reducing the amount of work in progress and, ultimately, waste.
Thanks to Agile software development methodology, the card wall (or notice board or whiteboard with index cards) has become popular, and you’ll use it to visualize the work in progress for user stories and backlog, and to determine who is working on what.
But card walls aren’t necessarily a Kanban system unless there’s an explicit limit on work in progress and a signaling system to pull new work.
The Kanban board you’ll build (figure 6.1) has this limit in place for the ready, dev, and test phases.
According to figure 6.1, you can move one more story from the ready phase to the dev phase.
A pair of developers looking for new work can select a card from the ready phase and move that card to the dev phase.
Once the development work is done, the card moves to the test phase where, in this stage, a tester, business analyst, or other members of the team will verify the work against the user story.
When the story is approved or verified, it’s moved to the deploy phase, which means it’s ready for production deployment.
This is how a card (work) flows through the system.
You’ll name your Kanban application weKanban, and here are the user stories you’ll implement:
As a customer, I want to create a new user story so I can add stories to the ready phase.
As a developer, I want to move cards (stories) from one phase to another so I can signal progress.
In this chapter and the next, you’ll implement these stories and build a full, working web application in Scala.
But first you’ll learn about SBT so you can compile and test your application.
SBT3 is a build tool for Scala and Java projects.
It is entirely written in Scala, and you can write Scala code or use SBT’s built-in DSL to configure your project and its dependencies.
The benefit of using Scala for configuring your build definition is that you have the full power and type-safety of the language.
This situation is quite different from Maven (http://maven.apache.org) or Ant (http://ant.apache.org), where the project build configuration is written in XML.
You’re going to use this feature to your advantage to autodeploy changes to the web server.
The following few sections introduce you to SBT, starting with installing to your environment.
Then you’ll explore the basics of SBT and learn how SBT projects are structured.
I focus on creating and building web projects with SBT for your weKanban application.
Because SBT is configured using Scala code, you’ll see examples of how that works and gradually build a project definition for your weKanban application, including creating new tasks.
The easiest way to get up and running with SBT is to download4 the latest version of the SBT .jar file from the website and create the script file, depending upon your OS.
To run SBT from any directory, put the sbt file in the ~/bin folder along with the downloaded .jar file and configure it in the PATH variable.
You might have to set the permission to make the file executable using the following command:
Put the downloaded .jar file and the batch file in the same directory and alter your path so that it’s accessible from any directory.
It’s also at times useful to have the encoding of the terminal set to UTF-8 so that you can work with Unicode method names inside the SBT console (REPL)
To verify that your setup is working properly, type sbt in the command prompt and click Enter.
If the setup is working, you’ll see the sbt prompt and something similar to the following line as an output:
You can start SBT on any empty folder and you don’t need a project structure to get up and running.
In your case the project name and the location could be different.
To make sure you’re in an SBT prompt, type in the following command to print the version of SBT you’re using:
If you see an output, you know your installation is working correctly.
The definition of the project has changed in SBT 0.11+ so that any folder can be a root folder of the project.
The benefit of the new approach is that now you can use SBT to build your Scala scripts without worrying about creating full-blown projects.
Any folder with one Scala file can be treated as an SBT project.
The next section explores the project structure and fundamentals of SBT.
It’s important to understand the basics of SBT before you start using it.
The goal of this section is make you aware of the basics so you can set up your Scala projects to use SBT.
The first option is the easiest way to get started with SBT.
It’s a DSL to declare the build definition for the project.
For a more complex build, you need to use .scala build files.
That’s why it’s common to see both .sbt and .scala build files in typical Scala projects.
Later I’ll explain when to use the .scala version of the build file.
For now let’s start your journey with SBT with a simple build file.
It looks for Scala source files in the base directory, inside the src/main/scala and src/main/java folders.
The minimum requirement for a valid SBT project is a source file under the base directory.
Let’s create a simple Hello world! program to play with SBT.
The following snippet creates an empty folder called test and creates a hello world application:
Now fire up SBT to compile and run the application.
Once in the SBT prompt, you can invoke the compile task to compile the source code.
And once the source code is compiled, invoke the run task to run the hello world example.
All the compiled classes are generated under the target directory.
To see all the available SBT tasks, invoke the tasks task from the SBT prompt.
By default, SBT will use the Scala version that shipped with SBT to compile the source code of the project.
You can easily change the default Scala version to some other version by using the set command.
The following commands will change the name and version of the project from within the SBT prompt:
Each time you call set, it changes the settings of the project.
To learn more about SBT settings, check the “Settings in SBT” sidebar; but in short: scalaVersion, name, and version are predefined keys in SBT that contain String type values associated with the build.
The type of each of these keys is SettingKey[T], where T is the type of the value it can accept.
To persist these changes in the settings, invoke the session save task from the SBT prompt.
This will take the settings changes and save them into the build.sbt file under the base directory:
Each line in the build file is an expression, and each needs to be separated by a blank line—otherwise, SBT can’t distinguish them.
The expressions in the build file create a list of settings for SBT.
A build definition in SBT is nothing but a list of settings represented by Setting[T] (refer to the sidebar “Settings in SBT”)
When all the settings are evaluated, SBT creates an immutable Map of key value pairs.
Here := is a method call on a key called name.
You could have written that preceding line like this as well:
All the available keys are defined in the sbt.Keys object, and it’s automatically imported for you in the build.sbt file.
You can also specify import statements inside build.sbt, but they should go at the top of the file.
The build.sbt file is a great place to configure build settings.
For example, you can enable unchecked and deprecation warnings for the Scala compiler by adding -unchecked and -deprecation values to the scalacOptions key:
Settings in SBT Settings are the way SBT stores the build definition.
A build definition defines a list of Setting[T] where Setting[T] is a transformation affecting SBT’s key value pair.
SettingKey[T] is a key with a value computed only once.
TaskKey[T] is a key with a value that has to be recomputed each time.
InputTask[T] is a task key which takes command-line arguments as input.
The ++= method lets you append multiple values to scalacOptions.
One more important thing to note here is that SBT build files are type-safe.
The type of key determines the type of the value.
Some argue about the benefits of type-safe build tools, but I say if type-safety is good for your code then surely it’s also good for build files.
In any medium or large project, you will write a considerable amount of code for your build system, and SBT can provide type-safety for faster feedback.
In the next section you’ll learn to build a more formal project structure for SBT.
If you’ve used SBT 0.7+ before, you might be a little surprised to know that SBT doesn’t create a Maven-style project structure for you.
But don’t worry, because now you have multiple ways to create your project structure.
You can use the following snippet to create all the folders typically found in an SBT project:
This will create all the folders you need for a typical Scala application.
As a second option, you can use an SBT plug-in to create a new project.
An SBT plug-in extends the build definition by adding new tasks and settings.
Since the plug-in creates new SBT projects it makes sense to add it as a global plug-in.
Global plug-ins are automatically added to all SBT projects; adding a plug-in to a project confines it to that project.
To use it, add the following lines to the plugins.sbt:
The resolvers key tells SBT of the locations to find the dependencies.
The += lets you append new resolvers to existing ones.
The addSbtPlugin function adds a new plug-in to the SBT build system.
The plug-in provides npSettings and by adding the above line to build.sbt this setting will be available to all the SBT projects.
Now to create a new project just execute the following commands:
The np plug-in also generates a default build.sbt file that you can modify to add your settings.
It’s a command-line tool to generate files and directories from templates published in Github.
This is slowly becoming a standard way of creating projects in Scala.
Once giter8 is installed, you can choose a template to generate the project structure.
For example, the sbt compile run command will execute both compile and run.
It doesn’t matter how you’ve created the project structure; the structure should look familiar if you’ve used the Maven build tool previously because SBT uses the Maven project directory structure (figure 6.2)
In fact, if you use Maven with a Scala plug-in (http://scala-tools.org/mvnsites/maven-scala-plugin/) to create a project, you’ll end up with almost the same project directory structure as SBT.
If your project has Java source files along with Scala source files, you need to have a folder called java under src/main and src/test.
Figure 6.2 shows a complete SBT project with all the possible build configurations.
It’s a simple build configuration that allows you to set various build-related settings and dependencies.
You haven’t configured dependencies yet but you’ll learn how to shortly.
This also means you can have multiple .sbt and .scala build files in one project.
The build.scala file gives you the full power of SBT.
Instead of using the DSL, you can write Scala code to configure your build using build.scala.
In the old SBT world, this was the only way to configure build.
But in the “new” SBT it’s recommended that you start with the simple build definition (build.sbt file) and only when needed create the build.scala file.
For your weKanban project, you’ll use both of them together.
The build.properties file allows you to set the version of SBT used to build the project.
For example, the contents of my build.properties are as follows:
This sets the version of SBT used by the project.
Please note that you don’t need the build.scala and plugin.sbt files to build projects with SBT.
Only add them to the project when the need arises.
The target folder is used to store generated classes, .jars, and other artifacts produced by the build.
The rule of thumb is to define all the settings in the .sbt file and use .scala files when you need to factor out a val or object or method definition.
For multiproject setup, the build.scala file is used to define common settings and tasks for multiple projects.
The project directory is another project inside your project that knows how to build your project.
And the project/project knows how to build the parent project.
There are two ways you can manage dependencies with SBT: manual and automatic.
To manually manage dependencies, copy the .jar files you want to the lib folder.
The downside is now you’re responsible for managing those .jars, updating them, or adding them.
The most common and recommended way to manage dependencies in SBT projects is to allow SBT to do it for you.
In automatic dependencies management, you specify the dependency in your build definition file, and SBT handles the rest.
For example, the following build.sbt file adds a jetty server as a dependency:
This key holds the sequence of all the dependencies for a given project.
This way of referring to dependencies is exactly how dependencies are resolved in Maven using Project Object Model (POM) files.
Any dependency can be uniquely found using the three properties in the preceding code.
In SBT, a resolver is mapped to a URL that hosts dependencies (like Maven repositories)
You can also easily add new resolvers to the list of existing resolvers using the resolvers key.
Apache Ivy is a dependency manager with flexibility and configurability.
You can also declare dependencies for a specific configuration (scope) by specifying an additional value after the version in the dependency declaration.
The following line declares the dependency to specs (unit testing framework for Scala) but only for a test configuration:
Now this dependency is only available for classes under src/main/test.
Using SBT on existing Maven Scala projects Because SBT follows Maven project structure and uses Maven dependencies, setting up SBT for a Maven project is easy.
Alternatively, you can create a project definition file configured to use a local Maven repository:
Another common thing you can do with SBT is create custom tasks for the project.
For custom tasks, the .scala build definition file is used because the .sbt build file doesn’t support it.
Put the task in the .scala build file under project.
TaskKey is similar to SettingKey, but it’s used to define tasks.
The main difference is the value of SettingKey is evaluated only once, but the value of TaskKey is evaluated every time the key is accessed.
It makes sense because you want to execute the task over and over again.
But both SettingKey and TaskKey produce settings (key-value pairs) for the build.
The following shows a simple Build.scala file that defines a hello world task:
If the name of the project is example, the Build.scala file should go under the example/project folder.
The name part will be used to invoke the task from the SBT prompt.
The build definition of the project should extend sbt.Build, and it gives access to default build settings.
Each build definition should also define one or more projects.
In this case you have only one project, but multiproject builds will declare all the subprojects here.
Multiproject build definitions are beyond the scope of this book, but you can always check out http://scala-sbt.org for details.
Because you want to add the hello task to the project, you set it by calling the settings method on the project.
Now you have a new task available for the project:
You should have all the basics you need to know about SBT to use it to build a web application.
In the next section, you’ll build the weKanban project structure and learn how to build web applications using SBT.
To set up the weKanban project, first create the project structure as shown in figure 6.3
This structure will look similar to the structure in figure 6.2 with additional folders for web projects.
As you build the project, you’ll fill up these folders and build files.
Start by setting the SBT version you’re going to use for the weKanban project in the project/ build.properties file:
The only purpose of the build.properties file is to set the value of the sbt.version key.
Debugging project definition in interactive mode Depending on the size of the Scala project you’re working on, the build definition could become quite big.
To troubleshoot any problems with a project definition, SBT provides a task called console-project.
If you execute this build command inside the SBT console, SBT will load the Scala interpreter with your build definition.
If you run console-project it will load all your build and plug-in definitions from your project and make them accessible.
If you run the console-project task on your example project, you can access its settings and run tasks:
Similarly, you can launch the Scala interpreter with your project classes using the console build command.
Remember to separate each setting expression with an empty new line so that SBT can parse each expression .sbt file.
When SBT loads a .sbt file, it creates a Seq[Setting[T]] of all the expressions defined in the .sbt file.
This plug-in adds tasks to the SBT build to start and stop the web server.
This adds the web plug-in as a dependency to the project.
Adding the plug-in is nothing more than adding a library dependency to the build definition.
The <+= method allows you to compute a new list element from other keys.
Here the sbtVersion key is used to determine the exact version number for the plug-in.
In fact, the apply method of the sbtVersion is used to compute the version of the plug-in:
Before using the plug-in to start and stop the project, you have to add Jetty dependencies to the build definition inside build.sbt:
Note that the Jetty dependencies are added into the container scope.
Think of scope as a name-spacing mechanism that allows a key to have different values in different scopes.
For example, in a multiproject build, you could have the sbtVersion key value set to a different version of Scala for each project.
This is useful for plug-ins because scoping allows plug-ins to create tasks that don’t conflict with other task names.
To include all the tasks from the plug-in to your project, you have to import the settings from the plug-in project into your build.sbt file as follows:
If everything goes well in your SBT prompt, you should see additional tasks under the container scope (you might have to invoke the reload task):
Because SBT forks a new process to run the Jetty server, you can execute other build actions in the SBT console while the server is running.
In http:// localhost:8080/ you should see the directory listing of the webapp folder.
Now let’s switch gears and talk about Scalaz, a framework for building web applications in Scala.
Scalaz (pronounced “Scala-zed”) is a library written in the Scala programming language.
The idea behind Scalaz is to provide general functions that aren’t provided by the standard Scala API.
This section introduces you to the HTTP module that comes with the core Scalaz distribution.
And while you’re using the HTTP module, I’ll touch on some of the Scalaz core APIs that are used by the Scalaz HTTP module.
Let me first introduce you to the Scalaz HTTP module and how you’ll use it for your weKanban application.
What the Scalaz HTTP library exposes is a way to write functions that transforms an HTTP request into a response.
This exactly matches what was discussed in section 6.1, where I talked about mapping HTTP URLs to functions that take requests and return responses.
The following is an example of what a web Application trait looks like in Scalaz:
The Application trait defines a single apply method that takes an instance of request and returns an instance of response.
The easiest way to implement this method would be to create a factory method that takes a function to transform request to response.
The application method creates a new instance of the Application trait by passing the function that takes a request and returns an instance of response.
The type parameters used by the Application trait look quite different than what you saw in section 4.1—they’re called higher-kinded types in Scala.
Think of higher-kinded types as a way to specify a type of a parameter type (type of types)
I know this is little confusing, so let’s break it down a bit.
Another example of higher-kinded types You’ve already seen an application for higher-kinded types in the Scalaz library, so now let’s study an example to understand why higher-kinded types are so powerful.
You’ve learned that higher-kinded types are nothing but a group of types, and when you have to write a function that could operate on a group of types, higher-kinded types are a common way to implement it.
How would you implement a sum function that could operate on various types of Scala collections? One way would be to implement sum for all the collection types:
This isn’t an effective way to implement a sum function, but if you create an abstraction for all the collections as a type, then you could write a generic sum function that works with that type.
First make your sum function work with all types that implement the + function.
To achieve that, create a trait called Summable that’s parameterized for type A:
Now for each type that supports the + function, I’ll implement this trait.
The following is the implementation for Int and String types:
Now, to implement the logic to sum all the elements of a collection, use the foldLeft function, but this time you’ll create a trait to abstract the foldLeft function for any higher-kinded type:
Note that you’re using the Summable trait created a minute ago.
Now, for each type of Scala collection, implement this trait:
Both request and response objects need to talk to the input stream and output stream to read and write HTTP parameters.
But wouldn’t it be nice if we could think of this input or output stream as a collection? The request and response would have a collection of bytes, and we could use all the collection API methods.
The advantage of this is that now you can use all the collection methods to read and write without worrying too much about the input and output stream.
And because Scala Stream is a nonstrict collection (see section 4.5), you only read from the input stream when you need it.
Why do you need a higher-kinded type again? Because Stream is a collection and has its own parameter type, you have to say Stream of something.
Your generic sum function will take three parameters: the collection, the appropriate implementation of the Foldable trait, and the Summable trait for a given type:
Here you’re parameterizing the sum function for the type of collection and the type of the objects the collection holds.
Now to sum the list of integers and array of strings, you can use the previous sum function as follows:
In a smaller context, this approach might look like a lot of work, but in a large context this is a powerful way to create abstractions, and you’ll see some real-world examples of it in the next chapter.
This class in turn uses the Scalaz core library to convert inputStream to Scala Stream.
To deploy your web application in Jetty or any Java web container, you have to conform to the Java Servlet API.
I mentioned earlier that Scalaz provides a wrapper around Java servlets, so you don’t have to worry about that too much.
Figure 6.4 shows how HTTP requests are handled in Scalaz when deployed in a web container.
Like a standard Java web application, Scalaz is configured using web.xml.
Usually this servlet is configured with the name of the application class (similar to the Application trait we saw earlier) that will handle all the request and response.
You have to write this application class for your weKanban application.
When the web server (we’re using Jetty) receives an HTTP request (see figure 6.4) it calls the service method of ScalazServlet.
Inside the service method it transforms the HTTP servlet request to the Scalaz Request object and then invokes the application method of the Application trait configured in web.xml.
Once the application method returns the Scalaz response, it transforms that response object back to the HTTP servlet response so that the web server can send the response back to the caller.
With this new knowledge, let’s move on and configure Scalaz to your SBT build.
After this, you’ll be ready to implement stories for your weKanban application.
Figure 6.4 The way HTTP requests are handled by Scalaz.
The following listing shows how it will look like after adding the Scalaz dependency.
After adding Scalaz dependencies, if you reload and update your project from the SBT console, SBT will download the necessary Scalaz .jar files from the Scala snapshot.
Servlet lifecycle The lifecycle of the servlet is controlled by the web container (in this case, Jetty) in which the servlet is deployed.
When the container receives a request that maps to a servlet, the container performs the following steps:
ScalazServlet overrides this init method to initialize the application class from the init parameter.
Servlet’s service method is invoked by passing a request and response object.
Typically servlet-based frameworks override this service method to invoke framework-specific classes.
In the case of ScalazServlet, the service method transforms the HTTP request and response to Scalazspecific request and response instances and invokes the application class to handle the request.
Each Scalaz-based web application will provide the implementation of this application.
In the preceding project definition, notice that for scalaz-core and scalaz-http I’m using double %% rather than single %
This tells SBT to look for dependencies matching the Scala version of the project.
If multiple Scala versions are configured, it will download dependencies for each configured version.
Ideally you should use this pattern for declaring dependencies, but not all libraries in the Maven repository support the naming convention required by SBT to make the pattern work.
In the previous section you learned about Scalaz and how it works in the Java web server environment, but you haven’t configured one.
I’m not going to explain the entire web.xml file here, only the parts that are interesting for our purposes.
The two most important things you need to configure in the web.xml file are the Scalaz servlet and the application class.
When the Scalaz servlet is initialized, the application class that’s passed as an init-param will be instantiated.
Let’s save this web.xml in the src/webapp/WEB-INF folder per the Java servlet convention.
The only abstract method defined in the ServletApplication trait is this:
This application method isn’t that much different from the one I started this discussion with (section 6.4.1)
Because you’re using a servlet to handle the HTTP request and response, Scalaz is providing access to the underlying HttpServlet and HttpServletRequest.
The only thing that will look new to you is the implicit keyword before the servlet parameter.
The beauty of declaring the implicit parameter6 is that, if such a method misses its arguments for implicit parameters, the compiler will automatically provide such an argument by looking up the implicit value matching the type of the argument in the enclosing scope.
I think you’re ready to implement your Scalaz application class.
What if I want to roll out my own servlet? It’s somewhat easy to create a servlet that extends ScalazServlet.
The only thing you have to do is provide parameter type values for the request, response, and type of application class you’re going to use.
The only requirement for the application class or trait is to provide a method or value called application that is of type ServletApplication.
The only abstract value you have to implement is application, and the preceding code does that by providing an implementation of the application method.
The quickest way to verify your configuration and setup is to add a static HTML file that you’ll load using your application class.
This way you’ll know that your environment is working properly.
To load any static resource from the web context (in this case, src/main/webapp), Scalaz provides a useful method called resource.
Using this method, you can load any existing resource requested:
Here the resource method will try to load the resource from the filesystem relative to your web context path and, if found, invoke the first parameter passed to it.
The first parameter is a function that takes Iterator[Byte] and returns a Response[Stream]
You can invoke the resource method in the following way as well:
Now when you invoke the << method on a Scalaz status code object, it converts that to an empty Scalaz Response object.
Once it’s converted to a Response object, << appends the stream to the body of the response.
With OK << x.toStream, you create a Scalaz Response object with the contents of the requested resource.
Similarly, NotFound is a case class representing HTTP status code 404; when calling the xhtml method, it implicitly gets converted to a Scalaz Response object with an HTTP header value for a content-type of "application/ xhtml+xml." This is a good example of how you can use higher-order functions and combine functions to create nice APIs like the preceding example.
Chapter 4 talks about higher-order functions and functional compositions at length.
After putting all these pieces together, your application looks like the following listing.
To create a valid Scalaz application class, you have extended this trait.
The ServletApplication trait also defines an abstract method called application that takes servlet, HTTP request, and Scalaz request as parameters.
This method is the core of the Scalaz-based web application and is invoked for each HTTP request that’s mapped to ScalazServlet.
So far, your application can only handle static content (you’ll change this little fact in the next chapter), and to load static content you’re using the Scalaz library method called resource that takes two parameters.
The first parameter to the method is the function Iterator[Byte] => A (here A is Stream) that looks for the static content for the path specified in the request and loads the content as bytes.
In the nested found function, you’re transforming the Iterator of bytes to a Scalaz response.
The second parameter is another function that gets called when no static content is found.
Let’s create a simple index.html file under src/main/webapp as a placeholder for your weKanban application, as follows:
Now go into your SBT console (you can always start the SBT console by typing sbt under the project folder) and run the jetty-run build action.
This will start the Jetty server in the background, and your application should be deployed automatically provided all the steps have been followed properly.
Point your browser at http://localhost:8080/index.html, and you’ll see the placeholder page.
Go ahead and pat yourself on the back, because you’re done with the Scalaz and SBT setup and are ready to implement your weKanban application.
This chapter was your first step in building a medium-sized Scala application.
For the first time you moved outside the RPEL to build a Scala application.
You learned how to work with it, configure it, and manage dependencies, which is important when it comes to building large Scala applications.
You used Scalaz’s HTTP module to build your web application.
You also learned how functional programming concepts could be used in web applications.
And you saw how Scalaz is using Scala’s higher-order functions and pattern matching to expose nice APIs.
This chapter has provided enough background for you to work with various Scala tools to build your next application.
In this chapter, you spent most of your time laying the groundwork for your weKanban application, and you haven’t finished any of the stories you started with.
To complete any of the stories mentioned in section 6.2, you have to figure out a way to talk to persistence storage.
The next chapter explores some of the tools available for talking to databases from Scala and completing your application.
The reason: to build a functional Kanban application, your application needs to store information such as stories and its status into persistent storage.
You’ll learn how to retrieve and store information in a relational database.
I introduce a Scala Object Relational Mapping (ORM) tool called Squeryl to communicate with the database.
You’ll also explore how to model database tables in a typesafe manner.
You’ll build a new screen for adding new stories to the application Connecting to a database.
In chapter 6 you learned how to create a simple web application using the Simple Build Tool (SBT) and the Scalaz HTTP module.
In the process of building these screens, you’ll explore how to work with databases from Scala applications.
Even though the focus of the chapter is working with a database, I will show you bits of Scalaz and SBT that are required to connect all the pieces.
Before building our application, let’s recap all the stories you need to implement the complete weKanban application:
As a customer I want to create a new user story so that I can add stories to the ready phase.
As a developer I want to move cards (stories) from one phase to another to signal progress.
Let’s start by building a screen that will allow users to add a new story to the weKanban board.
The first thing to work on is adding a new story to the board, because without that it would be difficult to do anything with the board.
Adding a story means you have to worry about the persistence store.
Actually, you’re free to pick any of the following databases: Postgres, Oracle, MySQL, and DB2
The reason I restrict you to this predefined list is because the Scala Object Relational Mapping (ORM) library you’ll use for your application, called Squeryl (http://squeryl.org/ index.html), can only support those databases at the time of writing.
So why use Squeryl to access a database? First it’s popular in the Scala community.
It also provides a nice, simple DSL to talk to a database.
Even though it’s fine to use JDBC directly from Scala, you’ll use Squeryl to learn an ORM tool that’s completely written in Scala.
Scala’s strong type system is perfect for creating a type-safe ORM tool.
A fork of SLICK to keep old links to the ScalaQuery repository alive, http://github.com/szeiger/scala-query.
Querulous, an agreeable way to talk to your database, http://github.com/nkallen/querulous.
Adding a new story to a weKanban board name := "weKanban"
By now you know what you have to do to update your SBT dependencies.
For your Kanban board the story should have three attributes: a story number that identifies the story uniquely, a title describing the story, and the phase the story is in.
The following is how you represent a story class in Scala:
To make this class work with Squeryl, you have to do a couple of simple setups.
First you have to tell Squeryl you need a table that will store all the stories for you in the database.
Think of this class as equivalent to a database schema where you’ll keep all the data definitions of an application.
The following code defines the schema with a table called “STORIES” for your Story class:
One thing to note here is I’m defining the table in a type-safe manner.
The stories value now represents the database table “STORIES,” and you can invoke various types of queries on the stories object without worrying about type and always get back story type objects.
But because Scala is a DSLfriendly and expressive language, it’s common for Scala tools to use the Scala language itself for configuration.
You’ve already seen examples of this in SBT and now in Squeryl.
Before connecting to the database, make sure that the H2 database server is running.
This is a little ugly because you have to dig into the location where SBT stores all your runtime dependencies (ivy cache)
It would be better if you could start and stop the H2 server as you can for the Jetty server.
The good news is that it’s easy to add new build tasks to SBT, but for that we have to make changes to the build.scala file.
Copy the code from the following listing into your build.scala file.
The build.scala file is doing a couple of things here.
First it defines the project B by providing name, locations, and settings.
These tasks are now part of the project settings and can be used to start and stop the H2 database.
The second thing is the versions of the scalazVersion and jettyVersion are declared as a lazy val.
The benefit of this is you don’t have to repeat the version number multiple times when declaring dependencies in the build.sbt file:
Yes, you can share the settings and vals from the build.scala file to build.sbt files.
In fact it’s a common practice to declare common things in build.scala files and use them in build.sbt files.
In the end, all the settings from various files are combined into one sequence of settings.
This will help you in working with H2 without leaving the comfort of the SBT console.
If the browser doesn’t open, try going to http://localhost:8082 directly from the browser.
Because the H2 server is running, let’s switch focus to make Squeryl connect to this running server.
To connect to the H2 server, use the following driver and database URL:
Because you want to play nice with other custom tasks and plug-ins, a new scope is created for H2 tasks:
The Compile config will provide the necessary classpath setting you need to run the tasks.
The new config will create a new scope for you, and the tasks will be available under it.
This method expects a sequence of paths that point to the H2 database .jar files you need to start the database.
Because the H2 config extends Compile config, you can easily use the fullClasspath setting at Compile scope to tap into the classpath.
And the <<= method in SBT helps to create a new setting that depends on other settings.
The following code snippet maps the path information from fullClasspath in Compile scope and creates a new function that will get executed for the start task:
The startDatabase method stores the reference of the process object so that it can be used in the stopDatabase method.
The following listing adds an init method to the KanbanSchema class to connect to the H2 database.
The KanbanSchema represents the schema definition of the weKanban application.
The first thing to do is map the Story DOM with the STORIES table in the database using the table method defined in the Squeryl Schema class.
The init method is responsible for establishing a connection with the running H2 database.
SessionFactory is similar to the database connection factory and is used in Squeryl to create new connections.
This driver will be used when you create a new database connection.
Creating a new connection in Squeryl means creating a new Session.
Think of a Squeryl session as a wrapper to a database-based connection with which you can control database transactions.
The Squeryl Session instance provides additional methods like log and methods for binding/unbinding the session to the current thread.
Note that Session saves the database connection to a thread local3 variable so that each.
This is useful in web applications where you can have multiple users accessing your application at any point.
In Squeryl, the mechanism for creating new sessions needs to be defined in a variable called concreteFactory, defined in the SessionFactory object.
If the value of the concreteFactory is something other than None, you know it’s initialized.
And Squeryl expects concreteFactory to be a function that will create new sessions.
Here you’re calling the utility method defined in the Session object called create by passing a database connection and the adapter.
The Java DriverManager takes the connection URL to the H2 database, username, and password to create a new connection.
Because you’ve defined the stories object to represent the "STORIES" table, this won’t create tables in the database.
In some cases you’d rather create database tables using SQL scripts, but here you’ll use Squeryl to create the schema for you.
Do that by adding a main method to your KanbanSchema class so that you can use SBT to run it whenever you need it:
The inTransaction method defined by Squeryl runs the given closure in a database transaction.
It creates a new transaction if none is in progress.
Here in the transaction block, you’re dropping all the tables and creating them back again.
Right now the only table defined is "STORIES." Now when you execute the SBT run build action, it will invoke the main method and will create a fresh schema for you.
Now let’s move on to saving a new story to the database.
You already have a table object called stories in your WeKanbanSchema object that points to the “STORIES” table in the database.
If you create an instance of Story and pass it to the insert method, you can save a Story to the database.
Before saving an instance of Story to the database, you have to add validation.
For example, both the number and title properties of Story should be nonempty, and because the story number should uniquely identify a story, you have to make sure that the number value is unique too.
Checking whether a field is empty is simple; here’s how it’s implemented:
Add this validate method to the Story class and invoke it before saving it to the database.
This is okay for the small application you’re building here, but in the real world you should have a surrogate key as primary key for your model classes.
To add an autoincrement id field to your domain class, you can extend the KeyedEntity[A] trait.
To check the uniqueness of the number field, you have to query the "STORIES" table to make sure there’s no other story with the same number.
Squeryl provides a nice method called where in table objects that you can easily use to achieve that.
The where method takes a predicate function to filter out rows from the result.
Here’s how to check the uniqueness of a story number using the where method:
Here using the function a => a.number === number (=== is the equality operator defined by Squeryl), you’re only selecting the stories that match the given story number.
If that results in anything other than an empty collection, then the given number isn’t unique.
The query is only sent to the database when you start the iteration.
After adding this validation, the validate method now looks like the following:
Validat numb Now, before inserting the new story into the database, you’ll invoke this validate method to make sure the story is valid.
You also have to run under a database transaction so you can commit or roll back your changes when saving the story instance (more on this later)
For now, let’s add a method called save to our Story class.
But what should you return from the method? Well, you could return a success message that the story is created successfully, but what if something goes wrong while saving to the database? In those cases I prefer scala.Either (discussed in chapter 4), which allows you to return both success and error responses.
This will also help the caller of the save method to handle both scenarios gracefully (you’ll see that shortly)
Here’s the complete Story class after adding both the validate and save methods.
Here in the Story class you added two new methods, validate and save.
Inside the validate method you do a couple of validation checks.
Adding a new story to a weKanban board the story number or title is empty because neither can be empty.
The second validation involves going to the database and making sure that the given story number is unique.
Here you’re using the built-in method called where, available to all table objects.
The where method takes a function that filters out stories based on a Boolean expression (similar to the filter method defined in the Scala collection library)
In your function you’re matching on story number a => a.number === number, where a represents a story saved in the database.
The save method first calls the validate method to make sure the story is valid and then invokes the insert method defined in the stories table object to save it to the database.
Both method calls are wrapped in a closure of the tx method.
This method is defined in KanbanSchema, and you’ll see it shortly.
Because save could result in both success and failure, I’m using scala.Either as a return type of the save method.
This helps to communicate to the caller of the method that expects both success and failure.
In Scala, using scala.Either is a more common idiom than throwing exceptions from public methods.
Additionally, you created a companion object for Story to create new story instances.
The default phase for a new story is “Ready,” because that’s the first phase in the Kanban board.
The tx method in the previous code snippet makes sure that the Squeryl SessionFactory is initialized properly and starts a new transaction if no transaction exists.
The tx method takes a function as a parameter and returns the response of the function.
This function could be any closure or block of code that you want to run within a transaction boundary.
The tx method takes a function and runs that function in the transaction.
In case of an in-progress transaction, the given function will be executed and the result returned.
In this section you’ll build the screen with which the user will create a new story and add it to the Kanban board.
You’ll also hook your Story model object with the input from the screen and complete the following feature of the weKanban application:
As a customer, I want to create a new user story so I can add stories to the ready phase.
You can create dynamic web pages in Scala in many ways.
You can use JSP, the Scala template engine (Scalate4), or Scala’s built-in support for XML to generate XHTML.
Here you’ll use Scala’s XML support to your advantage to generate XHTML web pages.
It’s simple and testable and will demonstrate some of Scala’s XML capabilities (covered in chapter 2)
In chapter 12 you’ll explore Scala web frameworks that make building large web applications easy.
Figure 7.2 shows what the Create a new Story screen looks like.
Here, in the apply method of the view object, you have the necessary HTML that when rendered will create a view like figure 7.2
Even though it’s HTML, it’s also valid XML or XHTML and can be used easily as an XML fragment inside Scala code.
Now let’s tie this view to a URL so you can render this view.
Copy all the static resources like CSS and JavaScript files from the accompanying code base of this book.
The main.css file should go in the webapp/css folder, and JavaScript files should go in the webapp/js folder.
So far in your Scalaz application class you’ve handled static resources using the resource method:
To handle dynamic resources like view objects, create a method called handle in the application class.
This method will take the same parameters as the application method but will match on the URL on the request object.
Typically web frameworks use a separate configuration file to map a URL to a resource or function.
In conventionbased frameworks like Rails, the playframework URL contains enough information to map to the appropriate function or action.
Scalaz takes a different approach—it uses Scala’s powerful pattern matching, where the URL is broken into an HTTP method and URL parts as List.
For example, a request object with the URL http://localhost:8080/ card/create can be matched like this:
The MethodParts is an extractor object (see section 3.7.1) that takes a Scalaz Request and returns Option with the HTTP method and URL parts as List.
In the previous code snippet, GET is the HTTP method used to request the resource, and the second parameter is the URL broken into the List.
How an Extractor object works In chapter 3 you learned how to use case classes for pattern matching, but pattern matching isn’t restricted to case classes.
You can use almost any object for pattern matching as long as it defines a method called unapply.
Any object with the unapply method is called an Extractor object.
For example, MethodParts in Scalaz is defined as the following:
Sometimes you’ll request this resource by passing a request parameter.
For example, when a story is created successfully, you’ll come back to the same “Create story” page but with a success message.
To read a request parameter from a URL, use the ! method defined in the Scalaz Request object.
Let’s create a private method called param in your application class that will return a string value of the parameter or an empty string if the parameter isn’t specified:
The ! method of Scalaz Request returns Option[List[Char]] and you’re converting that to a string.
Now let’s create the handle method by combining these pieces:
Here, when the request matches both the HTTP method type and the URL, you’re creating a response by invoking the CreateStory view object by passing the value of the request parameter message.
OK(ContentType, "text/html") creates an empty Scalaz Response with the HTTP response header content-type represented by the ContentType object.
The << method allows you to add additional information to the response.
Here the unapply method takes an instance of the Scalaz request and returns a Some value of method and URL parts.
The parts method returns all URL path elements separated by /
Note that the apply method isn’t necessary for pattern matching.
For example, you’re using the apply method in the Story object to create a new instance of the Story class.
One rule to notice here is if you want to return some value from unapply, it then needs to be wrapped around the scala.Option type.
And because Scala encourages creating immutable objects, every time you call the << method, a new response object is created.
It’s an instruction to the web browser about what version of the markup language the page is written in.
To adhere to strict HTML standards, the example is using the strict doctype.
In the application method (your entry point for all URLs) you’ll invoke your new handle method.
You can still keep the existing resource method so you can load static resources.
The Scalaz core provides a method called | for the Option class and using it you can combine both handle and resource methods so that when the handle method returns None you can invoke the resource method as a fallback to load resources.
Because all the parameters to the handle method are implicit, you don’t have to explicitly pass them, but you can.
If the handle method returns None as a response, the resource method will be called, and you do that with the | method.
If no resource is found matching the URL, then NotFound.xhtml is returned.
If you’re used to metaprogramming in Ruby, Groovy, or other programming languages, implicit conversions are Scala’s way of doing metaprogramming but in a more controlled way.
You’ll explore implicit conversion in detail in the next chapter.
The handle method matches the HTTP request to a function.
So far, it only knows how to handle the create story request.
Here you’re using the Scalaz Extractor object called MethodParts, which breaks the Scalaz request into request type and URL parts.
The HTTP GET request to the http://localhost:8080/ card/create URL will be matched to MethodParts(GET, "card" :: "create" :: Nil), where GET is the method type and "card" :: "create" :: Nil are the parts of the URL.
CreateStory is the name of the view object, and OK(ContentType, "text/html") << strict creates an empty Scalaz response with a strict HTML doctype.
The param method retrieves the parameter value for a given parameter name.
You’re using the ! method defined in the Scalaz request to retrieve the parameter value and transform into a String.
Scalaz by default uses List[Char] to represent a parameter value.
When the requested URL doesn’t match the cases defined in the handle method, it returns None (default case)
In case of None, the application method calls the resource method B to load any static resource that matches the requested URL.
Now go to your SBT console and start the Jetty server using the jetty-run build action if it’s not already running.
That’s great, but you still have to tie the Save button with the save method of the model object.
Clicking Save will result in an ugly error because you haven’t linked the URL to any function in your application class.
Fix that by adding a MethodParts to your handle method, which will match the URL ending with /card/save to the saveStory method defined in the application class:
The saveStory method will read the HTTP POST parameters from the request, instantiate an instance of the Story model class, and invoke the save method (see listing 7.3) on it.
To read POST parameters from the request, add another utility method like the param method to your application class, but this time with a ! because POST generally means a side-effect (section 5.1):
Left and Right are the only two subtypes of Either.
You can easily use pattern matching and take appropriate actions.
When save is successful, you’ll redirect to the “Create story” screen with a success message so the user can create another story; in case of error you return to the “Create story” page, but this time with an error message.
The redirects method is defined in the Scalaz Response object, which is already imported for the application class.
The redirects method takes the relative URL and the parameters to the URL as a tuple.
The next listing shows the complete application class you have so far.
Here you’re extending the application class to handle the save story POST request.
To handle the save story request, you added a new pattern matching expression to the handle method.
Now the POST request made to /card/save will be matched by.
MethodParts(POST, "card" :: "save" :: Nil), and when it matches it invokes the saveStory method defined in the application class.
The saveStory method extracts the story number and title parameters from the Scalaz request and creates a new instance of the Story model object.
The save method defined in the Story model object will validate and save the story in the database.
In case of error, the saveStory method renders the CreateStory view object with the error message.
When the save is successful it redirects to a new “Create story” screen.
Here using the scala.Either type allows you to easily handle the error condition.
Next you’ll build the Kanban board where all the stories will be displayed.
Now the focus will move to building the Kanban board.
As a developer I want to move cards (stories) from one phase to another so that I can signal progress.
Figure 6.1 shows the prototype of the Kanban board you’re supposed to build.
To implement this story, you have to provide an ability to move cards from one phase to another.
For example, a user of your Kanban board should be able to move a card from ready phase to development phase and vice versa.
To implement drag-and-drop, use the jQuery-ui plug-in (http://jqueryui.com) for jQuery (http://jquery.com), a JavaScript framework that simplifies HTML, document traversing, event handling, and Ajax interactions for web development.
I don’t focus on the jQuery library in detail here, but if you haven’t used jQuery I encourage you to check the links.
After you download the jquery-ui library, copy the JavaScript files to the js folder under the webapp folder of the weKanban project, as shown in figure 7.3
The draggable plug-in of jQuery adds draggable functionality to any DOM element, and all you have to do is invoke a draggable method to it.
For example, if you want to make your stories draggable, invoke the draggable method on each DOM element that represents a story in the board.
For now, you’ll add a css class called story to each story to easily find them and make them draggable.
To make DOM elements droppable, call the droppable method by passing a function that can handle the drop event.
In your board all the elements that represent phases (ready, dev, and so on) will be droppable.
But only implementing drag-and-drop in the web page isn’t enough—you also have to update the database when a story moves from one phase to another so that when you come back to the board you can see the updated version.
Because you’ve already decided to use jQuery, you’ll use its Ajax features to make an HTTP POST call when a story moves from one phase to another.
The next listing shows the completed main.js file that implements drag-and-drop for the Kanban board.
Listing 7.9 Implementing drag-and-drop for the weKanban board in the main.js file.
Figure 7.3 The weKanban project with JavaScript files for drag-and-drop.
In the init function you’re making the element with the story class draggable and adding droppable functionality to all the phases identified by their ids.
To implement droppable you’ve implemented the drop function, which gets invoked when an element is dropped.
Inside the drop function you’re calling the moveCard function by passing the id of the story that’s dropped and the target phase.
The job of moveCard is to make an Ajax POST call to the /card/move URL by passing both the story number and target phase.
Now you’ll create the view object for the Kanban board and the JavaScript code.
To create the Kanban board view you have to retrieve all the stories from the database by phase.
You have to show all the stories that are in the ready state, all the stories at dev state, and so on.
At first all the stories that you create using the “Create story” view will be in the ready state.
But once you implement the drag-and-drop features, you’ll have stories in various phases.
First use Squeryl to find all the stories from the database and then apply a filter to the result to find stories matching a particular phase.
To find all the stories from the database using Squeryl, you have to do the following:
Here the from method returns an instance of Query, which takes a function that takes an instance of the story object and returns the same story object as select.
But this isn’t  helpful because you want to filter the stories by phase; to do that, add a where method that will check for the phase, as in the following:
The === method is added by an implicit method conversion to a String class so that the DSL looks like SQL’s where clause.
The Squeryl query returns an immutable instance of Query that’s a lazy Iterable collection.
At this point Squeryl has only created the query—it hasn’t executed it in the database.
It will execute the query the moment you try to access the first element in the collection.
But because you’re going to use this collection to render stories in your Kanban board view, let’s change this to strict collection from lazy collection (covered in chapter 4) by invoking the map method, so that you access these instances of Story objects outside the transaction:
Here you’re calling map on the Query object to create a collection of stories.
But because you don’t need to transform your story objects, the map is returning the parameter.
Here I’m using map to demonstrate that you can take the response from Squeryl and transform it if necessary.
Now in your view object you can invoke the findAllByPhase method by passing various phases to render the stories in the Kanban board.
The first thing you’ll do is add a header method that will include all the JavaScript files you need to enable the drag-and-drop feature to your board:
This header method, apart from adding all the JavaScript, also invokes the init method that’s defined in the main.js to initialize the drag-and-drop functionality using jQuery.
To render the stories you get back from the findAllByPhase method, add another method that will loop through the stories and create an html div element:
Here you’re using a for-comprehension to loop through all the stories and creating a story html div element.
To tie all these pieces together, add the apply method to the KanbanBoard view object, which will create the Kanban board view, shown in the following listing.
The KanbanBoard view object is used to render the Kanban board in figure 7.4
Like the CreateStory view object, the apply method is responsible for rendering the Kanban board.
The apply method calls the header method to add all the JavaScript files you need to add drag-and-drop functionality to your Kanban board.
The contents of the header method get inserted in between the HTML head tags.
To render stories in each phase, the apply method invokes the stories method by passing the phase.
The stories method is invoked for each phase in the Kanban board.
The stories method uses a for-comprehension to generate the HTML required to render the result.
The findAllByPhase method in the Story model class returns all the stories in a given phase from the database.
Com num Even though you haven’t implemented the move card functionality on the server side, in the UI you should be able to drag and drop cards from one phase to another.
Because the server side logic isn’t implemented, when you refresh the page all the stories will show up in ready phase again.
To complete the move card story, you have to implement the move card logic in the server side.
To implement moving cards from one phase to another, you have to update the phase of the story.
For example, to move a story from the ready to the dev phase, you have to get the story number (the story number uniquely identifies a story) to find the story and update the phase.
But the problem is, you aren’t allowed to cross the limit set for the phase, so before updating the phase you have to check whether you’re crossing the threshold of the phase you’re moving to.
To validate that you aren’t crossing the limit, add a validate method to the Story class; doing so will compute the total number of stories in a phase and throw an exception if you exceed the limit.
For now, hardcode the limits for each phase inside the Story class:
Here you’re selecting the number of stories in a given phase and checking with the hardcoded values in the phaseLimits.
The compute(count) is a way to invoke a count function on the result of the where function.
The compute Squeryl function is similar to the Squeryl select function except compute can invoke other aggregate functions like count.
Check the Squeryl documentation5 for all the available functions and operators.
Now, to move a story to a different phase, add another method, called moveTo, to the Story class.
This method will take the target phase and validate whether you’re crossing the limit.
If everything is okay it will update the phase value of the story.
This method will also return Either[Throwable, String] so that your Scalaz application class responds accordingly.
To update a given row in the database, Squeryl provides an update method, which takes an instance of table (in this case it’s the stories object defined in the Schema class) and a function that will update the row.
To update a story identified by this, you can use the Squeryl update function like this:
Here, inside the function passed to update, you’re first identifying the story you need to update using the where method and then invoking the set method by passing the target phase value.
Here’s the completed moveTo method defined in the Story class:
To invoke this moveTo method from the application class you have to first find the story object using the story number.
To find a story by its number, add a method called findByNumber to your singleton Story object.
This method will query the "STORIES" table and find the one that matches the given story number.
Here you’re using the single method defined in the Squeryl Query object that returns the first row from the query.
At this point you’re all done with your model changes.
Before you hook this new method to your application class, look at the following listing, which shows the complete implementation of the Story class.
Now the only thing remaining is to handle the Ajax call in your application class so you can update the database when the card is moved in the UI.
You do this with an Ajax call using the moveCard function defined in the main.js file (see listing 7.10):
This method gets called when you drop a card in a phase from the drop method (see listing 7.8)
This function makes an HTTP POST call to the /card/move URL by passing the storyNumber and the target phase of the story that moved—and handles the response by appending the message to the DOM element identified by the id message.
In the application class add a moveCard function to handle the /card/move POST request.
This function first finds an instance of the Story class by its number and then calls the moveCard method you added to the Story class.
Based on whether the move failed or succeeded, you’ll return a response message.
The first thing you do in the moveCard method is retrieve both story number and phase from the Scalaz request.
Here you’re using the param_! method already defined in the application class.
Next, using the story number, you retrieve the Story model object associated with it and invoke the moveTo method to update the phase.
Listing 7.11 shows how these methods are implemented inside the Story model class.
Based on the response from the moveTo method, you show either a success message or an error message.
To invoke this moveCard method, let’s add another pattern matching case to the handle method to match the POST request to move a card:
Now if you run the weKanban application with all the changes, you’ll be able to move a card between phases, and your changes will persist.
For the first time you moved outside the RPEL to build your Scala application.
You learned how to work with SBT, configure it, and manage dependencies—which is important when it comes to building enterprise-level Scala applications.
You used Scalaz’s HTTP module to build your web application and also learned how functional programming concepts can be used in web applications.
You saw how Scalaz uses Scala’s higher-order functions and pattern matching to expose nice APIs.
Building enterprise-level applications most of the time means you have to work with relational databases.
You looked into Squeryl, the Scala ORM tool, to understand how a relational database can be used and modeled in your Scala application.
This chapter has provided enough of a foundation to work with various Scala tools to build your next application.
I encourage you to try different Scala ORM tools, view template engines, and web frameworks to build or extend this application.
I hope the concepts and the tools you’ve learned in these two chapters will make you comfortable working with the various Scala tools available in the market.
In this chapter you got a glimpse of implicit conversion and implicit parameters.
In the next chapter we’ll take a deep dive into the Scala type system and see how we can build abstraction layers using types.
So far we’ve been working with Scala without paying any serious attention to its type system.
The type system is a tractable syntactic method for proving the absence of certain program behaviors by classifying phrases according to the kinds of values they compute.1
The challenge of learning about a type system is understanding the theory behind it.
It’s always helpful to learn the fundamentals behind a good type system, but in this chapter my focus is on the practical benefits of a good type system without going too much into theory.
In the process I explore various types of the types.
Scala provides you, with examples so you can understand their applications.
Why is the type system so important? It provides the following features:
Error detection—Think of the compiler as a suite of test cases that can detect common type and other program errors.
You’ll learn how the type system provides abstractions to build components.
Documentation—The signature of a function or method tells you a lot about what it’s doing.
Efficiency—The type system helps the compiler generate optimized binary code.
My goal for this chapter is to show you how you can use Scala’s type system to build reusable components.
I’m using component as an umbrella term to refer to reusable libraries, classes, modules, frameworks, or web services.
The goal of building software by assembling components is still a dream and isn’t possible to the extent we’d like.
The challenge of building something reusable is to make the components refer to the context in which they are built.
Typically, you modify your component to suit the current need and end up with multiple versions of the same component.
In the first section of this chapter, you’ll learn about building simple, reusable components using Scala’s type system.
Next you’ll learn about different kinds of types provided by Scala so that you’re aware of all the building blocks you have to make your code more expressive and reusable.
You’ll also learn about a new kind of polymorphism using type classes that allows you to express and create abstractions that are easy to extend and scale—a powerful construct to help you solve your day-to-day programming problems.
It’s important to understand that a good type system doesn’t work against you.
Rather, it provides you with enough flexibility to be as creative as possible.
Settle in with your coffee and don’t worry if the ride feels a little bumpy.
I promise, by the end of this chapter the results will be valuable.
As I mentioned, building scalable and reusable components is hard.
Remember that Scala traits allow you to build small components and combine them to build larger components.
Let’s explore abstract type members and self type before you start building your component, because they’re important building blocks.
Scala takes the idea of abstract beyond methods and fields.
You can also declare abstract type members inside a class or trait.
Abstract types are those whose identity is unknown at the time of declaration.
Unlike concrete types, the type of an abstract type member is specified during the concrete implementation of the enclosing class.
The following example declares an abstract type member S inside a trait called Calculator:
Any concrete class that mixes in this trait has to now provide a type for an S type member:
The benefit of abstract type members is they can hide the internal information of a component.
You’re going to build a price calculator that can take a product ID and return the price of the product.
There can be many ways to calculate the price, and each way could use a different type of data source to retrieve the price.
You’re building this for a retail company that sells various types of products from a number of manufacturers.
The common steps across all the calculators are the following:
Connect to a data source (could be of many types)
A fairly successful way to encode common steps and program skeletons is a Template Method pattern, which lets you follow a common algorithm across multiple subclasses.
Here’s how you could implement your parent Calculator trait using the Template Method pattern:
This feature of Scala provides a mechanism for composing traits for designing reusable components without the problems of multiple inheritance.
You could define contracts using it and have multiples of them (such as interfaces), or you could have concrete method implementations.
Scala lets you declare abstract type members to class, trait, and subclasses that can provide concrete types.
Self type A mixin doesn’t depend on any methods or fields of the class that it’s mixed into.
But sometimes it’s useful to use fields or methods of the class it’s mixed into.
In this example, DbConnection is a component that knows how to retrieve data from a database.
Because all the necessary steps are implemented, each calculator can implement the overloaded calculate(s: DAO, productId: String) method.
The problem with the current implementation is that it’s hard-wired to a DAO, and a calculator that uses a different kind of data source won’t be able to use the calculator.
You can easily fix the problem of the hard link to DbConnection by creating an abstract type member that hides the type of component you use to retrieve the price from the data source.
The following listing shows the Calculator trait with the abstract type member.
The Calculator trait abstracts out the type that knows how to connect to the data source.
The initialize method makes the connection to a data source, and the close method closes the connection.
Now any concrete calculator implementation, along with implementing all the abstract methods, needs to provide type information for S.
Here’s one implementation of a calculator that uses MongoDB as a data source:
The abstract type member concept is particularly useful to model a family of types that varies covariantly.
The next section talks about self type, which helps in building components from smaller components.
The self type annotation allows you to access members of a mixin trait or class, and the Scala compiler ensures that all the dependencies are correctly wired before you’re allowed to instantiate the class.
Self type makes mixin composition more powerful because it allows you to statically define dependencies defined by other classes and traits.
In the following example, trait A is defining a dependency to trait B:
Trait A can’t be mixed in with a concrete class unless that class also extends B.
And because of that type-safety, you can access members of B as if it’s defined in A, as shown in the preceding example.
Also note that self is a name—it could be any valid parameter name.
The most common names used for self type annotation are this and self.
I’ll use an example to demonstrate how self type can work in a real-world application.
In this example, you’ll try to build a product finder that depends on a couple of required services: a way to access the database and a logger to log the result.
Because traits let you easily compose features, separate the required services and the logic to find products into separate traits.
The RequiredServices trait declares all the services that could be used by the product finder:
Because the required services are annotated with self type this, you can still access those services, and the Scala compiler will ensure that the final class gets mixed with a trait or a class that implements RequiredServices.
The following listing shows the complete example with test services.
This example shows how to build large components by combining smaller components in Scala using self type annotation and mixin composition.
We will explore self type once again in chapter 10 to help in unit testing.
Now let’s move on to build your first reusable component—a generic ordering system.
To see how to build a reusable component, let’s build a generic product ordering system.
It will be reusable in that a user can order any kind of product.
A general ordering system is built using the following components (see figure 8.1):
An order component that represents the order placed by the customer.
Figure 8.1 Ordering system with three components: order, inventory, and shipping.
A shipping component that knows how to ship an order to customer.
A real-world ordering system is more complex than this, but let’s stick to this simple system because you can easily scale it to fit into a larger context.
You can use abstract type members to abstract out the components your ordering system requires:
The OrderingSystem declares three abstract members—O, I, and S—and at the same time sets the upper bounds for each type.
The type O denotes a type that is a subtype of the Order type.
And Order, Inventory, and Shipping define the contracts for each component:
The benefit of nesting all these components under a trait is that they’re all aggregated and encapsulated in one place.
So far you have the interfaces for each component, but you still need to implement the steps for placing the order.
If the item doesn’t exist in inventory, return without placing the order and possibly notify Inventory to replenish the product.
Let’s implement these steps as part of an Ordering trait that is defined inside the OrderingSystem:
The placeOrder method implements all the steps mentioned with the help of the self type annotation.
Ordering now relies on Inventory for itemExists and Shipping for the scheduleShipping method.
Note that you can specify multiple self type annotations using the with keyword, similar to the way you mix in traits.
All these pieces together make up the ordering system component.
The abstract type members of the OrderingSystem represent the required services that this component relies on without providing concrete implementation.
The mixin feature allows it to build the Ordering trait by composing Inventory and Shipping traits.
And finally self type allows the Ordering trait to use services provided by the mixed in traits.
As you can see, all these abstracts provide a building block to build scalable and reusable components in Scala.
If you want to implement an ordering system for books, you could easily use the OrderingSystem as follows:
Listing 8.3 Generic ordering system with abstract type members and self type.
The BookOrderingSystem provides all the concrete implementations and creates the BookOrdering object to place orders for books.
Now all you have to do to use the BookOrderingSystem is import it:
The next section shows you how to use the concepts you learned here to solve the expression problem.
The ability to extend a software component and integrate it into an existing software system without changing existing source code is a fundamental challenge in software engineering.
Many people have used the expression problem to demonstrate that objectoriented inheritance fails in terms of extensibility of a software component.
The expression problem is one in which the challenge is to define a data type with cases, and in which one can add new cases of the type, and operations for the types, without recompiling and maintaining static type-safety.
Usually this challenge is used to demonstrate strength and weakness of programming languages.
Next I’ll show you how to solve the expression problem in Scala with the constructs you’ve learned so far.
But first lets look at the expression problem in detail.
The goal is to define a data type and operations on that data type in which one can add new data types and operations without recompiling existing code, but while retaining static type safety.
Any implementation of the expression problem should satisfy the following requirements:
You should be able to add new types and operations that work on all the types.
You have a payroll system that processes salaries for full-time employees in the United States and Canada:
The Payroll trait declares the processEmployees method that takes a collection of employees and processes their salaries.
Both USPayroll and CanadaPayroll implement the processEmployees method based on the way the salary is processed in the individual country.
With current changes in the business, you also have to process salaries of full-time employees in Japan.
That’s easy—all you have to do is add another class that extends the Payroll trait:
This is one type of extension the expression problem talks about.
The solution is typesafe, and you can add JapanPayroll as an extension and plug it in to an existing payroll system with a separate compilation.
What happens when you try to add a new operation? In this case the business has decided to hire contractors, and you also have to process their monthly pay.
The problem is you can’t go back and modify the trait because that will force you to rebuild everything—which you can’t do because of the constraint put on you by the expression problem.
This is a practical problem: how to add features to an existing system incrementally without doing modifications.
To understand the difficulty of solving the expression problem, let’s try another route: using the Visitor pattern to solve this problem.
Both the USPayroll and CanadaPayroll types accept a payroll visitor.
Using the Visitor pattern, it’s easy to add new operations, but what about type? If you try to add a new type called JapanPayroll, you have a problem.
You have to go back and change all the visitors to accept a JapanPayroll type.
In the first solution it was easy to add a new type, and in the second solution it’s easy to add an operation.
But you want a solution that lets you change in both dimensions.
In the next section you’ll learn how to solve this problem in Scala using abstract type members and trait mixins.
You’ll use Scala traits and abstract type members to solve the expression problem.
Using the same payroll system, I’ll show you how to easily add new operations to the payroll system without breaking type-safety and at the same time add a new type.
Start by defining the base payroll system as a trait with an abstract type member for payroll:
Again you’ll nest everything inside a trait so that you can treat it as a module.
The type P denotes some subtype of the Payroll trait, which declares an abstract method to process salaries for employees.
The processPayroll method needs to be implemented to process payrolls for a given Payroll type.
I’ve omitted the details of processing a payroll because it’s not important in this context.
In these settings it will be easy to add a new Payroll type for Japan.
Now add a new method to the Payroll trait without recompiling everything, using the shadowing feature of Scala:
Shadowing can introduce unintended errors in your code, but in this context it lets you extend the old definition of Payroll without overriding it.
Another thing to notice is that you’re redefining the abstract type member P.
Also note that you’re mixing in the new definition of the Payroll trait to implement the processContractors method.
Using Scala first-class module support, you can wrap all the traits and classes inside an object and extend an existing software component without forcing everything to recompile and at the same time maintain type-safety.
Note that both old and new interfaces of the Payroll are available, and the behavior is driven by what traits you compose.
The processPayroll method invokes both the processEmployees and processContractors methods of the Payroll trait, but you could instead have easily used an existing payroll system that knows how to process salaries for U.S.
All that remains is to implement the additional processContractors part.
This is a good example that demonstrates the power of Scala’s type system and the abstractions available to build both scalable and extensible components.
We solved this problem using the object-oriented abstractions available in Scala.
In section 8.3, I’ll show you how to solve this problem using the functional programming side of things.
But first I’ll go over another powerful type of abstraction available in Scala.
One of Scala’s unique features is its rich type system.
Like any good type system, it doesn’t work against you but rather provides the abstractions necessary to build reusable components.
This section explores various types offered by the Scala type system.
Structural typing in Scala is the way to describe types by their structure, not by their names, as with other typing.
If you’ve used dynamically typed languages, a structural type may give you the feel of duck typing (a style of dynamic typing) in a type-safe manner.
Let’s say you want to close any resource after use as long as it’s closable.
One way to do that would be to define a trait that declares a close method and have all the resources implement the trait.
But using a structural type, you can easily define a new type by specifying its structure, like the following:
Structural types aren’t limited to a single method, but when defining multiple methods make sure you use the type keyword to give it a name—otherwise, your function signatures will look confusing:
You can also create new values of a structural type using the new keyword.
You can use a structural type to reduce class hierarchies and simplify a code base.
Let’s say you have the following class hierarchies to represent various types of workers for a department store:
This is a small hierarchy, but you get the idea.
Each type of worker is different; there are hourly and full-time workers.
The only thing they have in common is they all get paid.
If you have to calculate the money spent on paying salaries to workers in a given month, you have to define another common type that represents salaried workers:
The benefit of duck typing is that it lets you abstract out commonalities without being part of the same type.
Using a structural type you can easily rewrite a function like the following without defining new types:
Now you can pass instances of any worker to the previous function without conforming to some common type.
Structural type is a good technique to get rid of unnecessary class hierarchies, but the downside is that it’s comparatively slow because it uses reflection under the hood.
Higher-kinded types are types that know how to create a new type from the type argument.
That’s why higherkinded types are also known as type constructors—they accept other types as a parameter and create a new type.
It takes a type parameter and creates a new concrete type.
List[String] and List[Int] are examples of types you can create from the List kind.
Kinds are to types as types are to values (see figure 8.2)
Scala defines large sets of powerful features but not every programmer needs to use all of them.
Starting with Scala 2.10 you must first enable the advanced features of the language.
This is part of the effort to modularize Scala’s language features.
The scala.language object controls the language features available to the programmer.
Take a look at the scaladoc of scala.language to find all the language features that you can control.
For large projects, for example, you can disable some advanced Scala features so that they don't get abused.
Most of the collections classes are good examples of why kinds are such a powerful abstraction tool.
You’ll try to build a function that takes another function as a parameter and applies that function to elements of a given type.
For example, you have a vector of elements and want to apply a function to each element of the vector:
Similarly, if you want to apply the function to Option, you have to create another function:
Both functions look similar and only differ by the first parameter.
The question is: how can you define a common fmap function signature for various types? Using a higherkinded type, you can abstract out the type of the first parameter, as in the following:
If you have to implement fmap for Vector, you’ll do something like the following:
Using higher-kinded types, you can raise the abstraction level higher and define interfaces that work across various types.
For instance, you can use the Mapper trait to implement fmap for the Function0 as follows:
To enable all the advanced features pass -language:_ parameter to the Scala compiler.
For example, you can use the preceding Function0Mapper to compose two functions and create a new one:
Remember that calling apply on a Function type invokes the function.
Type projection Before leaving this example, I want to explain one trick called type projection that comes in handy at times.
Type projection T#x references the type member x of type T.
The type projection allows for accessing members of the given type.
That creates a new type alias of type member E defined inside the trait X.
How could this be useful in the real world? Take the example of Either.
Either is a type constructor (higher-kinded type) that takes two type parameters, one for Left and another for Right.
You could create an instance of Left or Right like the following:
Depending on whether the first parameter returns true or false, it creates either an instance of Right or Left.
Can you use your fmap over Either type? Not easily because fmap only accepts types that take one type parameter, and Either takes two type parameters.
But you can use type projection to hide one type parameter and make it constant.
First, you’ll only apply the function if it’s Right, because Right by convention implies success and Left implies failure.
The interesting part of the implementation is type parameter X.
Here X is specified by the function that creates Mapper, and using type projection you’ll hide the X to the Mapper trait:
It’s a little hard to come up with generic functions like fmap on your first try.
I recommend always starting with specific implementations (even if you see duplications) to understand the pattern before creating abstractions.
Once you understand the pattern, higher-kinded types help to create abstractions.
I encourage you to look into Scala collections2 to see various usages of higher-kinded types.
Phantom types are types that don’t provide any constructors to create values.
You only need these types during compile time to enforce constraints.
An order is represented by an item and a shipping address:
To place an order, you have to specify both item and shipping address.
The client of the ordering system provides an item, specifies the shipping address, then places an order:
The problem with this approach is that the methods could get called out of order.
For example, some clients could by mistake call placeOrder without specifying the shipping address.
Well, you could implement necessary validations inside the placeOrder function, but using the type system to enforce an order would be even better.
This is where you could use phantom types to enforce some constraints on the order object and the way it’s used by various functions.
First let’s look into the following phantom types to represent states of order:
Each of these types represents a certain order state, and you’ll use them as you progress through the ordering process.
When the order is first initialized, it has no item and no address, and it’s incomplete.
The Order type now takes three type parameters, and an empty order is initialized with InCompleteOrder, NoItem, and NoAddress types.
To enforce some constraints on each operation performed on the order, you’ll use combinations of these types.
For example, you can only add an item to an order when it doesn’t have any items, and once an item is added its type parameter changes from NoItem to ItemProvided:
Similarly, addShipping creates a new order by updating the address:
To place an order, it needs to have both item and address, and you can easily verify that at compile time using types:
If you try to invoke placeOrder without an item or address, you’ll get a compile error.
If I invoke placeOrder without specifying a shipping address, I get the following error:
The following listing shows the complete ordering system example with phantom types.
To use this ordering system, you can create an empty order and then add the details as follows:
This time you know that if the client of the ordering system doesn’t properly populate the order, it will get a compilation error.
You can also use this technique to implement the type-safe Builder pattern where, using phantom types, you can ensure all the required values are populated.
In the next section, you’ll use phantom types to implement type classes.
It comes with many more varieties than I’ve covered here.
There’s a type called the method dependent type3 that allows you to specify the return type based on the type of the parameter, path-dependent types that allow you to constrain types by objects, and many more.
My advice is to keep playing with the language, and I am sure you’ll become comfortable with Scala types.
A type class is a type system construct that supports ad hoc polymorphism.
Ad hoc polymorphism is a kind of polymorphism in which polymorphic functions can be applied to arguments of different types.
Ad hoc polymorphism lets you add features to a type any time you want.
Don’t think of type classes as classes in OOP; think of them as a category.
Type classes are a way to define commonalities among sets of types.
In this section you’ll learn how type classes can help in building abstractions.
A simple example can demonstrate how to implement type classes in Scala.
The following example implements an adapter pattern using type classes.
In an object adapter pattern, the adapter (the wrapper object) contains an instance of the class it wraps.
The adapter pattern is a great way to add functionality to a type through composition.
Here’s the problem you’re trying to solve: you have a Movie type represented by a case class and you want to convert it to XML:
One quick and dirty solution could be to add a toXml method inside the case class.
But in most cases that would be inappropriate because converting to XML is a completely orthogonal responsibility for the Movie class and should not be part of the Movie type.
The second solution is to use the object adapter pattern.
Define a generic interface for XmlConverter and parameterize it with a type so you can use it for multiple types:
And provide an object adapter for the Movie instance like the following:
To convert an instance of Movie to XML, all you have to do from client code is the following:
The problem with the following implementation is the incidental complexity introduced by the MovieXmlConverter.
Ad hoc polymorphism with type classes with, which is movie.
Going to a toXml method inside the Movie class almost feels like an elegant solution.
The second problem with that implementation is the rigidity of the design.
It’s going to be hard to provide a separate XML converter for the Movie type.
Let’s see how you can improve the solution with type classes.
The first role of a type class is to define a concept.
The concept is XML conversion and could be easily represented by the XmlConverter trait:
The second role of a type class is to propagate constraints automatically to a generic algorithm.
For example, you can create a new method called toXml that takes an instance of a type and a converter to convert it to XML:
But this isn’t much of an improvement because you still have to create an instance of a converter and pass it to the method.
What makes type classes practical in Scala is the implicit keyword.
Making the converter parameter implicit allows the Scala compiler to jump in and provide the parameter when it’s missing:
Now you can invoke toXml by passing an instance of Movie, and the Scala compiler will automatically provide the converter for you.
In fact, you can pass in an instance of any type as long as you have an implicit definition of an XmlConverter that knows how to convert that type.
Listing 8.6 Type class to convert a type to XML.
You created a type class called XmlConverter and then provided an implicit definition of it B.
When using the toXml C method, you have to make sure the implicit definition is available in the compiler scope, and the Scala compiler will do the rest.
The flexibility of this implementation is that now if you want to provide a different XML conversion for Movie, you could do that and pass it to the toXml method as a parameter explicitly:
But make sure both definitions aren’t in the compiler scope at the same time—otherwise, you’ll get an “ambiguous implicit values” compile error.
One way to use multiple implicit definitions for a given type is to import them in a much narrower scope, such as inside a method.
The following two methods use a different XML converter for the Movie type:
Type classes are so useful that they’re used in many places across the standard library.
For example, look at the sum method available for List:
The Scala library provides implicit definitions for Numeric[Int] but not for Numeric[String], and that’s why you get the implicit missing compilation error.
Similarly, the following min method defined in the Scala collection library also uses Ordering[B] as a type class:
The common confusion about type classes is that people tend to think of them as an interface.
But the key difference between an interface and a type class is that with interface your focus is on subtype polymorphism, and with a type class your focus is on parametric polymorphism.
In the Java world you might know parametric polymorphism as generics, but a more appropriate name is parametric polymorphism.
Another way to understand the difference is that subtyping is common in the OOP world, and parametric polymorphism is common in the functional programming world.
In fact, the type-class concept is first found in the Haskell programming language, which is a purely functional language.
Type classes are a flexible way to model orthogonal concerns of an abstraction without hardwiring to the abstraction.
Type classes are also useful in retroactive modeling because you can add a type class for a type anytime you want.
New syntax for declaring implicit parameters Beginning with Scala 2.8, there’s a succinct way of declaring implicit parameters for methods and functions that makes the implicit parameter name anonymous:
Using A: XmlConverter, you’re declaring that the toXml method takes an implicit parameter of type XmlConverter[A]
Because the implicit parameter name isn’t available, you can use the implicitly method defined by scala.Predef to get reference to the implicit parameter.
But in cases where adding an additional implicit parameter doesn’t help in readability, you can start using the new syntax.
The upside of this limitation is that all the implicit resolution happens during compile time, and there’s no runtime cost associated with it.
Type classes have everything you need to solve the expression problem, so let’s see how.
One is the country for which you have to process the payroll, and the other is the payee.
The A type represents a type of payee; it could represent an employee or contractor.
Similarly, the payroll class for Canada would look something like the following:
To represent the type class for a family of payroll processors, you could define the following trait by parameterizing both country and the type of payee:
The reason it’s a higher-kinded type is because both USPayroll and CanadaPayroll take a type parameter.
Note that you aren’t using C anywhere except as a parameterized type, like a phantom type.
It will make sense once I introduce the second building block of type class, the implicit definitions of the PayrollProcessor trait:
Notice how you’re using the first type parameter of PayrollProcessor to identify the appropriate definition of PayrollProcessor based on the country.
Ad hoc polymorphism with type classes implicit definitions, you could use implicit class parameters for both USPayroll and CanadaPayroll types:
The preceding code snippet also demonstrates another important point: you can use implicit parameters in class definitions as well.
Now when you create an instance of USPayroll or CanadaPayroll, the Scala compiler will try to provide values for implicit parameters.
Again, all the implicit definitions are grouped together B to help you import them, as inside the RunPayroll object.
Notice when you’re instantiating USPayroll that you’re providing a collection of employees, and the implicit processor will be provided by the Scala compiler.
Because there’s no restriction on the type of payee (it’s denoted by A without any bounds), you could easily create a collection of contractors and pass it to USPayroll:
But the moment you try to compile the preceding line, you’ll get a compilation error, because there’s no implicit definition for USPayroll and Contractor type yet.
Let’s move on with the quest of solving the expression problem using type classes.
Adding a new type in the current settings is trivial; add a new class and the implicit definition of the payroll processor:
Adding a new operation to process pay for contractors B is also trivial because all you have to do is provide implicit definitions of payroll processors for contractors, as in the following:
The following code snippet shows how to use the preceding code to process the payroll of both employees and contractors:
You import all the necessary classes and implicit definition and process the payroll for Japan.
You again successfully solved the expression problem, this time using a functional programming technique.
If you’re a Java programmer, type classes might take a little while to get used to, but once you get comfortable with them they’ll provide the power of retroactive modeling, which in turn will allow you to respond to change quickly.
This chapter is an important milestone in understanding the various applications of Scala’s type system.
Once understood and explored, Scala’s type system helps in building reusable and scalable components.
And a good type system not only provides typesafety, it also provides enough abstraction to build components or libraries much more quickly.
You learned about abstract type members and self-type annotation and how to build components using them.
You also explored various types of types provided by Scala and saw how they can be used to build applications and create abstractions.
One of the most common ways to identify the weakness and strengths of programming languages is the expression problem.
You implemented the expression problem in two different ways, clearly demonstrating the power of Scala’s type system.
Scala being multiparadigm, you solved this problem using both object-oriented and functional paradigms.
The object-oriented solution is implemented using abstract type members and traits.
To solve this using functional programming, you learned about type classes.
Type classes are a powerful way to provide polymorphic behavior at runtime, which is an important design technique for programmers.
But I’ve only scratched the surface of Scala’s type system.
It has more things to offer than I could possibly cover in this chapter, but I’m certain that your curiosity will entice you to explore this subject further.
The next chapter covers one of the Scala’s most popular features: actors.
In this chapter I introduce the most exciting feature of Scala: its actor library.
Think of an actor as an object that processes a message (your request) and encapsulates state (state is not shared with other actors)
The ability to perform an action in response to an incoming message is what makes an object an actor.
At the high level, actors are the way you should do OOP.
Remember that the actor model encourages no shared state architecture.
In this chapter, I explain why that’s an important property to have for any concurrent program.
Future and Promise provide  abstractions to perform concurrent operations in a nonblocking fashion.
They are a great way to create multiple concurrent and parallel computations and join them to complete your job.
This is very similar to how you compose functions, but, in this case, functions are executed concurrently or in parallel.
Think of Future as a proxy object that you can create for a result that will be available at some later time.
You  can use Promise to complete a Future by providing the result.
We will explore Promise and Future as this chapter progresses.
First let’s understand what I mean by concurrent and parallel programming.
It doesn’t matter whether they’re running at the same instant.
You can write concurrent programs on a single CPU (single execution core) machine where only one task can execute at a given point of time.
Typically multiple tasks are executed in a time-slice manner, where a scheduler (such as the JVM) will guarantee each process a regular “slice” of operating time.
And the common de facto standard way to implement a multitasking application is to use threads.
Figure 9.1 shows how a multitasking application shares a single CPU.
As you can see in figure 9.1, two threads are executing instructions generated by the application in a time-sliced manner.
The group of instructions varies in size because you don’t know how much code will be executed before the scheduler decides to take the running thread out and give another thread the opportunity to execute.
Remember that other processes running at the same time might need some CPU time—you can see it’s pretty unpredictable.
Sometimes schedulers use a priority mechanism to schedule a thread to run when there’s more than one thread in a readyto-run state.1 Things become more interesting when you have code that blocks for.
Figure 9.1 A concurrent application running in a single CPU core with two threads.
What is concurrent programming? resources, such as reading data from a socket or reading from the filesystem.
In this case, even though the thread has the opportunity to use the CPU, it can’t because it’s waiting for the data, and the CPU is sitting idle.
Most people use concurrency and parallel programming interchangeably, but there’s a difference.
In parallel programming (figure 9.2), you can literally run multiple tasks at the same time, and it’s possible with multicore processors.
A concurrent program sometimes (in the next section I will explain why not always) becomes a parallel program when it’s running in a multicore environment.
This sounds great, because all the CPU vendors are moving toward manufacturing CPUs with multiple cores.
But it poses a problem for software developers because writing concurrent, parallel applications is hard.
In this case, Thread 1 will wait until it gets the data, and the application is no  longer parallel.
The more data and state sharing you have among threads, the more difficult it’ll be for the scheduler to run threads in parallel.
Throughout this chapter you’ll try to make your concurrent program run in parallel mode as much as you can.
Another term that’s used often with concurrency is distributed computing.
The way I define distributed computing is multiple computing nodes (computers, virtual machines) spanned across the network, working together on a given problem.
A parallel process could be a distributed process when it’s running on multiple network nodes.
You’ll see an example of this in chapter 12 when we  deploy actors in remote nodes so they can communicate across the network.
But now let’s look at the tools you can use to solve concurrency issues and the challenges associated with it.
Figure 9.2 A concurrent and parallel application running in a two-CPU core with two threads.
As a software engineer, I don’t think we have a choice but to support multicore processors.
And the types of problems we’re solving in enterprise software development are getting bigger and bigger.
But it’s hard to write a correct and bug-free concurrent program.
Only a handful of programmers know how to write a correct, concurrent application or program.
The same program that causes deadlock in production might not have any locking issues when debugging locally.
Sometimes threading issues show up after years of running in production.
Threading encourages shared state concurrency, and it’s hard to make programs run in parallel because of locks, semaphores, and dependencies between threads.
Even though multithreading comes up as the main reason why writing concurrent programs is difficult, the main culprit is mutable state.
The issue with using threads is it’s a low level of abstraction for concurrency.
Threads are too close to hardware and represent the way work is scheduled or executed by the CPU.
You need something that can encapsulate threads and give you something that’s easier to program with.
Take the example of Scala collections: the Traversable trait defines an abstract method called def foreach[U](f: Elem => U), which other collection library classes and traits implement.
Imagine you have to use only foreach to do any sort of manipulation on the collection without using other useful methods like map, fold, filters, and so on.
Well, in that case programming in Scala will become a little more difficult.
This is exactly what I think about threads: they’re too low-level for programmers.
Challenges with concurrent programming nice utilities and implements popular concurrent design patterns, but it’s still hard to avoid the fact that the main complexity behind using threads comes from using mutable shared data.
It’s a design issue that we programmers have to deal with when working with threads.
To protect against data corruption and have consistency across many threads, we use locks.
Using locks, we control the way the shared data is modified and accessed, but locks introduce problems to the program (see table 9.1)
Last but not least, shared mutable data makes it hard to run programs in parallel, as discussed in section 9.1
The bigger question is: if threads are so hard to use, why are so many programs written using them? Almost all multithreaded programs have bugs, but this hasn’t been a huge problem until recently.
As multicore architectures become more popular, these bugs will be more apparent.
Threading should be left to only a few experts; the rest of us need to find a much higher level of abstraction that will hide the complexity of the multithreading and provide an easy-to-use API.
Although there will be situations where threading may be your only option, for 99 percent of the cases you should be falling back to other alternatives.
This change will come with a price, and the price is that we all have to start learning a new way to write and design concurrent applications.
Enough talking about problems with threads and locks—let’s turn our attention to the solutions.
Table 9.2 lists the three most popular trends in implementing concurrent applications.
You can’t implement a higher-level, thread-safe behavior by composing smaller thread-safe operations.
The problem might not show until in production (sometimes after years).a Acquiring and releasing locks are expensive operations.
It’s almost impossible to make any deterministic reasoning of multithreaded code.
You can use design patterns, such as always acquiring locks in a certain order, to avoid deadlocks, but this mechanism adds more responsibility to the developer.
This is more of a threading issue than the shared-state issue, but it’s a huge issue nonetheless.
There’s no clear mechanism to recover from errors in multithreaded programs.
Usually a feedback loop is looking at the stack trace inside a log file.
The remainder of this chapter focuses on message-passing concurrency using Scala actors.
An actor processes incoming messages and executes the actions associated with it.
Typically, these messages are immutable because you shouldn’t share state between them for reasons discussed previously.
There are two main communication abstractions in actor: send and receive.
To send a message to an actor, you can use the following expression:
You’re sending the msg message to actor a by invoking the ! method.
When you send a message to an actor, it’s an asynchronous operation, and the call immediately returns.
The messages are stored in a queue and are processed in first-in, first-out fashion.
Think of this queue as a mailbox where messages get stored for an actor.
The receive operation is defined as a set of patterns matching messages to actions:
Instead of working with tables and rows, STM controls the access to shared memory.
An STM transaction executes a piece of code that reads and writes a shared memory.
This is typically implemented in a lock-free way and is composable.
I talk about STM in chapter 12 in more detail.
The principle behind the dataflow concurrency is to share variables across multiple tasks or threads.
These variables can only be assigned a value once in its lifetime.
But the values from these variables can be read multiple times, even when the value isn’t assigned to the variable.
This gives you programs that are more deterministic, with no race conditions and deterministic deadlocks.
Chapter 12 covers dataflow concurrency constructs available in the Akka framework.
This is where you’ll spend most of your time in this chapter.
Messages can be sent both synchronously and asynchronously, but asynchronously sending messages to other components is more common.
These messages are immutable and are separated from the state of individual components.
You don’t have to worry about shared state—in fact, message-passing concurrency encourages shared nothing (SN) architecture.
The most successful implementation of message passing concurrency is the actor model, and it became popular after the Erlang programming language demonstrated the success of using the actor model as a concurrency model for building large-scale, distributed, parallel telecom applications.
The Scala actor library is another implementation of the message passing concurrency model.
What differentiates an actor from any other object is the ability to perform actions in response to an incoming message.
The default actor library that ships with Scala, starting with Scala 2.10, is Akka (http://akka.io/) actors.
There are many actor libraries but Akka is the most popular and powerful.
To help with the migration Scala provides an Actor Migration Kit (AMK) and migration guide4 so old Scala actor code can be easily migrated to the Akka actor library.
To create an actor, extend the Actor trait provided by the Akka library and implement the receive method.
The following example creates a simple actor that prints a greeting message to the console when it receives a Name message:
The GreetingsActor can only process messages of type Name, and I will cover what will happen when you send messages that don’t match any pattern in a moment.
Please note that you don’t necessarily have to create messages from case classes—you can send strings, lists, or whatever you can match using Scala’s pattern matching.
For example, to match string type messages, you could do something like the following:
Before sending any messages to the GreetingsActor actor, the actor needs to be instantiated by creating an ActorSystem.
Think of an ActorSystem as the manager of one or more actors.
The actor system provides a method called actorOf that takes the configuration object (akka .actor.Props) and, optionally, the name of the actor:
The actor system will create the infrastructure required for the actor to run.
Before running the previous snippet, make sure you add the following dependency in your build file:
If everything works as planned you should see the “Hello Nilanjan” message in the console.
Now let’s step back to understand why we need an actor system.
An actor system is a hierarchical group of actors that share a common configuration.
It’s also the entry point for creating and looking up actors.
Typically a design of an actor-based application resembles the way an organization works in the real world.
Each department may further divide the work until it becomes a size manageable by an employee.
Similarly actors form a hierarchy where parent actors spawn child actors to delegate work until it is small enough to be handled by an individual actor.
For example, you can have one actor system to handle the backend database, another to handle all the web service calls, and so forth.
A given actor consumes only 300 bytes so you can easily create millions of them.
At the top of the hierarchy is the guardian actor, created automatically with each actor system.
All other actors created by the given actor system become the child of the guardian actor.
In the actor system, each actor has one supervisor (the parent actor) that automatically takes care of the fault handling.
So if an actor crashes, its parent will automatically restart that actor (more about this later)
The simplest way to create an actor is to create an ActorSystem and use its actorOf method:
The preceding snippet creates an ActorSystem named "word-count", and the actorOf method is used to create an actor instance for the SomeActor class.
Props is an ActorRef configuration object that’s thread-safe and shareable.
Props has a lot of utility methods to create actors.
The second part of the actor system is actor path.
An actor path uniquely identifies an actor in the actor system.
Because actors are created in a hierarchical structure, they form a similar structure to a filesystem.
As a path in a filesystem points to an individual resource, an actor path identifies an actor reference in an actor system.
Note that these actors don’t have to be in a Figure 0.1 Actor system with hierarchy of actors.
Using the methods defined in ActorSystem, you can look up an actor reference of an existing actor in the actor system.
The following example uses the system / method to retrieve the actor reference of the WordCountWorker actor:
The system / method returns the actor path, and the actorFor method returns the actor reference mapped to the given path.
If the actorFor fails to find an actor pointed to by the path, it returns a reference to the dead-letter mailbox of the actor system.
It’s a synthetic actor where all the undelivered messages are delivered.
You can also create the actor path from scratch and look up actors.
See the Akka documentation for more details on the actor path.5
To shut down all the actors in the actor system, invoke the shutdown method, which gracefully stops all the actors in the system.
The parent actor first stops all the child actors and sends all unprocessed messages to the dead-letter mailbox before terminating itself.
The last important part of the actor system is message dispatcher.
The MessageDispatcher is the engine that makes all the actors work.
Its responsibility is to send a message to the actor’s mailbox and execute the actor by invoking the receive block.
Every MessageDispatcher is backed by a thread pool, which is easily configured using the configuration file (more about this in chapter 12)
You can also configure various types of dispatchers for your actor system or specific actors.
For this chapter we are going to use the default dispatcher (a.k.a event-based dispatcher)
Figure 9.4 shows how sending and receiving messages works inside actors.
To send a message to an actor mailbox the ActorRef first sends the message to the MessageDispatcher associated with the actor (which in most cases is the MessageDispatcher configured for the actor system)
The MessageDispatcher immediately queues the message in the mailbox of the actor.
The control is immediately returned to the sender of the message.
This is exactly how it worked when we sent a message to our greetings actor.
Handling a message is a bit more involved so let’s follow the steps in figure 9.4:
When an actor receives a message in its mailbox, MessageDispatcher schedules the actor for execution.
If a free thread is available in the thread pool that thread is selected for execution of the actor.
If all the threads are busy, the actor will be executed when threads becomes available.
The receive method of the actor is invoked by passing one message at a time.
The message dispatcher always makes sure that a single thread always executes a given actor.
It might not be the same thread all the time but it is always going to be one.
This is a huge guarantee to have in the concurrent world because now you can safely use mutable state inside an actor as long as it’s not shared.
Now I think we are ready to build an application using actors.
Figure 9.4 Showing step by step how a new actor is started and how an already running actor bides its time.
In the following example, the challenge is to count the number of words in each file in a given directory and sort them in ascending order.
One way of doing it would be to loop through all the files in a given directory in a single thread, count the words in each file, and sort them all together.
We will have a set of worker actors handling individual files and a master actor sorting and accumulating the result.
Brian Goetz, “Java theory and practice: Stick a fork in it, Part 1,” developerWorks, Nov.
Additionally the Actor trait defines methods that are useful for lifecycle hooks and fault handling.
Here is the list of some of the important methods.
Please check the scaladoc for a complete list of methods.
The default behavior of this method is to publish the message to an actor system’s event stream.
You can configure the event stream to log these unhandled messages in the log file.
You can use self to send a message to itself.
This is the ActorRef of the actor that sent the last received message.
It is very useful when you want to reply to the sender of the message.
This provides the contextual information for the actor, the current message, and the factory methods to create child actors.
The context also provides access to the actor system and lifecycle hooks to monitor other actors.
This supervisor strategy defines what will happen when a failure is detected in an actor.
This method will be called before any message is handled.
This method could be used to initialize any resources the actor needs to function properly.
To solve the word count problem with actors, you’ll create two actor classes: one that will scan the directory for all the files and accumulate results, called WordCountMaster, and another one called WordCountWorker to count words in each file.
It’s always a good idea to start thinking about the messages that these actors will use to communicate with each other.
First you need a message that will initiate the counting by passing in the directory location and the number of worker actors:
The docRoot will specify the location of the files and numActors will  create the number of worker actors.
The main program will start the counting process by passing this message to the main actor.
WordCountMaster and WordCountWorker will communicate with each other via  messages.
The WordCountMaster needs a message that will send the filename to the worker actor to count, and in return WordCountWorker needs a message that will send the word count information with the filename back to the master actor.
To understand how these messages are consumed, look at figure 9.5
The figure shows only one worker actor, but the number of worker actors will depend upon the number you send through the StartCounting message.
Let’s start with the WordCountWorker actor because it’s the easiest one.
This actor processes only FileToCount type messages, and the action associated with the message is to open the file and count all the words in it.
Counting words in a file is exactly the same as the threading example you saw previously:
Actors might be restarted in case of an exception thrown while handling a message.
This method is called on the current instance of the actor.
The default implementation is to stop all the child actors and then invoke the postStop method.
The good news is, the Akka actor runtime sends the actor reference of sender implicitly with every message:
In reply you’re sending the WordCount message back to the WordCountMaster actor.
Figure 9.5 WordCountMaster and WordCountWorker actors are communicating by sending messages.
The main program in the figure starts the word count process.
What if an actor performs a blocking operation? Usually it’s recommended that you don’t perform any blocking operation from actors.
When you make a blocking call from an actor you are also blocking a thread.
So if you end up with many of these blocking actors you will soon run out of threads and halt the actor system.
At times you will not have any option other than blocking.
In that case the recommended approach is to separate blocking actors from nonblocking actors by assigning different message dispatchers.
This provides the flexibility to configure the blocking dispatcher with additional threads, throughput, and so on.
An added benefit of this approach is if a part of the system is overloaded with messages (all the threads are busy in a message dispatcher) other parts can still function.
In this case the postStop method is overridden to print a message in the console when the actor is stopped.
We will instead use this as a debug message to ensure that the actor is stopped correctly.
Currently the WordCountWorker actor responds only to the FileToCount message.
When it receives the message, it will count words inside the file and reply to the master actor to sort the response.
Any other message will be discarded and handled by the unhandled method as described in the following side note.
What is ActorDSL? If you are familiar with old Scala actors, ActorDSL will look quite similar to Scala actors.
This is a new addition to the Akka actor library to help create one-off workers or even try in the REPL.
To bring in all the DSL goodies into the scope import:
To create a simple actor use the actor method defined in ActorDSL by passing an instance of the Act trait:
The become method adds the message patterns the actor needs to handle.
Behind the scene Act extends the Actor trait and become adds the behavior of the receive block.
Using this DSL syntax you no longer have to create a class.
Here is an example of two actors communicating with each other by sending ping-pong messages:
The WordCountMaster actor will start counting when it receives a StartCounting message.
This message will contain the directory name that needs to be processed and the number of worker actors that could be used for the job.
The map method is used to create a list of all the filenames with a complete file path.
To create work actors, we use the numActors value passed to the StartCounting message and create that many actors:
Since the worker actors will be the children of the WordCountMaster, the actor context.actorOf factory method is used.
To begin sorting, we need a method that will loop through all the filenames and send a FileToCount message to these worker actors.
Because the number of files to process could be higher than the number of actors available, files are sent to each actor in a round-robin fashion:
The actor system is assigned to an implicit value so we don’t have to pass it explicitly to the actor method.
The whenStarting is the DSL for a lifecycle hook of the prestart method of the actor.
When the WordCountMaster actor receives the StartCounting message it will create the worker actors and scan the files, then send these files to each worker.
The fileNames field stores all the files we need to process.
We will use this field later on to ensure we have received all the replies.
An important point to note here is that it is safe to use mutable state inside an actor because the actor system will ensure that no two threads will execute an instance of an actor at the same time.
You must make sure you don’t leak the state outside the actor.
Next the WordCountMaster actor needs to handle the WordCount message sent from the WordCountWorker actor.
This message will have the filename and the word count.
The last step is to determine when all the files are processed.
One way to do that is to compare the size of sortedCount with the number of files to determine whether all the responses from the worker actors are received.
When that happens we need to print the result in the console and terminate all the actors:
We could use context.children to access all the worker actors and stop them like the following:
The simplest way to shut down an actor system is to use the shutdown method of the actor system.
We can access the actor system from context using context.system like the following:
The following listing shows the complete implementation of WordCountWorker and WordCountMaster actors.
When the WordCountMaster actor receives the StartCounting message, it creates worker actors based on the number passed in by the message.
Once the actors are started, the WordCountMaster actor sends FileToCount messages to all the worker actors in round-robin fashion.
When the worker actor is done counting the words inside the file, it sends the WordCount message back to the master actor.
If the size of the sortedCount matches the number of files, it kills all the worker actors including the master actor.
The final piece missing from the preceding code is the main actor you saw in figure 9.4
For that, you’re not going to create a new actor but instead create an object with the Main method.
You’ve learned lots of interesting things about actors in this section.
And you learned how to design your applications using actors.
Creating self-contained immutable messages and determining the communication between actors are important steps when working with actors.
It’s also important to understand that when working with actors, all the communication happens through messages, and only through messages.
What happens if something fails? So many things can go wrong in the concurrent/ parallel programming world.
What if we get an IOException while reading the file? Let’s learn how to handle faults in an actor-based application.
Akka encourages nondefensive programming in which failure is a valid state in the lifecycle of an application.
As a programmer you know you can’t prevent every error, so it’s better to prepare your application for the errors.
You can easily do this through fault-tolerance support provided by Akka through the supervisor hierarchy.
Think of this supervisor as an actor that links to supervised actors and restarts them when one dies.
The responsibility of a supervisor is to start, stop, and monitor child actors.
It’s the same mechanism as linking, but Akka provides better abstractions, called supervision strategies.
That way you can supervise a supervisor in case of a crash.
It’s hard to build a fault-tolerant system with one box, so I.
Implementing message-passing concurrency with actors recommend having your supervisor hierarchy spread across multiple machines.
That way, if a node (machine) is down, you can restart an actor in a different box.
Always remember to delegate the work so that if a crash occurs, another supervisor can recover.
Now let’s look into the fault-tolerant strategies available in Akka.
In the Onefor-One strategy (see figure 9.7), if one actor dies, it’s recreated.
This is a great strategy if actors are independent in the system.
If you have multiple actors that participate in one workflow, restarting a single actor might not work.
In that case, use the All-for-One restart strategy (see figure 9.8), in which all actors supervised by a supervisor are restarted when one of the actors dies.
So how do these look in code? In Akka, by default, each actor has one supervisor, and the parent actor becomes the supervisor for the child actors.
When no supervisor strategy is defined, it uses the default strategy (OneForOne), which restarts the failing child actor in case of Exception.
The following example configures the WordCountWorker with OneForOneStrategy with retries:
You’re overriding the supervisorStrategy property of the actor with your own fault handler.
If no pattern matches, the fault is escalated to the parent.
Similarly, the following example configures the WordCountMaster actor with AllForOneStrategy:
A working example of a supervisor with the word-count example is in this chapter’s code base.
The next section talks about working with mutable data in a concurrent world.
It essentially acts as proxy to an actual value that does not yet exist.
Usually this value is produced by some computation performed asynchronously.
The simplest way to create a Future is to use the apply method:
In this case someFuture will hold the result of the computation and T represents the type of the result.
ExecutionContext is an abstraction over a thread pool that is responsible for executing all the tasks submitted to it.
Here the task is the computation performed by the Future.
There are many ways to configure and create ExecutionContext but in this chapter we will use the default global execution context available in the Scala library.
When the Future has the value it is considered completed.
To do an operation after the Future is completed we can use the onComplete callback method as in following:
Since a Future could be a success or failed state, the onComplete allows you to handle both conditions.
You can use Promise to create a Future, which will be completed when Promise is fulfilled with a value:
Here we have created two Futures, one using the future method and the other from Promise.
Once the promise is completed you cannot invoke success again.
And promise will automatically complete the future and the onSuccess callback will be invoked automatically.
Please note that callbacks registered with Future will only be executed once the future is completed.
The Scala Future and Promise APIs have many useful methods so please check the scaladoc for details.
By now you might be wondering when to use Future and when to use actors.
A common use case of Future is to perform some computation concurrently without needing the extra utility of an actor.
The most compelling feature of the Scala Future library is it allows us to compose concurrent operations, which is hard to achieve with actors.
To see them in action let’s implement the word count problem using Future and Promise.
You are going to reimplement the word count problem using Future.
First let’s break down the word count problem into small steps so that we can solve them individually.
Since Future allows functional composition we should be able to combine small steps to solve our problem.
We already know how to scan the files in a given directory but this time we will perform it asynchronously:
Similarly we can count words for a given file inside a Future.
If something goes wrong we can use the recover method to register a fallback:
The recover callback will be invoked if IOException is thrown inside the Future.
Since each file is processed inside a Future we will end up with a collection of futures like the following:
How will we know when all the futures will complete? We cannot possibly register callbacks with each future since each one is independent and can complete at a different time.
It takes collections of futures and reduces them to one Future:
You can invoke the map method on future to sort the result:
If you haven’t guessed, Future is an example of a monad.
It implements map, flatMap, and filter operations, necessary ingredients of functional composition.
Now you can compose scanFiles and processFiles to produce the sorted result:
The for-comprehensions here are composing scanFiles and processFiles operations together to produce another future.
Note here that each operation is performed asynchronously and we are composing futures in a nonblocking fashion.
The for-comprehensions are creating another future that only completes when both the scanFiles and processFiles future complete.
It is also acting as the pipe between two operations where the output of the scanFiles is sent to processFiles.
For the last step we can use a Promise that will be fulfilled when futureWithResult completes.
Here is the complete implementation of the word count example using Future:
As you can see it’s very easy to get started with Future and it is very powerful because it allows you to do functional composition.
On the other hand, actors allow you to structure your application and provide fault-handling strategies.
You can have your application broken down into actors and then have actors use futures as building blocks to perform asynchronous operations.
In the next section we will see how we can use futures inside actors.
As you work your way through Akka actors two common patterns will evolve:
Send a message to an actor and receive a response from it.
So far we have only used fire-and-forget using the ! method.
But getting a response is also a very common use case (a.k.a ask pattern)
Reply to sender when some concurrent task (Future) completes (a.k.a pipe pattern)
Let’s take an example to demonstrate these two patterns in action.
In the following code snippet we have two actors, parent and the child:
GreetingsActor accepts name and sends the message to a child actor to generate a greeting message.
In this case we are using the ask method (you can use ? as well) of the ActorRef to send and receive a response.
Since messages are processed asynchronously the ask method returns a Future.
The mapTo message allows us to transform the message from Future[Any] to Future[String]
The challenge is we don’t know when the message will be ready so that we can send the reply to the sender.
The pipeTo pattern solves that problem by hooking up with the Future so that when the future completes it can take the response inside the future and send it to the sender.
To see the complete working example please look at the chapter codebase.
It’s also discussed why shared mutable data is the root cause of most of the concurrency problems, and how actors eliminate that using shared nothing architecture.
But what if you have to have a shared state across multiple components?
Shared state—A classic example is where you want to transfer money from one account to another, and you want to have a consistent view across the application.
Alternatives like STM would be a great fit for this kind of problem, or you have to build transactions over message passing.
Cost of asynchronous programming—For many programmers, it’s  a paradigm shift to get used to asynchronous programming.
It takes time and effort to get comfortable if you are not used to it.
At times, asynchronous message passing makes it difficult to track and isolate a problem (knowing the starting point of the message helps)
This has nothing to do with the actor model specifically, but more to do with the inherited complexity of messaging-based applications.
Performance—If your application has to have the highest performance, then because actors may add an overhead, you may be better off using a much lower level of abstraction, like threads.
But again, for 99.9 percent of applications, I think the performance of actors is good enough.
In this chapter you learned about new concurrency trends and the problems with shared-state concurrency.
It became clear that if you raise the level of abstraction.
This chapter focused on message-passing concurrency and how to use actors to implement it.
We also learned about the Future and Promise APIs and how we can use functional composition to construct larger programs by combining small concurrent operations.
One of the big challenges with building fault-tolerant applications is handling errors effectively and recovering from them.
You learned how the supervisor strategy works to handle errors in actor-based applications.
This makes it easy to build long-running applications that can automatically recover from errors and exceptions.
The next chapter focuses on writing automated tests for Scala applications and explores how Scala helps with writing tests and the various tools available to you for testing.
It also shows you how to write tests around actors.
So far, I’ve been showing you code without writing tests—so why worry about that right now? The answer is, I wrote tests around the code but didn’t mention doing so because I wanted you to focus more on the Scala language.
My goal for this chapter is to make you comfortable writing automated tests in Scala so that you can build production-quality software.
The path to writing well-crafted code1 is the path where you write tests for your code.
The common perception about writing tests is that it’s hard, but this chapter will change that mindset.
I’m going to show you how you can get started.
The idea of test-driven development (TDD) is to write the test before you write code.
I know this seems backward, but I promise you that by the end of this chapter it will make sense.
You’ll learn that writing tests is more like doing a design exercise than testing, and it makes sense to design your software.
I’ll start by introducing automated testing and how developers use it in real-world projects.
There are two kinds of automated tests: ones you write (the most common) and ones you generate for your code.
First I discuss how you can generate tests for your code using the ScalaCheck tool, because it’s easy.
Scala, being a statically typed language, enjoys a unique position where tools like ScalaCheck can generate tests for your functions or classes based on types.
ScalaCheck is a great way to get started with automated tests.
But to truly get the benefit of automated tests, you also have to write them manually.
The majority of this chapter focuses on writing automated tests.
If you’re a Java developer and have used JUnit before, using it to test your Scala code is easy.
Specs is a testing tool written in Scala for Scala and provides more expressiveness in your tests.
I’ll take you through the process of writing tests, the tools available to you, and design techniques you can use to make your design testable.
The testability property of your design determines how easy it is to write tests.
I’ll show you how to implement dependency injection in Scala.
Dependency injection is a design pattern used by developers to make their code more testable (read more about this in section 10.5)
As a hybrid language, Scala provides a number of abstraction techniques you can use to implement dependency injection.
Writing automated tests is commonly said to be hard, but the reality is if you use the right tools and techniques it’s easy.
Without any further delay, let’s get started by asking: what are automated tests? And how do they fit into the software development process?
I don’t care how good you think your design is, if I can’t walk in and write a test for an arbitrary method of yours in five minutes it’s not as good as you think it is, and whether you know it or not, you’re paying a price for it.
Automated tests are tests that are recorded or prewritten and can be run by a machine without any manual intervention.
The tool that allows you to run these tests is called an.
As mentioned earlier, there are two kinds of automated tests: ones you write and ones generated by a tool.
Regardless of how the automated tests are created, it’s important to understand the value of having them around and running them as often as you can.
To grasp their benefits, let’s explore how automated tests fit into the agile software development process.
Chances are you’re already doing agile software development,2 but if you aren’t, having these tests available is still valuable.
In the agile software development process, teams don’t analyze and design the application up front; they build it using an evolutionary design.3 In this process developers only design for today, not tomorrow.
They design an application based on what they know today, understanding that some of the design decisions made today might be wrong tomorrow.
In this model, application design evolves and goes through lots of changes.
The first question is more important in the context of this chapter.
Automated tests are important because your application goes through lots of changes and you might break existing functionality.
In this ever-changing environment, you won’t be able to keep up using manual testing.
You need automated tests that can run repeatedly to ensure that your application is behaving as expected and that nothing unexpected has changed.
Why not design the application up front so you don’t have to change so frequently? In some cases you have to do an upfront design, like integrating with an external commercial product, and you don’t have control over its source code.
But most of the time you have to cope with requirements that change over time.
Agile software development tries to reduce the cost of change, and getting a correct upfront design is hard in the face of changing requirements.
It becomes costly to maintain and change a big upfront design.4
You can’t think of all the features your application will implement or how the various components of your application will work with each other.
The larger the application becomes, the harder it becomes to design up front.
Agile processes embrace more of an incremental approach to software development, where you build a little, test a little, and review the application features with users to get their feedback.
In this process it’s vital to have automated tests that give you feedback to assure you that your application is working.
Automated tests not only help you find problems, they also work as documentation for the application.
Section 10.6 shows you how to develop executable documentation in Scala using Specs.
The problem with the traditional way of documenting code is that the documentation goes stale quickly because most of us forget to keep it up-to-date with code changes.
But if you have tests that act as documentation, you’ll keep them up to date with code changes because every code change is preceded by or is the result of a test change.
Other types of tests also play an important role in software development but are beyond the scope of this chapter.
In specificationbased testing you express the behavior of your application in an executable description, and the tool generates tests to break it.
On the other hand, unit tests are something you write to design and verify your application.
If you haven’t done any sort of automated testing, it might take a while to get used to it.
Don’t worry too much at the beginning, and don’t give up, because the benefits mentioned earlier will pay you back.
You’ll be able to respond to change quickly because now you have tests to provide feedback.
I begin by discussing how to generate automated tests using ScalaCheck so that you can start getting the benefits of automated tests while you learn how to write them for your Scala project.
ScalaCheck is a tool for testing Scala and Java programs by generating test data based on property specifications.
The basic idea is that you define a property that specifies the behavior of a piece of code, and ScalaCheck automatically generates random test data to check whether the property holds true.
To create a new property in ScalaCheck, you have to make a statement that describes the behavior you want to test.
For example, I’m making the following claim about the reverse method defined in the String class:
My claim is that if the reverse method is invoked twice on an instance of a String, I get the same value back.
The job of ScalaCheck would be to falsify this statement by generating random test data.
Without going any further, let’s try a little example with ScalaCheck.
Create a new directory called scalacheck and add the following build.sbt file to the root of the directory:
This project file will download and add the ScalaCheck dependency to your project (don’t forget to do a reload and an update)
You can also download the latest ScalaCheck (http://code.google.com/p/scalacheck/downloads/list) and play with it using Scala REPL.
In this chapter I show all the examples using the SBT project because it’s more convenient to build and compile.
In the next section, you’ll create your first ScalaCheck test to verify the claim about the reverse method.
The property in ScalaCheck is represented by instances of the Prop trait.
This method takes a function as an argument that should return a Boolean and can take any type of parameter as long as there’s a generator.
Generators are components used by ScalaCheck to generate test data.
Here’s how the property would look for the statement I made about reverse in the previous section:
ScalaCheck will use the generator for String type to generate random string data to validate the statement.
To run this property with SBT, you need to wrap it inside the Properties class (later I show you how to use ScalaCheck with your tests)
After 100 tests it should be safe to say that the property does hold true.
Does this hold true? Go ahead and try this and see whether ScalaCheck can prove the property to be wrong.
Let’s try to create a property that’s not always true and see whether ScalaCheck is able to catch it.
In this case ScalaCheck will fail and show the arguments for which the expression doesn’t hold true.
Figure 10.1 The output from running ScalaCheck from the SBT prompt.
In listing 10.1 you create a specification for the String class.
Granted, you haven’t specified the complete behavior of the class, but you can see how ScalaCheck specifications work.
You extend the Properties trait to make your specification runnable by SBT.
Each statement that you want to verify is wrapped around a ScalaCheck property.
You’re using the Prop.forAll factory method to create a new property by passing a function that captures the statement that needs to be verified by ScalaCheck.
ScalaCheck executes this function by passing random test data generated by built-in generators.
I hope by now you get the idea of how ScalaCheck properties are created and can be used to test the behavior of Scala code.
You provide a specification of a class or method in terms of properties.
Because Scala and Java are both statically typed languages, ScalaCheck would be a great way to create specifications and add them to your project.
The next section discusses ScalaCheck generators so that when the time comes, you can create your own custom generator for a new type you create.
In the previous section you wrote your first ScalaCheck specification without worrying about generators, so why bother now? The reason you didn’t worry about generators was that ScalaCheck knows how to generate test data for the String type (it knows about other types too6)—but how about a new type that you created? In this case you’re on your own.
The good news is ScalaCheck provides all the building blocks you need to roll your own generator.
Think of generators as functions that take some generation parameters and return a generated value sometimes.
For some combinations of parameters, the generator may not generate any value.
The ScalaCheck library already ships with various kinds of generators, but one in particular is quite important: the arbitrary generator.
This is a special generator that generates arbitrary values for any supported type.
It’s the generator ScalaCheck used when testing the String specification you created in the previous section.
To run any specification, ScalaCheck needs a generator to generate test data, so generators play an important role in ScalaCheck.
The next section shows you how to create a custom generator in ScalaCheck.
In this section I show you how to use ScalaCheck with a simple real-world use case.
In the real world, you don’t write specifications for the String class but rather for types (classes and traits) that you’ll create.
Instead of creating a new type on your own, let’s take a look at the scala.Either class.
This will be close, in terms of complexity, to the types you create or deal with in your project.
In Scala, the Either type allows you to represent a value for one of two possible types: Left and Right.
Usually, by convention, Left represents failure and Right represents success.
In this section you’ll add specification tests for some of its API methods.
This is clearly not an exhaustive list, but it’s a good starting point:
Either will have value on either Left or Right, but not both at any point in time.
The complexity of the specifications grows as you go down the list, but you’ll see how easy it is to implement them.
First, create a custom ScalaCheck generator for the Either type, because there’s no built-in generator for this type.
Creating new generators in ScalaCheck is as easy as combining the existing generators.
To keep things simple, only create generators that can generate Int values for Left and Right (later I show you how to parameterize the generator)
To create a new generator for Left, use the existing generator for the Int value to create instances for Left:
The preceding code snippet creates a new instance of the Int type generator and maps it to create values for Left.
Similarly, for creating instances of Right, use the following code:
To successfully generate instances of the Either type, you have to randomly generate instances of Left or Right.
To solve these kinds of problems, the ScalaCheck Gen object ships with helper methods like oneOf or frequency, called combinators.
And oneOf ensures that Left and Right values are generated randomly:
The generator you’ve defined here only generates Int values, but if you wanted to play with different types of values, you’d also define the generator like this:
The generators for both Left and Right are type-parameterized so they’ll take any type of parameter for which the arbitrary generator is defined in ScalaCheck.
You can also use Gen.frequency to get more control over each individual generator and its uses.
This specification is easy to implement—all you have to do is check that both Left and Right aren’t present at the same time.
In this case you’ll use the isLeft and the isRight methods available in the Either type that return true or false based on whether it contains a value:
If isLeft and isRight are equal, your specification fails because it clearly states that both Left and Right can’t have values at the same time.
For the second specification (“fold on the Left should produce the value contained by Left”) and the third (“fold on the Right should produce the value contained by Right”), use the fold method defined in the Either type:
Both cases will error out if they try to access the wrong value.
The contract of the fold is like the following, where it only applies the appropriate function parameter:
Go ahead and add these properties to a specification class and run them (see listing 10.2 for the complete specification)
The fourth specification (“swap returns the Left value to Right and vice versa”) is a little harder but nothing that can’t be fixed.
According to this specification, the swap method of the Either type could swap the value from Left to Right and vice versa.
Here you can use pattern matching to check whether the value corresponds to Left or Right.
For example, if it’s Left, then after swap the value should be available on the Right side and vice versa for the Right value:
Customizing the number of tests generated by ScalaCheck ScalaCheck provides configurable options which allow you to control how ScalaCheck verifies your property.
If you want to generate more than 100 successful tests before a property is declared successful, you can pass ScalaCheck arguments to your test through SBT.
This action allows you to provide test names as arguments and pass additional test arguments.
If you don’t specify any test names, it will run all the tests like the SBT test action.
Check the ScalaCheck documentation to learn about all the configuration options.
The previous listing creates a generator for the Either type by using the building blocks provided by ScalaCheck.
Using it, you create generators for both Left and Right values of the Either type.
Then, using the combinators available in the Gen object, you create a generator for the Either type.
The rest of the code is defining ScalaCheck properties for all the specifications declared at the beginning of the section.
You can always download them and go through their ScalaCheck tests to see various ways you can use them.
It’s also easy to use ScalaCheck to test Java codebases.
Because Scala and Java interoperate, you don’t have to do anything special to test java codebases.
As you’ve already figured out, ScalaCheck is a powerful framework.
With the ability to create custom generators, I’m sure you can think of places in your project where ScalaCheck will be valuable.
What about the new functionality you’ve yet to implement? You aren’t sure how it should look yet—the classes, traits, and functions you’d need to implement the functionality.
The next section introduces you to a design technique called test-driven development, which might solve your problem.
At first it sounds misleading because you usually associate tests with a verification of the software.
You test your software to make sure it’s working as expected.
It’s more like the last thing you do before releasing your software.
In agile software development, TDD is one of the most, if not the most, important practice.
But you don’t have to buy in to agile to reap the benefits of TDDyou can use it with any process.
In the end you get a test suite, but that’s more of a secondary effect.
Let’s go through and understand how TDD works, and then I’ll explain why it works.
The figure outlines how TDD works as a development practice.
An end-to-end test (sometimes called an integration test) exercises your application from top to bottom.
This could mean the test is making an HTTP request through a browser and checking the response back.
Then you write a bunch of unit tests to break the problem into smaller pieces and make them pass.
You only write production code when you have a failing test, and you only refactor when your tests are passing.
One way to think about it is to take one of the acceptance criteria of the feature you’re supposed to implement and write it as an executable test.
As a pricing analyst, you want to calculate a price for products so you can bill customers correctly:
Acceptance criteria: A 100 product code should use cost plus the percent amount.
In this case, if you pick the first acceptance criterion, your job would be to implement that criterion as a test.
When you start to implement the acceptance criterion as a test, the following are the some of the questions that might pop into your head:
If this is the case, you have already started to think about the design.
But at this point your focus should be only on the unit of work at hand.
The most common theme of TDD is to pick the simplest solution that could possibly work.
In this case the simplest solution would be to create a function that takes a product code, looks it up in a Map, and returns the price using the formula specified in the acceptance criterion.
Probably, using Map to look up the cost design decision you made might not hold true on the next test.
When that happens, you’ll make the necessary code changes and look up the cost from some persistence store—you get the idea.
Do the simplest thing that could possibly work, and then incrementally design and build your application.
Once your test is running, you have the opportunity to refactor or clean up.
This test-code-refactor cycle repeats for each feature or step that you implement.
When a test is failing, you’re in the red state; then you make the test pass and move to the green state.
As you go through some examples, it will become clearer.
The good news is that the Scala community is blessed with lots of testing tools to use.
I’m going to focus on two of the most popular: JUnit and Specs.
JUnit is more popular among Java developers and can be easily used to test Scala code.
But most Scala programmers use Specs to test their Scala code.
As you start writing tests, you’re also building a test suite.
If you don’t run them often, then you’re not extracting the benefits from them.
The next section discusses setting up a continuous integration9 environment to get continuous benefits from them.
Once you and your team get comfortable with TDD, you need a tool that checks out the latest code from your source code repository and runs all the tests after every check-in of the source control system.
This ensures that you always have a working software application.
A continuous integration (CI) tool does that automatically for you.
Almost all the existing CI tools will work for Scala projects.
Table 10.1 shows some Scala tools that you could use in your Scala project.
If you have a testing tool or CI environment that doesn’t work well with SBT, you can use Maven (http://maven.apache.org) as your build tool.
There’s a Maven Scala plug-in10 that makes your Maven project Scala-aware and allows you to compile and run your Scala tests.
You can also generate a .POM file (Maven build file) from your SBT project using the make-pom action.
I’ve mentioned only a handful of tools you can include in your project to have a continuous feedback cycle.
The toolset around Scala is always evolving, so try out a few tools and pick the one that best fits your project.
The next section explains how you can use JUnit to test your Scala code.
This is a popular framework used in many Java projects.
If you’ve used the JUnit testing tool previously to write tests for Java code, I’m happy to.
Open source continuous integration tool that could build and test your project continuously.
You can configure it to point to your source control and run builds every time the repository is updated.
In essence, almost all the CI tools have these features.
You could also use any other popular CI tool for your Scala project.
Allows you to run SBT build actions from Jenkins and lets you configure SBT using Jenkins.
For CI tools that don’t have native support for SBT but support Maven, you can easily generate a POM file for your SBT project using the make-pom SBT command.
Code coverage is a measurement of source code that’s under automated tests.
Code coverage tools help you to identify the area of the code that’s not tested.
Almost all Java code coverage tools will work for Scala projects, but using tools that work with your build tool, like SBT, is always better.
Better tests with dependency injection inform you that you can use it to test Scala code too.
To use JUnit inside your SBT project, add the following dependency to your project file:
By default, SBT doesn’t recognize JUnit-style test cases, so you have to add another dependency to your project to make SBT aware of JUnit test cases:
The junit-interface11 tool implemented the test interface of SBT so that SBT can run JUnit test cases.
After you reload and update your SBT project, you’re ready to add JUnit test cases and run them using the test action from SBT console.
This works out great if you have legacy JUnit tests that you want to retain while porting your application from Java to Scala, or you have both Java and Scala projects12 that you’re building with SBT.
JUnit is a good way to get started writing automated tests, but it’s not an appropriate testing tool for Scala projects because it still doesn’t understand Scala natively.
There are multiple open source Scala testing tools you can use to write more expressive tests.
Section 10.6 looks into a Scala testing tool called Specs that most Scala developers use, but for now let’s try to understand an important concept called dependency injection, which helps in designing more testable applications.
This example is about calculating the price of a product based on various pricing rules.
Typically any pricing system will have hundreds of rules, but to keep things simple I will only talk about two:
With these rules in place, the calculate price service would look something like the next listing.
At the same time this class is also resolving its dependencies.
Yes, there are some potential problems with this, in particular when your software is evolving.
What if your client decides to use a different external pricing source to calculate the price or redefines the cost-plus calculation logic for some customers? In.
This might be okay in some situations, but if you’re planning to build this as a component that will be shared by projects, you have a problem.
In its simplest form, you could pass these calculators through the constructor:
The only thing that’s changed compared to the previous code listing is that now the instances of two calculators are passed in as constructor arguments.
In this case, the caller of the service takes the responsibility to determine the dependent price calculators and inject them into the service.
This also gives you flexibility in terms of design because now you can easily incorporate the changes your customer is talking about and come up with different implementations of pricing rules.
What does DI have to do with testing? In unit testing it’s important to understand the unit you’re testing.
But if you don’t isolate the calculators, your test will end up testing them as well.
In this small example, it might be hard to see the difference, but in a large application without isolation of dependencies, you’ll end up initializing the system over and over again for each component you test.
Isolation is important if you want to write simple and manageable unit tests.
The second problem with a closely coupled system is the speed of testing.
Remember that your tests are your feedback mechanism, so if they run slowly you won’t get faster feedback.
In the test version of the calculator, you can return a hardcoded price or use an inmemory database to speed things up.
This will also give you more control over the test data.
If your tests are heavily dependent on the external data, they will become brittle because the external data could change and break your tests.
If you follow TDD as a driver for your design, you don’t have to worry too much about the coupling problem—your tests will force you to come up with a decoupled design.
You’ll notice that your functions, classes, and methods follow a DI pattern.
The following sections discuss ways you can implement dependency injection in Scala.
These techniques can help to write more testable code and provide a scalable solution in Scala.
Cake pattern Handles dependency using trait mixins and abstract members.
The Scala structural typing feature provides duck typinga in a type-safe manner.
Duck typing is a style of dynamic typing in which the object’s current behavior is determined by the methods and properties currently associated with the object.
Manages dependencies using implicit parameters so that as a caller you don’t have to pass them.
In this case, dependencies could be easily controlled using scope.
Function currying is a technique by which you can transform a function with multiple arguments into multiple functions that take a single argument and chain them together.
I show you how to use a DI framework in your Scala project.
A cake pattern13 is a technique to build multiple layers of indirection in your application to help with managing dependencies.
The cake pattern is built on the three abstraction techniques described in Table 10.3
The first thing you can do is extract the calculator instances from the service to its own namespace called Calculators:
Provides a way to abstract the concrete types of components.
Using abstract types you can create components that don’t depend on concrete types, and the type information could be provided by other components that use them.
Allows you to redefine this and is a way to declare the dependencies required by a component.
Using a trait mixin, you can inject various implementations of dependencies.
A mixin allows you to use Scala traits to override and add new functionality.
You’re using the self type this: Calculators to redefine this.
The self type will ensure that they’re available during runtime.
You must be wondering why both calculators are declared as abstract inside the Calculators trait.
It’s because you want to control how these calculators are created.
Remember from the tests, you don’t want to use the calculators; instead you want to use a fake or TestDouble version of the calculators.
At the same time, you want to use the real version of the calculators in production mode.
For production mode you could create a pricing system by composing all the real versions of these components, as in the following:
In the case of the TestPricingSystem, the calculators are implemented using TestDouble so that it helps to write tests around the calculate price service.
In your tests you’ll use the TestPricingSystem shown in the following listing.
You mix the test version of the pricing system into your test class.
This will automatically make the fake implementation of the calculators available inside the test.
This is a common technique used by Scala developers to manage dependencies.
In smaller projects, it’s reasonable to have the wiring of dependencies implemented like the PricingSystem and the TestPricingSystem, but for large projects it may become difficult to manage them.
For large projects it makes more sense to use a DI framework (section 10.5.2 shows how to use off-the-shelf DI) that allows you to completely separate object creation and injection from business logic.
Structural typing in Scala is the way to describe types by their structure.
To create a structure that captures this information, make Scala treat that as a new type:
Listing 10.4 JUnit test case for calculating price service (cake pattern)
The code creates a new type called Calculators by specifying the structure.
When using a structural type, you don’t necessarily have to name your type—you can use it inline, as in the following:
In this case, the type of the constructor parameter is defined as inlined B.
The advantage of structural typing in Scala is that it’s immutable and type-safe.
Again, you could create two types of configuration—one for testing and another for production:
Based on what you’re doing, you have the flexibility to pick the appropriate configuration.
This is one of my favorite ways to handle dependencies because it’s easy and simple.
Internally, structural typing is implemented using reflection, so it’s slower compared to other approaches.
Sometimes that’s acceptable, but be aware of it when using structural typing.
Implicit parameters provide a way to allow parameters to be found.
Using this technique you can have the Scala compiler inject appropriate dependencies into your code.
ScalaCheck uses implicit parameters to decide an appropriate generator to use for a given property.
To declare a parameter implicit, you have to mark the parameter with the implicit keyword.
If the compiler fails to find an appropriate implicit value, it fails the compilation.
Create an object called ProductionServices that defines these implicit values for production code:
To provide values for implicit parameters, you also have to mark each value with implicit—otherwise the compiler won’t recognize it.
You have to import this object when running in production mode, and the easiest way to do that is use a configuration object like the following:
Similarly, for testing, create a separate configuration object and provide a test implementation of the services:
You don’t necessarily have to always use implicit values for implicit parameters because you can always explicitly pass parameters the old-fashioned way.
Using implicit to handle dependencies can easily get out of hand as your application grows in size, unless they’re grouped together like the preceding configuration objects.
Otherwise, your implicit declaration and imports will be scattered around the code and will make it hard to debug compilation issues.
Sometimes this constraint can be too restrictive to build a scalable design.
The general idea behind DI is inversion of control.14 Instead of a component controlling its dependencies, it’s passed from outside (usually by some container or framework)
When you work with functions, then, DI is already happening automatically.
If you consider a function as a component, then its dependencies are its parameters.
If you create function currying, you can also hide the dependencies as you did with other patterns.
Function currying is a technique of transforming functions that takes multiple arguments into a chain of functions each with a single argument.
The following is the new interface of Calculators that only uses functions:
The type Calculator is an alias of function that takes product ID and returns the price.
The findCalculator function determines the calculator for a given price type.
And finally calculate is a function that takes an instance of Calculator and productId to calculate the price of the product.
This is quite similar to the interfaces you designed earlier, but this time with only functions.
You can turn the calculate function into a curried function by invoking the curried method defined for all function types in Scala:
The curried method takes a function of n parameters and transforms it to n functions with one parameter.
In this case it created a function that takes Calculator and returns a function that calculates the price for a productid.
The benefit of doing this now is you have a function that knows how to calculate price but hides the Calculator from the users.
The following is the example test implementation of the Calculators:
The priceCalculator method returns a function that takes the productId and returns the price of the product that encapsulates the dependencies used to compute the price.
This is an example of how you can do dependency injection using functional programming.
Scala’s abstract members, self type and mixin provide more abstraction techniques than are available in Java, but DI frameworks provide the following additional services that aren’t available in these abstraction techniques:
They create a clean separation between object initialization and creation from the business logic.
These frameworks provide a separate lifecycle to create dependencies as part of the application initialization.
This way, your wiring between components becomes transparent from the code.
The good news is you can use any Java DI framework with your Scala project.
This section shows you how to use the Spring framework as a DI framework in your Scala project.
I won’t explain how the Spring dependency injection framework works, but if you’re new to it read the tutorials16 available on the Spring framework website.
The Spring framework allows you to configure dependencies in multiple ways.
I’ll show you how to configure it using the external XML configuration file.
In the Spring world, all the dependencies are called beans, because all the objects follow the JavaBean17 convention.
According to this convention a class should provide a default constructor, and class properties should be accessible using get, set, and is methods.
To make a property a bean property, Scala provides a handy annotation called @BeanProperty.
This annotation tells the Scala compiler to generate getter and setter methods automatically so you don’t have to worry about it.
Both price calculators are already beans because they provide a default constructor.
The only missing piece is to wire up dependencies to the service, and in Spring you can do this by specifying a configuration file, as shown in the next listing.
This is the main configuration file for the pricing application.
This file will be used to initialize the application beans.
Similarly, you also have a test version of the configuration file under src/test/resources that refers to the fake implementation of calculators.
You could also create fake instances inside the test and inject them.
You’ll use the latter one to see how you could inject fake versions of the calculators.
But first add the following dependencies to your SBT project file:
Both the Spring framework and the Spring test framework are added as dependencies.
Because you haven’t learned about Specs yet, let’s use JUnit as the testing tool.
Again, junitInterface is the testing interface for SBT so that it can run the JUnit tests.
To use Spring with the JUnit test, add the following annotations along with the test class declaration:
The RunWith annotation allows JUnit tests to get access to instantiated beans as defined in the application context file.
If you have a test version of the configuration file, specify that.
At this point, this test class is set up for testing the calculate price service.
The following is the JUnit test to check that the calculate price service uses the cost-plus calculator to calculate price:
The real implementation of the costPlusCalculator is replaced by a fake implementation.
The test is passing "costPlus" as a price type, and according to the logic (see listing 10.5) it will use the cost-plus calculator.
Similarly, the following is the test for an external price source calculator:
In a similar fashion, the real implementation is swapped with a fake version before invoking the service.
You annotate your JUnit test to allow it to see all the Spring beans using RunWith and specify the configuration file used to create the beans.
Note that if you have a test version of the configuration, you should specify it here—that way you don’t have to create a fake implementation per test.
As you can see, there’s nothing much to change to use Scala classes and traits with the Spring framework.
And this is true for all other dependency injection frameworks available in Java.
There’s some up-front work to use a DI framework, but for large projects it’s worth it—unless you’re using some Scala framework that provides native support for managing dependencies.
I covered a lot of ground in this section, and I’m sure the techniques you’ve learned here will help you to write more decoupled and testable systems in Scala.
JUnit was great to get quickly up and running with testing Scala code, but now it’s time to get used to the Scala-based testing framework, which is more expressive and easier to use.
So far I’ve been talking about test-driven development in this chapter, so why bother discussing BBD? How is it different from TDD?
The first thing to notice is that the definition of BDD doesn’t talk about testing at all.
This is on purpose, because one pitfall of doing TDD is that some people put more emphasis on testing than solving the business problem.
In fact, it recommends looking at the application from the stakeholder’s perspective.
The end result of doing BDD in a project has the following two important outcomes:
Delivering value quickly—Because you’re focused on viewing the application from the stakeholder’s point of view, you understand and deliver value quickly.
It helps you to understand the problem and recommend appropriate solutions.
Focus on behavior—This is the most important improvement because at the end of the day, behaviors that you implement are the ones your stakeholders want.
Having a focus on behavior also reduces the effort spent on up-front design, analysis, and documentation, which rarely adds value to the project.
To get developers and stakeholders on the same page, you need the Ubiquitous19 language, a common language everybody speaks when describing the behavior of an application.
And you also need a tool so you can express these behaviors and write automated specifications that assert the behavior.
Behavior-driven development using Specs2 In BDD, you still follow the red-green-refactor cycle during your development.
The only thing that changes is the way you look at these tests or specifications.
It’s time to see some BDD in action, and the next section introduces you to the BDD tool that most Scala developers use: Specs2
Specs220 is the BDD library for Scala, and it’s written in Scala.
At the time of this writing it’s the de facto BDD library used by Scala developers.
The easiest way to get started with Specs is to add it as a dependency to your SBT project.
If you’re planning to use some other version of Specs, make sure it’s compatible with the Scala version set in your SBT project.
Once you reload and update your project, you’re ready to use Specs.
The best part is that SBT knows how to run the Specs specification natively.
Write the first Specs specification using the same calculate price service you saw in the previous section.
First you use the should method to define the system, followed by the description of two behaviors of the service.
The Specs framework adds methods like should and in to the String class using implicit conversion so that your specification can become more expressive and readable.
When you run this specification using the SBT test action, you’ll see the output shown in figure 10.4
If you have a color-enabled terminal window, Specs will show the test output in different colors.
Because I haven’t implemented the specification, in figure 10.4, it’s yellow.
If I had implemented the specification, green would indicate a passed test, red a failure.
Implement these pending specifications using the cake pattern implementation of the service.
To use this version of the pricing system, you need to mix this in with the specification, as shown in the following listing.
The must method is again added by Specs using implicit conversions to almost all the types to make the specification more readable.
This example demonstrates how easy it is to write a good expressive specification with Specs.
The next section explores Specs features available to you for writing expressive specifications.
To effectively work with Specs, you need to get comfortable with specifications and the available matchers.
The matchers are the way you add expectations in your specification.
Specs ships with many built-in matchers, and you can find the complete list in the Specs documentation.21
Depending on the kind of behavior you’re describing, pick the appropriate one.
The basic format of the Specs specification is that you extend the Specification trait and then provide examples:
One way to look at a specification is as a group of examples that describe the behavior of your application.
But typically, when writing specifications you’ll have a component for which you’re describing the behavior; this is your system under specification.
You can organize the examples as a group for a system under specification:
You can also nest examples if you want to refine your examples.
You may want to add an example to describe the behavior of a cost-plus calculator when the product ID is.
Specs, MatchersGuide, “How to add expectations to your examples,”  http://code.google.com/p/specs/ wiki/MatchersGuide.
According to the stakeholder, the price in this case should be 0.0
You’re nesting the example for the special case when the product is empty.
By default, examples are run in isolation, and they don’t share any state.
What that means is that you have to take extra measures to share the variables and state.
Because having shared state between examples is a bad idea, I’m not going to cover that here.
Another interesting way to declare specifications in Specs is to use data tables.22
Data tables allow you to execute your example with a set of test data.
For example, if you have to describe an example of how the cost-plus rule calculates price, having one example with one sample data isn’t enough.
To describe its behavior properly, you need a set of data that evaluates the rule.
They let you specify your sample data in a table format like the following:
The first row of the table is the header and is used for readability purposes.
The second and following rows have sample data followed by a closure that’s invoked for each row of data.
To use data tables in your specification, you have to mix in the DataTables trait.
And the > at the beginning of the table is also required—think of it as a play command.
The > makes the table executable as part of the example.
Specs data tables are a great way to create examples with sets of example data.
You can also use ScalaCheck with Specs and have it generate sample data for your example.
The next section explores how automated testing fits into the asynchronous messaging world.
In chapter 9 you learned about actors as a specific example of messaging systems.
So far this chapter has talked about testing or created examples for systems that are synchronous, where the test invokes the system and control comes back to the test when the system is done performing an action.
But in asynchronous fire-and-forget systems, the control will come back to the test while the system is executing.
From the test, you don’t get the feedback you’re looking for.
To overcome this challenge, developers sometimes extract the business logic outside of the messaging layer (always a good idea) and test it separately.
One drawback with this kind of approach is that you’re no longer testing your system end to end.
For example, to verify that one actor is sending a message to another actor after some action, you need to write an integration test that sends a message to one actor and waits for the reply.
The general rule for writing integration tests around asynchronous systems is to detect invalid system state or wait for some expected notification with a timeout.
Writing automated tests around asynchronous systems is quite new, and the tools for it are still maturing.
One tool worth mentioning here is Awaitility,23 which provides a nice testing DSL for testing asynchronous systems.
Imagine that you have an order-placing service that saves orders to the database asynchronously, and you place an order by sending a PlaceOrder message.
Inside the specification you’ll use Awaitility’s await method to wait until the order is saved into the database.
If the order isn’t saved in the database, then you know that something went wrong while processing the message.
The preceding example sends an asynchronous message to the ordering service B and waits until the order is saved into the database.
The default timeout for Awaitility is 10 seconds, and you can easily set your timeout by invoking the overloaded version of await.
Awaitility doesn’t provide any infrastructure to help you test asynchronous systems, but it does make your examples readable.
This chapter covered an important topic that is critical to developing high-quality software.
Picking up a new programming language and trying to use it to build a large application is difficult.
And one of the common hurdles is to find a way to write automated tests in the new language or programming environment.
This chapter gave you that introduction and introduced tools you can use in Scala projects.
First I introduced you to automated testing and how you can generate automated tests using ScalaCheck.
You learned how to define specifications in ScalaCheck and create custom test data generators.
ScalaCheck is a great way to get test protection for your Scala project.
You learned about agile software development and the role test-driven development  plays inside it.
You also explored how TDD is beneficial to building reliable software and how it helps in evolving design.
To use TDD as a practice in a Scala project, you need tool support.
I explained how to set up a continuous environment and use SBT as a build tool.
I listed some of the common tools used by Scala developers.
Building applications using automated tests requires that your design be testable.
One of the critical properties for a testable design is inversion of control, used in Java, Ruby, and other languages.
Scala, being both object-oriented and functional, has more options to create abstractions.
Section 10.5 showed you ways of doing dependency injection in Scala.
Concepts like self type and abstract members aren’t only restricted to dependency injection—in fact, you can take these abstract ideas and build reusable components in Scala.
The most common mistake made by developers when doing TDD is putting focus on testing, whereas the most important thing is the behavior of the application.
I introduced you to a tool called Specs that allows you to write expressive specifiSend message to actor.
I mentioned that you can use JUnit to test your Scala code, but noted that it isn’t recommended.
On the surface, writing automated tests looks difficult, but I’m confident you don’t feel that way anymore.
With Scala’s rich ecosystem of tools, it’s easy to get started with automated tests or specifications, and you don’t have any excuse not to use them.
You’ve seen some functional programming features of Scala in previous chapters and examples, but chapter 11 ties them together with functional programming concepts so you can write more reliable and correct Scala programs.
These last 2 chapters lead you through advanced steps in Scala.
One of the most exciting features of Scala is that it runs on the JVM.
The benefit of running on a JVM, as you know by this point, is that you can integrate with other languages on it and take advantage of all the frameworks and tools built into other JVM languages.
Chapter 11, while it encourages you to think Scala first, covers the interoperability between Scala and Java.
Chapter 12 introduces Akka, a Scala toolkit that allows you to build nextgeneration, event-based, fault-tolerant, scalable, and distributed applications for the JVM.
To understand how the individual pieces of Akka fit together, you’re going to build a large real-time product search application using Akka called Akkaoogle.
This application is similar to what used to be called Froogle.
One of the most exciting features of Scala is that it runs on the JVM.
The benefit of running on a JVM is that you can take advantage of all the frameworks and tools built into other JVM languages.
More companies are moving to the JVM even if they don’t use Java as their primary programming language.
I strongly believe that any language that doesn’t have support for the JVM is almost a nonstarter, for most of the software projects in the enterprise.
One of the main design goals of Scala is to run on a JVM and provide interoperability with Java.
Scala is compiled to Java bytecodes, and you can use tools like javap (Java class file disassembler) to disassemble bytecodes generated by the Scala compiler.
In most cases, Scala features are translated to Java features so that Scala Interoperability between Scala and Java.
For example, Scala uses type erasure1 to be compatible with Java.
Type erasure also allows Scala to be easily integrated with dynamically typed languages for the JVM.
Some Scala features (such as traits) don’t directly map to Java, and in those cases you have to use workarounds (more about this in section 11.3)
Even though integration with Java is easy for the most part, I encourage you to use pure Scala as much as possible.
When I’m working with the Java library or framework, I try to find something equivalent in Scala first, and use Java if there’s no equivalent Scala library available.
The downside of using the Java library is that you have to deal with mutability, exceptions, and nulls that are absolutely discouraged in the Scala world.
Be extra careful when choosing a Java library or framework before using it in Scala.
A good example of a well-written Java library is Joda-Time (http://jodatime.sourceforge.net)
The most common integration of Scala and Java has part of the project written in Scala.
Section 11.4 shows how to use Scala with existing Java frameworks such as Hibernate and Spring to build a web application.
In most cases, the integration between Scala and Java is seamless, but be aware of some corner cases because they will occur when you’re integrating Java code with Scala and vice versa.
The goal of this chapter is to show you how easily you can integrate Scala with Java and which practices to follow to avoid integration problems.
You’ve been integrating with Java classes and frameworks throughout the book without my pointing them out, but here you’ll focus on integration so you can take advantage of both worlds.
The easiest way to introduce Scala in an existing Java project is to write some part of it in Scala and demonstrate the benefits the language has to offer over Java—and then gradually rewrite the Java parts to Scala.
Let’s kick off the chapter with some integration examples between Java and Scala.
You’ll learn how to handle features that are available in Java but not in Scala, such as static members and checked exceptions, and how to use Scala features like traits in Java code.
You’ll also learn how Scala annotations help in integration—for example, generating JavaBean-style get and set.
At the end of the chapter you’ll build a web application using Java frameworks.
Because working with dates in Java is always a painful process, the following Java code snippet uses the Joda-Time library to calculate the number of days between two dates:
To use this class in Scala, extend the Java class as follows:
The Scala class is calculating the payment using the daysBetween method defined in the DateCalculator Java class.
The integration is so seamless, you won’t even notice the difference.
In the next section you’ll learn how to use Java static members in Scala.
When working with Java classes that declare static members, you need to understand how they’re interpreted in Scala.
Scala doesn’t have any static keywords, and Scala interprets Java static methods by thinking of them as methods of a companion object.
Take a look at the following example to see how it works.
This code adds a static method that returns the chronology (chronological calendar system) used by Joda-Time to represent time:
Compiling Java and Scala together SBT knows how to build mixed Scala and Java projects out of the box.
The Scala compiler allows you to build against both Java classes and Java source code.
That way, if you have bidirectional dependency between Java and Scala, you can build them together without worrying about order.
You can also use the Maven build tool to build mixed Java and Scala projects.
To do so, you have to add an additional Maven plug-in.
In this chapter’s final example you’ll use Maven to build an example project.
To access the static member, you have to refer it as if it’s defined in a companion object, like the following:
You’re accessing the static method defined in DateCalculator by using the class name like you access a companion object.
Up next, you’ll see how to work with Java checked exceptions, because Scala doesn’t have them.
Scala’s lack of checked exceptions at times creates confusion when working with Java codebases where the compiler enforces checked exceptions.
If you invoke the following Java method in Scala, you don’t have to wrap the call in a try/catch block:
Scala enforces visibility at compile time but makes everything public at runtime.
There’s a reason for that: in Scala, companion objects are allowed to access protected members of companion classes, and that can’t be encoded at the bytecode level without making everything public.
Java enforces visibility rules both at compile time and runtime.
For example, if you have a protected static member defined in a Java class, there’s no way to access that member in Scala.
The only workaround is to wrap it in a public member so that can be accessed.
In Scala, you can invoke the method without a try/catch block:
As a programmer, it’s your responsibility to determine whether you need to catch the exception.
In cases where you think you should catch the exception, don’t rethrow the exception from Scala.
A better way is to create an instance of the Either or Option type.
The following code snippet invokes the writeToFile method and returns an instance of Either[Exception, Boolean]:
The benefit is now you can compose with the result.
But there may be cases where you have to throw an exception because some framework or client code expects it, and in these cases you can use Scala annotations to generate bytecodes that throw an exception (section 11.2.1 has more on this)
Understanding how Java generics work is important because they’re used in Java collections.
But things become interesting when you have classes defined in Java with wildcard types.
Here are two examples of Java collections with wildcard types:
Let’s look at an example to see how to use Java raw types in Scala.
The following creates a Java vector with a wildcard type:
Working with Java collections Working with Java collection classes in Scala is painful once you get used to the power of the Scala collections library.
Ideally, you should work with Scala collections in Scala code and transform them into the Java collection equivalent when crossing into Java code, and vice versa.
That way, you can use the power of the Scala collections library when needed and easily integrate with Java codebases that only understand Java collection classes.
The Scala library ships with two utility classes that do exactly that for you:
Both of these classes provide the same set of features, but they’re implemented differently.
JavaConversions provides a series of implicit conversions that convert between a Java collection and the closest corresponding Scala collection, and vice versa.
JavaConverters uses a “Pimp my Library” pattern to add the asScala method to Java collection and asJava method to Scala collection types.
My recommendation would be to use JavaConverters because it makes the conversion explicit.
The following example uses JavaConverters to convert java.util.List to Scala and back:
The existential type sets the upper bound of the type C and prints all the elements of the Java vector.
One of the most interesting language features of Scala is traits, which are used a lot in Scala codebases.
If you define a trait with only abstract methods, it gets compiled in the Java interface, and you can use it in Java without any issues.
But if you have a trait with concrete methods, things become interesting.
Let’s take an example where you’ll have a trait with concrete methods and see how that’s compiled into Java bytecode.
The following example has a Scala trait that makes objects persistable to a database when mixed in:
Note that when working with stackable traits, it’s much better to create a concrete class in Scala and then directly use it or extend it in Java.
One of the first hurdles people face when integrating Scala with Java frameworks is that Scala classes don’t have JavaBean-style get and set methods.
Scala annotations provide the flexibility to specify how you want the Scala compiler to generate bytecodes and are helpful in cases like these.
Scala doesn’t follow the standard Java getter and setter pattern.
For example, to create a Scala class with a Scala-style getter and setter, all you have to do is declare members as var, as in the following:
If you compare the following code with the code generated in the preceding snippet, it will make sense:
In the case of name, it will generate getName and setName methods:
Note here that when using the BeanProperty annotation, the Scala compiler will generate both Scala and Java-style get and set methods.
Using BeanProperty does increase the size of the class file generated, but that’s a small price to pay for the interoperability with Java.
Section 11.1 showed that Scala doesn’t have checked exceptions, and Scala doesn’t have a throws keywords to declare methods that throw exceptions.
Look into the generated bytecode, and you’ll see the throws clause:
You can also use Scala’s target meta annotation to control where annotations on fields and class parameters are copied.
In the following code the Id annotation will only be added to the Bean getter getX:
Otherwise, by default, annotations on fields end up on the fields.
This becomes important when you’re dealing with Java frameworks that are particular about where the annotation is defined.
In the next section you’ll see some usage of the target annotation and how to use popular frameworks like Spring and Hibernate in the Scala codebase.
Scala using Java frameworks In this section you’ll build a web application using Scala classes with Java frameworks.
This example will show how you can use Scala in a Java-heavy environment, so when adopting or migrating to Scala you don’t have to throw away all your investment in Java frameworks and infrastructure.
Obviously, some of the boilerplate code goes away when you use frameworks built for Scala.
Nonetheless, learning to work with Java frameworks is important for cases where you don’t have the option to select Scalabased frameworks.
You’ll also let go of your favorite build tool, SBT, and use Maven to build your Java because it’s the most common build tool used in Java projects.
It’s also safe to skip this section if you aren’t interested in working with Java frameworks.
Before going any further, let’s identify the type of application you’re going to build.
You’ll build a small web application called topArtists that displays top artists from Last.fm (www.last.fm)
Last.fm is a popular music website that lets visitors access internet radio stations.
Last.fm also provides an API you can use to retrieve various charts about songs and artists.
You’ll also display all the artists stored in your local database to the user.
Make sure you have it before you run this example.
If you’ve done Java development, then most likely you’ve already used Maven.
Maven knows how to compile Java source files, but to make it compile Scala source files you need to add the Maven Scala plug-in.2 To create an empty web application using Maven, execute the following command:
That command will create an empty web project for you.
The structure of the project should be familiar because it’s exactly the same as an SBT project (SBT follows Maven conventions)
Once you have the pom.xml file (the Maven build file generated by the preceding command), you can configure all the dependencies.
As mentioned, you’ll use the Hibernate and Spring frameworks to build the application.
To save time, you can copy the pom.xml file from the codebase associated with the project.
There’s no need to explain how Maven works because once you configure all the dependencies, it gets out of your way.
But if you’ve never used Maven, remember that pom.xml is the build file you use to configure Maven and specify all the dependencies.
For the topArtists application you’ll use Spring to build the web layer and also use it as a dependency injection framework.
Hibernate will be your ORM layer and will save all the artists retrieved from Last.fm to the database.
For your toy application, you’ll be using the database HSQLDB (http://hsqldb.org)
Let’s move on and write the code necessary to build the application.
The topArtists application displays artists retrieved from the REST API call from Last.fm.
To see what information you can retrieve from Last.fm, invoke the following URL from any web browser window:
Make sure you use your Last.fm API key before invoking the URL.
If the request is successful, you’ll see information about artists, including name, number of times the songs have been played, listeners, URL, and other attributes.
To keep things simple, use only the result from the first page and store the name of the artist, play count, and listeners.
This model class represents an artist by name, number of times songs by the artist have been played, and number of listeners.
Because you’re using Hibernate as your ORM tool, you need to make your domain object compatible with Hibernate.
If you take a look at the pom.xml file associated with the codebase of this chapter, you’ll notice the following snippet:
As you can see, this code calls for the 2.15.2 version of the plug-in, but make sure you always use the latest version.
Once you add the plug-in, it will create goals (tasks) to compile and run Scala classes.
For example, you can use Maven’s scala:compile to compile Scala code and scala:cc to have a continuous compilation (similar to ~compile in SBT)
You can find more options at the Maven Scala plug-in documentation page.
Hibernate implements the Java Persistence API (JPA), and by using the JPA annotation Entity you’re specifying Hibernate to persist the object to the database.
Now, to save and retrieve the artist from the database, you have to work with the Hibernate session factory.
Create a new class to encapsulate that and call it ArtistDb.
This class will help you hide Hibernate-specific details from the rest of the code.
Because you’re using Spring, you can easily inject the necessary Hibernate dependencies into this new class.
The following listing shows the complete implementation of the ArtistDb class.
The ArtistRepository class is marked with the Spring stereotype annotation Repository so that the Spring framework can automatically scan and load the component from the classpath.
When Spring loads the class, it also sets the sessionFactory dependency.
In the next section, you’ll see how all these components are configured.
For now, assume that the sessionFactory will be available in the ArtistRepository class to make database calls using Hibernate.
The save method is straightforward: using the current Hibernate session, it saves an instance of Artist to the database.
In this case, you know that the save operation will return the Id value of the object saved.
You typecast using asInstanceOf because by default the Hibernate list method returns a List of objects.
At this point, you have enough code to save and retrieve domain objects from the database.
As discussed earlier, you’re going to use Spring to build your web tier.
Again, you’ll use Spring’s stereotype annotation @Controller to mark a class as a controller.
The job of the controller you’ll build will be to get the top artists from Last.fm and display artists stored in the local database.
Start off with the action that retrieves the list from the local database and sends it to the view to render.
You already have an ArtistDb that knows how to retrieve artists from the database, and you’ll use Spring to inject an instance of ArtistDb to the controller:
Add a method in the controller that maps to a URL and returns a list of artists to the view:
The @RequestMapping annotation maps the "/artists" URI to the method loadArtists.
And in the method it uses db.findAll to find all the artists from the database.
The first parameter to ModelAndView is the name of the view that will be rendered as a result of the method.
The topArtists parameter is a handy way to name the response of db.findAll.
Using the topArtists name inside the view, you can access all artists returned from the findAll call.
But before you can return a list of artists successfully, you have to first get the list from Last.fm.
Allow the user to refresh the artists saved in the local database.
To implement refresh, invoke the REST API specified by Last.fm.
Use the Dispatch library to make a REST call to Last.fm.
Dispatch provides a nice DSL or wrapper over the Apache HttpClient library.
The following code snippet creates an Http request object from the URL:
When running the application, you have to specify the API as a system property.
The url method takes a string URL as an input and returns an instance of Http request.
But creating an Http request won’t do much unless you tell Dispatch how to process the response received from the request.
In this case we will use the built-in handler as.xml.Elem to handle the XHTML response:
Http returns the Promise of scala.xml.Elem (since every HTTP request is handled asynchronously) and we are using map to access the contents of the Promise object.
Since we are not using Spring’s asynchronous support, we will wait for the Promise (by invoking the apply method) to finish before rendering the result.
The response from Last.fm consists of an XML with a list of artists and looks something like the following:
You’ll use Scala’s awesome native XML support to parse the result.
Dispatch already converted the response to an instance of NodeSeq, and now you have to extract all the.
The refresh action of the controller needs to use the retrieveAndLoad method to load and save the artists to the database and display the artists view.
Now that you have the code for the model and controller, it’s time to turn to the view.
This simple view will take the response from the controller and render that using JavaServer Pages (JSP)
Your JSP view will take the topArtists parameter returned from the controller and iterate through it to render the response.
You use the topArtists to access the list of artists returned from the controller and display them.
In the next section you’ll integrate all the pieces using Spring configuration files.
This way Spring will ensure that all the necessary dependencies you need for your model and controller objects are properly initialized and injected.
Because you followed all the conventions using Scala and Java annotations at the Spring configuration level, you won’t have any problem configuring Scala classes.
This is a great benefit of smooth interoperability between Scala and Java.
Using this file, you’re configuring Hibernate with HSQLDB and the domain objects that are used with Hibernate.
You’re also using Spring’s component scan to look for ArtistDb so that it gets initialized with the necessary Hibernate dependencies.
Check Spring’s documentation for more about the tags used in the configuration file.
You also configure suffix with .jsp C so that when you return ModelAndView with "artists" from the controller, it can look for the artists.jsp file in the WEB-INF/jsp folder.
To load these configuration files when the application starts, add them to the web.xml file.
Most of the content of this file should be familiar to you if you’ve built a web application for the JVM.
All the Java web containers read the web.xml to initialize Java-based web applications.
The listener attribute C allows applications to listen to events generated by containers, such as when an application is loaded or unloaded.
To run the application, you can use the already configured Jetty web server using the following command:
As you can see, setting up and creating web applications using Scala and Java frameworks is easy.
Some of the boilerplate configuration is unavoidable when working with Java frameworks, but you can still have fun writing Scala code.
One thing that should be clear from this chapter is that Scala’s interoperability with Java is pain-free.
There are a few places where you have to take extra precautions, but for the most part you can integrate with existing Java codebases without thinking too much.
The extra carefulness comes when you have to integrate some features of Scala that aren’t supported in Java and vice versa.
You learned how to deal with situations like that throughout this chapter.
Because Scala is designed from the ground up to interoperate with Java, most of the workarounds are also simple to implement and learn.
The benefit of simple integration with Java means you can easily get started with Scala in existing codebases.
As the final example demonstrates, you can use Scala with existing popular Java frameworks without rewriting entire applications.
The next chapter looks into one of the most exciting Scala frameworks: Akka.
This framework lets you build large, scalable, and distributed applications using various concurrency models.
I know you’ve been waiting for this for a long time—so without further delay, let’s jump into the exciting world of Akka.
This chapter introduces an exciting Scala toolkit called Akka, which allows you to build next-generation, event-based, fault-tolerant, scalable, and distributed applications for the JVM.
Akka provides multiple concurrency abstractions (mentioned in section 9.2.3), and this chapter explores each one of them.
So far, you’ve only seen how actors can be used to build message-oriented concurrency.
Here we’ll go beyond actors and look into concurrency abstractions like STM, Agent, and Dataflow.
To understand how the pieces of Akka fit together, you’re going to build a realtime product search application using Akka called Akkaoogle.
The philosophy behind Akka lowest price on products you search for.
You’ll build this product incrementally so you can see which Akka features you can use in which situations.
I can’t cover the Java side of things in this chapter, but you can check out the latest documentation at http://akka.io/docs/ for details.
Akka is written in Scala but exposes all its features through both the Java and Scala APIs.
Because this is a Scala book, in this chapter I mainly discuss the Scala API, but I include Java examples as well.
You can build the Akkaoogle application in Java by following the Scala examples because both APIs look almost the same.
First I’ll talk about the philosophy behind Akka so you understand the goal behind the Akka project and the problems it tries to solve.
The philosophy behind Akka is simple: make it easier for developers to build correct, concurrent, scalable, and fault-tolerant applications.
To that end, Akka provides a higher level of abstractions to deal with concurrency, scalability, and faults.
Figure 12.1 shows the three core modules provided by Akka for concurrency, scalability, and fault tolerance.
By now I’m sure you’re comfortable with actors (message-oriented concurrency)
You need to understand alternative concurrency models available in Akka, and in the next section you’ll explore all of.
At the core, Akka is an event-based platform and relies on actors for message passing and scalability.
Akka puts both local and remote actors at your disposal.
Using local actors with routing (the ability to send work with multiple instances of an actor) you can scale up and you can use remote actors to help you scale out.
We’ll look into this in more detail when you build a sample application at the end of the chapter.
In chapter 9 you learned that threads are a difficult and error-prone way to implement concurrency and should be the tool you choose last.
The question then becomes what are your first, second, or third options? This section introduces you to those options and helps you decide which are appropriate.
Table 12.1 describes all the concurrency techniques available in  Akka.
The good news is you can combine all these concurrency techniques, which is what most Akka developers end up doing.
These options provide the flexibility you need to design your concurrency applications correctly.
For example, you can model an application using actors, handle mutable state with STM or agents, and use dataflow concurrency to compose multiple concurrent processes.
Let’s begin the journey into the world of Akka concurrency—it will be a fun ride.
Actors An actor is an object that processes messages asynchronously and encapsulates state.
Software transactional memory is a concurrency model analogous to database transactions for controlling access to a shared state.
They only allow you to mutate the data through an asynchronous write action.
This means that it behaves the same every time you execute it.
So if your problem deadlocks the first time, it will always deadlock, helping you to debug the problem.
Actor programming is not restricted to only a single JVM, so actors can communicate with each other across multiple JVMs (figure 12.2)
Akka remote actors allow you to deploy actors in remote machines and send messages back and forth transparently.
Remote actors are a great way to make your application scalable and distributed.
Think of the Google protocol buffer as XML but smaller and faster, and Netty as a non-blocking I/O (NIO) implementation, which allows Akka to efficiently use threads for I/O operations.
Akka implements transparent remoting, where the remoteness of the actor is completely configured at deployment time.
You can work with local actors while building the solution and configure remote details of each individual actor during deployment.
Before we proceed further let’s add dependencies for remote actors.
Akka is modular, so instead of pulling in the entire Akka library, you’re only depending on the Akka actors.
You can find the complete build.sbt file in the accompanying codebase for this chapter.
We will take the same word count example we built in chapter 9 and change the worker actor to implement it in Java and, instead of files, we will work with a list of.
Figure 12.2 Two actor systems running in two different nodes.
The goal is to connect to the URL and count all the words on the page.
The class is called UntypedActor because Akka includes the concept of a TypedActor.
The typed actors implement the active object2 pattern, which turns any POJO interface into an asynchronous API using actors.
The one advantage using typed actors has over untyped actors is that you can have a static compile-type contract and you don’t have to define messages.
Read more about Akka typed actors in the Akka documentation.
Because your WordCountWorker needs to handle the FileToCount message, you need to typecast the message received as a parameter to FileToCount:
The code is checking the type of message received using the instanceof operator, and if the message isn’t of type FileToCount, an exception is thrown.
Because you want to write most, if not all, of your code in Scala, add the countWords method to the FileToCount case class that counts all the words in a resource, to which the URL points:
From the WordCountWorker actor, you can invoke the countWords method to count words:
The tell method allows actors to reply to the sender.
To reply to the master, the worker actor needs to construct the WordCount message by passing the filename and the word count:
The getSelf method returns the actor reference to the current actor.
The following listing shows the complete WordCountWorker actor in Java.
Save this in the src/main/ java folder of your SBT project.
To take advantage of the remote actors, we run all the worker actors in a JVM separate from the master actor.
To achieve that, let’s create two actor systems with different properties.
The easiest way to configure Akka actors is by providing a configuration file in the classpath.
You can find the details of all the configuration properties in the Akka documentation.3 The following example defines two actor systems: the main actor system and the worker actor system:
Separating the configuration by actor systems provides the flexibility to define settings for each actor system.
Now save the preceding configuration into the application.conf file under the src/ main/resources folder of the project.
This will make the application.conf file available in the classpath.
To make the workersystem run on a different JVM, run the following code in a different terminal:
Now let’s create a new actor that will tie all the pieces together.
Now if you start the WorkerSystem and the MainSystem in two different JVM instances you will have the workers running on one JVM and the main actor running on another.
This opens up myriad possibilities to scale, because now you can distribute work to multiple machines.
Software transactional memory (STM) turns a Java heap into a transactional dataset.
Because memory isn’t durable with STM, you only get the first three properties of ACID (atomicity, consistency, isolation, durability):
Atomicity—This property states that all modifications should follow the “all or nothing” rule.
In STM, all the modification is done through an atomic transaction, and if one change fails all the other changes are rolled back.
Consistency—This property ensures that an STM transaction takes the system from one consistent state to another.
If you want to delete one element from a Map and insert into another Map, then at the end of the STM transaction both Maps will be modified appropriately.
Isolation—This property requires that no other STM transaction sees partial changes from other transactions.
You can also take two smaller STM operations and combine them to create.
Before I show you STM examples, let’s step back in order to understand what state is and how it’s represented in STM.
Let’s look at how state is handled in imperative programming.
Figure 12.3 shows how state is handled in an imperative world.
You directly access the data in memory and mutate it.
In the figure, an object, A, is directly accessing the data represented by B and C.
The problem with this approach is that it doesn’t work in the concurrent world.
What will happen when some other thread or process tries to access the data residing in B or C when A is trying to mutate that data? The result is unexpected behavior.
To solve the problem with this approach, STM defines mutable state differently.
In STM, state is defined as the value that an entity with a specific identity has at a particular point.
And identity is a stable reference to a value at a given point in time.
Figure 12.4 shows how the previous structure would be represented in STM.
The mutable part here is the identity, which gets associated with a series of values.
And STM makes the mutation of reference from one value to another atomic.
What will happen in this case when some other thread or process tries to access the data residing in B or C when A is trying to mutate it? You’ll see the value associated with B or C, because STM transactions are isolated and no partial change is visible outside the transaction.
This idea of defining state in terms of identities and values is derived from the programming language Clojure (http://clojure.org)
Now let’s see how STM works in Akka through examples.
To add the library to your SBT project use the following snippet:
To demonstrate how STM works let’s take a simple example in which you create atomic operations for deleting and inserting elements into an immutable Map.
Refs are nothing but mutable references to values that you can share safely with multiple concurrent participants.
The preceding snippet creates two refs pointing to the immutable HashMap.
To perform any operation on Ref you have to use the atomic method defined in the STM package by passing an in-transaction parameter.
The Scala STM library creates the transaction object and grants the caller permission to perform transactional reads and writes.
Any refs you change in the closure will be done in an STM transaction.
For example, in the following code you’re trying to add a new element to the Map managed by ref2:
By invoking ref2.get you’re getting the value currently associated with the Ref and using swap to replace the old value with the new value.
If the operation fails, the changes will be rolled back.
The transaction parameter is marked as implicit so you don’t have to pass it around.
To implement atomic deletion of key from ref1, you can use the transform method defined in Ref.
The transform method allows you to transform the value referenced by Ref by applying the given function:
Why return the old value from the function? I have a plan to use it later, so hang in there.
I keep talking about the composability of STM, but haven’t yet shown you an example.
Imagine you have to build an atomic swap function that moves one item from one Map to another.
With STM, it’s easy: all you have to do is wrap both the atomicDelete and atomicInsert functions in an atomic function, as in the following:
Because ref2 only holds an Int type value, you have to parse it to Int before insertion.
To fully understand the beauty of the swap function, look at the following specification:
The single method of STM lets you access the contents of Ref without requiring a transaction.
When you try to swap "service3" (which maps to a null value), the Integer .parseInt will throw an exception.
At that point the delete is already successful, but thanks to STM it will roll back the entire transaction.
To learn more about STM, consult the Scala STM documentation.4 Let’s move our attention to another concurrency abstraction called Agent.
Agents provide asynchronous changes to any individual storage location bound to it.
An agent only lets you mutate the location by applying an action.
Actions in this case are functions that asynchronously are applied to the state of Agent and in which the return value of the function becomes the new value of Agent.
However, reading a value from Agent is synchronous and instantaneous.
The difference between Ref and Agent is that Ref is a synchronous read and write; Agent is reactive.
To apply any action asynchronously, Akka provides two methods: send and sendOff.
The send method uses the reactive thread pool allocated for agents, and sendOff uses a dedicated thread, ideal for a long-running processes.
Here’s an example of Agent associated with a file writer that logs messages to the log file through send actions:
Agent will be running until you invoke the close method B.
An actor system is created for the agent because, behind the scenes, agents are implemented using actors.
If you have to do more than logging to a file, something that will take time, use the sendOff method:
Note that at any time, only one send action is invoked.
Even if actions are sent from multiple concurrent processes, the actions will be executed in sequential order.
Agents also participant in STM transactions when used in the atomic block and messages are held until the transaction is completed.
This is important because if you have a side-effect action, like logging to a file, you don’t want to do that with STM.
Why? Because if STM transactions fail, they retry automatically, meaning your sideeffecting operation is executed multiple times.
This might not be what you want, so combining agents with STM is a great pattern to execute side-effecting actions along with STM transactions.
Sometimes the asynchronous nature of Agent confuses people into thinking that agents are similar to actors, but they’re completely different in the way you design them.
Agent is associated with data, and you send behavior to Agent from outside, in the form of a function.
In the case of actors, the behavior is defined inside the actor, and you send data in the form of a message.
You’ll revisit agents when using Akkaoogle to log transactions, but now let’s continue with our next concurrency model: dataflow.
Dataflow is a great way to encapsulate concurrency from a program.
If you run it and it works, it will always work without deadlock.
Alternatively, if it deadlocks the first time, it will always deadlock.
This is a powerful guarantee to have in a concurrent application because you can easily understand the code.
The dataflow concurrency allows you to write sequential code that performs parallel operations.
The limitation is that your code should be completely side-effect free.
You can’t have deterministic behavior if your code is performing side-effecting operations.
Dataflow is implemented in Akka using Scala’s delimited continuations compiler plug-in.
To enable the plug-in within your SBT project, add the following lines to the build.sbt file:
To work with dataflow concurrency, you have to work with dataflow variables.
Here Akka Promise is used to create a dataflow variable.
A Promise is a read handle to a value that will be available at some point in the future.
The preceding call will wait in a thread unless a value is bound to messageFromFuture.
Future.flow returns a Future so you can perform other operations without blocking the main thread of execution.
Think of a Future as a data structure to retrieve the result of some concurrent operation.
To assign a value to a dataflow variable, use the << method as in the following:
Once a value is bound to a dataflow variable, all the Futures that are waiting on the value will be unblocked and able to continue with execution.
The following listing shows a complete example of using the dataflow variable.
The next section dives into building an application using some of the Akka concepts you have learned so that you can see practical use cases of Akka concurrency.
You’ve covered a lot of ground so far in this chapter, you’ve learned new concepts, and you’ve seen several examples.
But now it’s time to see how these concepts are applied.
Building a real-time pricing system: Akkaoogle in a large application.
In this section you’ll build a large web-based product search site called Akkaoogle (see figure 12.4)
So how does this application work? It gets the product price from two types of vendors that are offering the product.
You can pay money to Akkaoogle and become an internal vendor.
In this case, the product information is stored in Akkaoogle, and you pay a small service charge.
You can also sign up as external vendor, in which case Akkaoogle makes a RESTful web service call to fetch the price—but the downside is you pay a bigger service charge.
When the user is looking for the cheapest deal, Akkaoogle checks with all the vendors (internal and external) and finds the lowest price for the user.
That’s a challenge if you have to implement it using primitive concurrency constructs.
But with Akka? Let’s see how you could implement this.
Figure 12.5 shows you the high-level view of how Akkaoogle will be implemented.
At first glance it may look complicated, but don’t worry—you’re going to build it in small pieces.
Keep this figure in mind when building the application so you know where you’re heading.
Here are the descriptions for each important component in figure 12.5 that you’ll now start building:
Request handler—This is an actor that handles HTTP requests from the user.
You’ll use an asynchronous HTTP library called Mist, provided by Akka, to implement this actor.
Search cheapest product—This is the main entry point to execute a search to find the cheapest deal.
External load balancer—This actor invokes all the external vendor services and finds the cheapest price among them.
Find product price and find vendor price—The worker actors do the work of finding the price.
Monitor—A simple monitor actor logs the failures that happen in external vendor services.
This could be used to load product data for internal vendors.
You’re also going to build a supervisor hierarchy to handle failures.
You don’t want Akkaoogle going down because you’ll lose new business and money.
Let’s begin the fun by setting up the product with all the dependencies you need.
Use the Build.scala file as your definition file and build.properties to configure the version of SBT.
This project uses SBT 0.11.2, but you can easily upgrade to a newer version by modifying the build.properties file.
To make your life easier, use the custom tasks in the following listing to start and stop the H2 database.
The following listing shows the complete project definition file with all the dependencies you need for the Akkaoogle project.
The build.scala file declares all the dependences and settings you need to start working on the Akkaoogle project.
Once all the dependencies are downloaded you’re ready to implement the Akkaoogle application.
The next section builds the domain models you need to implement the internal vendor service.
I test drove most of the application, but I won’t show you test cases here.
I encourage you to go through the test cases in this chapter’s accompanying codebase.
Even better, write tests for the code used throughout the rest of the chapter.
You need a way to implement the products provided by internal vendors and also a model that represents external vendors.
To reduce duplication between domain models, create a common trait called Model that extends the KeyedEntity trait provided by Squeryl.
This trait provides an id field that acts as a primary key in the database for all the domain models:
You’re declaring a self type here because later on it will let you add generic methods that work on all the model objects (more about this later)
Now you can create model classes that represent products and external vendors by extending the Model trait:
You’re keeping these models simple so you can focus more on the Akka-related features you’ll implement.
The url property of the external vendor specifies the url you’ll use to invoke the remote RESTful service.
Add a method to the Product class to calculate the price using the basePrice and the plusPercentage fields:
You’ll need this method to determine the price of products offered by internal vendors.
Because Akkaoogle cares about quality, you need to track the service availability of all the external vendors so you can rate them quarterly.
You’ll log (to the database) every time a call to an external vendor service fails and you’ll need a domain model to represent it:
The following listing creates an Akkaoogle schema object that will create the necessary tables in the database and provide helper methods to work with domain models.
The init method makes the database connection (in this case to the H2 database) and is used by the tx method E to make sure the application is connected before initiating the transaction.
To fetch values back from the database, you can define finder methods in the companion objects of the domain models:
TransactionFailure .findAll B returns all the transaction failures stored in the database (for large database tables, you should define finders that take some criteria to filter data)
Using the same technique you used in chapter 7, you’ll add a save method to each domain model.
The save method for all the domain objects is almost identical, except for the Squeryl table object.
Create a generalized version of the save method that works with all the domain models that extend the Model trait:
For example, when the Model is an instance of Product, then A is going to be of type Product, and Table[A] is going to be a table that knows how to save Product, which is the products property defined in the AkkaoogleSchema object.
Because the parameter is defined as an implicit parameter, the compiler will now look for a value that matches the parameter type within the scope.
This way, the caller won’t have to specify the parameter when saving an instance of a domain model.
The models package defines all the classes you need to make Akkaoogle work with the database.
It defines a parameterized version of save E that works with all the table objects created inside the package.
To make save work for all these table types, you have defined implicit values (B C D) for each table so that the Scala compiler picks the appropriate one when saving your domain objects.
It’s time to move to the core of the application: implementing the price lookup actors for both the internal and external vendors.
The core of Akkaoogle is to find the cheapest deal on the web and track the availability of the external services for quality purposes.
To support these functionalities, the Akkaoogle application needs to handle the message types shown in the following listing.
Listing 12.8 List of message types supported by Akkaoogle (messages.scala)
The FindPrice message type represents a request triggered by a user looking for the cheapest deal.
Needless to say, it’s the most important message type in the application.
The response of the FindPrice message is represented by the LowestPrice message, and it contains all the information the user needs about the cheapest deal.
Akkaoogle internally uses the rest of the message types to track the availability of external services.
Every time an external service times out, the LogTimeout message is sent to an actor to log the details.
The FindStats and Stats messages are used for administration purposes.
First, implement a way to find the cheapest price for the products offered by the internal providers.
Remember: products offered by internal providers are stored in a database.
When it receives the FindPrice message, it uses the product description provided to find the product in the database and calculate the price using the calculatePrice method, as defined in the Product domain class.
The current implementation only considers the first matching product, which in some cases isn’t the right behavior.
Listing 12.9 Actor that calculates lowest price for internal products.
I leave it to you to make the necessary changes to the calculation.
Because you can have many external vendors for your application, you can’t make the remote service calls sequentially because it would increase the response time and affect your users.
You have to invoke the service calls in parallel and find a way to set a timeout for each service so you can respond to the user within a reasonable time.
Actors are a nice and easy way to achieve parallelism.
You’ll next create an actor for each external vendor and broadcast the FindPrice message to these actors.
These actors will act as a proxy client to the remote service.
The following listing shows how the proxy actor for each external vendor is implemented.
Source.fromURL makes the REST call to the vendor web service B.
Because the web service might take some time to respond, it’s wrapped in a Future.
A Future is a data structure to retrieve the result of some concurrent operation without blocking.
Typically, some other actor does the work and completes the Future so you aren’t blocked.
Future is a great and easy way to execute code asynchronously.
Akka Future can also monadically (using map and flatMap) compose with other Futures.
The reference of the Future is then piped to a sender D.
This is a common pattern in Akka, called the pipeTo pattern.
Instead of waiting for the Future to complete and send the result to the sender, you’re piping Future to the sender.
If the Future fails with an exception, the recover C method returns the empty option.
Make sure you explore it in the Akka documentation( http://mng.bz/wc7D)
You need the actor in the following listing to broadcast the FindPrice message to each proxy actor and, at the end, find the lowest price out of all the responses.
The FindPrice message is broadcast to all the proxy actors using the ? method (ask pattern)
But instead of waiting for the reply, it returns a Future.
This way you’re even-handed in terms of dispatching messages to each proxy so that all the external vendors get their fair share of time to respond with prices.
The findLowestPrice method at line B determines the lowest price out of all the responses, and here’s how it’s implemented:
The findLowestPrice method uses the fold operation over all the Futures to find the lowest price.
The beauty of this fold is it’s performed in a nonblocking fashion.
The fold method creates a new Future that wraps the entire operation and is performed on the thread of the Future that completes last.
If any of the Futures throws an exception, the result of the fold becomes that exception.
So far, you’ve implemented actors to get both the internal and external lowest price.
The following listing shows one more actor you need; it can find the lowest price from both internal and external vendors and return the result.
The CheapestDealFinder actor finds the lowest price from both internal and external vendors and then finds the cheapest price among them.
You use the router actors to increase the scalability of the application because they help in routing messages to multiple instances of actors, based on an algorithm.
The next section discusses routers and dispatchers, two of the neat features of Akka.
I haven’t discussed Akka dispatchers and message routing on purpose, because to truly understand them you need a context where they’re valuable—like our current example.
What will happen if you deploy the current application in production?
For one user at a time, the current setup would work fine, but with multiple concurrent users it won’t scale.
When the CheapestDealFinder actor is processing a message, other messages are waiting in the mailbox for processing.
In some cases you may want that behavior, but in this case you can’t do that.
If you could create multiple instances of the CheapestDealFinder actor, you could process messages in parallel.
Then you’d have to route messages to these actors effectively so you don’t overload some actors.
But how will the caller know which actor instance has the fewest messages to process? The good news is Akka comes with special kinds of actors called routers, which can effectively route messages between multiple instances of actors.
The router actor acts as a gateway to a collection of actors.
You send a message to the router actor, and the router actor forwards the message to one of the actors, based on some routing policy.
The actor with the least number of messages in the mailbox wins.
To create your own routing logic, you need to extend RouterConfig.
Instead of allowing the router to create the actor instances, the instances are passed as a parameter (they are called routees)
The reason is that you want to specify additional parameters to customize them further (discussed in the next section)
These routers let you scale and handle multiple users at a time.
But what about performance? You still need the underlying threads to run all the event-based actors you’ve created.
The next section explores how to customize Akka to allocate dedicated threads for each actor type.
Every actor system has a default dispatcher that’s used if nothing is configured.
In Akka, message dispatchers are the engine behind the actors that makes Actor run.
Think of a dispatcher as a service with a thread pool that knows how to execute actors when a message is received.
In fact, when you’re building an Akka application, I recommend starting with that—don’t create a special configuration.
But if you notice some contention on a single dispatcher, you can start creating dedicated dispatchers for a group of actors.
Remember, all the actors are created from the same actor system.
You can easily configure the default dispatcher by adding more threads to it, but for learning purposes you’re going to use different dispatchers.
It’s an event-based dispatcher that binds actors to a thread pool.
All the actors of the same type share one mailbox.
Using dispatchers in Akka is a simple two-step process: first, specify them in the configuration file, then set up the actor with the dispatcher.
Akka dispatchers are configurable (read the Akka documentation for details)
In the real world, you should do performance testing before choosing a configuration that works for everybody.
To use these dispatchers you will use the withDispatcher method of Props, as in the following:
Now these actors will no longer use the default dispatcher that comes with ActorSystem but rather the one that’s configured.
Where do the remote actors fit in? Making an actor remote is a matter of changing the configuration at deployment time.
This lets you scale further by deploying them into multiple remote machines, if required.
Refer to the codebase for this chapter for an example.
To build the monitoring piece for the Akkaoogle application, you have to rely on a shared mutable state, and this section shows you how to put Agent to use.
The monitor actor needs to log any transaction failure with external vendors.
You can always extend its functionality for internal use, but for now it needs to handle the following two message types:
On receiving a LogTimeout message, it needs to save the transaction failure information to the database and also keep track of the number of times a particular service failed.
It can store shared data and, if required, participate in the STM transaction.
To keep things simple you’ll use the Map provided by Akka.
The side effect that’s saving information to the database can’t be done safely within an STM transaction, because an STM transaction could retry the operations in a transaction multiple times if there’s any read/write inconsistency.
If you try to save information to the database within the transaction, it may get saved more than once.
If you use Agent, it can participate in the STM transaction and get executed only when the STM transaction completes successfully.
The logTimeout method first gets the actorId and the message that needs to be logged in the database.
The send method of Agent takes a function that increments the failure counts and saves the message into the database.
Get the latest count from the map and return it.
In the real world, you’ll monitor other information, but as of now you’re done with the monitor actor.
MonitorActor checks the health of external vendor services and provides stats.
The preRestart B is a callback method defined by the Akka actor trait, which is invoked when the actor is about to be restarted.
In the preRestart you’re clearing up the log count but ideally you may want to save the existing error count in some persistence storage so you can fetch the errors for later use.
Now let’s hook all these actors with a simple UI.
It maps an HTTP request to a function that takes an HTTP request and returns a response.
Behind the scenes, all the requests are handled using actors.
It also provides support for nonblocking, asynchronous HTTP support of the Play2 framework.
Behind the scenes, the Play2 framework uses the Netty server that implements the Java NIO API.
The next section sets up the Akkaoogle project to use the Play2-mini framework.
To run the Akkaoogle application, all you have to do is enter "sbt run"
The following trait captures the necessary settings to convert the Akkaoogle project to Play2-mini:
The scalaMiniProject method creates an SBT project with all the Play2-mini dependencies.
For Akkaoogle, you’ll mix in this trait and use the scalaMiniProject method to create the project, as in the following listing.
After you save and reload the build definition, you should have everything you need to give a UI look to your application.
In the next section, you’ll build your first Play2mini action, which can take the HTTP request and send messages to the actors.
This class takes an instance of Application as a parameter.
Think of Application as a controller of the MVC model that handles all the requests.
In the case of Play2-mini, the only abstract method you have to implement is the routes method:
For your application to work with Play2-mini, you need to implement the Setup by passing a concrete implementation of the Application trait.
Global is also a great place to initialize the various parts of the system.
The routes method is a partial function that matches the HTTP URL to an action.
Action is nothing but a function object that takes an HTTP request and returns a Result.
The Play2-mini framework provides a nice DSL to parse the HTTP URL and verb.
The following action returns Ok(HTTP 200 response code) with the output of the views.index() method:
The following GET pattern match for product search is more interesting:
In this case, you’re using the QueryString helper of Play2-mini to parse the query parameters and give you a value mapped to a parameter.
You can find the complete working version of the Akkaoogle project in the chapter’s codebase.
The Akkaoogle application isn’t completed yet, so you can take over and add more features to make it better.
Akka is a powerful toolkit that you can use to build frameworks or applications.
Akka makes concurrency easy for programmers by raising the abstraction level.
Akka is a concurrency framework built on actors, but it provides all the popular concurrency abstractions available on the market.
It provides the flexibility you need to build your next enterprise application.
Akka’s STM support lets you safely operate on mutable data structures without leaving the comfort of actor-based concurrency.
Most importantly, you learned that STM composes, so you can build smaller atomic operations and compose them to solve problems.
You also explored agents as another concurrency model that lets you send behavior from outside to manipulate data in a safe manner.
Exploring dataflow concurrency was also interesting because it lets you write sequential programs without worrying about concurrency.
And dataflow concurrency code is very easy to understand and follow.
By building Akkaoogle, you explored various considerations, constraints, and design decisions that typically arise when building large concurrent applications.
I’m sure that the insights you gained will enable you to build your next Akka application.
Always remember that Akka provides lots of options and configuration flexibility, and you should pick the features and options that work best for your application requirements.
Akka is already used in various real-world applications,5 and now you can use it too.
From here on, let’s keep all our CPU cores busy.
Akka, Akkaoogle application example (continued) architecture of 357–358 handling shared resources with Agent.
Scala classes in Java 329–332 Scala web applications using Java.
It’s designed to produce succinct, type-safe code, which is crucial for enterprise applications.
Scala implements Actor-based concurrency through the amazing Akka framework, so you can avoid Java’s messy threading while interacting seamlessly with Java.
Scala in Action is a comprehensive tutorial that introduces the language through clear explanations and numerous hands-on examples.
It takes a “how-to” approach, explaining language concepts as you explore familiar programming tasks.
You’ll tackle concurrent programming in Akka, learn to work with Scala and Spring, and learn how to build DSLs and other productivity tools.
You’ll learn both the language and how to use it.
Ruby and Python programmers will also fi nd this book accessible.
Nilanjan Raychaudhuri is a skilled developer, speaker, and an avid polyglot programmer who works with Scala on production systems.
A great way to get started on building real-world applications using Scala and.
